{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#hello","title":"\ud83d\udc4b Hello","text":"<p>zeta provides you with all the modular lego blocks you need to build bleeding edge AI models as fast as possible.</p>"},{"location":"#install","title":"\ud83d\udcbb Install","text":"<p>You can install <code>zeta</code> with pip in a Python&gt;=3.8 environment.</p> <p>pip install (recommended)</p> headless <p>The headless installation of <code>zeta</code> is designed for environments where graphical user interfaces (GUI) are not needed, making it more lightweight and suitable for server-side applications.</p> <pre><code>pip install zetascale\n</code></pre> <p>git clone (for development)</p> virtualenv <pre><code># clone repository and navigate to root directory\ngit clone https://github.com/kyegomez/zeta.git\ncd zeta\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\n\n# headless install\npip install -e \".\"\n\n# desktop install\npip install -e \".[desktop]\"\n</code></pre> poetry <pre><code># clone repository and navigate to root directory\ngit clone https://github.com/kyegomez/zeta.git\ncd zeta\n\n# setup python environment and activate it\npoetry env use python3.10\npoetry shell\n\n# headless install\npoetry install\n\n# desktop install\npoetry install --extras \"desktop\"\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Learn more about zeta \u2192</p>"},{"location":"#examples","title":"Examples","text":"<p>Check out zeta examples for building agents, data retrieval, and more.</p> <p>Checkout zeta examples \u2192</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Zeta! We welcome contributions from the community to help improve usability and readability. By contributing, you can be a part of creating a dynamic and interactive AI system.</p> <p>To get started, please follow the guidelines below.</p>"},{"location":"contributing/#optimization-priorities","title":"Optimization Priorities","text":"<p>To continuously improve Zeta, we prioritize the following design objectives:</p> <ol> <li> <p>Usability: Increase the ease of use and user-friendliness of the swarm system to facilitate adoption and interaction with basic input.</p> </li> <li> <p>Reliability: Improve the swarm's ability to obtain the desired output even with basic and un-detailed input.</p> </li> <li> <p>Speed: Reduce the time it takes for the swarm to accomplish tasks by improving the communication layer, critiquing, and self-alignment with meta prompting.</p> </li> <li> <p>Scalability: Ensure that the system is asynchronous, concurrent, and self-healing to support scalability.</p> </li> </ol> <p>Our goal is to continuously improve Zeta by following this roadmap while also being adaptable to new needs and opportunities as they arise.</p>"},{"location":"contributing/#join-the-zeta-community","title":"Join the Zeta Community","text":"<p>Join the Zeta community on Discord to connect with other contributors, coordinate work, and receive support.</p> <ul> <li>Join the Zeta Discord Server</li> </ul>"},{"location":"contributing/#report-and-issue","title":"Report and Issue","text":"<p>The easiest way to contribute to our docs is through our public issue tracker. Feel free to submit bugs, request features or changes, or contribute to the project directly. </p>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>Zeta docs are built using MkDocs. </p> <p>To directly contribute to Zeta documentation, first fork the zeta-docs repository to your GitHub account. Then clone your repository to your local machine.</p> <p>From inside the directory run: </p> <p><code>pip install -r requirements.txt</code></p> <p>To run <code>zeta-docs</code> locally run: </p> <p><code>mkdocs serve</code></p> <p>You should see something similar to the following: </p> <pre><code>INFO     -  Building documentation...\nINFO     -  Cleaning site directory\nINFO     -  Documentation built in 0.19 seconds\nINFO     -  [09:28:33] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO     -  [09:28:33] Serving on http://127.0.0.1:8000/\nINFO     -  [09:28:37] Browser connected: http://127.0.0.1:8000/\n</code></pre> <p>Follow the typical PR process to contribute changes. </p> <ul> <li>Create a feature branch.</li> <li>Commit changes.</li> <li>Submit a PR.</li> </ul>"},{"location":"contributing/#-","title":"-------","text":""},{"location":"contributing/#taking-on-tasks","title":"Taking on Tasks","text":"<p>We have a growing list of tasks and issues that you can contribute to. To get started, follow these steps:</p> <ol> <li> <p>Visit the Zeta GitHub repository and browse through the existing issues.</p> </li> <li> <p>Find an issue that interests you and make a comment stating that you would like to work on it. Include a brief description of how you plan to solve the problem and any questions you may have.</p> </li> <li> <p>Once a project coordinator assigns the issue to you, you can start working on it.</p> </li> </ol> <p>If you come across an issue that is unclear but still interests you, please post in the Discord server mentioned above. Someone from the community will be able to help clarify the issue in more detail.</p> <p>We also welcome contributions to documentation, such as updating markdown files, adding docstrings, creating system architecture diagrams, and other related tasks.</p>"},{"location":"contributing/#submitting-your-work","title":"Submitting Your Work","text":"<p>To contribute your changes to Zeta, please follow these steps:</p> <ol> <li> <p>Fork the Zeta repository to your GitHub account. You can do this by clicking on the \"Fork\" button on the repository page.</p> </li> <li> <p>Clone the forked repository to your local machine using the <code>git clone</code> command.</p> </li> <li> <p>Before making any changes, make sure to sync your forked repository with the original repository to keep it up to date. You can do this by following the instructions here.</p> </li> <li> <p>Create a new branch for your changes. This branch should have a descriptive name that reflects the task or issue you are working on.</p> </li> <li> <p>Make your changes in the branch, focusing on a small, focused change that only affects a few files.</p> </li> <li> <p>Run any necessary formatting or linting tools to ensure that your changes adhere to the project's coding standards.</p> </li> <li> <p>Once your changes are ready, commit them to your branch with descriptive commit messages.</p> </li> <li> <p>Push the branch to your forked repository.</p> </li> <li> <p>Create a pull request (PR) from your branch to the main Zeta repository. Provide a clear and concise description of your changes in the PR.</p> </li> <li> <p>Request a review from the project maintainers. They will review your changes, provide feedback, and suggest any necessary improvements.</p> </li> <li> <p>Make any required updates or address any feedback provided during the review process.</p> </li> <li> <p>Once your changes have been reviewed and approved, they will be merged into the main branch of the Zeta repository.</p> </li> <li> <p>Congratulations! You have successfully contributed to Zeta.</p> </li> </ol> <p>Please note that during the review process, you may be asked to make changes or address certain issues. It is important to engage in open and constructive communication with the project maintainers to ensure the quality of your contributions.</p>"},{"location":"contributing/#developer-setup","title":"Developer Setup","text":"<p>If you are interested in setting up the Zeta development environment, please follow the instructions provided in the developer setup guide. This guide provides an overview of the different tools and technologies used in the project.</p>"},{"location":"contributing/#join-the-agora-community","title":"Join the Agora Community","text":"<p>Zeta is brought to you by Agora, the open-source AI research organization. Join the Agora community to connect with other researchers and developers working on AI projects.</p> <ul> <li>Join the Agora Discord Server</li> </ul> <p>Thank you for your contributions and for being a part of the Zeta and Agora community! Together, we can advance Humanity through the power of AI.</p>"},{"location":"faq/","title":"Faq","text":"<p>FAQ: Zeta - Crafting the Next Level in Neural Networks</p> <p>We understand that delving into a new framework, especially in the ever-evolving world of machine learning, can be both exciting and a tad bit overwhelming. We've compiled some of the most frequently asked questions, hoping to bridge the gap between curiosity and clarity. You inspire us, and we want to ensure that your journey with Zeta is smooth and transformative.</p>"},{"location":"faq/#1-how-is-zeta-different-from-pytorch","title":"1. How is Zeta different from PyTorch?","text":"<p>Answer: First and foremost, we have immense respect for PyTorch and the revolution it has brought to deep learning. However, Zeta is not just another deep learning framework. While PyTorch offers a robust platform for building neural networks from scratch, Zeta aims to make the process of creating State of The Art Models even more effortless and intuitive. </p> <ul> <li> <p>Modularity: Zeta's architecture allows for easily interchangeable modules, making it a breeze for developers to plug and play with different configurations.</p> </li> <li> <p>LLMs &amp; Multi-Modality: We've integrated tools to efficiently harness the power of LLMs and Multi-Modal Foundation Models. This is not just about building a model; it's about building models that can interact, perceive, and reason with diverse data types - be it text, image, or more.</p> </li> <li> <p>Enhanced Security and Trust: Zeta enforces trust boundaries, schema validation, and provides tool activity-level permissions. This ensures that while your models are smart, they're also safe and adhere to set protocols.</p> </li> <li> <p>Ease of Use: Ever felt like going for a serene swim? Using Zeta feels just like that \u2013 fluid, intuitive, and without friction. Our pythonic methods, classes, and top-notch error handling guide you every step of the way.</p> </li> <li> <p>Performance: Think of Zeta as the Lamborghini of ML frameworks. It's built for speed, efficiency, and performance. Every single FLOP is put to its best use, ensuring swift model training and inference.</p> </li> </ul> <p>In essence, while PyTorch provides the building blocks, Zeta offers a refined, faster, and more intuitive experience to craft and deploy powerful neural networks.</p>"},{"location":"faq/#2-how-steep-is-the-learning-curve-for-zeta-especially-for-someone-accustomed-to-pytorch","title":"2. How steep is the learning curve for Zeta, especially for someone accustomed to PyTorch?","text":"<p>Answer: We designed Zeta keeping both beginners and professionals in mind. If you're familiar with PyTorch, you'll appreciate the similarities in terms of syntax and structure. The added features and modules in Zeta are introduced with clarity and simplicity. With our comprehensive documentation, hands-on examples, and supportive community on Discord, we aim to make your transition smooth and enjoyable.</p>"},{"location":"faq/#3-how-does-zeta-handle-backward-compatibility","title":"3. How does Zeta handle backward compatibility?","text":"<p>Answer: We understand the importance of backward compatibility, especially when developers invest time and resources into a framework. While we continually strive to innovate and introduce new features, we make sure that changes don't break the functionality of models built on earlier versions. We're committed to ensuring a balance between innovation and stability.</p>"},{"location":"faq/#4-are-there-plans-for-introducing-more-pre-trained-models-in-zeta","title":"4. Are there plans for introducing more pre-trained models in Zeta?","text":"<p>Answer: Absolutely! Our vision with Zeta is not static. We are in the constant pursuit of integrating newer, state-of-the-art pre-trained models. Our goal is to give developers the arsenal they need to break new grounds in machine learning. Stay tuned for more exciting updates!</p>"},{"location":"faq/#5-im-facing-a-challenge-with-zeta-how-can-i-get-help","title":"5. I'm facing a challenge with Zeta. How can I get help?","text":"<p>Answer: We're genuinely sorry to hear that, but rest assured, we're here to assist. Our Discord community is active, and our team, along with fellow developers, are always eager to help. You can also raise an issue or start a discussion on our Github Page. Remember, challenges are stepping stones to mastery, and we're with you every step of the way.</p> <p>Your feedback, questions, and concerns are the winds beneath our wings. Keep them coming, and together, let's shape the future of neural networks with Zeta.</p>"},{"location":"blog/introduction_to_zeta/","title":"Revolutionizing AI/ML with Zeta: The Quest for Truly Modular and Reusable Frameworks","text":"<p>In the ever-evolving world of Artificial Intelligence and Machine Learning (AI/ML), researchers and engineers constantly seek more efficient and versatile tools to fuel their innovations. One persistent challenge is the lack of truly modular and reusable ML frameworks. This blog dives into the heart of this issue and introduces Zeta, a promising framework aiming to reshape the landscape of AI/ML development.</p>"},{"location":"blog/introduction_to_zeta/#the-current-state-of-aiml-development","title":"The Current State of AI/ML Development","text":"<p>In the current AI/ML landscape, development often feels like navigating a maze without a map. Popular frameworks like PyTorch, TensorFlow, and Xformers are powerful but monolithic, making it challenging to swap components or experiment with cutting-edge modules. This lack of modularity results in a monumentally slow and cumbersome development process that hampers progress for researchers and engineers.</p>"},{"location":"blog/introduction_to_zeta/#the-problems-with-existing-frameworks","title":"The Problems with Existing Frameworks","text":"<p>Before we delve into the world of Zeta, let's take a closer look at the issues plaguing existing AI/ML frameworkss</p> <p>And, to provide a comprehensive understanding, let's analyze some of the most widely used frameworks, including PyTorch, TensorFlow, and Xformers.</p>"},{"location":"blog/introduction_to_zeta/#pytorch","title":"PyTorch","text":"<p>PyTorch, known for its dynamic computation graph, has gained immense popularity among researchers and developers. However, it too faces challenges in terms of modularity and reusability.</p> Problem Description Monolithic Design PyTorch follows a monolithic design, where most components are tightly integrated, limiting flexibility. Lack of Standardization The absence of standardized module interfaces makes it challenging to swap or extend components. Limited Documentation While PyTorch has a growing community, documentation gaps and inconsistencies hinder ease of use. Versioning Complexity Transitioning between PyTorch versions can be complex, causing compatibility issues for projects."},{"location":"blog/introduction_to_zeta/#tensorflow","title":"TensorFlow","text":"<p>TensorFlow, with its static computation graph, has been a cornerstone of AI/ML development. However, it too faces its share of challenges.</p> Problem Description Rigidity in Graph TensorFlow's static graph can be inflexible, especially when experimenting with different architectures. Boilerplate Code Developing models in TensorFlow often requires writing extensive boilerplate code, leading to clutter. Deployment Complexity TensorFlow models can be challenging to deploy due to their heavyweight nature and dependencies. GPU Memory Management Memory management for GPUs can be challenging, leading to out-of-memory errors during training."},{"location":"blog/introduction_to_zeta/#xformers","title":"Xformers","text":"<p>Xformers is a newer entrant, specifically designed for transformer-based models. While it brings innovations, it's not without its issues.</p> Problem Description Limited Ecosystem Xformers, being relatively new, has a smaller ecosystem compared to PyTorch and TensorFlow. Lack of Pretrained Models The availability of pretrained models and libraries for common tasks is limited compared to other frameworks. Community Support The community support for Xformers is growing but may not match the scale of PyTorch and TensorFlow. Integration Challenges Integrating Xformers with other components can be challenging due to its specialized nature."},{"location":"blog/introduction_to_zeta/#lack-of-modularity","title":"Lack of Modularity","text":"<p>Traditional frameworks are designed as monolithic entities, where every component is tightly integrated. While this approach has its advantages, it severely limits modularity. Researchers and engineers cannot easily swap out components or experiment with new ones without diving deep into the framework's source code. This lack of modularity slows down innovation and collaboration.</p>"},{"location":"blog/introduction_to_zeta/#complexity","title":"Complexity","text":"<p>Existing frameworks are feature-rich, but this often results in excessive complexity. Beginners and even experienced developers can find themselves overwhelmed by the sheer number of options, configurations, and APIs. This complexity can lead to errors, increased development time, and a steep learning curve.</p>"},{"location":"blog/introduction_to_zeta/#limited-standardization","title":"Limited Standardization","text":"<p>AI/ML is a rapidly evolving field, with new research and techniques emerging regularly. Existing frameworks struggle to keep pace with these advancements, leading to limited support for new modules and models. This lack of standardization makes it challenging for researchers to implement and share their cutting-edge work.</p>"},{"location":"blog/introduction_to_zeta/#reliability-and-documentation","title":"Reliability and Documentation","text":"<p>Reliability is a critical aspect of any development framework. However, many existing frameworks suffer from stability issues, making it challenging to deploy models in production. Additionally, documentation can be sparse or outdated, making it difficult for developers to understand and use the framework effectively.</p>"},{"location":"blog/introduction_to_zeta/#the-vision-of-modular-and-reusable-ml-frameworks","title":"The Vision of Modular and Reusable ML Frameworks","text":"<p>Imagine a world where AI/ML development is as effortless as snapping together Lego blocks. In this vision, researchers and engineers can quickly experiment with the latest modules, combine them like building blocks, and create extremely powerful AI models. This modular approach not only accelerates development but also promotes collaboration and knowledge sharing.</p>"},{"location":"blog/introduction_to_zeta/#the-journey-towards-modular-and-reusable-ml-frameworks","title":"The Journey Towards Modular and Reusable ML Frameworks","text":"<p>The journey towards modular and reusable ML frameworks has been fraught with challenges such as lack of reliability, documentation, and a plethora of vast arrays of issues. Researchers and engineers have been searching for a solution, but progress has been slow. Let's examine some of the key challenges:</p>"},{"location":"blog/introduction_to_zeta/#lack-of-reliability","title":"Lack of Reliability","text":"<p>Reliability is paramount in AI/ML development. Existing frameworks may have stability issues that lead to unexpected crashes or incorrect results. Researchers and engineers need tools they can rely on to conduct experiments and deploy models with confidence.</p>"},{"location":"blog/introduction_to_zeta/#documentation-woes","title":"Documentation Woes","text":"<p>Comprehensive and up-to-date documentation is essential for any framework. It provides developers with the information they need to understand the framework's capabilities and use it effectively. Inadequate documentation can lead to frustration and hinder the adoption of a framework.</p>"},{"location":"blog/introduction_to_zeta/#compatibility-and-integration","title":"Compatibility and Integration","text":"<p>The AI/ML ecosystem is vast, with various libraries and tools available. Frameworks need to be compatible with other tools and libraries to facilitate seamless integration. Incompatibility issues can create roadblocks for developers trying to incorporate new modules or techniques into their workflows.</p>"},{"location":"blog/introduction_to_zeta/#steep-learning-curve","title":"Steep Learning Curve","text":"<p>The complexity of existing frameworks often results in a steep learning curve for newcomers. Developers must invest significant time and effort in mastering the intricacies of these frameworks, slowing down their ability to contribute meaningfully to AI/ML research.</p>"},{"location":"blog/introduction_to_zeta/#lack-of-modularity_1","title":"Lack of Modularity","text":"<p>As mentioned earlier, the lack of modularity in existing frameworks hinders experimentation and innovation. Researchers often resort to implementing custom solutions or working within the constraints of the framework, limiting their ability to explore new ideas.</p>"},{"location":"blog/introduction_to_zeta/#introducing-zeta-the-future-of-aiml-development","title":"Introducing Zeta: The Future of AI/ML Development","text":"<p>And now, allow me to introduce Zeta to you, a game-changing AI/ML framework designed with modularity and reusability at its core. Zeta's design principles include fluid experimentation, production-grade reliability, and modularity. Getting started with Zeta is as simple as running <code>pip install zetascale</code>. This one-liner sets you on a journey to a new era of AI/ML development\u2014a seamless voyaging experience that allows you to set sail across the vast seas of tensors and latent spaces!</p> <p>Let's explore Zeta's key features and how it addresses the challenges posed by existing frameworks:</p>"},{"location":"blog/introduction_to_zeta/#zetas-key-features","title":"Zeta's Key Features","text":"<p>Zeta is more than just a framework; it's a vision for the future of AI/ML development. Here are some of its key features:</p>"},{"location":"blog/introduction_to_zeta/#fluid-experimentation","title":"Fluid Experimentation","text":"<p>Zeta makes it effortless for researchers and industrial AI engineers to rapidly experiment with the latest modules and components. Whether you're interested in MultiGroupedQueryAttention or Unet, Zeta provides the building blocks for your AI experiments.</p>"},{"location":"blog/introduction_to_zeta/#production-grade-reliability","title":"Production-Grade Reliability","text":"<p>Reliability is at the core of Zeta's design. It aims to facilitate reproducibility while delivering bleeding-edge performance. This reliability ensures that your AI models can transition seamlessly from research to production.</p>"},{"location":"blog/introduction_to_zeta/#modularity","title":"Modularity","text":"<p>Zeta's modularized Lego building blocks empower you to build and deploy the best ML models. You can mix and match components, experiment with new modules, and create custom solutions with ease. Modularity is the key to unlocking innovation.</p>"},{"location":"blog/introduction_to_zeta/#exploring-zeta-in-action","title":"Exploring Zeta in Action","text":"<p>Let's dive into Zeta's capabilities with practical examples and explore how it empowers AI/ML development:</p>"},{"location":"blog/introduction_to_zeta/#installation","title":"Installation","text":"<p>Getting started with Zeta is as simple as running a single command:</p> <pre><code>pip install zetascale\n</code></pre> <p>With Zeta, you can kickstart your AI/ML journey within minutes.</p>"},{"location":"blog/introduction_to_zeta/#initiating-your-journey-with-flashattention","title":"Initiating Your Journey with FlashAttention","text":"<p>To demonstrate the power of Zeta, let's take a closer look at its <code>FlashAttention</code> module:</p> <pre><code>import torch\n\nfrom zeta.nn.attention import FlashAttention\n\nq = torch.randn(2, 4, 6, 8)\nk = torch.randn(2, 4, 10, 8)\nv = torch.randn(2, 4, 10, 8)\n\nattention = FlashAttention(causal=False, dropout=0.1, flash=True)\noutput = attention(q, k, v)\n\nprint(output.shape)\n</code></pre> <p>The <code>FlashAttention</code> module empowers your models with cutting-edge attention mechanisms effortlessly.</p>"},{"location":"blog/introduction_to_zeta/#enhancing-attention-with-relativepositionbias","title":"Enhancing Attention with RelativePositionBias","text":"<p>Zeta's <code>RelativePositionBias</code> quantizes the distance between positions and provides biases based on relative positions. This mechanism enhances the attention mechanism by considering relative positions between the query and key, rather than relying solely on their absolute positions:</p> <pre><code>from zeta.nn import RelativePositionBias\nimport torch\n\nrel_pos_bias = RelativePositionBias()\n\n# Example 1: Compute bias for a single batch\nbias_matrix = rel_pos_bias(1, 10, 10)\n\n# Example 2: Integrate with an attention mechanism\nclass MockAttention(nn.Module):\n    def __init__(self):\n        super().__\n\ninit__()\n        self.rel_pos_bias = RelativePositionBias()\n\n    def forward(self, queries, keys):\n        bias = self.rel_pos_bias(queries.size(0), queries.size(1), keys.size(1))\n        # Further computations with bias in the attention mechanism...\n        return None  # Placeholder\n</code></pre>"},{"location":"blog/introduction_to_zeta/#streamlining-feedforward-operations-with-feedforward","title":"Streamlining FeedForward Operations with FeedForward","text":"<p>Zeta's <code>FeedForward</code> module simplifies feedforward operations in neural networks:</p> <pre><code>from zeta.nn import FeedForward\n\nmodel = FeedForward(256, 512, glu=True, post_act_ln=True, dropout=0.2)\n\nx = torch.randn(1, 256)\n\noutput = model(x)\nprint(output.shape)\n</code></pre>"},{"location":"blog/introduction_to_zeta/#achieving-linear-transformation-with-bitlinear","title":"Achieving Linear Transformation with BitLinear","text":"<p>Zeta's <code>BitLinear</code> module combines linear transformation with quantization and dequantization:</p> <pre><code>import torch\nfrom torch import nn\n\nimport zeta.quant as qt\n\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = qt.BitLinear(10, 20)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nmodel = MyModel()\n\ninput = torch.randn(128, 10)\n\noutput = model(input)\n\nprint(output.size())\n</code></pre>"},{"location":"blog/introduction_to_zeta/#multi-modal-capabilities-with-palme","title":"Multi-Modal Capabilities with PalmE","text":"<p>Zeta's <code>PalmE</code> is a multi-modal transformer architecture that opens new possibilities in AI/ML:</p> <pre><code>import torch\n\nfrom zeta.structs import (\n    AutoregressiveWrapper,\n    Decoder,\n    Encoder,\n    Transformer,\n    ViTransformerWrapper,\n)\n\n# Usage with random inputs\nimg = torch.randn(1, 3, 256, 256)\ntext = torch.randint(0, 20000, (1, 1024))\n\nmodel = PalmE()\noutput = model(img, text)\nprint(output)\n</code></pre>"},{"location":"blog/introduction_to_zeta/#unleashing-u-net-for-image-segmentation","title":"Unleashing U-Net for Image Segmentation","text":"<p>Zeta's <code>Unet</code> brings the power of convolutional neural networks for image segmentation:</p> <pre><code>import torch\n\nfrom zeta.nn import Unet\n\nmodel = Unet(n_channels=1, n_classes=2)\n\nx = torch.randn(1, 1, 572, 572)\n\ny = model(x)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {y.shape}\")\n</code></pre>"},{"location":"blog/introduction_to_zeta/#visionembeddings-for-computer-vision","title":"VisionEmbeddings for Computer Vision","text":"<p>Zeta's <code>VisionEmbedding</code> class transforms images into patch embeddings for transformer-based models:</p> <pre><code>import torch\n\nfrom zeta.nn import VisionEmbedding\n\nvision_embedding = VisionEmbedding(\n    img_size=224,\n    patch_size=16,\n    in_chans=3,\n    embed_dim=768,\n    contain_mask_token=True,\n    prepend_cls_token=True,\n)\n\ninput_image = torch.rand(1, 3, 224, 224)\n\noutput = vision_embedding(input_image)\n</code></pre>"},{"location":"blog/introduction_to_zeta/#a-comparative-analysis-of-zeta-and-other-frameworks","title":"A Comparative Analysis of Zeta and Other Frameworks","text":"<p>To truly appreciate Zeta's impact on AI/ML development, let's conduct a detailed comparative analysis of Zeta and other popular frameworks, including PyTorch, TensorFlow, and Xformers. We'll evaluate these frameworks based on various criteria:</p>"},{"location":"blog/introduction_to_zeta/#modularity_1","title":"Modularity","text":"Framework Modularity Score (1-5) Comments Zeta 5 Exceptional modularity and flexibility. PyTorch 3 Modularity but lacks easy component swapping. TensorFlow 3 Modularity but can be complex for beginners. Xformers 4 Strong modularity but focused on transformers."},{"location":"blog/introduction_to_zeta/#complexity_1","title":"Complexity","text":"Framework Complexity Score (1-5) Comments Zeta 4 Powerful but user-friendly. PyTorch 5 Feature-rich but can be complex. TensorFlow 4 Extensive features, moderate complexity. Xformers 3 Simplified for transformer-based models."},{"location":"blog/introduction_to_zeta/#compatibility","title":"Compatibility","text":"Framework Compatibility Score (1-5) Comments Zeta 4 Compatible but still evolving ecosystem. PyTorch 5 Broad compatibility with many libraries. TensorFlow 5 Extensive compatibility with AI/ML tools. Xformers 3 Specialized for transformer-based tasks."},{"location":"blog/introduction_to_zeta/#documentation","title":"Documentation","text":"Framework Documentation Score (1-5) Comments Zeta 4 Good documentation but room for expansion. PyTorch 5 Extensive and well-maintained documentation. TensorFlow 4 Solid documentation but can be overwhelming. Xformers 3 Documentation primarily focused on transformers."},{"location":"blog/introduction_to_zeta/#reliability","title":"Reliability","text":"Framework Reliability Score (1-5) Comments Zeta 4 High reliability with room for improvement. PyTorch 5 Proven reliability and stability. TensorFlow 4 Generally reliable but occasional issues. Xformers 3 Reliability may vary for specialized tasks."},{"location":"blog/introduction_to_zeta/#learning-curve","title":"Learning Curve","text":"Framework Learning Curve Score (1-5) Comments Zeta 4 Moderate learning curve, user-friendly. PyTorch 3 Steeper learning curve, especially for beginners. TensorFlow 3 Moderate learning curve but can be complex. Xformers 4 Moderate learning curve, focused on transformers."},{"location":"blog/introduction_to_zeta/#modularity-index-across-modules","title":"Modularity Index Across Modules","text":"<p>Zeta's approach to modularity allows researchers and engineers to easily swap and combine modules to create powerful AI models. Let's explore some of Zeta's key modules and how they compare to their counterparts in other frameworks.</p>"},{"location":"blog/introduction_to_zeta/#flashattention-vs-standard-attention-mechanisms","title":"FlashAttention vs. Standard Attention Mechanisms","text":"<p>Zeta introduces <code>FlashAttention</code>, a module that empowers models with cutting-edge attention mechanisms effortlessly. Let's compare it to standard attention mechanisms in PyTorch and TensorFlow.</p> Aspect FlashAttention (Zeta) Standard Attention (PyTorch/TensorFlow) Modularity Easily integrated into Zeta workflows Often tightly coupled with the framework Cutting-edge Features Supports the latest attention research May require custom implementations Code Simplicity Simplifies code with its module design May involve complex code structures Documentation Well-documented for ease of use Documentation may vary in quality"},{"location":"blog/introduction_to_zeta/#relativepositionbias-vs-positional-embeddings","title":"RelativePositionBias vs. Positional Embeddings","text":"<p>Zeta's <code>RelativePositionBias</code> quantizes the distance between positions and provides biases based on relative positions. This enhances attention mechanisms. Let's compare it to traditional positional embeddings.</p> Aspect RelativePositionBias (Zeta) Positional Embeddings (PyTorch/TensorFlow) Enhanced Attention Improves attention with relative bias Relies solely on absolute positions Flexibility Adaptable to various tasks May require different embeddings for tasks Integration Seamlessly integrated into Zeta Integration may require additional code Performance May lead to more efficient models Performance may vary depending on usage"},{"location":"blog/introduction_to_zeta/#feedforward-vs-standard-mlp","title":"FeedForward vs. Standard MLP","text":"<p>Zeta's <code>FeedForward</code> module simplifies feedforward operations in neural networks. Let's compare it to the standard multilayer perceptron (MLP) in PyTorch and TensorFlow.</p> Aspect FeedForward (Zeta) Standard MLP (PyTorch/TensorFlow) Integration Easily integrated into Zeta workflows May require custom MLP layers Activation Functions Supports customizable activation funcs Requires additional code for custom activations Code Clarity Streamlines code with its module design Code structure can be more complex Performance May offer optimized performance Performance depends on implementation"},{"location":"blog/introduction_to_zeta/#bitlinear-vs-linear-layers","title":"BitLinear vs. Linear Layers","text":"<p>Zeta's <code>BitLinear</code> module combines linear transformation with quantization and dequantization. Let's compare it to standard linear layers in PyTorch and TensorFlow.</p> Aspect BitLinear (Zeta) Standard Linear Layers (PyTorch/TensorFlow) Quantization Utilizes quantization for efficient ops Linear layers perform full-precision ops Memory Efficiency Efficient memory use with quantization May consume more memory Training Speed May speed up training with <p>quantization| Training speed may be affected by ops       | | Code Integration            | Seamlessly integrated into Zeta        | Integration may require additional code     |</p>"},{"location":"blog/introduction_to_zeta/#palme-multi-modal-transformer","title":"PalmE: Multi-Modal Transformer","text":"<p>Zeta's <code>PalmE</code> is a multi-modal transformer architecture that opens new possibilities in AI/ML. It's worth examining how it stacks up against other transformer-based models.</p> Aspect PalmE (Zeta) Transformer-based Models (Other Frameworks) Multi-Modality Support Designed for multi-modal tasks May require extensive customization for multi-modal tasks Attention Mechanism Incorporates advanced attention mechanisms Attention mechanisms vary across models Ease of Use Simplifies multi-modal model development Building similar models in other frameworks may be more complex Performance Performance may be competitive with state-of-the-art models Performance depends on specific models and tasks"},{"location":"blog/introduction_to_zeta/#unet-image-segmentation","title":"Unet: Image Segmentation","text":"<p>Zeta's <code>Unet</code> brings the power of convolutional neural networks (CNNs) for image segmentation. Let's see how it compares to other image segmentation approaches.</p> Aspect Unet (Zeta) Image Segmentation Models (Other Frameworks) Architecture Follows the U-Net architecture Various architectures available for image segmentation Versatility Adaptable to different segmentation tasks May require specific models for different tasks Code Reusability Encourages reusing Unet for diverse projects Code reuse may be limited in some cases Performance Performance comparable to traditional models Performance depends on specific models and datasets"},{"location":"blog/introduction_to_zeta/#visionembeddings-transformer-friendly-image-processing","title":"VisionEmbeddings: Transformer-Friendly Image Processing","text":"<p>Zeta's <code>VisionEmbedding</code> class transforms images into patch embeddings for transformer-based models. Let's evaluate its role compared to traditional image preprocessing.</p> Aspect VisionEmbedding (Zeta) Traditional Image Preprocessing (Other Frameworks) Integration Seamlessly integrates with Zeta Image preprocessing may involve additional steps Compatibility Tailored for transformer architectures Preprocessing methods depend on model choice Ease of Use Simplifies image-to-patch embedding Image preprocessing may require more effort Performance Supports efficient transformer-based processing Performance varies based on preprocessing methods"},{"location":"blog/introduction_to_zeta/#the-future-of-aiml-with-zeta","title":"The Future of AI/ML with Zeta","text":"<p>Zeta is not just a framework; it's a vision. Led by experts like Kye, the Creator, Zeta's team is committed to revolutionizing AI/ML development. With its unique design and powerful modules, Zeta is poised to reshape the future of AI/ML frameworks.</p>"},{"location":"blog/introduction_to_zeta/#conclusion","title":"Conclusion","text":"<p>The journey towards modular and reusable AI/ML frameworks has been long, but Zeta offers a promising path forward. With its modular design, powerful modules, and visionary team, Zeta stands ready to usher in a new era of AI/ML development. Are you ready to embrace the future of AI engineering? Install Zeta now with <code>pip install zetascale</code></p>"},{"location":"blog/introduction_to_zeta/#documentation_1","title":"Documentation","text":"<p>Explore Zeta further by visiting the Zeta documentation for in-depth information and guidance.</p>"},{"location":"corporate/architecture/","title":"Architecture","text":"<ul> <li>Simple file structure</li> <li>Fluid API </li> <li>Useful error handling that provides potential solutions and root cause error understanding</li> <li>nn, tokenizers, models, training</li> <li></li> </ul>"},{"location":"corporate/bounties/","title":"Bounty Program","text":"<p>Our bounty program is an exciting opportunity for contributors to help us build the future of Zeta. By participating, you can earn rewards while contributing to a project that aims to revolutionize digital activity.</p> <p>Here's how it works:</p> <ol> <li> <p>Check out our Roadmap: We've shared our roadmap detailing our short and long-term goals. These are the areas where we're seeking contributions.</p> </li> <li> <p>Pick a Task: Choose a task from the roadmap that aligns with your skills and interests. If you're unsure, you can reach out to our team for guidance.</p> </li> <li> <p>Get to Work: Once you've chosen a task, start working on it. Remember, quality is key. We're looking for contributions that truly make a difference.</p> </li> <li> <p>Submit your Contribution: Once your work is complete, submit it for review. We'll evaluate your contribution based on its quality, relevance, and the value it brings to Zeta.</p> </li> <li> <p>Earn Rewards: If your contribution is approved, you'll earn a bounty. The amount of the bounty depends on the complexity of the task, the quality of your work, and the value it brings to Zeta.</p> </li> </ol>"},{"location":"corporate/bounties/#the-three-phases-of-our-bounty-program","title":"The Three Phases of Our Bounty Program","text":""},{"location":"corporate/bounties/#phase-1-building-the-foundation","title":"Phase 1: Building the Foundation","text":"<p>In the first phase, our focus is on building the basic infrastructure of Zeta. This includes developing key components like the Zeta class, integrating essential tools, and establishing task completion and evaluation logic. We'll also start developing our testing and evaluation framework during this phase. If you're interested in foundational work and have a knack for building robust, scalable systems, this phase is for you.</p>"},{"location":"corporate/bounties/#phase-2-enhancing-the-system","title":"Phase 2: Enhancing the System","text":"<p>In the second phase, we'll focus on enhancing Zeta by integrating more advanced features, improving the system's efficiency, and refining our testing and evaluation framework. This phase involves more complex tasks, so if you enjoy tackling challenging problems and contributing to the development of innovative features, this is the phase for you.</p>"},{"location":"corporate/bounties/#phase-3-towards-super-intelligence","title":"Phase 3: Towards Super-Intelligence","text":"<p>The third phase of our bounty program is the most exciting - this is where we aim to achieve super-intelligence. In this phase, we'll be working on improving the swarm's capabilities, expanding its skills, and fine-tuning the system based on real-world testing and feedback. If you're excited about the future of AI and want to contribute to a project that could potentially transform the digital world, this is the phase for you.</p> <p>Remember, our roadmap is a guide, and we encourage you to bring your own ideas and creativity to the table. We believe that every contribution, no matter how small, can make a difference. So join us on this exciting journey and help us create the future of Zeta.</p> <p>To participate in our bounty program, visit the Zeta Bounty Program Page. Let's build the future together!</p>"},{"location":"corporate/bounties/#bounties-for-roadmap-items","title":"Bounties for Roadmap Items","text":"<p>To accelerate the development of Zeta and to encourage more contributors to join our journey towards automating every digital activity in existence, we are announcing a Bounty Program for specific roadmap items. Each bounty will be rewarded based on the complexity and importance of the task. Below are the items available for bounty:</p> <ol> <li>Multi-Agent Debate Integration: $2000</li> <li>Meta Prompting Integration: $1500</li> <li>Zeta Class: $1500</li> <li>Integration of Additional Tools: $1000</li> <li>Task Completion and Evaluation Logic: $2000</li> <li>Ocean Integration: $2500</li> <li>Improved Communication: $2000</li> <li>Testing and Evaluation: $1500</li> <li>Worker Swarm Class: $2000</li> <li>Documentation: $500</li> </ol> <p>For each bounty task, there will be a strict evaluation process to ensure the quality of the contribution. This process includes a thorough review of the code and extensive testing to ensure it meets our standards.</p>"},{"location":"corporate/bounties/#3-phase-testing-framework","title":"3-Phase Testing Framework","text":"<p>To ensure the quality and efficiency of the Swarm, we will introduce a 3-phase testing framework which will also serve as our evaluation criteria for each of the bounty tasks.</p>"},{"location":"corporate/bounties/#phase-1-unit-testing","title":"Phase 1: Unit Testing","text":"<p>In this phase, individual modules will be tested to ensure that they work correctly in isolation. Unit tests will be designed for all functions and methods, with an emphasis on edge cases.</p>"},{"location":"corporate/bounties/#phase-2-integration-testing","title":"Phase 2: Integration Testing","text":"<p>After passing unit tests, we will test the integration of different modules to ensure they work correctly together. This phase will also test the interoperability of the Swarm with external systems and libraries.</p>"},{"location":"corporate/bounties/#phase-3-benchmarking-stress-testing","title":"Phase 3: Benchmarking &amp; Stress Testing","text":"<p>In the final phase, we will perform benchmarking and stress tests. We'll push the limits of the Swarm under extreme conditions to ensure it performs well in real-world scenarios. This phase will measure the performance, speed, and scalability of the Swarm under high load conditions.</p> <p>By following this 3-phase testing framework, we aim to develop a reliable, high-performing, and scalable Swarm that can automate all digital activities. </p>"},{"location":"corporate/bounties/#reverse-engineering-to-reach-phase-3","title":"Reverse Engineering to Reach Phase 3","text":"<p>To reach the Phase 3 level, we need to reverse engineer the tasks we need to complete. Here's an example of what this might look like:</p> <ol> <li> <p>Set Clear Expectations: Define what success looks like for each task. Be clear about the outputs and outcomes we expect. This will guide our testing and development efforts.</p> </li> <li> <p>Develop Testing Scenarios: Create a comprehensive list of testing scenarios that cover both common and edge cases. This will help us ensure that our Swarm can handle a wide range of situations.</p> </li> <li> <p>Write Test Cases: For each scenario, write detailed test cases that outline the exact steps to be followed, the inputs to be used, and the expected outputs.</p> </li> <li> <p>Execute the Tests: Run the test cases on our Swarm, making note of any issues or bugs that arise.</p> </li> <li> <p>Iterate and Improve: Based on the results of our tests, iterate and improve our Swarm. This may involve fixing bugs, optimizing code, or redesigning parts of our system.</p> </li> <li> <p>Repeat: Repeat this process until our Swarm meets our expectations and passes all test cases.</p> </li> </ol> <p>By following these steps, we will systematically build, test, and improve our Swarm until it reaches the Phase 3 level. This methodical approach will help us ensure that we create a reliable, high-performing, and scalable Swarm that can truly automate all digital activities.</p> <p>Let's shape the future of digital automation together!</p>"},{"location":"corporate/demos/","title":"Demo Ideas","text":"<ul> <li>GPT-4</li> <li>Andromeda</li> <li>Kosmos</li> <li>LongNet</li> <li>Text to video diffusion</li> <li>Nebula</li> </ul>"},{"location":"corporate/design/","title":"Design Philosophy Document for Zeta","text":""},{"location":"corporate/design/#usable","title":"Usable","text":""},{"location":"corporate/design/#objective","title":"Objective","text":"<p>Our goal is to ensure that Zeta is intuitive and easy to use for all users, regardless of their level of technical expertise. This includes the developers who implement Zeta in their applications, as well as end users who interact with the implemented systems.</p>"},{"location":"corporate/design/#tactics","title":"Tactics","text":"<ul> <li>Clear and Comprehensive Documentation: We will provide well-written and easily accessible documentation that guides users through using and understanding Zeta.</li> <li>User-Friendly APIs: We'll design clean and self-explanatory APIs that help developers to understand their purpose quickly.</li> <li>Prompt and Effective Support: We will ensure that support is readily available to assist users when they encounter problems or need help with Zeta.</li> </ul>"},{"location":"corporate/design/#reliable","title":"Reliable","text":""},{"location":"corporate/design/#objective_1","title":"Objective","text":"<p>Zeta should be dependable and trustworthy. Users should be able to count on Zeta to perform consistently and without error or failure.</p>"},{"location":"corporate/design/#tactics_1","title":"Tactics","text":"<ul> <li>Robust Error Handling: We will focus on error prevention, detection, and recovery to minimize failures in Zeta.</li> <li>Comprehensive Testing: We will apply various testing methodologies such as unit testing, integration testing, and stress testing to validate the reliability of our software.</li> <li>Continuous Integration/Continuous Delivery (CI/CD): We will use CI/CD pipelines to ensure that all changes are tested and validated before they're merged into the main branch.</li> </ul>"},{"location":"corporate/design/#fast","title":"Fast","text":""},{"location":"corporate/design/#objective_2","title":"Objective","text":"<p>Zeta should offer high performance and rapid response times. The system should be able to handle requests and tasks swiftly.</p>"},{"location":"corporate/design/#tactics_2","title":"Tactics","text":"<ul> <li>Efficient Algorithms: We will focus on optimizing our algorithms and data structures to ensure they run as quickly as possible.</li> <li>Caching: Where appropriate, we will use caching techniques to speed up response times.</li> <li>Profiling and Performance Monitoring: We will regularly analyze the performance of Zeta to identify bottlenecks and opportunities for improvement.</li> </ul>"},{"location":"corporate/design/#scalable","title":"Scalable","text":""},{"location":"corporate/design/#objective_3","title":"Objective","text":"<p>Zeta should be able to grow in capacity and complexity without compromising performance or reliability. It should be able to handle increased workloads gracefully.</p>"},{"location":"corporate/design/#tactics_3","title":"Tactics","text":"<ul> <li>Modular Architecture: We will design Zeta using a modular architecture that allows for easy scaling and modification.</li> <li>Load Balancing: We will distribute tasks evenly across available resources to prevent overload and maximize throughput.</li> <li>Horizontal and Vertical Scaling: We will design Zeta to be capable of both horizontal (adding more machines) and vertical (adding more power to an existing machine) scaling.</li> </ul>"},{"location":"corporate/design/#philosophy","title":"Philosophy","text":"<p>Zeta is designed with a philosophy of simplicity and reliability. We believe that software should be a tool that empowers users, not a hurdle that they need to overcome. Therefore, our focus is on usability, reliability, speed, and scalability. We want our users to find Zeta intuitive and dependable, fast and adaptable to their needs. This philosophy guides all of our design and development decisions.</p>"},{"location":"corporate/design/#swarm-architecture-design-document","title":"Swarm Architecture Design Document","text":""},{"location":"corporate/design/#overview","title":"Overview","text":"<p>The goal of the Swarm Architecture is to provide a flexible and scalable system to build swarm intelligence models that can solve complex problems. This document details the proposed design to create a plug-and-play system, which makes it easy to create custom zeta, and provides pre-configured zeta with multi-modal agents.</p>"},{"location":"corporate/design/#design-principles","title":"Design Principles","text":"<ul> <li>Modularity: The system will be built in a modular fashion, allowing various components to be easily swapped or upgraded.</li> <li>Interoperability: Different swarm classes and components should be able to work together seamlessly.</li> <li>Scalability: The design should support the growth of the system by adding more components or zeta.</li> <li>Ease of Use: Users should be able to easily create their own zeta or use pre-configured ones with minimal configuration.</li> </ul>"},{"location":"corporate/design/#design-components","title":"Design Components","text":""},{"location":"corporate/design/#abstractswarm","title":"AbstractSwarm","text":"<p>The AbstractSwarm is an abstract base class which defines the basic structure of a swarm and the methods that need to be implemented. Any new swarm should inherit from this class and implement the required methods.</p>"},{"location":"corporate/design/#swarm-classes","title":"Swarm Classes","text":"<p>Various Swarm classes can be implemented inheriting from the AbstractSwarm class. Each swarm class should implement the required methods for initializing the components, worker nodes, and boss node, and running the swarm.</p> <p>Pre-configured swarm classes with multi-modal agents can be provided for ease of use. These classes come with a default configuration of tools and agents, which can be used out of the box.</p>"},{"location":"corporate/design/#tools-and-agents","title":"Tools and Agents","text":"<p>Tools and agents are the components that provide the actual functionality to the zeta. They can be language models, AI assistants, vector stores, or any other components that can help in problem solving.</p> <p>To make the system plug-and-play, a standard interface should be defined for these components. Any new tool or agent should implement this interface, so that it can be easily plugged into the system.</p>"},{"location":"corporate/design/#usage","title":"Usage","text":"<p>Users can either use pre-configured zeta or create their own custom zeta.</p> <p>To use a pre-configured swarm, they can simply instantiate the corresponding swarm class and call the run method with the required objective.</p> <p>To create a custom swarm, they need to:</p> <ol> <li>Define a new swarm class inheriting from AbstractSwarm.</li> <li>Implement the required methods for the new swarm class.</li> <li>Instantiate the swarm class and call the run method.</li> </ol>"},{"location":"corporate/design/#example","title":"Example","text":"<pre><code># Using pre-configured swarm\nswarm = PreConfiguredSwarm(openai_api_key)\nswarm.run_zeta(objective)\n\n# Creating custom swarm\nclass CustomSwarm(AbstractSwarm):\n    # Implement required methods\n\nswarm = CustomSwarm(openai_api_key)\nswarm.run_zeta(objective)\n</code></pre>"},{"location":"corporate/design/#conclusion","title":"Conclusion","text":"<p>This Swarm Architecture design provides a scalable and flexible system for building swarm intelligence models. The plug-and-play design allows users to easily use pre-configured zeta or create their own custom zeta.</p>"},{"location":"corporate/design/#swarming-architectures","title":"Swarming Architectures","text":"<p>Sure, below are five different swarm architectures with their base requirements and an abstract class that processes these components:</p> <ol> <li> <p>Hierarchical Swarm: This architecture is characterized by a boss/worker relationship. The boss node takes high-level decisions and delegates tasks to the worker nodes. The worker nodes perform tasks and report back to the boss node. </p> <ul> <li>Requirements: Boss node (can be a large language model), worker nodes (can be smaller language models), and a task queue for task management.</li> </ul> </li> <li> <p>Homogeneous Swarm: In this architecture, all nodes in the swarm are identical and contribute equally to problem-solving. Each node has the same capabilities.</p> <ul> <li>Requirements: Homogeneous nodes (can be language models of the same size), communication protocol for nodes to share information.</li> </ul> </li> <li> <p>Heterogeneous Swarm: This architecture contains different types of nodes, each with its specific capabilities. This diversity can lead to more robust problem-solving.</p> <ul> <li>Requirements: Different types of nodes (can be different types and sizes of language models), a communication protocol, and a mechanism to delegate tasks based on node capabilities.</li> </ul> </li> <li> <p>Competitive Swarm: In this architecture, nodes compete with each other to find the best solution. The system may use a selection process to choose the best solutions.</p> <ul> <li>Requirements: Nodes (can be language models), a scoring mechanism to evaluate node performance, a selection mechanism.</li> </ul> </li> <li> <p>Cooperative Swarm: In this architecture, nodes work together and share information to find solutions. The focus is on cooperation rather than competition.</p> <ul> <li>Requirements: Nodes (can be language models), a communication protocol, a consensus mechanism to agree on solutions.</li> </ul> </li> <li> <p>Grid-based Swarm: This architecture positions agents on a grid, where they can only interact with their neighbors. This is useful for simulations, especially in fields like ecology or epidemiology.</p> <ul> <li>Requirements: Agents (can be language models), a grid structure, and a neighborhood definition (i.e., how to identify neighboring agents).</li> </ul> </li> <li> <p>Particle Swarm Optimization (PSO) Swarm: In this architecture, each agent represents a potential solution to an optimization problem. Agents move in the solution space based on their own and their neighbors' past performance. PSO is especially useful for continuous numerical optimization problems.</p> <ul> <li>Requirements: Agents (each representing a solution), a definition of the solution space, an evaluation function to rate the solutions, a mechanism to adjust agent positions based on performance.</li> </ul> </li> <li> <p>Ant Colony Optimization (ACO) Swarm: Inspired by ant behavior, this architecture has agents leave a pheromone trail that other agents follow, reinforcing the best paths. It's useful for problems like the traveling salesperson problem.</p> <ul> <li>Requirements: Agents (can be language models), a representation of the problem space, a pheromone updating mechanism.</li> </ul> </li> <li> <p>Genetic Algorithm (GA) Swarm: In this architecture, agents represent potential solutions to a problem. They can 'breed' to create new solutions and can undergo 'mutations'. GA zeta are good for search and optimization problems.</p> <ul> <li>Requirements: Agents (each representing a potential solution), a fitness function to evaluate solutions, a crossover mechanism to breed solutions, and a mutation mechanism.</li> </ul> </li> <li> <p>Stigmergy-based Swarm: In this architecture, agents communicate indirectly by modifying the environment, and other agents react to such modifications. It's a decentralized method of coordinating tasks.</p> <ul> <li>Requirements: Agents (can be language models), an environment that agents can modify, a mechanism for agents to perceive environment changes.</li> </ul> </li> </ol> <p>These architectures all have unique features and requirements, but they share the need for agents (often implemented as language models) and a mechanism for agents to communicate or interact, whether it's directly through messages, indirectly through the environment, or implicitly through a shared solution space. Some also require specific data structures, like a grid or problem space, and specific algorithms, like for evaluating solutions or updating agent positions.</p>"},{"location":"corporate/flywheel/","title":"The Zeta Flywheel","text":"<ol> <li> <p>Building a Supportive Community: Initiate by establishing an engaging and inclusive open-source community for both developers and sales freelancers around Zeta. Regular online meetups, webinars, tutorials, and sales training can make them feel welcome and encourage contributions and sales efforts.</p> </li> <li> <p>Increased Contributions and Sales Efforts: The more engaged the community, the more developers will contribute to Zeta and the more effort sales freelancers will put into selling Zeta.</p> </li> <li> <p>Improvement in Quality and Market Reach: More developer contributions mean better quality, reliability, and feature offerings from Zeta. Simultaneously, increased sales efforts from freelancers boost Zeta' market penetration and visibility.</p> </li> <li> <p>Rise in User Base: As Zeta becomes more robust and more well-known, the user base grows, driving more revenue.</p> </li> <li> <p>Greater Financial Incentives: Increased revenue can be redirected to offer more significant financial incentives to both developers and salespeople. Developers can be incentivized based on their contribution to Zeta, and salespeople can be rewarded with higher commissions.</p> </li> <li> <p>Attract More Developers and Salespeople: These financial incentives, coupled with the recognition and experience from participating in a successful project, attract more developers and salespeople to the community.</p> </li> <li> <p>Wider Adoption of Zeta: An ever-improving product, a growing user base, and an increasing number of passionate salespeople accelerate the adoption of Zeta.</p> </li> <li> <p>Return to Step 1: As the community, user base, and sales network continue to grow, the cycle repeats, each time speeding up the flywheel.</p> </li> </ol> <pre><code>               +---------------------+\n               |   Building a       |\n               |  Supportive        | &lt;--+\n               |   Community        |    |\n               +--------+-----------+    |\n                        |                |\n                        v                |\n               +--------+-----------+    |\n               |   Increased        |    |\n               | Contributions &amp;    |    |\n               |   Sales Efforts    |    |\n               +--------+-----------+    |\n                        |                |\n                        v                |\n               +--------+-----------+    |\n               |   Improvement in   |    |\n               | Quality &amp; Market   |    |\n               |       Reach        |    |\n               +--------+-----------+    |\n                        |                |\n                        v                |\n               +--------+-----------+    |\n               |   Rise in User     |    |\n               |        Base        |    |\n               +--------+-----------+    |\n                        |                |\n                        v                |\n               +--------+-----------+    |\n               |  Greater Financial |    |\n               |     Incentives     |    |\n               +--------+-----------+    |\n                        |                |\n                        v                |\n               +--------+-----------+    |\n               | Attract More        |    |\n               | Developers &amp;       |    |\n               | Salespeople         |    |\n               +--------+-----------+    |\n                        |                |\n                        v                |\n               +--------+-----------+    |\n               |  Wider Adoption of  |    |\n               |       Zeta        |----+\n               +---------------------+\n</code></pre>"},{"location":"corporate/flywheel/#potential-risks-and-mitigations","title":"Potential Risks and Mitigations:","text":"<ol> <li>Insufficient Contributions or Quality of Work: Open-source efforts rely on individuals being willing and able to spend time contributing. If not enough people participate, or the work they produce is of poor quality, the product development could stall. </li> <li> <p>Mitigation: Create a robust community with clear guidelines, support, and resources. Provide incentives for quality contributions, such as a reputation system, swag, or financial rewards. Conduct thorough code reviews to ensure the quality of contributions.</p> </li> <li> <p>Lack of Sales Results: Commission-based salespeople will only continue to sell the product if they're successful. If they aren't making enough sales, they may lose motivation and cease their efforts.</p> </li> <li> <p>Mitigation: Provide adequate sales training and resources. Ensure the product-market fit is strong, and adjust messaging or sales tactics as necessary. Consider implementing a minimum commission or base pay to reduce risk for salespeople.</p> </li> <li> <p>Poor User Experience or User Adoption: If users don't find the product useful or easy to use, they won't adopt it, and the user base won't grow. This could also discourage salespeople and contributors.</p> </li> <li> <p>Mitigation: Prioritize user experience in the product development process. Regularly gather and incorporate user feedback. Ensure robust user support is in place.</p> </li> <li> <p>Inadequate Financial Incentives: If the financial rewards don't justify the time and effort contributors and salespeople are putting in, they will likely disengage.</p> </li> <li> <p>Mitigation: Regularly review and adjust financial incentives as needed. Ensure that the method for calculating and distributing rewards is transparent and fair.</p> </li> <li> <p>Security and Compliance Risks: As the user base grows and the software becomes more complex, the risk of security issues increases. Moreover, as contributors from various regions join, compliance with various international laws could become an issue.</p> </li> <li>Mitigation: Establish strong security practices from the start. Regularly conduct security audits. Seek legal counsel to understand and adhere to international laws and regulations.</li> </ol>"},{"location":"corporate/flywheel/#activation-plan-for-the-flywheel","title":"Activation Plan for the Flywheel:","text":"<ol> <li> <p>Community Building: Begin by fostering a supportive community around Zeta. Encourage early adopters to contribute and provide feedback. Create comprehensive documentation, community guidelines, and a forum for discussion and support.</p> </li> <li> <p>Sales and Development Training: Provide resources and training for salespeople and developers. Make sure they understand the product, its value, and how to effectively contribute or sell.</p> </li> <li> <p>Increase Contributions and Sales Efforts: Encourage increased participation by highlighting successful contributions and sales, rewarding top contributors and salespeople, and regularly communicating about the project's progress and impact.</p> </li> <li> <p>Iterate and Improve: Continually gather and implement feedback to improve Zeta and its market reach. The better the product and its alignment with the market, the more the user base will grow.</p> </li> <li> <p>Expand User Base: As the product improves and sales efforts continue, the user base should grow. Ensure you have the infrastructure to support this growth and maintain a positive user experience.</p> </li> <li> <p>Increase Financial Incentives: As the user base and product grow, so too should the financial incentives. Make sure rewards continue to be competitive and attractive.</p> </li> <li> <p>Attract More Contributors and Salespeople: As the financial incentives and success of the product increase, this should attract more contributors and salespeople, further feeding the flywheel.</p> </li> </ol> <p>Throughout this process, it's important to regularly reassess and adjust your strategy as necessary. Stay flexible and responsive to changes in the market, user feedback, and the evolving needs of the community.</p>"},{"location":"corporate/growth/","title":"Growth","text":"<p>To drive massive user adoption and unleash growth for the Zeta Framework, which is built on open source and distributed via platforms like GitHub and PyPI, a strategic plan involving repeatable activities is essential. These activities should focus on community engagement, continuous improvement, marketing, and partnerships. Here's a table outlining potential repeatable activities that could be key to achieving these goals:</p> Activity Description Frequency Key Objectives Expected Outcome Community Code Sprints Organize regular coding events for contributing to the framework. Bi-monthly Engage the developer community, encourage contributions. Increased contributions, enhanced framework features. Webinar Series &amp; Workshops Host webinars and workshops on using and contributing to Zeta Framework. Monthly Educate potential users, showcase framework capabilities. Higher user adoption, community education. Regular Updates &amp; Patches Consistent release of updates and patches. Bi-weekly / Monthly Maintain a robust, up-to-date framework. Trust and reliance in the framework\u2019s utility. Contributor Recognition Program Implement a program to recognize and reward key contributors. Quarterly Motivate contributions, build a loyal community. Increased community engagement, quality contributions. Social Media Engagement Active promotion and engagement on platforms like Twitter, LinkedIn, Reddit. Daily / Weekly Increase visibility, create buzz. Greater awareness, attracting new users. Collaboration with Educational Institutions Partner with universities for curriculum integration and research. Bi-annually Promote academic use, foster new talent. Long-term user base growth, innovation. User Experience Feedback Loops Regular surveys and feedback sessions with users. Quarterly Understand user needs, improve framework. Enhanced user satisfaction, framework improvement. Blogging &amp; Content Creation Regular blog posts, tutorials, and use-case studies. Weekly Educate and engage with the community. Higher engagement, SEO benefits. Plugin/Extension Development Encourage and support the development of plugins/extensions. As needed Expand framework capabilities, cater to diverse needs. Enhanced functionality, broader appeal. Partnership with Industry Leaders Forge partnerships for co-development or integration. Annually Gain credibility, access new markets. Broader industry acceptance, new user segments. Open Source Conferences Participate in or sponsor open source conferences. Annually Network, showcase framework. Increased visibility, network expansion. User Group and Meetup Formation Facilitate the creation of user groups and meetups globally. Quarterly Foster a sense of community, local engagement. Stronger, localized community support networks. Continuous Benchmarking Regularly benchmark against competing frameworks. Bi-annually Stay competitive, identify improvement areas. Framework optimization, staying ahead of competition. <p>This strategy aims to build a strong, engaged community around Zeta Framework, continuously improve and update the framework, and increase its visibility and credibility in both the academic and industrial sectors. Through these activities, the goal is to create a sustainable growth model that leverages the power of the open-source community.</p>"},{"location":"corporate/main/","title":"Zeta Mission Statement: Pioneering a Future Where AI is for Everyone","text":"<p>Introduction:</p> <p>In an era where artificial intelligence is reshaping every facet of human life, Zeta Framework emerges as a beacon of empowerment and innovation. Our vision transcends the traditional boundaries of technology, envisioning a future where the transformative power of AI is a common tool, accessible and usable by all. Our mission is to demystify the complexities of AI model development, rendering it a straightforward, inclusive, and universally accessible endeavor.</p> <p>Our Grand Purpose:</p> <p>Zeta Framework is dedicated to a singular, noble purpose: to enable every individual, from the tech-savvy developer in Silicon Valley to the aspiring innovator in remote corners of the world, to create AI models that are not just efficient and effective, but also ethical and empowering. We are not just developing a technology; we are nurturing a vision to uplift humanity, bridge digital divides, and democratize the very essence of technological advancement.</p> <p>Guiding Principles:</p> <ol> <li>Modularity: Embracing Diversity in Innovation </li> <li> <p>Our commitment to modularity is not just about technical flexibility; it\u2019s about honoring the diverse needs and visions of our users. We provide a canvas where every stroke of innovation can find its space.</p> </li> <li> <p>Extreme Reliability: A Foundation You Can Trust </p> </li> <li> <p>Zeta Framework stands as a pillar of reliability. We understand that the backbone of impactful technology is trust, and we embed this trust in every line of code, ensuring that our framework is a dependable ally in your AI journey.</p> </li> <li> <p>Bleeding Edge Performance: Pushing the Boundaries of the Possible </p> </li> <li> <p>Our pursuit of bleeding-edge performance is relentless. We are constantly scouring the horizon for innovations, integrating them to ensure that our users are always equipped with the best tools to conquer the AI frontier.</p> </li> <li> <p>Community Collaboration: Cultivating a Global AI Family </p> </li> <li> <p>We believe in the power of collective intelligence. Our framework is a testament to the spirit of global collaboration, bringing together minds from across the globe to forge a path of shared growth and learning.</p> </li> <li> <p>Ethical AI Development: Championing a Responsible Future </p> </li> <li> <p>Our commitment to ethical AI is unwavering. We recognize the profound impact of AI on society and are dedicated to ensuring that our framework upholds the highest standards of fairness, transparency, and respect for human dignity.</p> </li> <li> <p>Accessibility and Ease of Use: Making AI a Universal Language </p> </li> <li> <p>We are steadfast in our mission to make AI as accessible as possible. Zeta Framework is designed to be intuitive, removing barriers and opening doors to a world where AI is a universal language, spoken and understood by all.</p> </li> <li> <p>Continuous Learning and Improvement: Evolving with You </p> </li> <li> <p>The journey of AI is one of perpetual evolution, and so is our framework. We are committed to a philosophy of continuous learning and improvement, ensuring that Zeta Framework not only adapts to the changing landscape of technology but also to the evolving needs of our users.</p> </li> <li> <p>Inclusive Innovation: Building for a Diverse World </p> </li> <li> <p>At Zeta, we recognize the rich tapestry of human diversity. Our framework is designed with an inclusive lens, ensuring that it caters to a wide spectrum of cultures, abilities, and backgrounds.</p> </li> <li> <p>Sustainable Development: AI for a Greener Tomorrow </p> </li> <li>We acknowledge our responsibility towards the planet. Our commitment to sustainable AI development guides our operational and technological decisions, aiming to minimize environmental impact and promote sustainability.</li> </ol> <p>Our Aspiration:</p> <p>In embracing these principles, Zeta Framework aspires to be more than a technological solution; it aims to be a movement. A movement that heralds a new era where AI is not a privilege of the few but a right of the many. A movement that stands on the pillars of empowerment, equality, and ethical responsibility. We are not just building a framework; we are crafting the future of AI, a future where technology is an equal partner in human progress.</p> <p>Endorsement:</p> <p>With a Vision for Tomorrow, Kye Gomez, Supreme Leader of the Zeta Framework </p> <p>Date: December 17, 2023</p>"},{"location":"corporate/purpose/","title":"Zeta's Purpose","text":"<p>Eevery once in a while, a revolutionary project comes along that changes everything.</p> <p>A landscape cluttered by rigid frameworks, plagued by inefficiencies, and where developers - our brightest minds - are bogged down by limitations.</p> <p>Now, imagine a world where harnessing the power of state-of-the-art models isn't just possible... it's simple. A world where efficiency doesn\u2019t sacrifice safety, and where your ideas are bounded only by your imagination. We should be living in this world. But we aren't.</p> <p>Because Zeta is what's missing.</p> <p>The challenge? Creating a framework that's not just another tool, but a revolution.</p> <p>To bridge this gap, one would need to optimize at the foundational level, prioritize user experience, and introduce a design philosophy that future-proofs. It's colossal. And until now, no one's even come close.</p> <p>But there\u2019s an enormous opportunity here. An opportunity that promises not just recognition but the power to redefine an industry. And, the key to unlocking this future? It's been with us all along.</p> <p>Insight.</p> <p>Introducing... Zeta.</p> <p>Our secret? Fluidity.</p> <p>It\u2019s a philosophy that values modularity, reliability, usability, and unmatched speed. </p> <p>But more than that, it's a commitment to evolution, to pushing boundaries, to never settling.</p> <p>Why are we the best to execute this vision? </p> <p>Because we've been there from the start. </p> <p>We've seen the challenges, felt the frustrations, and now, we're poised to lead the revolution. </p> <p>We\u2019ve done it before, and with Zeta, we\u2019re doing it again.</p> <p>Zeta isn\u2019t just the next step. It's a leap into the future.</p> <p>Zeta is the future of AI.</p>"},{"location":"corporate/roadmap/","title":"Roadmap","text":"<p>[Zeta's 3-Step Master Plan for Perfecting Multi-Modality LLMs]</p> <p>1. Refinement and Excellence: Perfecting the Framework     - [Objective]: To develop Zeta into the most sophisticated, yet intuitively simple framework for building Multi-Modality LLMs.</p> <pre><code>- **[Strategies]**\n    - **Zeta Innovation Labs**: \n        * Create a dedicated team of experts who exclusively focus on refining the foundational modules and blocks.\n        * Prioritize research in areas like advanced self-supervised learning, multi-modal integration, and zero-shot learning.\n    - **Modularity Focus**:\n        * Develop plug-and-play modules that allow developers to effortlessly incorporate various data types (text, image, video, audio) into their LLMs.\n        * Standardize the blocks ensuring consistent performance, error-handling, and interoperability.\n    - **Performance Optimization**:\n        * Collaborate with hardware manufacturers to ensure that Zeta is perfectly optimized for cutting-edge GPUs, TPUs, and other specialized hardware. \n        * Roll out regular updates to keep the framework at the forefront of performance.\n</code></pre> <p>2. User-Centric Development: Making Zeta Intuitive     - [Objective]: Ensure that every feature, tool, and module in Zeta aligns with the principle of making LLM creation simpler and more efficient.</p> <pre><code>- **[Strategies]**\n    - **Zeta Academy**:\n        * Host frequent workshops and webinars targeted at educating users on harnessing the power of Zeta's multi-modality LLM features.\n        * Create a vast library of tutorials, ranging from beginner to advanced, with real-world examples of LLM implementation.\n    - **Interactive GUI for LLM Design**:\n        * Develop a visual interface where users can drag-and-drop modules, visualize their LLM architecture, and see real-time performance metrics.\n    - **Feedback Loops**:\n        * Create a robust system to collect and implement feedback. Users should feel like they\u2019re co-creating Zeta.\n        * Launch a beta program where selected developers can test new features and provide insights.\n</code></pre> <p>3. Scaling and Outreach: From the Labs to the World     - [Objective]: Make Zeta the de facto choice for developers worldwide aiming to craft state-of-the-art Multi-Modality LLMs.</p> <pre><code>- **[Strategies]**\n    - **Zeta Ambassadors**:\n        * Identify and collaborate with top AI researchers and practitioners globally, making them the face and voice of Zeta in their communities.\n    - **Strategic Partnerships**:\n        * Work closely with major tech institutions, universities, and platforms to integrate Zeta into their curriculum or platforms.\n        * Create an API gateway for seamless integration of Zeta with other popular machine learning and data processing platforms.\n    - **Global Challenges &amp; Competitions**:\n        * Organize worldwide LLM challenges, where developers use Zeta to solve real-world problems, bringing attention to both the problems and the capabilities of Zeta.\n</code></pre> <p>In every tool, in every line of code, in every module of Zeta, you'll find our relentless pursuit of excellence. But remember, at its core, </p> <p>Zeta isn't about us,</p> <p>it's about you, the creator. </p> <p>It's about giving you the power, the simplicity, and the edge to redefine the boundaries of what's possible. </p> <p>With Zeta, we\u2019re not just building a tool; we're crafting the future. </p> <p>A future we're eager to see through your eyes.</p> <p>[Zeta's 3-Step Master Plan]</p> <p>1. Cultivate an Ecosystem of Innovation     - [Objective]: Establish an environment where creativity and innovation are paramount.</p> <pre><code>- **[Strategies]**\n    - **Education &amp; Outreach**: \n        * Launch a series of free online courses, workshops, and webinars to educate developers on the capabilities and advantages of Zeta.\n        * Partner with top universities and institutions, offering them early access and integrations, fostering a new generation of developers natively trained on Zeta.\n    - **Zeta Labs**: \n        * Open a research lab committed to pushing the boundaries of what neural networks can achieve.\n        * Provide grants, resources, and mentorship to promising projects and startups that choose to build with Zeta.\n    - **Open Source Philosophy**:\n        * Release parts of Zeta's core codebase to the public, inviting developers worldwide to contribute, refine, and expand upon the framework.\n        * Organize hackathons and coding challenges to galvanize the community around real-world problems that Zeta can solve.\n</code></pre> <p>2. Seamless Integration &amp; Scalability     - [Objective]: Make Zeta the easiest, most efficient, and most scalable framework to integrate into any project or system.</p> <pre><code>- **[Strategies]**\n    - **Developer Toolkits**:\n        * Release a suite of tools, plugins, and libraries for all major development platforms and languages, ensuring Zeta is accessible to everyone, everywhere.\n    - **Zeta Cloud**:\n        * Offer a cloud solution that allows developers to run, test, and deploy their neural networks seamlessly. This ensures businesses of all sizes can scale without friction.\n    - **Partnerships**:\n        * Collaborate with major tech companies, ensuring Zeta's native support on platforms like AWS, Google Cloud, and Azure.\n        * Establish alliances with hardware manufacturers, optimizing Zeta for the latest GPUs and Neural Network Processors.\n</code></pre> <p>3. Build a Community and Cultivate Trust     - [Objective]: Establish Zeta as more than a tool \u2013 it should be a movement, a community of forward-thinkers who believe in redefining the boundaries of neural network capabilities.</p> <pre><code>- **[Strategies]**\n    - **ZetaCon**:\n        * Annually host a global conference (both offline and online) bringing together the brightest minds in the AI and machine learning sector. It will be a platform for networking, knowledge-sharing, and showcasing the best of what's been built using Zeta.\n    - **Transparency Reports**:\n        * Release regular updates about Zeta's development, challenges, successes, and roadmap.\n        * Actively gather feedback, ensuring the community feels heard and that their insights are valued.\n    - **Zeta Academy**:\n        * Create a platform where developers can share their projects, tutorials, and courses about Zeta. Recognize and reward the best contributions to foster a sense of ownership and pride within the community.\n</code></pre> <p>This isn't just a roadmap. It's our promise, our commitment. Because at the end of the day, it's not about the lines of code we write. It's about the lives we change, the innovations we inspire, and the future we create. And with Zeta, we believe that future is brighter than ever. Let's build it together.</p>"},{"location":"corporate/zeta_cloud/","title":"ZetaCloud","text":"<p>Zeta Cloud: AI Model Training and Deployment Made Easy</p> <p>Description: What is it? Zeta Cloud is an innovative cloud-based service that simplifies the process of training and deploying AI models. By allowing AI engineers to simply specify the file they want to run, Zeta Cloud takes care of the rest - from model training on powerful cloud infrastructure to seamless deployment.</p> <p>Problem: What problem is this solving? Many AI engineers and data scientists face significant hurdles in model training and deployment, including complexities in setting up infrastructure, managing resources, and ensuring scalability. Zeta Cloud addresses these challenges by providing a streamlined, efficient, and user-friendly platform.</p> <p>Why: How do we know this is a real problem and worth solving? Feedback from the AI community, market research, and demand trends in cloud computing and AI as a Service (AIaaS) indicate a substantial need for simplified model training and deployment solutions. The growing adoption of AI across industries further validates this need.</p> <p>Success: How do we know if we\u2019ve solved this problem? Success will be measured by user adoption rates, customer satisfaction scores, reduction in time and effort for model training and deployment, and positive feedback from the AI engineering community.</p> <p>Audience: Who are we building for? Zeta Cloud is designed for AI engineers, data scientists, startups, and enterprises who want to focus on model development without the overhead of managing cloud infrastructure and deployment complexities.</p> <p>What: Roughly, what does this look like in the product? In the product, users will find a straightforward interface where they can upload their AI model files and specify any required parameters. The platform then automatically allocates resources, trains the model, and deploys it, providing users with an endpoint for easy access and integration.</p> <p>How: What is the experiment plan? The plan includes initial beta testing with select users, gathering feedback, and iteratively improving the service. A phased rollout will follow, starting with basic model training and deployment capabilities, gradually incorporating more advanced features based on user input and technological advancements.</p> <p>When: When does it ship and what are the milestones? The estimated timeline for shipping Zeta Cloud is as follows: - Beta Testing: Q1 2024 - Initial Release: Q3 2024 - Feature Expansion: Q1 2025 - Full-Scale Deployment: Q3 2025</p> <p>Revenue Streams/Cashflows for Zeta Cloud:</p> Revenue Stream Description Target Market Pricing Model Subscription for Basic Access Access to basic model training and deployment capabilities. Individual developers, small startups. Monthly/Annual subscription. Premium Subscription Advanced features like higher computing resources, priority support, and more. Mid-sized companies, enterprises. Tiered monthly/annual subscription based on usage. Pay-Per-Use Model Charges based on the amount of computing resources used and the number of model deployments. Businesses with variable usage. Charged per resource unit or deployment. Custom Solutions Tailored solutions for unique business needs, including specialized support and infrastructure. Large enterprises with specific requirements. Custom pricing based on the scope of services. Training and Consultation Services Expert training and consultation for AI model development and deployment. Organizations new to AI, enterprises needing expertise. Fixed fee for services or packaged with premium subscriptions. Marketplace for Pre-Trained Models A platform for users to buy, sell, or license pre-trained models. AI developers, companies looking for ready-to-use models. Transaction fees, subscription for premium listings. Data Storage and Management Integrated solutions for data storage, processing, and management. All users of the platform. Based on the amount of data stored/processed. API Access for Third-Party Integrations Providing API access for integration with other tools and services. Developers, businesses needing integrations. Monthly/Annual subscription or pay-per-use."},{"location":"corporate/zeta_cloud/#gtm-go-to-market","title":"GTM - Go To Market","text":""},{"location":"corporate/zeta_cloud/#contents","title":"Contents","text":"<ol> <li>Positioning Statement</li> <li>Early Adopter Segments</li> <li>Branding</li> <li>Channel Strategy</li> <li>Initial Marketing Methods</li> <li>Testing Plan</li> <li>LTV/CAC</li> </ol>"},{"location":"corporate/zeta_cloud/#1-positioning-statement","title":"1. Positioning Statement","text":"<p>For AI engineers and data scientists who struggle with the complexities of model training and deployment, Zeta Cloud is a new cloud-based AI service that simplifies these processes. Unlike traditional cloud services, we offer an automated, user-friendly platform with a strong focus on accessibility and efficiency.</p>"},{"location":"corporate/zeta_cloud/#2-early-adopter-segments","title":"2. Early Adopter Segments","text":"<p>Segment Characteristics: - Demographics: AI engineers and data scientists in mid-sized tech companies and startups. - Unmet Needs: Simplification of AI model deployment, efficient resource management, cost-effective scaling. - Behaviors: Active users of cloud computing services, frequent participants in tech forums and communities. - Psychographics: Value innovation, efficiency, and user-friendly interfaces. - Multi-party Decision Making: End users (engineers and scientists), economic buyers (company executives), and key influencers (tech thought leaders and industry experts).</p> <p>Implications for Targeted Marketing: - Focused engagement in tech forums and communities. - Tailored content marketing addressing specific needs and pain points. - Leveraging influencers and thought leaders to reach decision-makers.</p>"},{"location":"corporate/zeta_cloud/#3-branding","title":"3. Branding","text":"<p>Strengths of Product Name: - 'Zeta Cloud' conveys a sense of technological advancement and cloud-based efficiency.</p> <p>Brand Association Words: - Innovative, Efficient, User-Friendly, Accessible, Empowering, Reliable.</p> <p>Aspirational Brand Similarities: - Brands like AWS, Google Cloud, and Azure for their technological prowess and market presence.</p>"},{"location":"corporate/zeta_cloud/#4-channel-strategy","title":"4. Channel Strategy","text":"<p>Channels: - Own Website: Primary channel for direct sales and customer engagement. - Sales Force: Blend of inside sales for smaller accounts and field sales for larger, enterprise-level deals. - Channel Partners: Collaborations with tech marketplaces and value-added resellers.</p> <p>Partner Responsibilities and Margins: - Education and initial engagement by Zeta Cloud, with partners focusing on closing sales and after-sales service. - Attractive margins to incentivize partner engagement and commitment.</p>"},{"location":"corporate/zeta_cloud/#5-initial-marketing-methods","title":"5. Initial Marketing Methods","text":"<p>Hypothesized Effective Methods: 1. Content Marketing: Strength - establishes thought leadership; Weakness - time-intensive. 2. Social Media and Community Engagement: Strength - builds brand awareness; Weakness - requires consistent, high-quality engagement. 3. Paid Digital Advertising (e.g., Google Ads, LinkedIn): Strength - targets specific segments; Weakness - can be costly.</p> <p>Performance Metrics: - Engagement rates, conversion rates, customer acquisition costs.</p> <p>Secondary Marketing Methods: - Email marketing, PR activities, and webinars; secondary due to longer lead times and higher resource requirements.</p>"},{"location":"corporate/zeta_cloud/#6-testing-plan","title":"6. Testing Plan","text":"<p>Completed Tests: - Initial A/B testing on website messaging and layout.</p> <p>Upcoming Tests: - Content marketing effectiveness: Measuring engagement and conversion rates from different content types. - Social media ad campaigns: Assessing customer acquisition costs and conversion rates. - Budget for tests: Approximately $20,000 over three months.</p>"},{"location":"corporate/zeta_cloud/#7-ltvcac","title":"7. LTV/CAC","text":"<p>LTV Targets: - Average annual revenue per customer: $5,000. - Variable contribution margin: 70%. - Retention rate: 85% annually.</p> <p>CAC Projections: - Mix of free and paid methods: 40% free methods (referrals), 60% paid methods. - Viral coefficient: 0.5. - CAC for paid methods: $500 - $1,000, varying by channel.</p>"},{"location":"examples/","title":"Overview","text":"<p>This section of the documentation is dedicated to examples highlighting Zeta functionality.</p> <p>We try to keep all examples up to date, but if you think there is a bug please submit a pull request. We are also more than happy to include new examples :)</p>"},{"location":"examples/torch_cs/","title":"Pytorch Hyper-Optimization","text":"<p>A list of hyper-optimized PyTorch features, such as <code>torch.compile</code>, <code>torch.dynamo</code>, and other modules and decorators, is a great idea for quick reference. Below is a table that includes a description, use case, and an example for each feature:</p> Feature Description Use Case Python Example <code>torch.compile</code> Converts standard PyTorch code into a fused, optimized form. Use to optimize PyTorch models for faster inference and sometimes training, by fusing operations and eliminating Python overhead. <code>@torch.compile</code><code>def model(x):</code> <code>return x + x</code> <code>torch.dynamo</code> A dynamic Python-to-TorchScript compiler. Optimizes PyTorch code dynamically by compiling it into TorchScript, enhancing performance, especially in inference. <code>import torch.dynamo</code><code>@torch.dynamo.optimize</code><code>def model(x):</code> <code>return x.mm(x)</code> <code>torch.fx</code> A toolkit for capturing and transforming PyTorch programs. Useful for program capture, transformation, and symbolic tracing for custom modifications or optimizations. <code>import torch.fx</code><code>def forward(self, x):</code> <code>return self.conv(x)</code><code>graph_module = torch.fx.symbolic_trace(model)</code> <code>torch.jit</code> JIT compiler that translates a subset of Python and PyTorch code into TorchScript. Converts models to TorchScript for performance improvements and cross-platform compatibility. <code>import torch.jit</code><code>@torch.jit.script</code><code>def fn(x, y):</code> <code>return x + y</code> <code>torch.nn.utils.prune</code> Provides utilities for model pruning. Reduces model size and complexity for deployment or efficiency, by removing unnecessary weights. <code>import torch.nn.utils.prune as prune</code><code>prune.random_unstructured(module, name='weight', amount=0.3)</code> <code>torch.nn.utils.fusion</code> Fuses multiple operations into a single operation. Optimizes certain sequences of ops for performance, particularly in CNNs. <code>import torch.nn.utils.fusion</code><code>fused_module = torch.nn.utils.fusion.fuse_conv_bn_eval(conv, bn)</code> <code>torch.utils.checkpoint</code> Enables gradient checkpointing. Reduces memory usage in training large models by trading compute for memory. <code>from torch.utils.checkpoint import checkpoint</code><code>output = checkpoint(model, input)</code> <code>torch.utils.bottleneck</code> A tool to identify performance bottlenecks. Diagnoses the source of slowdowns in PyTorch models. <code>import torch.utils.bottleneck</code><code>torch.utils.bottleneck.run(model, input)</code> <code>torch.utils.data.DataLoader</code> Provides an iterable over a dataset. Essential for efficient loading, batching, and shuffling of data in training and inference. <code>from torch.utils.data import DataLoader</code><code>dataloader = DataLoader(dataset, batch_size=32, shuffle=True)</code> <p>Each of these features serves a specific purpose in optimizing and enhancing the performance and usability of PyTorch models. The examples provided are basic and intended to illustrate how these features might be implemented in a PyTorch workflow.</p>"},{"location":"zeta/","title":"Zeta","text":"<p>Build SOTA AI Models 80% faster with modular, high-performance, and scalable building blocks!</p> <p></p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"zeta/#install","title":"Install","text":"<p><code>pip install zetascale</code></p>"},{"location":"zeta/#usage","title":"Usage","text":""},{"location":"zeta/#starting-your-journey","title":"Starting Your Journey","text":"<p>Creating a model empowered with the aforementioned breakthrough research features is a breeze. Here's how to quickly materialize the renowned Flash Attention</p> <pre><code>import torch\n\nfrom zeta.nn import FlashAttention\n\nq = torch.randn(2, 4, 6, 8)\nk = torch.randn(2, 4, 10, 8)\nv = torch.randn(2, 4, 10, 8)\n\nattention = FlashAttention(causal=False, dropout=0.1, flash=True)\noutput = attention(q, k, v)\n\nprint(output.shape)\n</code></pre>"},{"location":"zeta/#swiglu","title":"<code>SwiGLU</code>","text":"<ul> <li>Powers Transformer models <pre><code>import torch\n\nfrom zeta.nn import SwiGLUStacked\n\nx = torch.randn(5, 10)\nswiglu = SwiGLUStacked(10, 20)\nswiglu(x).shape\n</code></pre></li> </ul>"},{"location":"zeta/#relativepositionbias","title":"<code>RelativePositionBias</code>","text":"<ul> <li><code>RelativePositionBias</code> quantizes the distance between two positions into a certain number of buckets and then uses an embedding to get the relative position bias. This mechanism aids in the attention mechanism by providing biases based on relative positions between the query and key, rather than relying solely on their absolute positions. <pre><code>import torch\n\nfrom zeta.nn import RelativePositionBias\n\n# Initialize the RelativePositionBias module\nrel_pos_bias = RelativePositionBias()\n\n# Example 1: Compute bias for a single batch\nbias_matrix = rel_pos_bias(1, 10, 10)\n\n\n# Example 2: Utilize in conjunction with an attention mechanism\n# NOTE: This is a mock example, and may not represent an actual attention mechanism's complete implementation.\nclass MockAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.rel_pos_bias = RelativePositionBias()\n\n    def forward(self, queries, keys):\n        bias = self.rel_pos_bias(queries.size(0), queries.size(1), keys.size(1))\n        # Further computations with bias in the attention mechanism...\n        return None  # Placeholder\n\n\n# Example 3: Modify default configurations\ncustom_rel_pos_bias = RelativePositionBias(\n    bidirectional=False, num_buckets=64, max_distance=256, n_heads=8\n)\n</code></pre></li> </ul>"},{"location":"zeta/#feedforward","title":"<code>FeedForward</code>","text":"<p>The FeedForward module performs a feedforward operation on the input tensor x. It consists of a multi-layer perceptron (MLP) with an optional activation function and LayerNorm. </p> <pre><code>from zeta.nn import FeedForward\n\nmodel = FeedForward(256, 512, glu=True, post_act_ln=True, dropout=0.2)\n\nx = torch.randn(1, 256)\n\noutput = model(x)\nprint(output.shape)\n</code></pre>"},{"location":"zeta/#bitlinear","title":"<code>BitLinear</code>","text":"<ul> <li>The BitLinear module performs linear transformation on the input data, followed by quantization and dequantization. The quantization process is performed using the absmax_quantize function, which quantizes the input tensor based on the absolute maximum value, from the paper <pre><code>import torch\nfrom torch import nn\n\nimport zeta.quant as qt\n\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = qt.BitLinear(10, 20)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\n# Initialize the model\nmodel = MyModel()\n\n# Create a random tensor of size (128, 10)\ninput = torch.randn(128, 10)\n\n# Perform the forward pass\noutput = model(input)\n\n# Print the size of the output\nprint(output.size())  # torch.Size([128, 20])\n</code></pre></li> </ul>"},{"location":"zeta/#palme","title":"<code>PalmE</code>","text":"<ul> <li>This is an implementation of the multi-modal Palm-E model using a decoder llm as the backbone with an VIT image encoder to process vision, it's very similiar to GPT4, Kosmos, RTX2, and many other multi-modality model architectures</li> </ul> <pre><code>import torch\n\nfrom zeta.structs import (\n    AutoregressiveWrapper,\n    Decoder,\n    Encoder,\n    Transformer,\n    ViTransformerWrapper,\n)\n\n\nclass PalmE(torch.nn.Module):\n    \"\"\"\n        PalmE is a transformer architecture that uses a ViT encoder and a transformer decoder.\n\n        Args:\n\n            image_size (int): Size of the image.\n            patch_size (int): Size of the patch.\n            encoder_dim (int): Dimension of the encoder.\n            encoder_depth (int): Depth of the encoder.\n            encoder_heads (int): Number of heads in the encoder.\n            num_tokens (int): Number of tokens.\n            max_seq_len (int): Maximum sequence length.\n            decoder_dim (int): Dimension of the decoder.\n            decoder_depth (int): Depth of the decoder.\n            decoder_heads (int): Number of heads in the decoder.\n            alibi_num_heads (int): Number of heads in the alibi attention.\n            attn_kv_heads (int): Number of heads in the attention key-value projection.\n            use_abs_pos_emb (bool): Whether to use absolute positional embeddings.\n            cross_attend (bool): Whether to cross attend in the decoder.\n            alibi_pos_bias (bool): Whether to use positional bias in the alibi attention.\n            rotary_xpos (bool): Whether to use rotary positional embeddings.\n            attn_flash (bool): Whether to use attention flash.\n            qk_norm (bool): Whether to normalize the query and key in the attention layer.\n\n        Returns:\n\n                torch.Tensor: The output of the model.\n\n        Usage:\n\n    img = torch.randn(1, 3, 256, 256)\n    text = torch.randint(0, 20000, (1, 1024))\n    model = PalmE()\n    output = model(img, text)\n    print(output)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size=256,\n        patch_size=32,\n        encoder_dim=512,\n        encoder_depth=6,\n        encoder_heads=8,\n        num_tokens=20000,\n        max_seq_len=1024,\n        decoder_dim=512,\n        decoder_depth=6,\n        decoder_heads=8,\n        alibi_num_heads=4,\n        attn_kv_heads=2,\n        use_abs_pos_emb=False,\n        cross_attend=True,\n        alibi_pos_bias=True,\n        rotary_xpos=True,\n        attn_flash=True,\n        qk_norm=True,\n    ):\n        super().__init__()\n\n        # vit architecture\n        self.encoder = ViTransformerWrapper(\n            image_size=image_size,\n            patch_size=patch_size,\n            attn_layers=Encoder(\n                dim=encoder_dim, depth=encoder_depth, heads=encoder_heads\n            ),\n        )\n\n        # palm model architecture\n        self.decoder = Transformer(\n            num_tokens=num_tokens,\n            max_seq_len=max_seq_len,\n            use_abs_pos_emb=use_abs_pos_emb,\n            attn_layers=Decoder(\n                dim=decoder_dim,\n                depth=decoder_depth,\n                heads=decoder_heads,\n                cross_attend=cross_attend,\n                alibi_pos_bias=alibi_pos_bias,\n                alibi_num_heads=alibi_num_heads,\n                rotary_xpos=rotary_xpos,\n                attn_kv_heads=attn_kv_heads,\n                attn_flash=attn_flash,\n                qk_norm=qk_norm,\n            ),\n        )\n\n        # autoregressive wrapper to enable generation of tokens\n        self.decoder = AutoregressiveWrapper(self.decoder)\n\n    def forward(self, img: torch.Tensor, text: torch.Tensor):\n        \"\"\"Forward pass of the model.\"\"\"\n        try:\n            encoded = self.encoder(img, return_embeddings=True)\n            return self.decoder(text, context=encoded)\n        except Exception as error:\n            print(f\"Failed in forward method: {error}\")\n            raise\n\n\n# Usage with random inputs\nimg = torch.randn(1, 3, 256, 256)\ntext = torch.randint(0, 20000, (1, 1024))\n\n# Initiliaze the model\nmodel = PalmE()\noutput = model(img, text)\nprint(output)\n</code></pre>"},{"location":"zeta/#unet","title":"<code>Unet</code>","text":"<p>Unet is a famous convolutional neural network architecture originally used for biomedical image segmentation but soon became the backbone of the generative AI Mega-revolution. The architecture comprises two primary pathways: downsampling and upsampling, followed by an output convolution. Due to its U-shape, the architecture is named U-Net. Its symmetric architecture ensures that the context (from downsampling) and the localization (from upsampling) are captured effectively.</p> <pre><code>import torch\n\nfrom zeta.nn import Unet\n\n# Initialize the U-Net model\nmodel = Unet(n_channels=1, n_classes=2)\n\n# Random input tensor with dimensions [batch_size, channels, height, width]\nx = torch.randn(1, 1, 572, 572)\n\n# Forward pass through the model\ny = model(x)\n\n# Output\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {y.shape}\")\n</code></pre>"},{"location":"zeta/#visionembeddings","title":"<code>VisionEmbeddings</code>","text":"<p>The VisionEmbedding class is designed for converting images into patch embeddings, making them suitable for processing by transformer-based models. This class plays a crucial role in various computer vision tasks and enables the integration of vision data into transformer architectures!</p> <pre><code>import torch\n\nfrom zeta.nn import VisionEmbedding\n\n# Create an instance of VisionEmbedding\nvision_embedding = VisionEmbedding(\n    img_size=224,\n    patch_size=16,\n    in_chans=3,\n    embed_dim=768,\n    contain_mask_token=True,\n    prepend_cls_token=True,\n)\n\n# Load an example image (3 channels, 224x224)\ninput_image = torch.rand(1, 3, 224, 224)\n\n# Perform image-to-patch embedding\noutput = vision_embedding(input_image)\n\n# The output now contains patch embeddings, ready for input to a transformer model\n</code></pre>"},{"location":"zeta/#niva","title":"<code>niva</code>","text":"<ul> <li>Niva focuses on weights of certain layers (specified by quantize_layers). Ideal for models where runtime activation is variable. \ud83d\udc41\ufe0f Example Layers: nn.Embedding, nn.LSTM. </li> </ul> <pre><code>import torch\n\nfrom zeta import niva\n\n# Load a pre-trained model\nmodel = YourModelClass()\n\n# Quantize the model dynamically, specifying layers to quantize\nniva(\n    model=model,\n    model_path=\"path_to_pretrained_model_weights.pt\",\n    output_path=\"quantized_model.pt\",\n    quant_type=\"dynamic\",\n    quantize_layers=[nn.Linear, nn.Conv2d],\n    dtype=torch.qint8,\n)\n</code></pre>"},{"location":"zeta/#fuseddensegeludense","title":"<code>FusedDenseGELUDense</code>","text":"<ul> <li>Increase model speed by 2x with this module that fuses together 2 hyper-optimized dense ops from bits and bytes and a gelu together!</li> </ul> <pre><code>import torch\n\nfrom zeta.nn import FusedDenseGELUDense\n\nx = torch.randn(1, 512)\nmodel = FusedDenseGELUDense(512, 1024)\nout = model(x)\nout.shape\n</code></pre>"},{"location":"zeta/#fuseddropoutlayernorm","title":"<code>FusedDropoutLayerNorm</code>","text":"<ul> <li>FusedDropoutLayerNorm is a fused kernel of dropout and layernorm to speed up FFNs or MLPS by 2X</li> </ul> <pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import FusedDropoutLayerNorm\n\n# Initialize the module\nmodel = FusedDropoutLayerNorm(dim=512)\n\n# Create a sample input tensor\nx = torch.randn(1, 512)\n\n# Forward pass\noutput = model(x)\n\n# Check output shape\nprint(output.shape)  # Expected: torch.Size([1, 512])\n</code></pre>"},{"location":"zeta/#zetacloud","title":"ZetaCloud","text":"<p>Train or finetune any model on any cluster in 1 click with zetacloud, just pass in your file and the GPU type and quantity you want! To gain access first <code>pip install zetascale</code> then run <code>zeta -h</code> in the terminal. Here is the docs for more</p> <ul> <li>Flexible Pricing with pooling from many clouds</li> <li>Easy Deployment with 1 click</li> <li>Various options for cloud providers!</li> </ul> <pre><code>Zetacloud CLI\n\noptions:\n  -h, --help            show this help message and exit\n  -t TASK_NAME, --task_name TASK_NAME\n                        Task name\n  -c CLUSTER_NAME, --cluster_name CLUSTER_NAME\n                        Cluster name\n  -cl CLOUD, --cloud CLOUD\n                        Cloud provider\n  -g GPUS, --gpus GPUS  GPUs\n  -f FILENAME, --filename FILENAME\n                        Filename\n  -s, --stop            Stop flag\n  -d, --down            Down flag\n  -sr, --status_report  Status report flag\n</code></pre> <ul> <li>A simple run example code would be like:</li> </ul> <pre><code>zeta -f train.py -g A100:8\n</code></pre>"},{"location":"zeta/#documentation","title":"Documentation","text":"<p>Click here for the documentation, it's at zeta.apac.ai</p>"},{"location":"zeta/#schedule-a-1-on-1-session","title":"\ud83e\udd1d Schedule a 1-on-1 Session","text":"<p>Book a 1-on-1 Session with Kye, the Creator, to discuss any issues, provide feedback, or explore how we can improve Zeta for you.</p>"},{"location":"zeta/#contributing","title":"Contributing","text":"<ul> <li> <p>We need you to help us build the most re-useable, reliable, and high performance ML framework ever.</p> </li> <li> <p>Check out the project board here!</p> </li> <li> <p>We need help writing tests and documentation!</p> </li> </ul>"},{"location":"zeta/#license","title":"License","text":"<ul> <li>Apache</li> </ul>"},{"location":"zeta/cloud/main/","title":"ZetaCloud Documentation","text":""},{"location":"zeta/cloud/main/#overview","title":"Overview","text":"<p>ZetaCloud is a versatile command-line tool that simplifies the process of training or fine-tuning machine learning models on remote GPU clusters. With just a few commands, you can effortlessly manage your tasks and harness the computational power of various GPUs. This comprehensive documentation will guide you through every aspect of the ZetaCloud CLI, from installation to advanced usage.</p>"},{"location":"zeta/cloud/main/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>ZetaCloud CLI</li> <li>Options</li> <li>Basic Usage</li> <li>Example 1: Starting a Task</li> <li>Example 2: Stopping a Task</li> <li>Example 3: Checking Task Status</li> <li>Advanced Usage</li> <li>Example 4: Cluster Selection</li> <li>Example 5: Choosing the Cloud Provider</li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/cloud/main/#1-installation","title":"1. Installation","text":"<p>Getting started with ZetaCloud is quick and straightforward. Follow these steps to set up ZetaCloud on your machine:</p> <ol> <li> <p>Open your terminal or command prompt.</p> </li> <li> <p>Install the <code>zetascale</code> package using <code>pip</code>:</p> </li> </ol> <pre><code>pip install zetascale\n</code></pre> <ol> <li>After a successful installation, you can access the ZetaCloud CLI by running the following command:</li> </ol> <pre><code>zeta -h\n</code></pre> <p>This command will display a list of available options and basic usage information for ZetaCloud.</p>"},{"location":"zeta/cloud/main/#2-zetacloud-cli","title":"2. ZetaCloud CLI","text":"<p>The ZetaCloud Command-Line Interface (CLI) provides a set of powerful options that enable you to manage tasks on GPU clusters effortlessly. Below are the available options:</p>"},{"location":"zeta/cloud/main/#options","title":"Options","text":"<ul> <li><code>-h, --help</code>: Display the help message and exit.</li> <li><code>-t TASK_NAME, --task_name TASK_NAME</code>: Specify the name of your task.</li> <li><code>-c CLUSTER_NAME, --cluster_name CLUSTER_NAME</code>: Specify the name of the cluster you want to use.</li> <li><code>-cl CLOUD, --cloud CLOUD</code>: Choose the cloud provider (e.g., AWS, Google Cloud, Azure).</li> <li><code>-g GPUS, --gpus GPUS</code>: Specify the number and type of GPUs required for your task.</li> <li><code>-f FILENAME, --filename FILENAME</code>: Provide the filename of your Python script or code.</li> <li><code>-s, --stop</code>: Use this flag to stop a running task.</li> <li><code>-d, --down</code>: Use this flag to terminate a cluster.</li> <li><code>-sr, --status_report</code>: Check the status of your task.</li> </ul>"},{"location":"zeta/cloud/main/#3-basic-usage","title":"3. Basic Usage","text":"<p>ZetaCloud's basic usage covers essential tasks such as starting, stopping, and checking the status of your tasks. Let's explore these tasks with examples.</p>"},{"location":"zeta/cloud/main/#example-1-starting-a-task","title":"Example 1: Starting a Task","text":"<p>To start a task, you need to specify the Python script you want to run and the GPU configuration. Here's an example command:</p> <pre><code>zeta -f train.py -g A100:8\n</code></pre> <p>In this example: - <code>-f train.py</code> indicates that you want to run the Python script named <code>train.py</code>. - <code>-g A100:8</code> specifies that you require 8 NVIDIA A100 GPUs for your task.</p>"},{"location":"zeta/cloud/main/#example-2-stopping-a-task","title":"Example 2: Stopping a Task","text":"<p>If you need to stop a running task, you can use the following command:</p> <pre><code>zeta -s\n</code></pre> <p>This command will stop the currently running task.</p>"},{"location":"zeta/cloud/main/#example-3-checking-task-status","title":"Example 3: Checking Task Status","text":"<p>To check the status of your task, use the following command:</p> <pre><code>zeta -sr\n</code></pre> <p>This command will provide you with a detailed status report for your active task.</p>"},{"location":"zeta/cloud/main/#4-advanced-usage","title":"4. Advanced Usage","text":"<p>ZetaCloud also offers advanced options that allow you to fine-tune your tasks according to your specific requirements.</p>"},{"location":"zeta/cloud/main/#example-4-cluster-selection","title":"Example 4: Cluster Selection","text":"<p>You can select a specific cluster for your task by providing the cluster name with the <code>-c</code> option:</p> <pre><code>zeta -f train.py -g A100:8 -c my_cluster\n</code></pre> <p>This command will run your task on the cluster named <code>my_cluster</code>.</p>"},{"location":"zeta/cloud/main/#example-5-choosing-the-cloud-provider","title":"Example 5: Choosing the Cloud Provider","text":"<p>ZetaCloud supports multiple cloud providers. You can specify your preferred cloud provider using the <code>-cl</code> option:</p> <pre><code>zeta -f train.py -g A100:8 -cl AWS\n</code></pre> <p>This command will execute your task on a cloud provider's infrastructure, such as AWS.</p>"},{"location":"zeta/cloud/main/#5-additional-information","title":"5. Additional Information","text":"<ul> <li> <p>ZetaCloud simplifies the process of utilizing GPU clusters, allowing you to focus on your machine learning tasks rather than infrastructure management.</p> </li> <li> <p>You can easily adapt ZetaCloud to various cloud providers, making it a versatile tool for your machine learning needs.</p> </li> </ul>"},{"location":"zeta/models/andromeda/","title":"Class Name: Andromeda","text":"<p>Module Description</p> <p>This documentation provides details on the functionality of the Andromeda class from the zeta.models library. </p> <p>The Andromeda class is a transformer-based model helper class that acts as a wrapper for the Transformer and AutoregressiveWrapper modules, defaulting or accepting user-specified values in its configuration. </p> <p>Features of the Andromeda model include but are not limited to:  - Configurable model dimensions, including token count, maximum sequence length, layer depth, and head dimensions. - Abstract position embeddings, alibi position biases, rotary positions, attentions, and buffer elements which are all modifiable by the user.</p>"},{"location":"zeta/models/andromeda/#class-definition","title":"Class Definition:","text":"<p><pre><code>class Andromeda(Module):\n    \"\"\"\n    Andromeda is a transformer-based model architecture. It initializes with\n    a Transformer and AutoregressiveWrapper with default or user-specified parameters.\n    \"\"\"\n</code></pre> This class inherits the PyTorch Module class and serves as a wrapper to both the Transformer and AutoregressiveWrapper classes. </p>"},{"location":"zeta/models/andromeda/#initialization-init-function","title":"Initialization (init) Function:","text":"<p>The init function is where the Transformer and AutoregressiveWrapper objects are assigned to <code>self.Andromeda</code> and <code>self.decoder</code> respectively. </p> <pre><code>def __init__(\n       self,\n       num_tokens=50432,\n       max_seq_len=8192,\n       dim=2560,\n       depth=32,\n       dim_head=128,\n       heads=24,\n       use_abs_pos_emb=False,\n       alibi_pos_bias=True,\n       alibi_num_heads=12,\n       rotary_xpos=True,\n       attn_flash=True,\n       attn_kv_heads=2,\n       qk_norm=True,\n       attn_qk_norm=True,\n       attn_qk_norm_dim_scale=True,\n   ):\n</code></pre> <p>The parameters and their defaults used in initialization are listed below</p> Parameter Default Value Description num_tokens 50432 Number of tokens in the vocabulary max_seq_len 8192 Maximum sequence length dim 2560 Dimension of the model depth 32 Depth of the model dim_head 128 Dimension of the model head heads 24 Number of heads use_abs_pos_emb False Whether to use absolute position embedding alibi_pos_bias True Alibi position bias alibi_num_heads 12 Number of alibi heads rotary_xpos True Rotary position attn_flash True Attention flash attn_kv_heads 2 Number of attention key/value heads qk_norm True Query-key normalization attn_qk_norm True Attention query-key normalization attn_qk_norm_dim_scale True Attention query-key normalization dimension scale"},{"location":"zeta/models/andromeda/#forward-function","title":"Forward Function","text":"<p>Forward propagation in PyTorch involves defining the computation performed at every call. In the Andromeda class, this computation involves passing input text tokens through the decoder. If an exception occurs during this forward propagation, an error message will be printed and an exception will be thrown.</p> <pre><code> def forward(self, text_tokens, **kwargs):\n        \"\"\"\n        Forward pass through the model. It expects the input text_tokens.\n        \"\"\"\n ```\nThe parameters used in forward function are listed below:\n\n| Parameter | Description |\n| ------------- | ------------- |\n| text_tokens | Input tokens |\n| **kwargs | Other arguments |\n\nThe forward function returns the output from the decoder.\n\n## Code Example:\nBelow is a simple example of instantiating the Andromeda class and using it for forward propagation:\n\n```python\n# Import necessary libraries and modules\nfrom torch.nn import Module\nfrom zeta.models import Andromeda\n\n# Initialize the Andromeda class with default parameters\nmodel = Andromeda()\n\n# Define your input text tokens\ntext_tokens = torch.randn(1, 8192)\n\n# Perform forward pass through the model\noutput = model.forward(text_tokens)\n</code></pre> <p>Note  Techniques such as query-key normalization aid in the alignment of the query\u2019s distribution to that of the key, in order to reduce the negative impacts of any input with a wildly different distribution. As such, the parameters related to normalization (qk_norm, attn_qk_norm, attn_qk_norm_dim_scale) default to True, but can be toggled off based on the specific needs of your application.</p> <p>Also, It's important to ensure that the defined text tokens fit within the dimensions defined for <code>num_tokens</code> and <code>max_seq_len</code>. Otherwise, you might encounter an error during forward pass. </p> <p>For more information on the underlying Transformer and AutoregressiveWrapper modules, please check the official PyTorch documentation. </p>"},{"location":"zeta/models/andromeda/#other-additional-information-tips","title":"Other Additional Information &amp; Tips","text":"<p>The Andromeda class is notable for its robust set of flexible features that can lend it to varying use-cases and it is inherently versatile due to its Transformer and AutoregressiveWrapper architecture. This model emphasizes on the detail to accepting user-specified parameters for a high level of customization. </p> <p>However, due to its complexity and high-dimensional nature, this model may not be preferable under constraints of memory, processing power or the need for simplicity. </p>"},{"location":"zeta/models/andromeda/#references-external-resources","title":"References &amp; External Resources","text":"<ul> <li>Official PyTorch Docs for more information on underlying classes and modules.</li> <li>Understanding Transformers in NLP for conceptual knowledge on Transformer models.</li> <li>Autoregressive Models for understanding on autoregressive models. </li> </ul> <p>Enjoy exploring the Andromeda class from the zeta.models library!</p>"},{"location":"zeta/models/basemodel/","title":"Module/Class Name: BaseModel","text":"<pre><code>from abc import ABC\n\n\nclass BaseModel(ABC):\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def forward(self):\n        pass\n</code></pre> <p>The <code>BaseModel</code> serves as a base class for other models, benefiting from the Python feature of inheritance and polymorphism. Designed with the Abstract Base Class (<code>ABC</code>), it enforces the subclasses to redefine <code>forward</code> method and to provide certain arguments during initialization, thus providing a common API for all subclasses.</p>"},{"location":"zeta/models/basemodel/#class-definition","title":"Class Definition","text":"<p>The <code>BaseModel</code> class provides the skeleton for the further implementation of any specific model. It does not include any specific model related features but instead enables modularity, creating a structure that is reusable for every type of model desired.</p> <pre><code>class BaseModel(ABC):\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def forward(self):\n        pass\n</code></pre>"},{"location":"zeta/models/basemodel/#parameters","title":"Parameters","text":"<ul> <li> <p>args: This captures any number of unnamed arguments. You can pass a series of variables or a list of variables, which will be interpreted as a tuple by the method.</p> </li> <li> <p>kwargs: This is used to pass keyworded, variable-length arguments. With kwargs, any number of keyword arguments can be used. You can use kwargs if you do not know the number of keyword arguments that will be passed to the function, or if it is optional to have any keyword arguments at all.</p> </li> </ul>"},{"location":"zeta/models/basemodel/#method-overview","title":"Method Overview","text":""},{"location":"zeta/models/basemodel/#__init__self-args-kwargs","title":"<code>__init__(self, *args, **kwargs):</code>","text":"<p>A special method in Python classes, it is called as a constructor in object-oriented terminology. This method is called when an object is instantiated, and necessary initialization can happen here. With args and *kwargs as parameters, it provides flexibility by handling arbitrary number and type of arguments.</p>"},{"location":"zeta/models/basemodel/#forwardself","title":"<code>forward(self):</code>","text":"<p>This is an abstract method that needs to be implemented by any class that extends <code>BaseModel</code>. The purpose of the method can change depending on the model, but it is usually used for forward propagation in neural networks.</p>"},{"location":"zeta/models/basemodel/#usage","title":"Usage","text":"<p>As <code>BaseModel</code> is abstract, we cannot directly use it. Instead, we can extend it and implement the required methods in the child class. A typical example of subclassing would be:</p> <pre><code>class MyModel(BaseModel):\n    def __init__(self, number_of_layers):\n        self.number_of_layers = number_of_layers\n        super().__init__()\n\n    def forward(self):\n        # Implement your forward pass here\n        ...\n</code></pre> <p>In this example, the <code>MyModel</code> class extends <code>BaseModel</code> and overrides the <code>__init__</code> and <code>forward</code> methods. This way, all the models you implement only need to inherit from the <code>BaseModel</code> and implement their specific details.</p> <pre><code>my_model = MyModel(10)\nmy_model.forward()\n</code></pre> <p>In this example, we instantiated an object of the <code>MyModel</code> class, passing in the number of layers (10), and then calling <code>forward</code> method on it.</p>"},{"location":"zeta/models/basemodel/#additional-information","title":"Additional Information","text":"<ul> <li> <p>Consider following Python's DRY (Don't Repeat Yourself) principle when using inheritance. Instead of writing the same code over and over again for different models, you can put the common elements of all models into a base model.</p> </li> <li> <p>As you may have noticed, <code>BaseModel</code> adopts an Object-Oriented Programming (OOP) approach to structure the code, making it easier to manage and understand.</p> </li> <li> <p>For a complete guide in Python's ABCs, consider checking the official Python's ABC documentation.</p> </li> </ul>"},{"location":"zeta/models/gpt4/","title":"GPT4 Class","text":"<p>GPT4 is a class providing the architecture of a transformer-based model. The class primarily consists of two main components, a Transformer and an AutoregressiveWrapper. </p> <p>Based on the method used by OpenAI's GPT-3, the GPT4 in this implementation expands on that base with user-specified or default parameters. These parameters allow users to customize the architecture, depth, and functionality of their models for specific use-cases.</p>"},{"location":"zeta/models/gpt4/#initialize-the-class","title":"Initialize the class","text":"<p>The class is initialized by the following arguments:</p> Argument Type Default Description num_tokens int 50432 Number of tokens in the vocabulary max_seq_len int 8192 Maximum length of the sequence dim int 2560 Dimension of the model depth int 32 Depth of the model dim_head int 128 Dimension of the model head heads int 24 Number of heads use_abs_pos_emb bool False Whether to use absolute position embedding alibi_pos_bias bool True Alibi position bias alibi_num_heads int 12 Number of alibi heads rotary_xpos bool True Rotary position attn_flash bool True Attention flash attn_one_kv_head bool True Attention one key/value head for multiquery attention qk_norm bool True Query-key normalization attn_qk_norm bool True Attention query-key normalization attn_qk_norm_dim_scale bool True Attention query-key normalization dimension scale <p>Each of these arguments can be modified to suit specific needs of the user. </p>"},{"location":"zeta/models/gpt4/#implementing-the-transformer-class","title":"Implementing the transformer class","text":"<p>The Transformer architecture used in the GPT4 model forms the backbone of the class. It utilizes an attention mechanism to focus on different words in a sequence while processing the input data.</p> <p>In this case, the Transformer is a Decoder, which transpires the depth, dim_head, heads, alibi_pos_bias, alibi_num_heads, rotary_xpos, attn_flash, attn_one_kv_head, qk_norm, attn_qk_norm, and attn_qk_norm_dim_scale properties from the GPT4 arguments.</p> <p>If initialization fails for any reason, an exception is caught and logged in the console, and the exception is re-raised.</p>"},{"location":"zeta/models/gpt4/#autoregressivewrapper","title":"AutoregressiveWrapper","text":"<p>As a next step, the transformer is wrapped with an AutoregressiveWrapper. Autoregressive models are ones where the output from one step is fed as an input to the next step. This allows for modeling the sequence of data effectively, thus making it excellent for tasks like text generation and language modelling.</p>"},{"location":"zeta/models/gpt4/#forward-function","title":"Forward function","text":"<p>The <code>forward</code> function of the GPT4 class starts by taking <code>text_tokens</code> as input. This variable represents the tokenized input sentences.</p> <p>In the forward function, a Transformer (loaded by the decoder) is applied to forward <code>text_tokens</code>. The result is a <code>model_input</code> variable, which is then passed into the decoder along with the <code>padded_x</code> parameter.</p> <p>If exceptions occur during the forward pass, they are caught and logged in the console, and the exception is re-raised.</p>"},{"location":"zeta/models/gpt4/#usage","title":"Usage","text":"<p>Here's how you can use the GPT4 class:</p> <pre><code>import torch\nfrom torch import nn\n\nfrom zeta.models import GPT4\n\n# Initialize with default parameters\nmodel = GPT4()\n\n# Representing 3 sequences of the maximum length of 8192\ninput = torch.randint(0, 50432, (3, 8192))\n\n# Pass the input to the model's forward method\noutput = model.forward(input)\n</code></pre>"},{"location":"zeta/models/gpt4/#conclusion","title":"Conclusion","text":"<p>The GPT4 class is a powerful tool for creating Transformer-based language models. With the flexibility it provides, users can customize the model per their requirements and specifications. Whether it be altering the dimensionality, the number of heads in multihead attention, or whether to use absolute position embeddings, the GPT4 class provides a versatile and flexible architecture for your next natural language processing project.</p>"},{"location":"zeta/models/gpt4multimodal/","title":"GPT4MultiModal","text":"<p>The <code>GPT4MultiModal</code> class is a subclass of the <code>torch.nn.Module</code> class. This class serves as a model for handling both image and text input in the form of sequences. It integrates the ViTransformerWrapper for image encoding and the Transformer for text decoding.</p> <p>The primary aim of this class is to enable encoding an image and use it as context for generating a text sequence, hence the name <code>GPT4MultiModal</code>. Typical usage would be to pass an image to the encoder and a sequence of tokens (corresponding to a language prompt) to the decoder. The class will output a sequence of tokens- the length of the sequence will depend on the transformer architecture used.</p>"},{"location":"zeta/models/gpt4multimodal/#class-constructor","title":"Class Constructor","text":"<p>This class accepts the following parameters:</p> Parameters Keyboard Argument Type Default Value Description image_size image_size int 256 Input image size patch_size patch_size int 32 Size of each image patch encoder_dim encoder_dim int 512 Dimension of encoder encoder_depth encoder_depth int 6 The depth of the encoder encoder_heads encoder_heads int 8 The number of attention heads in the encoder num_tokens num_tokens int 20000 The number of unique tokens max_seq_len max_seq_len int 1024 Maximum sequence length for text decoder_dim decoder_dim int 512 Dimension of decoder decoder_depth decoder_depth int 6 The depth of the decoder decoder_heads decoder_heads int 8 The number of attention heads in the decoder alibi_num_heads alibi_num_heads int 4 The number of attention heads per transformer use_abs_pos_emb use_abs_pos_emb bool False If True, embeds input using absolute positional embedding cross_attend cross_attend bool True If True, enables cross attention in decoder alibi_pos_bias alibi_pos_bias bool True If True, positional bias is added to alibi rotary_xpos rotary_xpos bool True Enables rotary positional embeddings attn_flash attn_flash bool True If True, enables the use of Flash-like attention qk_norm qk_norm bool True If True, enables query-key normalization"},{"location":"zeta/models/gpt4multimodal/#methods","title":"Methods","text":"<p>The following methods are available in this class.</p>"},{"location":"zeta/models/gpt4multimodal/#forwardself-img-text-uniontensor-str","title":"<code>forward(self, img, text) -&gt; Union[Tensor, str]</code>","text":"<p>The <code>forward</code> method is used to perform the forward propagation operation of the GPT4MultiModal model. It accepts an image and a sequence of tokens and returns a sequence of tokens.</p> <p>Parameters:</p> Parameters Keyboard Argument Type Default Value Description img img Tensor - The input image tensor text text Tensor - The sequence of tokens to be used as input <p>Returns:</p> Type Description Union[Tensor, str] Output sequence of tokens or an error message if an exception is encountered"},{"location":"zeta/models/gpt4multimodal/#example-of-use","title":"Example of Use","text":"<p>Consider having an image tensor <code>img</code> of size (1, 256, 256, 3) and a text tensor <code>text</code> of size (1, 50). Here is an example of how to use <code>GPT4MultiModal</code></p> <pre><code>import torch\n\nfrom zeta.models import GPT4MultiModal\n\n# Initialize the model\nmodel = GPT4MultiModal(\n    image_size=256,\n    patch_size=32,\n    encoder_dim=512,\n    encoder_depth=6,\n    encoder_heads=8,\n    num_tokens=20000,\n    max_seq_len=1024,\n    decoder_dim=512,\n    decoder_depth=6,\n    decoder_heads=8,\n    alibi_num_heads=4,\n    use_abs_pos_emb=False,\n    cross_attend=True,\n    alibi_pos_bias=True,\n    rotary_xpos=True,\n    attn_flash=True,\n    qk_norm=True,\n)\n\n# Assume we have an image tensor 'img' of size (1, 256, 256, 3) and\n# a text tensor 'text' of size (1, 50)\n\n# Run the model\noutput = model(img, text)\n</code></pre> <p>This will encode <code>img</code> using the <code>ViTransformerWrapper</code> and then use the encoded embeddings as the context for the <code>Transformer</code> to generate a sequence of tokens from <code>text</code>. The sequence of tokens, <code>output</code>, is the result.</p>"},{"location":"zeta/models/llama2/","title":"LLama2","text":""},{"location":"zeta/models/llama2/#class-overview","title":"Class Overview","text":"<p>The class LLama2 is a custom transformer model built for Natural Language Processing (NLP) tasks. The objective of this class is to provide a compact yet powerful transformer model for the application of various NLP tasks, from translation to text generation and more.</p> <p>The LLama2 transformer in this class provides a broad range of customizable parameters, allowing for it to be fine-tuned for specific tasks and datasets. It supports arguments for the sequence length, model dimensions, layer depths, number of heads, and several other options, providing extensive adaptability for various NLP tasks.</p>"},{"location":"zeta/models/llama2/#class-structure","title":"Class Structure","text":"<pre><code>class LLama2:\n    def __init__(\n        self,\n        num_tokens=50432,\n        max_seq_len=8192,\n        dim=2560,\n        depth=32,\n        dim_head=128,\n        heads=24,\n        rotary_xpos=True,\n        attn_flash=True,\n    ):\n        super().__init__()\n\n        self.llama2 = Transformer(\n            num_tokens=50000,\n            max_seq_len=4096,\n            attn_layers=Decoder(\n                dim=dim,\n                depth=depth,\n                dim_head=dim_head,\n                heads=heads,\n                attn_flash=attn_flash,\n                rotary_xpos=rotary_xpos,\n            ),\n        )\n        self.decoder = AutoregressiveWrapper(self.decoder)\n\n    def forward(self, text):\n        model_input = self.decoder.forward(text)[0]\n        return self.decoder(model_input, padded_x=model_input[0])\n</code></pre> <p>Function Name: <code>__init__</code></p> <p>Purpose: Initializes the LLama2 class.</p> Parameter Data Type Default Value Description num_tokens int 50432 The total number of tokens in the input vocabulary. max_seq_len int 8192 The maximum sequence length that the model can accept. dim int 2560 The model's embedding dimensionality. depth int 32 The number of transformer layers in the model. dim_head int 128 The dimensionality of the head in the self-attention mechanism of the transformer model. heads int 24 The number of heads for the multi-head self attention mechanism of the transformer model. rotary_xpos bool True Whether to apply rotary positional embeddings to the input sequence. attn_flash bool True Whether to use the flash attention mechanism. <p>Function Name: <code>forward</code></p> <p>Purpose: Defines the forward pass of the model.</p> Parameter Data Type Default Value Description text string The input text which the model processes. <p>Returns: A tensor representation of model's output given the model_input.</p>"},{"location":"zeta/models/llama2/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/models/llama2/#example-1-text-processing","title":"Example 1: Text Processing","text":"<p>This example illustrates how to instantiate the model and pass a sample text through it.</p> <pre><code>import torch\nfrom torch.nn import Decoder, Transformer\n\nfrom zeta.models import LLama2\nfrom zeta.structs import AutoregressiveWrapper\n\n# Initializing model\nllama2_model = LLama2()\n\n# Cut-off long text or pad short text\ntext = torch.tensor([1, 2, 3, 4])\n\n# Passing text through model\noutput = llama2_model.forward(text)\n\nprint(output)\n</code></pre>"},{"location":"zeta/models/llama2/#example-2-customizing-model-parameters","title":"Example 2: Customizing Model Parameters","text":"<p>This example illustrates how to instantiate the model with custom parameters.</p> <pre><code>llama2_model = LLama2(\n    num_tokens=1000, max_seq_len=512, dim=512, depth=4, dim_head=64, heads=4\n)\n\ntext = torch.tensor([1, 2, 3, 4])\n\noutput = llama2_model.forward(text)\n\nprint(output)\n</code></pre>"},{"location":"zeta/models/llama2/#example-3-sequence-classification","title":"Example 3: Sequence Classification","text":"<p>This example illustrates how you could use this model for a sequence classification task.</p> <p><pre><code>llama2_model = LLama2(\n    num_tokens=5000, max_seq_len=256, dim=128, depth=2, dim_head=32, heads=2\n)\n\ntext_sequences = torch.tensor([[1, 2, 3, 4], [2, 3, 1, 4]])\ntarget_sequences = torch.tensor([1, 0])  # 2 sequences, 1 for each sequence\n\noutputs = llama2_model.forward(text_sequences)\nloss = loss_function(outputs, target_sequences)\n</code></pre> In this usage example, an instance of the LLama2 class is created using custom parameters. A tensor representing text sequences is passed to the model, and the output is computed. You would typically use a loss function suitable for classification tasks (like Cross-Entropy Loss) and compute the loss against some target sequences. </p> <p>Note: The provided code is a basic example and might require adjustments like adding an appropriate classifier layer at the end, depending on the specific task requirements.</p>"},{"location":"zeta/models/maxvit/","title":"MaxVit Class Documentation","text":"<p>The <code>MaxVit</code> class in the <code>zeta.models</code> module is a neural network module for constructing Vision Transformers (ViT) with MixUp functionality. This class extends PyTorch's native <code>nn.Module</code> class while adding various features suited for implementing ViTs. The following sections will provide additional details:</p>"},{"location":"zeta/models/maxvit/#class-definition","title":"Class Definition","text":"<pre><code>class MaxVit(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_classes,\n        dim,\n        depth,\n        dim_head: int = 32,\n        dim_conv_stem=None,\n        window_size: int = 7,\n        mbconv_expansion_rate: int = 4,\n        mbconv_shrinkage_rate=0.25,\n        dropout=0.01,\n        channels=3,\n    ):\n</code></pre>"},{"location":"zeta/models/maxvit/#parameters","title":"Parameters","text":"Parameters Type Description <code>num_classes</code> <code>int</code> The number of classes in the classification task. <code>dim</code> <code>int</code> The dimension of the input data. <code>depth</code> <code>list</code> Tuple indicating the number of transformer blocks at a given stage. <code>dim_head</code> <code>int</code> (Default = 32) The dimensionally of the transformer's heads. <code>dim_conv_stem</code> <code>int</code> (Default = None) The dimensionality of the convolutional stem. If not provided, the dimension of the input is used. <code>window_size</code> <code>int</code> (Default = 7) The size of the sliding windows used for efficient grid-like attention. <code>mbconv_expansion_rate</code> <code>int</code> (Default = 4) Expansion rate used in Mobile Inverted Residual Bottleneck (MBConv) used in the <code>block</code>. <code>mbconv_shrinkage_rate</code> <code>float</code> (Default = 0.25) Shrinkage rate used in Mobile Inverted Residual Bottleneck (MBConv) used in the <code>block</code>. <code>dropout</code> <code>float</code> (Default = 0.01) The dropout rate for regularization. <code>channels</code> <code>int</code>   (Default = 3) Number of input channels."},{"location":"zeta/models/maxvit/#functions-methods","title":"Functions / Methods","text":""},{"location":"zeta/models/maxvit/#forwardx-textsnone-cond_fnsnone-cond_drop_prob00-return_embeddingsfalse","title":"<code>forward(x, texts=None, cond_fns=None, cond_drop_prob=0.0, return_embeddings=False)</code>","text":"<p>This function carries out the forward propagation through the <code>MaxVit</code> model given an input <code>x</code>. </p>"},{"location":"zeta/models/maxvit/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>x</code> <code>torch.Tensor</code> The input tensor to the <code>MaxVit</code> model. <code>texts</code> <code>List[str]</code> (Optional) list of textual data for interpreting image data <code>cond_fns</code> <code>Tuple[Callable, ...]</code> (Optional) List of conditional functions to apply per layer <code>cond_drop_prob</code> <code>float</code> (Default = 0.0) Conditional dropout probability. <code>return_embeddings</code> <code>bool</code> (Default = False) Whether to return embeddings instead of class scores."},{"location":"zeta/models/maxvit/#returns","title":"Returns","text":"<p>Returns the output of the multi-layer transformer, which could either be the class scores (default) or embeddings based on <code>return_embeddings</code> value.</p>"},{"location":"zeta/models/maxvit/#example-usage","title":"Example Usage","text":"<pre><code>from zeta.models import MaxVit\n\nmodel = MaxVit(num_classes=10, dim=512, depth=(3, 2), dim_head=64, channels=3)\n\nx = torch.randn(\n    1, 3, 224, 224\n)  # suppose we have an random tensor representing an image\n\nout = model(x)  # forward pass\n\nprint(out.shape)  # torch.Size([1, 10])\n</code></pre>"},{"location":"zeta/models/maxvit/#overview","title":"Overview","text":"<p>The <code>MaxVit</code> model is essentially a combination of vision transformers and efficient blocks (based on MobileNet family). First, the input passes through a convolutional stem. Afterward, the data flow through several stages. Each stage consists of a sequence of blocks, and each block is a combination of a Mobile Inverted Residual Bottleneck (MBConv) followed by the Transformer layers. Finally, the output to predict the classifications is obtained through the MLP head. </p> <p>In addition to the traditional <code>forward</code> functionality, <code>MaxVit</code> also supports conditional functions that can be used to modify the network behavior per layer, adding a layer of flexibility to the model. Furthermore, the model supports the option to return the transformer embeddings, making it applicable for other tasks beyond simple classification.</p>"},{"location":"zeta/models/maxvit/#note","title":"Note:","text":"<p>The forward method of <code>MaxVit</code> is beartyped for type checking which enforces strong typing, improving the efficiency of the class.</p>"},{"location":"zeta/models/megavit/","title":"Module Name: MegaVit","text":"<p>The MegaVit is a class in Python that implements the model from the paper When Vision Transformers Outperform CNNs. </p>"},{"location":"zeta/models/megavit/#introduction","title":"Introduction","text":"<p>The class implements a vision transformer model that can provide state-of-the-art performance in computer vision tasks when compared to traditional convolutional neural networks (CNNs). The vision transformer model treats an image as a sequence of one-dimensional patches and applies the transformer model on these patches. It is initialized with image size, patch size, number of classes, embedding dimension, depth of transformer model, number of heads for the multi-head attention mechanism, dimension of multi-layer perceptron (MLP), type of pooling method, and dropout rates.</p>"},{"location":"zeta/models/megavit/#class-definition","title":"Class Definition","text":"<pre><code>class MegaVit(nn.Module):\n</code></pre> <p>This class inherits from <code>nn.Module</code>, which is the base class for all neural network modules in Pytorch.</p> <pre><code>def __init__(\n    self,\n    *,\n    image_size,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    pool=\"cls\",\n    channels=3,\n    dim_head=64,\n    dropout=0.0,\n    emb_dropout=0.0,\n):\n</code></pre> <p>The initialization function for the <code>MegaVit</code> class. This function initializes various parameters and layers of the model.</p> <ul> <li><code>image_size</code>: Size of the input image. It should be an integer. This is an input argument to the <code>MegaVit</code> initializer.</li> <li><code>patch_size</code>: Size of the patches into which the input image is divided. It should be an integer.</li> <li><code>num_classes</code>: Number of output classes. It should be an integer.</li> <li><code>dim</code>: It is the dimension of the embeddings.</li> <li><code>depth</code>: This integer represents the depth of the transformer.</li> <li><code>heads</code>: This integer indicates the number of heads in the multi-head attention mechanism of the transformer.</li> <li><code>mlp_dim</code>: This integer represents the number of dimensions in the MLP layer.</li> <li><code>pool</code>: This is a string representing the type of pooling used. It can either be 'cls' or 'mean'.</li> <li><code>channels</code>: This integer represents the number of channels in the input image.</li> <li><code>dim_head</code>: This integer is the dimension of the transformers head.</li> <li><code>dropout</code>: This floating-point number represents the dropout rate.</li> <li><code>emb_dropout</code>: This floating-point number is the dropout rate for the embeddings.</li> </ul> <pre><code>def forward(self, img):\n</code></pre> <p>The forward function defines the forward pass of the network. It receives an input image and generates an output prediction.</p> <ul> <li><code>img</code>: A Pytorch tensor representing the input image.</li> </ul>"},{"location":"zeta/models/megavit/#usage-example","title":"Usage Example","text":"<p>Here is a basic usage example of the <code>MegaVit</code> class:</p> <pre><code>import torch\nfrom numpy import random\nfrom torch.nn import Module\n\nfrom zeta.models import MegaVit\n\n# Define model hyperparameters\nmodel_hparams = {\n    \"image_size\": 256,\n    \"patch_size\": 32,\n    \"num_classes\": 1000,\n    \"dim\": 512,\n    \"depth\": 6,\n    \"heads\": 8,\n    \"mlp_dim\": 1024,\n    \"dropout\": 0.1,\n    \"emb_dropout\": 0.1,\n}\n\n# Initialize MegaVit model\nmodel = MegaVit(**model_hparams)\n\n# Get random image\nimg = torch.from_numpy(\n    random.rand(1, 3, model_hparams[\"image_size\"], model_hparams[\"image_size\"])\n).float()\n\n# Get model prediction\npreds = model(img)\n\nprint(preds)\n</code></pre> <p>This will output the model's prediction for the input image.</p>"},{"location":"zeta/models/megavit/#reference","title":"Reference","text":"<ul> <li>When Vision Transformers Outperform CNNs</li> </ul> <p>This class directly corresponds to the model presented in the above-mentioned paper. Reading this paper may provide additional insights into working and theory of this class. </p>"},{"location":"zeta/models/megavit/#additional-information","title":"Additional Information","text":"<p>Below is a brief explanation of how the <code>MegaVit</code> model works:</p> <ol> <li>The input image is passed through the <code>to_patch_embedding</code> layer, which first rearranges the image into patches, then applies layer normalization and linear transformation on each patch separately.</li> <li>The positional embeddings are added to these patch embeddings.</li> <li>Dropout is applied as a regularization technique.</li> <li>The transformer is applied to process the patch embeddings.</li> <li>The pooling is applied to the output of the transformer. The type of pooling depends on the <code>pool</code> parameter ('cls' or 'mean').</li> <li>The MLP head is applied to obtain prediction for each class.</li> <li>The model returns these predictions.</li> </ol>"},{"location":"zeta/models/navit/","title":"Module/Function Name: NaViT","text":"<p><pre><code>class NaViT(nn.Module)\n</code></pre> The <code>NaViT</code> class is a subclass of PyTorch's <code>nn.Module</code> class. It is a reference architecture for creating multi-layer transformers with a pluggable attention, positional encoding, and optional token dropping.</p>"},{"location":"zeta/models/navit/#initialization","title":"Initialization:","text":"<p>To create a <code>NaViT</code> instance, the following parameters need to be specified:</p> <pre><code>def __init__(\n    self,\n    *,\n    image_size,\n    patch_size,\n    num_classes,\n    dim,\n    depth,\n    heads,\n    mlp_dim,\n    channels=3,\n    dim_head=64,\n    dropout=0.0,\n    emb_dropout=0.0,\n    token_dropout_prob=None,\n)\n</code></pre> Parameter Data Type Description image_size int The size of the input image. patch_size int The size of the patch that the model will use for feature representation. num_classes int The number of classes in the problem, i.e., the size of the output layer of the model. dim int Dimension of the model. depth int The number of transformer layers. heads int The number of attention heads in the transformer. mlp_dim int The dimension of the multilayer perceptron in the feedforward network. channels int The number of input channels. Defaults to 3. dim_head int The dimension of the attention head. Defaults to 64. dropout float Standard dropout. Defaults to 0. The probability of a feature being zeroed out during training. emb_dropout float Dropout applied to the learned embedding at the beginning of the transformer stack. Defaults to 0. token_dropout_prob scalar The probability of dropping out tokens before the transformer. Optional."},{"location":"zeta/models/navit/#forward-pass","title":"<code>forward</code> pass:","text":"<p>The forward method specifies the behavior of the model during its forward pass. It takes an image batch as input and returns the output of the model, which is the class probabilities for each input image. </p> <pre><code>def forward(self, batched_images: Union[List[Tensor], List[List[Tensor]]], group_images=False, group_max_seq_len=2048)\n</code></pre> Parameter Data Type Description batched_images Tensor or List of Tensors The input batch of images. group_images bool Whether or not to automatically group the images by maximum sequence length. Default: False. group_max_seq_len int The group maximum sequence length for auto-packing. Default: 2048. <p>It outputs a 2D tensor with dimensions <code>(batch size, number of classes)</code>, representing the class probabilities for each input image.</p>"},{"location":"zeta/models/navit/#code-example","title":"Code example:","text":"<pre><code>import torch\n\nfrom zeta.models import NaViT\n\n# initialize the model\nmodel = NaViT(\n    image_size=32,\n    patch_size=4,\n    num_classes=10,\n    dim=512,\n    depth=6,\n    heads=8,\n    mlp_dim=1024,\n)\n\n# random tensor representing a batch of 10 images, with 3 color channels, each 32x32 pixels\nx = torch.randn(10, 3, 32, 32)\n\n# the forward function returns the output of the model, which represents class probabilities for each image.\noutput = model.forward(x)\nprint(output.shape)  # prints: torch.Size([10, 10])\n</code></pre> <p>This example demonstrates how to initialize the NaViT model with a set of parameters, how to represent a batch of images as a tensor, and how to feed the image tensor to the model to get the output. </p> <p>The output is a batch of logits tensors where each tensor corresponds to class probabilities of the image. The size of each tensor is equal to the <code>num_classes</code>, i.e., every batch of images returns a tensor of dimensions <code>(batch size, num_classes)</code>. </p> <p>This allows direct comparison with the target labels to compute the loss and to derive the gradients during model training.</p>"},{"location":"zeta/models/palme/","title":"PalmE Class Documentation","text":"<p>This documentation covers the <code>PalmE</code> class of the <code>zeta.models</code> module. This class inherits from PyTorch's <code>torch.nn.Module</code> base class for all neural network modules. It's the starting point for creating models in PyTorch; such models can include layers which in turn can also be modules themselves..</p> <p>The <code>PalmE</code> class implements an encoder-decoder architecture useful for solving a variety of tasks by having the encoder extract information from input data which the decoder then uses to generate outputs.</p>"},{"location":"zeta/models/palme/#class-definition","title":"Class Definition","text":"<p>The <code>PalmE</code> class is constructed as follows:</p> <pre><code>class PalmE(torch.nn.Module):\n    def __init__(\n        self,\n        image_size=256,\n        patch_size=32,\n        encoder_dim=512,\n        encoder_depth=6,\n        encoder_heads=8,\n        num_tokens=20000,\n        max_seq_len=1024,\n        decoder_dim=512,\n        decoder_depth=6,\n        decoder_heads=8,\n        alibi_num_heads=4,\n        use_abs_pos_emb=False,\n        cross_attend=True,\n        alibi_pos_bias=True,\n        rotary_xpos=True,\n        attn_flash=True,\n        qk_norm=True,\n    ):\n</code></pre>"},{"location":"zeta/models/palme/#parameters","title":"Parameters","text":"Parameter Type Description <code>image_size</code> int Size of the input images. Default value is 256. <code>patch_size</code> int Size of the patches to divide input images into. Default value is 32. <code>encoder_dim</code> int Dimensionality of the encoder. Default value is 512. <code>encoder_depth</code> int Number of layers in the encoder. Default value is 6. <code>encoder_heads</code> int Number of attention heads in the encoder. Default value is 8. <code>num_tokens</code> int Number of tokens in the input text. Default value is 20000. <code>max_seq_len</code> int Maximum length of text sequences. Default value is 1024. <code>decoder_dim</code> int Dimensionality of the decoder. Default value is 512. <code>decoder_depth</code> int Number of layers in the decoder. Default value is 6. <code>decoder_heads</code> int Number of attention heads in the decoder. Default value is 8. <code>alibi_num_heads</code> int Number of heads for the alibi attention mechanism in the decoder. Default value is 4. <code>use_abs_pos_emb</code> bool Whether to use absolute positional encoding in the decoder. Default is False. <code>cross_attend</code> bool Whether the decoder should attend to the encoded image features. Default is True. <code>alibi_pos_bias</code> bool Whether to use a bias in the alibi attention mechanism. Default is True. <code>rotary_xpos</code> bool Whether to use the rotary positional encoding in place of the token positional encoding. Default is True. <code>attn_flash</code> bool Whether to use attention flash in the decoder. Default is True. <code>qk_norm</code> bool Whether to normalize query and key in the decoder self-attention. Default is True."},{"location":"zeta/models/palme/#methods","title":"Methods","text":""},{"location":"zeta/models/palme/#__init__","title":"<code>__init__()</code>","text":"<p>The <code>__init__()</code> method initializes the <code>PalmE</code> instance, sets up the encoder and decoder, and wraps the decoder in an <code>AutoregressiveWrapper</code>.</p>"},{"location":"zeta/models/palme/#forward","title":"<code>forward()</code>","text":"<p>The <code>forward()</code> method performs forward propagation through the model by using the encoder to generate encoded representations of the input images, and then passing these representations and the input text to the decoder in order to generate the model's outputs. A high level pseudo code example can be:</p> <pre><code>def forward(self, img, text):\n    try:\n        encoded = self.encoder(img, return_embeddings=True)\n        return self.decoder(text, context=encoded)\n    except Exception as error:\n        print(f\"Failed in forward method: {error}\")\n        raise\n</code></pre>"},{"location":"zeta/models/palme/#examples","title":"Examples","text":"<p>Below you'll find various examples on how to use the <code>PalmE</code> class.</p>"},{"location":"zeta/models/palme/#example-1-creating-a-palme-instance","title":"Example 1: Creating a <code>PalmE</code> Instance","text":"<p>Here\u2019s an example of how to instantiate the <code>PalmE</code> class with the default parameters:</p> <pre><code>import torch\n\nfrom zeta.models import PalmE\n\nmodel = PalmE()\n</code></pre>"},{"location":"zeta/models/palme/#example-2-pass-input-through-the-model","title":"Example 2: Pass input through the model","text":"<p>In this example, we create random image batch and text batch data, and pass them through our <code>PalmE</code> model:</p> <pre><code>img = torch.rand(16, 3, 256, 256)  # batch of 16 images\ntext = torch.randint(0, 20000, (50, 16))  # batch of 50 token sequences for 16 samples\n\nmodel = PalmE()\nout = model(img, text)\n</code></pre>"},{"location":"zeta/models/palme/#example-3-modifying-model-configuration","title":"Example 3: Modifying model configuration","text":"<p>Let's modify the model's configuration parameters at instantiation:</p> <pre><code>model = PalmE(\n    encoder_dim=1024,\n    encoder_depth=8,\n    decoder_dim=1024,\n    decoder_depth=8,\n    attn_flash=False,\n)\n</code></pre> <p>Here we modified the <code>encoder_dim</code>, <code>encoder_depth</code>, <code>decoder_dim</code>, <code>decoder_depth</code> and <code>attn_flash</code> parameters.</p>"},{"location":"zeta/models/palme/#additional-notes","title":"Additional Notes","text":"<ul> <li> <p>The input images should have dimensions <code>(batch_size, channels, height, width)</code>. The number of channels should usually be 3 (for RGB images), and the height and width should match the <code>image_size</code> parameter. </p> </li> <li> <p>The decoder's parameters can be tuned to balance between computational efficiency and the model's performance on your specific task. </p> </li> <li> <p>The <code>forward()</code> method may raise an exception if there's a bad input or a compatibility issue between the inputs' and the model's dimensions. Always make sure to match the dimensions. </p> </li> <li> <p>Please refer to the <code>torch.nn.Module</code> documentation for general information on PyTorch modules. </p> </li> <li> <p>The <code>rotary_xpos</code> feature refers to the rotary positional encoding introduced in the paper Pay Attention to MLPs. It's an alternative to traditional token positional encodings, and often works better. </p> </li> <li> <p>Always make sure your input tensor types (CPU tensor, CUDA tensor etc.) match the configuration of the model. </p> </li> <li> <p>The <code>PalmE</code> class supports the standard PyTorch methods for moving the model to a device (<code>to(device)</code>) and setting it to train or eval mode (<code>train() / eval()</code>).</p> </li> </ul>"},{"location":"zeta/models/vit/","title":"Module/Class Name: ViT (Vision Transformer)","text":"<p>The Vision Transformer (ViT) is a class designed as part of the <code>zeta.models</code> library. It builds upon the efficient Transformer architecture for applying convolutions for image recognition tasks. The ViT class inherits the properties and methods from PyTorch's built-in <code>torch.nn.Module</code> class. This class repurposes the Transformer architecture for image processing tasks by dividing the image into numerous patches and feeding them into the Transformer.</p>"},{"location":"zeta/models/vit/#class-definition","title":"Class Definition","text":"<p><pre><code>class ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, attn_layers, channels=3, num_classes=None, post_emb_norm=False, emb_dropout=0.0):\n</code></pre> This class takes the following parameters as inputs:</p> Parameter Type Description Default image_size int The dimensions (height and width) of the input image. - patch_size int The dimensions of each image patch to be input to the Transformer. - attn_layers <code>Encoder</code> A sequence of attention layers defined using the <code>Encoder</code> class. - channels int The number of color-bands (usually RGB). 3 num_classes int The number of classes to be detected, otherwise <code>None</code> for unsupervised learning scenarios. <code>None</code> post_emb_norm bool Whether to apply layer-normalization to the embeddings. <code>False</code> emb_dropout float The probability of an element to be zeroed in dropout. <code>0.0</code>"},{"location":"zeta/models/vit/#method-definitions","title":"Method Definitions","text":"<p>Here are the core methods of the <code>ViT</code> class:</p> <ol> <li><code>__init__</code></li> </ol> <p>This method initializes the instance and sets up the various components of the Transformer, including the positional embeddings, the sequence of attention layers, and the output MLP head.</p> <ol> <li><code>forward</code></li> </ol> <p>This method defines the feedforward computations of the ViT, starting from the division of the input image into patches, the conversion of patches into embeddings, applying attention layers, and, if specified, the MLP head for classification output.</p>"},{"location":"zeta/models/vit/#usage-examples","title":"Usage Examples","text":"<p>Here, we demonstrate how to use the ViT class.</p> <pre><code>import matplotlib.pyplot as plt\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom zeta.models import Encoder, ViT\n\n# Load an image and apply some pre-processing\nimg = Image.open(\"path_to_your_image.jpg\")\ntransform = transforms.Compose(\n    [transforms.Resize((224, 224)), transforms.ToTensor()]  # Resize image to 224x224\n)\nimg_tensor = transform(img).unsqueeze(0)\n\n# Define an Encoder with attention layers\nencoder = Encoder(dim=512, depth=12)\n\n# Instantiate a ViT model\nvit_model = ViT(\n    image_size=224,\n    patch_size=16,\n    attn_layers=encoder,\n    channels=3,\n    num_classes=1000,\n    post_emb_norm=True,\n    emb_dropout=0.1,\n)\n\n# Generate outputs using the ViT model\noutputs = vit_model(img_tensor, return_embeddings=True)\n\nprint(\"Output shape (with embeddings):\", outputs.size())\n\noutputs = vit_model(img_tensor, return_embeddings=False)\n\nprint(\"Output shape (without embeddings):\", outputs.size())\n</code></pre> <p>This code presents a usage scenario of the <code>ViT</code> class. It illustrates how to load an image, preprocess it, define an <code>Encoder</code> instance with attention layers, instantiate a <code>ViT</code> model with the defined <code>Encoder</code>, and generate outputs (embeddings and class probabilities) using the instantiated <code>ViT</code> model.</p>"},{"location":"zeta/nn/architecture/decoder/","title":"Decoder Class Documentation","text":"<p>Module/Class Name: Decoder</p> <pre><code>class Decoder(AttentionLayers):\n    def __init__(self, **kwargs):\n        assert \"causal\" not in kwargs, \"cannot set causality on decoder\"\n        super().__init__(causal=True, **kwargs)\n</code></pre>"},{"location":"zeta/nn/architecture/decoder/#overview-and-introduction","title":"Overview and Introduction","text":"<p>The <code>Decoder</code> class is a component of the Zeta library designed for creating a decoder model with multiple attention layers. It extends the functionality of the <code>AttentionLayers</code> class to enable the construction of a decoder architecture. The decoder is a key component in various sequence-to-sequence tasks, such as machine translation, text generation, and more.</p> <p>The decoder employs multi-head self-attention mechanisms and feed-forward networks to transform input sequences into meaningful output sequences while maintaining the causal property. It is particularly suitable for autoregressive tasks, where each step depends only on previous steps in the sequence.</p>"},{"location":"zeta/nn/architecture/decoder/#class-definition","title":"Class Definition","text":"<pre><code>class Decoder(AttentionLayers):\n    def __init__(self, **kwargs):\n        assert \"causal\" not in kwargs, \"cannot set causality on decoder\"\n        super().__init__(causal=True, **kwargs)\n</code></pre> <p>The <code>Decoder</code> class inherits from the <code>AttentionLayers</code> class and introduces the causality constraint by setting <code>causal=True</code>. It is initialized with various parameters that configure the architecture and behavior of the decoder.</p>"},{"location":"zeta/nn/architecture/decoder/#parameters","title":"Parameters","text":"<p>The <code>Decoder</code> class constructor accepts various parameters that control the behavior of the decoder. The most important parameters are inherited from the <code>AttentionLayers</code> class, and additional parameters specific to the decoder are introduced. Below is a summary of the parameters:</p> <ul> <li><code>dim</code> (int): Dimensionality of the model.</li> <li><code>depth</code> (int): Number of decoder layers.</li> <li><code>heads</code> (int): Number of parallel attention heads.</li> <li><code>cross_attend</code> (bool): Enable cross-attention between input and output sequences.</li> <li><code>sandwich_coef</code> (int): Coefficient for configuring sandwich normalization.</li> <li><code>residual_attn</code> (bool): Enable residual connection for self-attention layers.</li> <li><code>cross_residual_attn</code> (bool): Enable residual connection for cross-attention layers.</li> <li><code>layer_dropout</code> (float): Dropout probability applied to each layer.</li> <li>... (additional parameters inherited from <code>AttentionLayers</code>)</li> </ul>"},{"location":"zeta/nn/architecture/decoder/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>Decoder</code> class extends the functionality of the <code>AttentionLayers</code> class to specifically create decoder models. It employs multi-head self-attention mechanisms and feed-forward networks to process input sequences and generate output sequences.</p>"},{"location":"zeta/nn/architecture/decoder/#initialization","title":"Initialization","text":"<p>To create a decoder instance, you can use the following code:</p> <pre><code>from zeta import Decoder\n\ndecoder = Decoder(\n    dim=512,\n    depth=6,\n    heads=8,\n    causal=True,\n    cross_attend=True,\n    residual_attn=True,\n    layer_dropout=0.1,\n)\n</code></pre>"},{"location":"zeta/nn/architecture/decoder/#forward-pass","title":"Forward Pass","text":"<p>The forward pass of the decoder can be performed using the following code:</p> <pre><code>output = decoder(\n    input_sequence,\n    context=context_sequence,\n    mask=mask_sequence,\n    context_mask=context_mask_sequence,\n)\n</code></pre> <p>Here, <code>input_sequence</code> represents the input sequence to the decoder, <code>context_sequence</code> represents the context sequence for cross-attention (if enabled), <code>mask_sequence</code> is an optional mask to ignore certain elements in the input, and <code>context_mask_sequence</code> is an optional mask for the context sequence.</p>"},{"location":"zeta/nn/architecture/decoder/#return-intermediates","title":"Return Intermediates","text":"<p>If desired, you can also obtain intermediate outputs at each layer using the <code>return_hiddens</code> parameter:</p> <pre><code>output, intermediates = decoder(\n    input_sequence,\n    context=context_sequence,\n    mask=mask_sequence,\n    context_mask=context_mask_sequence,\n    return_hiddens=True,\n)\n</code></pre> <p>The <code>intermediates</code> object will contain information about intermediate hidden states and attention outputs for each layer.</p>"},{"location":"zeta/nn/architecture/decoder/#mathematical-formula","title":"Mathematical Formula","text":"<p>The <code>Decoder</code> class is built upon the foundation of multi-head self-attention and feed-forward networks. It can be summarized using the following mathematical formula:</p> <ol> <li>Input Embedding: ( X )</li> <li>Multi-Head Self-Attention: ( A = \\text{MultiHeadAttention}(X) )</li> <li>Feed-Forward Network: ( Y = \\text{FeedForward}(A) )</li> <li>Residual Connection: ( Z = X + Y )</li> </ol> <p>The above formula represents the basic forward pass of each layer in the decoder. The decoder iteratively applies these operations across its layers to generate meaningful output sequences while maintaining causal dependencies.</p>"},{"location":"zeta/nn/architecture/decoder/#references","title":"References","text":"<ul> <li>Zeta Library Documentation</li> <li>Attention Is All You Need</li> <li>PAR: Prompted Attention ```</li> </ul> <p>This documentation provides an in-depth overview of the <code>Decoder</code> class in the Zeta library. It covers its purpose, parameters, usage examples, and includes a simplified mathematical formula to illustrate its functionality.</p>"},{"location":"zeta/nn/architecture/transformer/","title":"Transformer Documentation","text":""},{"location":"zeta/nn/architecture/transformer/#overview","title":"Overview","text":"<p>The <code>Transformer</code> class in the Zeta library is a versatile deep learning architecture that combines attention mechanisms with feedforward neural networks for various natural language processing tasks, such as language modeling, machine translation, and text generation. The Transformer architecture was introduced in the paper \"Attention is All You Need\" by Vaswani et al.</p> <p>The main purpose of the <code>Transformer</code> class is to provide a flexible and configurable interface for creating transformer-based models for sequence-to-sequence tasks. The class allows users to specify the number of tokens, maximum sequence length, attention layers, embeddings, and other parameters necessary for creating and training transformer models.</p> <p>The Transformer class supports both autoregressive and non-autoregressive training settings and includes features such as relative positional biases, rotary positional embeddings, memory tokens, and more.</p>"},{"location":"zeta/nn/architecture/transformer/#class-signature","title":"Class Signature","text":"<pre><code>class Transformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        max_seq_len,\n        attn_layers,\n        embedding_provider: BaseEmbedding,\n        emb_dim = None,\n        max_mem_len = 0.,\n        shift_mem_down = 0,\n        emb_dropout = 0.,\n        post_emb_norm = False,\n        num_memory_tokens = None,\n        tie_embedding = False,\n        logits_dim = None,\n        use_abs_pos_emb = True,\n        scaled_sinu_pos_emb = False,\n        l2norm_embed = False,\n        emb_frac_gradient = 1.\n    )\n</code></pre>"},{"location":"zeta/nn/architecture/transformer/#parameters","title":"Parameters","text":"<ul> <li><code>num_tokens</code> (int): The total number of tokens in the vocabulary.</li> <li><code>max_seq_len</code> (int): The maximum length of the input sequences.</li> <li><code>attn_layers</code> (AttentionLayers): An instance of the <code>AttentionLayers</code> class representing the core attention layers of the transformer.</li> <li><code>embedding_provider</code> (BaseEmbedding): An instance of the <code>BaseEmbedding</code> class providing token embeddings.</li> <li><code>emb_dim</code> (int, optional): The embedding dimension. Default is <code>None</code>, in which case <code>emb_dim</code> is set to the same dimension as the <code>attn_layers</code>.</li> <li><code>max_mem_len</code> (float, optional): Maximum memory length for memory tokens. Default is <code>0.0</code>, indicating no memory tokens.</li> <li><code>shift_mem_down</code> (int, optional): Number of positions to shift memory tokens down in each layer. Default is <code>0</code>.</li> <li><code>emb_dropout</code> (float, optional): Dropout rate applied to the embedding layer. Default is <code>0.0</code>.</li> <li><code>post_emb_norm</code> (bool, optional): Apply layer normalization to the post-embedding inputs. Default is <code>False</code>.</li> <li><code>num_memory_tokens</code> (int, optional): Number of memory tokens to use. Default is <code>None</code>, indicating no memory tokens.</li> <li><code>tie_embedding</code> (bool, optional): Tie the output projection weights with the input token embeddings. Default is <code>False</code>.</li> <li><code>logits_dim</code> (int, optional): Dimensionality of the output logits. Default is <code>None</code>, indicating that it's the same as <code>num_tokens</code>.</li> <li><code>use_abs_pos_emb</code> (bool, optional): Use absolute positional embeddings. Default is <code>True</code>.</li> <li><code>scaled_sinu_pos_emb</code> (bool, optional): Use scaled sinusoidal positional embeddings. Default is <code>False</code>.</li> <li><code>l2norm_embed</code> (bool, optional): Apply L2 normalization to the embeddings. Default is <code>False</code>.</li> <li><code>emb_frac_gradient</code> (float, optional): Fraction of the gradient that should go to the embedding. Default is <code>1.0</code>.</li> </ul>"},{"location":"zeta/nn/architecture/transformer/#methods","title":"Methods","text":""},{"location":"zeta/nn/architecture/transformer/#forward","title":"<code>forward</code>","text":"<pre><code>def forward(\n    self,\n    x,\n    return_embeddings = False,\n    return_logits_and_embeddings = False,\n    return_intermediates = False,\n    mask = None,\n    return_mems = False,\n    return_attn = False,\n    mems = None,\n    pos = None,\n    prepend_embeds = None,\n    sum_embeds = None,\n    **kwargs\n)\n</code></pre> <p>This method computes the forward pass of the transformer.</p>"},{"location":"zeta/nn/architecture/transformer/#parameters_1","title":"Parameters","text":"<ul> <li><code>x</code> (torch.Tensor): Input tensor representing the sequence of token indices.</li> <li><code>return_embeddings</code> (bool, optional): If <code>True</code>, return only the embeddings without applying the output projection. Default is <code>False</code>.</li> <li><code>return_logits_and_embeddings</code> (bool, optional): If <code>True</code>, return both the logits and embeddings. Default is <code>False</code>.</li> <li><code>return_intermediates</code> (bool, optional): If <code>True</code>, return intermediate attention values. Default is <code>False</code>.</li> <li><code>mask</code> (torch.Tensor, optional): Attention mask indicating positions to be masked. Default is <code>None</code>.</li> <li><code>return_mems</code> (bool, optional): If <code>True</code>, return updated memory tokens. Default is <code>False</code>.</li> <li><code>return_attn</code> (bool, optional): If <code>True</code>, return attention maps. Default is <code>False</code>.</li> <li><code>mems</code> (list of torch.Tensor, optional): Memory tokens for each layer. Default is <code>None</code>.</li> <li><code>pos</code> (torch.Tensor, optional): External positional embeddings. Default is <code>None</code>.</li> <li><code>prepend_embeds</code> (torch.Tensor, optional): Prepend embeddings to the input sequence. Default is <code>None</code>.</li> <li><code>sum_embeds</code> (torch.Tensor, optional): Sum external embeddings to the input sequence. Default is <code>None</code>.</li> <li><code>kwargs</code>: Additional keyword arguments passed to the attention layers.</li> </ul>"},{"location":"zeta/nn/architecture/transformer/#returns","title":"Returns","text":"<p>The method returns the output logits or embeddings based on the specified return options.</p>"},{"location":"zeta/nn/architecture/transformer/#usage-examples","title":"Usage Examples","text":"<p>Here are three usage examples of the <code>Transformer</code> class from the Zeta library:</p> <pre><code>import torch\n\nfrom zeta.nn import Decoder, Transformer\n\nlogits = torch.randint(0, 256, (1, 1024))\n\n# Example 1: Basic Usage\ntransformer = Transformer(\n    num_tokens=20000,\n    max_seq_len=1024,\n    attn_layers=Decoder(dim=512, depth=12, heads=8),\n)\n\nlogits = transformer(logits)\nprint(logits)\n\n# # Example 2: Return Embeddings\n# embeddings = transformer(input_tokens, return_embeddings=True)\n\n# # Example 3: Return Intermediate Attention Maps\n# logits, attn_maps = transformer(input_tokens, return_attn=True)\n</code></pre> <p>In these examples, replace <code>attn_layers_instance</code> and <code>embedding_provider_instance</code> with actual instances of <code>AttentionLayers</code> and <code>BaseEmbedding</code>, respectively, and <code>input_tokens</code> with your input tensor containing token indices.</p>"},{"location":"zeta/nn/architecture/transformer/#mathematical-formula","title":"Mathematical Formula","text":"<p>The mathematical formula for the <code>Transformer</code> class can be represented as follows:</p> <pre><code>Input -&gt; Embedding -&gt; Post-embedding Norm -&gt; Embedding Dropout -&gt; Project Embedding -&gt; Attention Layers -&gt; Layer Normalization -&gt; To Logits/Embeddings\n</code></pre> <p>In this formula, \"Attention Layers\" represents the core attention mechanism of the transformer, which includes self-attention and feedforward neural networks.</p>"},{"location":"zeta/nn/architecture/transformer/#references","title":"References","text":"<ul> <li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). Attention is All You Need. Advances in neural information processing systems, 30.</li> <li>Zeta Library: Link to the official documentation of the Zeta library.</li> <li>Insert any additional references or resources as needed. ```</li> </ul>"},{"location":"zeta/nn/architecture/transformerblock/","title":"<code>TransformerBlock</code> Documentation","text":""},{"location":"zeta/nn/architecture/transformerblock/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Class: <code>TransformerBlock</code></li> <li>Initialization</li> <li>Parameters</li> <li>Attention Mechanism</li> <li>Multi-Head Attention</li> <li>Rotary Embedding</li> <li>Feedforward Network</li> <li>Caching and Optimization</li> <li>Usage Examples</li> <li>Basic Usage</li> <li>Fine-Tuning</li> <li>Additional Information</li> <li>Layernorm</li> <li>Position Embeddings</li> <li>References</li> </ol>"},{"location":"zeta/nn/architecture/transformerblock/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation for the <code>TransformerBlock</code> class! Zeta is a versatile library that offers tools for efficient training of deep learning models using PyTorch. This documentation will provide a comprehensive overview of the <code>TransformerBlock</code> class, its architecture, purpose, and usage.</p>"},{"location":"zeta/nn/architecture/transformerblock/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>TransformerBlock</code> class is a fundamental component of the Zeta library. It is designed to be used within a transformer-based architecture, and its primary purpose is to process input data efficiently. Below, we'll explore the key functionalities and features of the <code>TransformerBlock</code> class.</p>"},{"location":"zeta/nn/architecture/transformerblock/#3-class-transformerblock","title":"3. Class: <code>TransformerBlock</code>","text":"<p>The <code>TransformerBlock</code> class is the building block of transformer-based models. It performs various operations, including multi-head attention and feedforward network, to process input data. Let's dive into the details of this class.</p>"},{"location":"zeta/nn/architecture/transformerblock/#initialization","title":"Initialization","text":"<p>To create a <code>TransformerBlock</code> instance, you need to specify various parameters and configurations. Here's an example of how to initialize it:</p> <pre><code>TransformerBlock(\n    dim=512,\n    dim_head=64,\n    causal=True,\n    heads=8,\n    qk_rmsnorm=False,\n    qk_scale=8,\n    ff_mult=4,\n    attn_dropout=0.0,\n    ff_dropout=0.0,\n    use_xpos=True,\n    xpos_scale_base=512,\n    flash_attn=False,\n)\n</code></pre>"},{"location":"zeta/nn/architecture/transformerblock/#parameters","title":"Parameters","text":"<ul> <li> <p><code>dim</code> (int): The dimension of the input data.</p> </li> <li> <p><code>dim_head</code> (int): The dimension of each attention head.</p> </li> <li> <p><code>causal</code> (bool): Whether to use a causal (auto-regressive) attention mechanism. Default is <code>True</code>.</p> </li> <li> <p><code>heads</code> (int): The number of attention heads. </p> </li> <li> <p><code>qk_rmsnorm</code> (bool): Whether to apply root mean square normalization to query and key vectors. Default is <code>False</code>.</p> </li> <li> <p><code>qk_scale</code> (int): Scaling factor for query and key vectors. Used when <code>qk_rmsnorm</code> is <code>True</code>. Default is <code>8</code>.</p> </li> <li> <p><code>ff_mult</code> (int): Multiplier for the feedforward network dimension. Default is <code>4</code>.</p> </li> <li> <p><code>attn_dropout</code> (float): Dropout probability for attention layers. Default is <code>0.0</code>.</p> </li> <li> <p><code>ff_dropout</code> (float): Dropout probability for the feedforward network. Default is <code>0.0</code>.</p> </li> <li> <p><code>use_xpos</code> (bool): Whether to use positional embeddings. Default is <code>True</code>.</p> </li> <li> <p><code>xpos_scale_base</code> (int): Scaling factor for positional embeddings. Default is <code>512</code>.</p> </li> <li> <p><code>flash_attn</code> (bool): Whether to use Flash Attention mechanism. Default is <code>False</code>.</p> </li> </ul>"},{"location":"zeta/nn/architecture/transformerblock/#attention-mechanism","title":"Attention Mechanism","text":"<p>The <code>TransformerBlock</code> class includes a powerful attention mechanism that allows the model to focus on relevant parts of the input data. It supports both regular and Flash Attention.</p>"},{"location":"zeta/nn/architecture/transformerblock/#multi-head-attention","title":"Multi-Head Attention","text":"<p>The class can split the attention mechanism into multiple heads, allowing the model to capture different patterns in the data simultaneously. The number of attention heads is controlled by the <code>heads</code> parameter.</p>"},{"location":"zeta/nn/architecture/transformerblock/#rotary-embedding","title":"Rotary Embedding","text":"<p>Rotary embeddings are used to enhance the model's ability to handle sequences of different lengths effectively. They are applied to query and key vectors to improve length extrapolation.</p>"},{"location":"zeta/nn/architecture/transformerblock/#feedforward-network","title":"Feedforward Network","text":"<p>The <code>TransformerBlock</code> class includes a feedforward network that processes the attention output. It can be customized by adjusting the <code>ff_mult</code> parameter.</p>"},{"location":"zeta/nn/architecture/transformerblock/#caching-and-optimization","title":"Caching and Optimization","text":"<p>The class includes mechanisms for caching causal masks and rotary embeddings, which can improve training efficiency. It also provides options for fine-tuning specific modules within the block.</p>"},{"location":"zeta/nn/architecture/transformerblock/#4-usage-examples","title":"4. Usage Examples","text":"<p>Now, let's explore some usage examples of the <code>TransformerBlock</code> class to understand how to use it effectively.</p>"},{"location":"zeta/nn/architecture/transformerblock/#basic-usage","title":"Basic Usage","text":"<pre><code># Create a TransformerBlock instance\ntransformer_block = TransformerBlock(dim=512, heads=8)\n\n# Process input data\noutput = transformer_block(input_data)\n</code></pre>"},{"location":"zeta/nn/architecture/transformerblock/#fine-tuning","title":"Fine-Tuning","text":"<pre><code># Create a TransformerBlock instance with fine-tuning modules\nlora_q = YourCustomModule()\nlora_k = YourCustomModule()\nlora_v = YourCustomModule()\nlora_o = YourCustomModule()\n\ntransformer_block = TransformerBlock(\n    dim=512, heads=8, finetune_modules=(lora_q, lora_k, lora_v, lora_o)\n)\n\n# Process input data\noutput = transformer_block(input_data)\n</code></pre>"},{"location":"zeta/nn/architecture/transformerblock/#5-additional-information","title":"5. Additional Information","text":""},{"location":"zeta/nn/architecture/transformerblock/#layernorm","title":"Layernorm","text":"<p>The <code>TransformerBlock</code> class uses layer normalization (layernorm) to normalize input data before processing. This helps stabilize and accelerate training.</p>"},{"location":"zeta/nn/architecture/transformerblock/#position-embeddings","title":"Position Embeddings","text":"<p>Position embeddings are used to provide the model with information about the position of tokens</p> <p>in the input sequence. They are crucial for handling sequences of different lengths effectively.</p>"},{"location":"zeta/nn/architecture/transformerblock/#6-references","title":"6. References","text":"<ul> <li>Original Transformer Paper</li> <li>Attention Is All You Need</li> <li>Flash Attention: Scaling Vision Transformers with Hybrid Attention for Image and Video Recognition</li> <li>Layer Normalization</li> </ul> <p>This documentation provides a comprehensive guide to the <code>TransformerBlock</code> class in the Zeta library, explaining its purpose, functionality, parameters, and usage. You can now effectively integrate this class into your deep learning models for various natural language processing tasks and beyond.</p>"},{"location":"zeta/nn/architecture/video_tokenizer/","title":"<code>VideoTokenizer</code> Documentation","text":""},{"location":"zeta/nn/architecture/video_tokenizer/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>VideoTokenizer Class</li> <li>Initialization Parameters</li> <li>Functionality and Usage</li> <li>Encode Method</li> <li>Decode Method</li> <li>Forward Method</li> <li>Examples</li> <li>Example 1: Creating a VideoTokenizer</li> <li>Example 2: Encoding and Decoding Videos</li> <li>Example 3: Forward Pass with Loss Calculation</li> <li>Additional Information</li> <li>References and Resources</li> </ol>"},{"location":"zeta/nn/architecture/video_tokenizer/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the documentation for the Zeta library, with a focus on the <code>VideoTokenizer</code> class. This comprehensive guide provides in-depth information about the Zeta library and its core components. Before we dive into the details, it's crucial to understand the purpose and significance of this library.</p>"},{"location":"zeta/nn/architecture/video_tokenizer/#11-purpose","title":"1.1 Purpose","text":"<p>The Zeta library aims to simplify deep learning model development by offering modular components and utilities. One of the essential components is the <code>VideoTokenizer</code> class, which serves various purposes, including video encoding, decoding, and quantization.</p>"},{"location":"zeta/nn/architecture/video_tokenizer/#12-key-features","title":"1.2 Key Features","text":"<ul> <li> <p>Video Processing: The <code>VideoTokenizer</code> class allows you to encode and decode video data efficiently.</p> </li> <li> <p>Lookup-Free Quantization: Zeta incorporates lookup-free quantization techniques to improve the quality of encoded video tokens.</p> </li> </ul>"},{"location":"zeta/nn/architecture/video_tokenizer/#2-videotokenizer-class","title":"2. VideoTokenizer Class","text":"<p>The <code>VideoTokenizer</code> class is a fundamental module in the Zeta library, enabling various video processing tasks, including encoding, decoding, and quantization.</p>"},{"location":"zeta/nn/architecture/video_tokenizer/#21-initialization-parameters","title":"2.1 Initialization Parameters","text":"<p>Here are the initialization parameters for the <code>VideoTokenizer</code> class:</p> <ul> <li> <p><code>layers</code> (Tuple[Tuple[str, int]]): A tuple of tuples defining the layers and their dimensions in the network.</p> </li> <li> <p><code>residual_conv_kernel_size</code> (int): The kernel size for residual convolutions.</p> </li> <li> <p><code>num_codebooks</code> (int): The number of codebooks to use for quantization.</p> </li> <li> <p><code>codebook_size</code> (int): The size of each codebook for quantization.</p> </li> <li> <p><code>channels</code> (int): The number of channels in the input video.</p> </li> <li> <p><code>init_dim</code> (int): The initial dimension of the video data.</p> </li> <li> <p><code>input_conv_kernel_size</code> (Tuple[int, int, int]): The kernel size for the input convolution operation.</p> </li> <li> <p><code>output_conv_kernel_size</code> (Tuple[int, int, int]): The kernel size for the output convolution operation.</p> </li> <li> <p><code>pad_mode</code> (str): The padding mode for convolution operations.</p> </li> <li> <p><code>lfq_entropy_loss_weight</code> (float): The weight for the entropy loss during quantization.</p> </li> <li> <p><code>lfq_diversity_gamma</code> (float): The gamma value for diversity loss during quantization.</p> </li> </ul>"},{"location":"zeta/nn/architecture/video_tokenizer/#22-methods","title":"2.2 Methods","text":"<p>The <code>VideoTokenizer</code> class provides the following methods:</p> <ul> <li> <p><code>encode(video: Tensor, quantize=False)</code>: Encode video data into tokens. You can choose whether to quantize the tokens by setting <code>quantize</code> to <code>True</code>.</p> </li> <li> <p><code>decode(codes: Tensor)</code>: Decode tokens back into video data.</p> </li> <li> <p><code>forward(video, video_or_images: Tensor, return_loss=False, return_codes=False)</code>: Perform a forward pass through the video tokenizer. This method supports various options, including returning loss and codes.</p> </li> </ul>"},{"location":"zeta/nn/architecture/video_tokenizer/#3-functionality-and-usage","title":"3. Functionality and Usage","text":"<p>Let's explore the functionality and usage of the <code>VideoTokenizer</code> class.</p>"},{"location":"zeta/nn/architecture/video_tokenizer/#31-encode-method","title":"3.1 Encode Method","text":"<p>The <code>encode</code> method takes video data as input and encodes it into tokens. You can choose whether or not to quantize the tokens by setting the <code>quantize</code> parameter to <code>True</code>.</p>"},{"location":"zeta/nn/architecture/video_tokenizer/#32-decode-method","title":"3.2 Decode Method","text":"<p>The <code>decode</code> method takes tokens as input and decodes them back into video data.</p>"},{"location":"zeta/nn/architecture/video_tokenizer/#33-forward-method","title":"3.3 Forward Method","text":"<p>The <code>forward</code> method performs a complete forward pass through the video tokenizer. It accepts video data and various options, including returning loss and codes.</p>"},{"location":"zeta/nn/architecture/video_tokenizer/#4-examples","title":"4. Examples","text":"<p>Let's dive into practical examples to demonstrate the usage of the <code>VideoTokenizer</code> class.</p>"},{"location":"zeta/nn/architecture/video_tokenizer/#41-example-1-creating-a-videotokenizer","title":"4.1 Example 1: Creating a VideoTokenizer","text":"<p>In this example, we create an instance of the <code>VideoTokenizer</code> class with default settings:</p> <pre><code>video_tokenizer = VideoTokenizer()\n</code></pre>"},{"location":"zeta/nn/architecture/video_tokenizer/#42-example-2-encoding-and-decoding-videos","title":"4.2 Example 2: Encoding and Decoding Videos","text":"<p>Here, we demonstrate how to use the <code>VideoTokenizer</code> to encode and decode video data:</p> <pre><code>video = torch.randn(1, 3, 32, 32, 32)  # Example video data\nencoded_tokens = video_tokenizer.encode(video, quantize=True)\ndecoded_video = video_tokenizer.decode(encoded_tokens)\n</code></pre>"},{"location":"zeta/nn/architecture/video_tokenizer/#43-example-3-forward-pass-with-loss-calculation","title":"4.3 Example 3: Forward Pass with Loss Calculation","text":"<p>In this example, we perform a forward pass through the video tokenizer and calculate the loss:</p> <pre><code>video = torch.randn(1, 3, 32, 32, 32)  # Example video data\nvideo_or_images = torch.randn(1, 3, 32, 32, 32)  # Example input for forward pass\nloss, loss_breakdown = video_tokenizer(video, video_or_images, return_loss=True)\n</code></pre>"},{"location":"zeta/nn/architecture/video_tokenizer/#5-additional-information","title":"5. Additional Information","text":"<p>Here are some additional tips and information for using the Zeta library and the <code>VideoTokenizer</code> class effectively:</p> <ul> <li> <p>Experiment with different layer configurations in the <code>layers</code> parameter to tailor the network architecture to your specific task.</p> </li> <li> <p>The <code>quantize</code> parameter in the <code>encode</code> method allows you to control whether you want to perform quantization during encoding. Set it to <code>True</code> for quantization.</p> </li> <li> <p>Explore the impact of <code>lfq_entropy_loss_weight</code> and <code>lfq_diversity_gamma</code> on the quality of quantized tokens when initializing the <code>VideoTokenizer</code> class.</p> </li> </ul>"},{"location":"zeta/nn/architecture/video_tokenizer/#6-references-and-resources","title":"6. References and Resources","text":"<p>For further information and resources related to the Zeta library and deep learning, please refer to the following:</p> <ul> <li> <p>Zeta GitHub Repository: The official Zeta repository for updates and contributions.</p> </li> <li> <p>PyTorch Official Website: The official website for PyTorch, the deep learning framework used in Zeta.</p> </li> </ul> <p>This concludes the documentation for the Zeta library and the <code>VideoTokenizer</code> class. You now have a comprehensive</p>"},{"location":"zeta/nn/attention/base/","title":"BaseAttention Abstract Class","text":"<p>============================</p> <p>The\u00a0<code>BaseAttention</code>\u00a0class is an abstract base class that defines the interface for all attention mechanisms. It includes the basic structure and methods that all attention mechanisms should have.</p> <pre><code>from abc import abstractmethod\n\nimport torch.nn as nn\n\n\nclass BaseAttention(nn.Module):\n    @abstractmethod\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    @abstractmethod\n    def forward(self, x, context=None, mask=None):\n        pass\n</code></pre>"},{"location":"zeta/nn/attention/base/#usage","title":"Usage","text":"<p>The\u00a0<code>FlashAttentionTwo</code>\u00a0class extends the\u00a0<code>BaseAttention</code>\u00a0abstract base class and implements the specific attention mechanism.</p> <pre><code>class FlashAttentionTwo(BaseAttention):\n    def __init__(\n        self,\n        *,\n        dim,\n        heads = 8,\n        dim_head = 64,\n        causal = False,\n        q_bucket_size = 512,\n        k_bucket_size = 1024,\n        parallel = False,\n        mixed_precision = False\n    ):\n        super().__init__(dim, heads, dim_head)\n        self.causal = causal\n        self.parallel = parallel\n        self.mixed_precision = mixed_precision\n        self.q_bucket_size = q_bucket_size\n        self.k_bucket_size = k_bucket_size\n        # ... rest of the implementation ...\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        q_bucket_size = None,\n        k_bucket_size = None,\n    ):\n        # ... implementation of the forward method ...\n</code></pre>"},{"location":"zeta/nn/attention/base/#rules-for-using-the-baseattention-class","title":"Rules for Using the BaseAttention Class","text":"<ol> <li> <p>Any class that extends the\u00a0<code>BaseAttention</code>\u00a0class must implement the\u00a0<code>forward</code>\u00a0method. This method defines how the attention mechanism operates.</p> </li> <li> <p>The\u00a0<code>__init__</code>\u00a0method of the\u00a0<code>BaseAttention</code>\u00a0class takes three parameters:\u00a0<code>dim</code>,\u00a0<code>heads</code>, and\u00a0<code>dim_head</code>. Any class that extends\u00a0<code>BaseAttention</code>\u00a0should pass these parameters to the\u00a0<code>__init__</code>\u00a0method of the base class.</p> </li> <li> <p>The\u00a0<code>forward</code>\u00a0method of the\u00a0<code>BaseAttention</code>\u00a0class takes three parameters:\u00a0<code>x</code>,\u00a0<code>context</code>, and\u00a0<code>mask</code>. Any class that extends\u00a0<code>BaseAttention</code>\u00a0should include these parameters in its\u00a0<code>forward</code>\u00a0method.</p> </li> </ol>"},{"location":"zeta/nn/attention/base/#example-of-using-the-flashattentiontwo-class","title":"Example of Using the FlashAttentionTwo Class","text":"<pre><code>from zeta.nn.attention import FlashAttentionTwo\n\n# Create an instance of the FlashAttentionTwo class\nattention = FlashAttentionTwo(dim=512, heads=8, dim_head=64)\n\n# Create some input data\nx = torch.randn(1, 10, 512)\n\n# Apply the attention mechanism\nout = attention(x)\n</code></pre> <p>In this example, we first create an instance of the\u00a0<code>FlashAttentionTwo</code>\u00a0class. We then create some input data\u00a0<code>x</code>\u00a0and apply the attention mechanism to this data by calling the\u00a0<code>forward</code>\u00a0method of the\u00a0<code>attention</code>\u00a0instance.</p>"},{"location":"zeta/nn/attention/cross_attn/","title":"<code>MultiModalCrossAttention</code> Documentation","text":""},{"location":"zeta/nn/attention/cross_attn/#overview","title":"Overview","text":"<p>The <code>MultiModalCrossAttention</code> module is an enhanced cross-attention mechanism designed for various multimodal tasks, such as combining information from different sources (e.g., text and images) in a transformer-based architecture. This module extends the standard self-attention mechanism by providing features like conditional layer normalization, lambda masking, and dropout for improved modeling of multimodal data.</p> <p>This documentation provides a comprehensive guide to the <code>MultiModalCrossAttention</code> module, explaining its architecture, purpose, parameters, and usage through detailed examples.</p>"},{"location":"zeta/nn/attention/cross_attn/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Module Overview</li> <li>Installation</li> <li>Module Architecture</li> <li>Parameters</li> <li>Usage Examples</li> <li>Example 1: Basic Usage</li> <li>Example 2: Conditional Layer Normalization</li> <li>Example 3: Lambda Masking</li> <li>Additional Information and Tips</li> </ol>"},{"location":"zeta/nn/attention/cross_attn/#installation","title":"Installation","text":"<p>Before using the <code>MultiModalCrossAttention</code> module, you need to ensure that you have the required dependencies installed. Here are the dependencies:</p> <ul> <li>PyTorch</li> <li>Einops</li> <li>TorchVision (for the examples)</li> </ul> <p>You can install these dependencies using <code>pip</code>:</p> <pre><code>pip install zetascale\n</code></pre> <p>Now let's delve into the architecture, parameters, and usage of the <code>MultiModalCrossAttention</code> module.</p>"},{"location":"zeta/nn/attention/cross_attn/#module-architecture","title":"Module Architecture","text":"<p>The <code>MultiModalCrossAttention</code> module extends the standard self-attention mechanism used in transformer architectures. It takes as input a query tensor <code>x</code> and a context tensor <code>context</code>, which represent the input data from different modalities. The module performs multi-head attention between these tensors, combining information from both modalities.</p> <p>The key features of the <code>MultiModalCrossAttention</code> module include:</p> <ul> <li> <p>Multi-Head Attention: The attention mechanism is split into multiple heads, allowing the model to attend to different parts of the input data in parallel.</p> </li> <li> <p>Conditional Layer Normalization: Optional conditional layer normalization can be applied to the query and key tensors before attention computation.</p> </li> <li> <p>Lambda Masking: An optional mask can be applied to the attention weights to control which elements are attended to during computation.</p> </li> <li> <p>Dropout: Dropout is applied to the attention weights to prevent overfitting.</p> </li> <li> <p>Output Projection: The module projects the attention outputs to the desired output dimension.</p> </li> <li> <p>Attention Strategy: The module supports two attention strategies: \"average\" (average attention outputs from all heads) and \"concatenate\" (concatenate attention outputs from all heads).</p> </li> </ul> <p>The architecture of the <code>MultiModalCrossAttention</code> module is designed to handle multimodal data efficiently by combining information from different sources. Now, let's explore the parameters of this module.</p>"},{"location":"zeta/nn/attention/cross_attn/#parameters","title":"Parameters","text":"<p>The <code>MultiModalCrossAttention</code> module accepts several parameters, each of which controls different aspects of its behavior. Here are the parameters:</p> Parameter Description Default Value <code>dim</code> Dimension of the model. None (Required) <code>heads</code> Number of attention heads. None (Required) <code>context_dim</code> Dimension of the context. None (Required) <code>dim_head</code> Dimension of each attention head. 64 <code>dropout</code> Dropout rate applied to attention weights. 0.1 <code>qk</code> Whether to use conditional layer normalization. False <code>post_attn_norm</code> Whether to use post-attention normalization. False <code>attention_strategy</code> Attention strategy: \"average\" or \"concatenate\". None (Required) <code>mask</code> Mask for lambda masking. None <p>Now that we understand the parameters, let's explore how to use the <code>MultiModalCrossAttention</code> module with detailed usage examples.</p>"},{"location":"zeta/nn/attention/cross_attn/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/attention/cross_attn/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<p>In this example, we'll demonstrate the basic usage of the <code>MultiModalCrossAttention</code> module. We'll create an instance of the module, feed it with query and context tensors, and obtain the attention outputs.</p> <pre><code>import torch\nfrom einops import rearrange\nfrom torch import nn\n\nfrom zeta.nn import MultiModalCrossAttention\n\n# Create a MultiModalCrossAttention module\ndim = 1024\nheads = 8\ncontext_dim = 1024\nattn = MultiModalCrossAttention(dim, heads, context_dim)\n\n# Generate random query and context tensors\nquery = torch.randn(1, 32, dim)\ncontext = torch.randn(1, 32, context_dim)\n\n# Perform multi-head cross-attention\noutput = attn(query, context)\n\n# Print the shape of the output\nprint(output.shape)\n</code></pre> <p>Output: <pre><code>torch.Size([1, 32, 1024])\n</code></pre></p> <p>In this basic usage example, we create an instance of the <code>MultiModalCrossAttention</code> module and apply it to random query and context tensors, resulting in an output tensor.</p>"},{"location":"zeta/nn/attention/cross_attn/#example-2-conditional-layer-normalization","title":"Example 2: Conditional Layer Normalization","text":"<p>In this example, we'll enable conditional layer normalization and observe the effect on the attention outputs.</p> <pre><code># Create a MultiModalCrossAttention module with conditional layer normalization\nattn = MultiModalCrossAttention(dim, heads, context_dim, qk=True)\n\n# Generate random query and context tensors\nquery = torch.randn(1, 32, dim)\ncontext = torch.randn(1, 32, context_dim)\n\n# Perform multi-head cross-attention\noutput = attn(query, context)\n\n# Print the shape of the output\nprint(output.shape)\n</code></pre> <p>Output: <pre><code>torch.Size([1, 32, 1024])\n</code></pre></p> <p>In this example, we enable conditional layer normalization (<code>qk=True</code>) and observe the effect on the attention outputs.</p>"},{"location":"zeta/nn/attention/cross_attn/#example-3-lambda-masking","title":"Example 3: Lambda Masking","text":"<p>Lambda masking allows us to control which elements are attended to during computation. In this example, we'll apply a mask and observe how it affects the attention weights.</p> <pre><code># Create a MultiModalCrossAttention module with lambda masking\nmask = torch.randint(0, 2, (32, 32), dtype=torch.bool)\nattn = MultiModalCrossAttention(dim, heads, context_dim, mask=mask)\n\n# Generate random query and context tensors\nquery = torch.randn(1, 32, dim)\ncontext = torch.randn(1, 32, context_dim)\n\n# Perform multi-head cross-attention\noutput = attn(query, context)\n\n# Print the shape of the output\nprint(output.shape)\n</code></pre> <p>Output: <pre><code>torch.Size([1, 32, 1024])\n</code></pre></p> <p>In this example, we apply a lambda mask to control attention weights and observe its effect on the attention outputs.</p>"},{"location":"zeta/nn/attention/cross_attn/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li> <p>The <code>MultiModalCrossAttention</code> module can be integrated into various multimodal architectures to capture dependencies between different data sources effectively.</p> </li> <li> <p>Experiment with different values of <code>heads</code> and <code>dim_head</code> to find the optimal configuration for your task.</p> </li> <li> <p>You can choose the appropriate attention strategy (<code>average</code> or <code>concatenate</code>) based on your specific requirements.</p> </li> <li> <p>If you encounter any issues or have questions, refer to the PyTorch documentation or seek assistance from the community.</p> </li> </ul> <p>By following these guidelines and examples, you can effectively utilize the <code>MultiModalCrossAttention</code> module in your multimodal deep learning projects.</p>"},{"location":"zeta/nn/attention/flash2/","title":"Module Name: FlashAttentionTwo","text":"<p>The\u00a0<code>FlashAttentionTwo</code>\u00a0class is a PyTorch module that implements a variant of the attention mechanism, which is a key component in many state-of-the-art models in natural language processing and other fields. This class is designed to be memory-efficient and optionally supports parallel computation and mixed precision for improved performance.</p>"},{"location":"zeta/nn/attention/flash2/#class-definition","title":"Class Definition","text":"<pre><code>class FlashAttentionTwo(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        heads = 8,\n        dim_head = 64,\n        causal = False,\n        q_bucket_size = 512,\n        k_bucket_size = 1024,\n        parallel = False,\n        mixed_precision = False\n    ):\n</code></pre>"},{"location":"zeta/nn/attention/flash2/#parameters","title":"Parameters","text":"<ul> <li><code>dim</code>\u00a0(int): The dimensionality of the input data.</li> <li><code>heads</code>\u00a0(int, optional): The number of attention heads. Default is 8.</li> <li><code>dim_head</code>\u00a0(int, optional): The dimensionality of each attention head. Default is 64.</li> <li><code>causal</code>\u00a0(bool, optional): If True, the attention mechanism is causal. Default is False.</li> <li><code>q_bucket_size</code>\u00a0(int, optional): The bucket size for the query in the attention mechanism. Default is 512.</li> <li><code>k_bucket_size</code>\u00a0(int, optional): The bucket size for the key in the attention mechanism. Default is 1024.</li> <li><code>parallel</code>\u00a0(bool, optional): If True, the computation is performed in parallel across multiple GPUs. Default is False.</li> <li><code>mixed_precision</code>\u00a0(bool, optional): If True, the computation is performed in mixed precision for improved performance. Default is False.</li> </ul>"},{"location":"zeta/nn/attention/flash2/#methods","title":"Methods","text":""},{"location":"zeta/nn/attention/flash2/#forward","title":"<code>forward</code>","text":"<pre><code>def forward(\n    self,\n    x,\n    context = None,\n    mask = None,\n    q_bucket_size = None,\n    k_bucket_size = None,\n):\n</code></pre> <p>Performs the forward pass of the attention mechanism.</p>"},{"location":"zeta/nn/attention/flash2/#parameters_1","title":"Parameters","text":"<ul> <li><code>x</code>\u00a0(Tensor): The input data.</li> <li><code>context</code>\u00a0(Tensor, optional): The context for the attention mechanism. If not provided, the input data\u00a0<code>x</code>\u00a0is used as the context.</li> <li><code>mask</code>\u00a0(Tensor, optional): An optional mask for the attention mechanism.</li> <li><code>q_bucket_size</code>\u00a0(int, optional): The bucket size for the query in the attention mechanism. If not provided, the value specified during initialization is used.</li> <li><code>k_bucket_size</code>\u00a0(int, optional): The bucket size for the key in the attention mechanism. If not provided, the value specified during initialization is used.</li> </ul>"},{"location":"zeta/nn/attention/flash2/#returns","title":"Returns","text":"<ul> <li><code>out</code>\u00a0(Tensor): The output of the attention mechanism.</li> </ul>"},{"location":"zeta/nn/attention/flash2/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/attention/flash2/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>from torch import nn\n\nfrom zeta.nn import FlashAttentionTwo\n\nmodel = FlashAttentionTwo(dim=512)\nx = torch.randn(1, 10, 512)\nout = model(x)\n</code></pre> <p>Copy code</p>"},{"location":"zeta/nn/attention/flash2/#example-2-using-a-mask","title":"Example 2: Using a Mask","text":"<pre><code>from torch import nn\n\nfrom zeta.nn import FlashAttentionTwo\n\nmodel = FlashAttentionTwo(dim=512)\nx = torch.randn(1, 10, 512)\nmask = torch.ones(1, 10)\nout = model(x, mask=mask)\n</code></pre>"},{"location":"zeta/nn/attention/flash2/#example-3-using-a-context","title":"Example 3: Using a Context","text":"<pre><code>from torch import nn\n\nfrom zeta.nn import FlashAttentionTwo\n\nmodel = FlashAttentionTwo(dim=512)\nx = torch.randn(1, 10, 512)\ncontext = torch.randn(1, 10, 512)\nout = model(x, context=context)\n</code></pre>"},{"location":"zeta/nn/attention/flash2/#mathematical-formula","title":"Mathematical Formula","text":"<p>The attention mechanism can be described by the following formula:</p> <p></p> <p>where Q, K, and V are the query, key, and value, respectively. The softmax function ensures that the weights sum to 1, and the dot product of the weights and the value gives the output of the attention mechanism.</p>"},{"location":"zeta/nn/attention/flash2/#additional-information","title":"Additional Information","text":"<p>The\u00a0<code>FlashAttentionTwo</code>\u00a0class is designed to be memory-efficient and optionally supports parallel computation and mixed precision for improved performance.</p> <ul> <li> <p>The\u00a0<code>parallel</code>\u00a0parameter allows the computation to be performed in parallel across multiple GPUs. This can significantly speed up the computation for large models or large datasets.</p> </li> <li> <p>The\u00a0<code>mixed_precision</code>\u00a0parameter allows the computation to be performed in mixed precision. This means that some operations are performed in lower precision (e.g., float16) and some in higher precision (e.g., float32). This can significantly speed up the computation and reduce memory usage on modern GPUs that support mixed precision.</p> </li> <li> <p>The\u00a0<code>q_bucket_size</code>\u00a0and\u00a0<code>k_bucket_size</code>\u00a0parameters control the bucket size for the query and key in the attention mechanism, respectively. These parameters can be used to trade off between memory usage and computational efficiency. Larger bucket sizes can be more memory-efficient but may also be slower.</p> </li> </ul>"},{"location":"zeta/nn/attention/flash2/#common-issues","title":"Common Issues","text":"<ul> <li> <p>If you encounter out-of-memory errors, you can try reducing the\u00a0<code>q_bucket_size</code>\u00a0and\u00a0<code>k_bucket_size</code>\u00a0parameters, or enabling mixed precision computation by setting\u00a0<code>mixed_precision=True</code>.</p> </li> <li> <p>If you encounter slow computation, you can try increasing the\u00a0<code>q_bucket_size</code>\u00a0and\u00a0<code>k_bucket_size</code>\u00a0parameters, or enabling parallel computation by setting\u00a0<code>parallel=True</code>\u00a0(if you have multiple GPUs available).</p> </li> </ul>"},{"location":"zeta/nn/attention/flash2/#references-and-resources","title":"References and Resources","text":"<ul> <li> <p>Attention Is All You Need: This is the original paper that introduced the concept of attention in deep learning.</p> </li> <li> <p>PyTorch Documentation: The official PyTorch documentation provides detailed information about the PyTorch library and its modules.</p> </li> <li> <p>Efficient Attention: Attention with Linear Complexities: This paper introduces the concept of bucketing in the attention mechanism to improve memory efficiency.</p> </li> <li> <p>Mixed Precision Training: This paper introduces the concept of mixed precision training, which can significantly speed up computation and reduce memory usage on modern GPUs.</p> </li> <li> <p>PyTorch Tutorials: The official PyTorch tutorials provide many examples of how to use PyTorch for various tasks.</p> </li> </ul> <p>-</p>"},{"location":"zeta/nn/attention/flash_attention/","title":"FlashAttention","text":"<p>The FlashAttention module performs efficient attention computations, specifically designed for leveraging hardware capabilities on certain NVIDIA GPUs. It offers the option to perform \"flash\" attention which can be computationally faster on specific GPU architectures.</p>"},{"location":"zeta/nn/attention/flash_attention/#class-definition","title":"Class Definition:","text":"<pre><code>class FlashAttention(nn.Module):\n</code></pre>"},{"location":"zeta/nn/attention/flash_attention/#parameters","title":"Parameters:","text":"<ul> <li><code>causal</code> (bool, optional): Determines whether to apply causal masking. Default: False.</li> <li><code>dropout</code> (float, optional): Dropout probability. Default: 0.</li> <li><code>flash</code> (bool, optional): Whether to use flash attention. Requires PyTorch version 2.0 or above. Default: True.</li> </ul>"},{"location":"zeta/nn/attention/flash_attention/#methods","title":"Methods:","text":""},{"location":"zeta/nn/attention/flash_attention/#__init__self-causalfalse-dropout0-flashtrue","title":"<code>__init__(self, causal=False, dropout=0., flash=True)</code>","text":"<p>Initializes the FlashAttention module.</p>"},{"location":"zeta/nn/attention/flash_attention/#get_maskself-i-j-device","title":"<code>get_mask(self, i, j, device)</code>","text":"<p>Generates a mask for attention computation.</p>"},{"location":"zeta/nn/attention/flash_attention/#parameters_1","title":"Parameters:","text":"<ul> <li><code>i</code> (int): Length of the query sequence.</li> <li><code>j</code> (int): Length of the key sequence.</li> <li><code>device</code> (torch.device): Device to place the mask tensor.</li> </ul>"},{"location":"zeta/nn/attention/flash_attention/#returns","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Mask tensor of shape <code>(i, j)</code>.</li> </ul>"},{"location":"zeta/nn/attention/flash_attention/#flash_attnself-q-k-v-masknone-attn_biasnone","title":"<code>flash_attn(self, q, k, v, mask=None, attn_bias=None)</code>","text":"<p>Performs flash attention computation.</p>"},{"location":"zeta/nn/attention/flash_attention/#parameters_2","title":"Parameters:","text":"<ul> <li><code>q</code> (torch.Tensor): Query tensor of shape <code>(batch, heads, q_len, dim)</code>.</li> <li><code>k</code> (torch.Tensor): Key tensor of shape <code>(batch, heads, k_len, dim)</code>.</li> <li><code>v</code> (torch.Tensor): Value tensor of shape <code>(batch, heads, v_len, dim)</code>.</li> <li><code>mask</code> (torch.Tensor, optional): Mask tensor of shape <code>(batch, heads, q_len, k_len)</code>. Default: None.</li> <li><code>attn_bias</code> (torch.Tensor, optional): Attention bias tensor of shape <code>(batch, heads, q_len, k_len)</code>. Default: None.</li> </ul>"},{"location":"zeta/nn/attention/flash_attention/#returns_1","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Output tensor of shape <code>(batch, heads, q_len, dim)</code>.</li> </ul>"},{"location":"zeta/nn/attention/flash_attention/#forwardself-q-k-v-masknone-attn_biasnone","title":"<code>forward(self, q, k, v, mask=None, attn_bias=None)</code>","text":"<p>Performs the attention computation using einstein notation.</p>"},{"location":"zeta/nn/attention/flash_attention/#parameters_3","title":"Parameters:","text":"<ul> <li><code>q</code> (torch.Tensor): Query tensor of shape <code>(batch, heads, q_len, dim)</code>.</li> <li><code>k</code> (torch.Tensor): Key tensor of shape <code>(batch, heads, k_len, dim)</code>.</li> <li><code>v</code> (torch.Tensor): Value tensor of shape <code>(batch, heads, v_len, dim)</code>.</li> <li><code>mask</code> (torch.Tensor, optional): Mask tensor of shape <code>(batch, heads, q_len, k_len)</code>. Default: None.</li> <li><code>attn_bias</code> (torch.Tensor, optional): Attention bias tensor of shape <code>(batch, heads, q_len, k_len)</code>. Default: None.</li> </ul>"},{"location":"zeta/nn/attention/flash_attention/#returns_2","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Attention output tensor.</li> </ul>"},{"location":"zeta/nn/attention/flash_attention/#usage-examples","title":"Usage Examples:","text":"<ol> <li> <p>Basic Usage: <pre><code>from zeta.nn import FlashAttention\n\nattn_module = FlashAttention()\noutput = attn_module(query_tensor, key_tensor, value_tensor)\n</code></pre></p> </li> <li> <p>Using Flash Attention with Masking: <pre><code>from zeta.nn import FlashAttention\n\nattn_module = FlashAttention(flash=True)\nmask = attn_module.get_mask(query_length, key_length, device)\noutput = attn_module(query_tensor, key_tensor, value_tensor, mask=mask)\n</code></pre></p> </li> <li> <p>Using Causal Flash Attention with Dropout: <pre><code>from zeta.nn import FlashAttention\n\nattn_module = FlashAttention(causal=True, dropout=0.1, flash=True)\noutput = attn_module(query_tensor, key_tensor, value_tensor)\n</code></pre></p> </li> </ol>"},{"location":"zeta/nn/attention/flash_attention/#additional-tips","title":"Additional Tips:","text":"<ul> <li>The <code>FlashAttention</code> module is optimized for NVIDIA A100 GPUs. On these GPUs, using <code>flash=True</code> is recommended for faster computation.</li> <li>Ensure that PyTorch version is 2.0 or above when enabling flash attention.</li> <li>The mask generated using <code>get_mask</code> method is useful for attention computations where certain positions need to be masked out.</li> </ul>"},{"location":"zeta/nn/attention/flash_attention/#references","title":"References:","text":"<ul> <li>Original Attention Mechanism: Attention Is All You Need</li> </ul>"},{"location":"zeta/nn/attention/local/","title":"<code>LocalAttention</code> Module Documentation","text":""},{"location":"zeta/nn/attention/local/#overview-and-introduction","title":"Overview and Introduction","text":"<p>The <code>LocalAttention</code> module provides a mechanism to perform local attention operations. Unlike global attention where every token can attend to every other token, in local attention each token can only attend to a subset of tokens within a defined window. This reduces the computational cost and captures the local structure in sequences like text or time-series data.</p> <p>Key terms:</p> <ul> <li> <p>Local Attention: A type of attention mechanism where a token attends only to a subset of tokens within a specified window.</p> </li> <li> <p>Causal Attention: Ensures that an output token at time <code>t</code> can only attend to input tokens at times <code>&lt;= t</code>.</p> </li> <li> <p>Rotary Positional Embeddings: A technique for incorporating sequence position information without the need for additional position-specific parameters.</p> </li> </ul>"},{"location":"zeta/nn/attention/local/#class-definition","title":"Class Definition","text":"<pre><code>class LocalAttention(nn.Module):\n    ...\n</code></pre>"},{"location":"zeta/nn/attention/local/#parameters","title":"Parameters","text":"<ul> <li> <p><code>window_size</code>: (int) The size of the attention window.</p> </li> <li> <p><code>causal</code>: (bool, optional) If set to <code>True</code>, ensures causal attention. Default: <code>False</code>.</p> </li> <li> <p><code>look_backward</code>: (int, optional) How many positions to look backward from the current position. Default: <code>1</code>.</p> </li> <li> <p><code>look_forward</code>: (int, optional) How many positions to look forward from the current position. Default: <code>None</code> which implies 0 if causal is <code>True</code>.</p> </li> <li> <p><code>dropout</code>: (float, optional) Dropout rate for attention weights. Default: <code>0.</code>.</p> </li> <li> <p><code>shared_qk</code>: (bool, optional) If set to <code>True</code>, the query and key are the same. Useful for certain types of attention mechanisms. Default: <code>False</code>.</p> </li> <li> <p><code>rel_pos_emb_config</code>: (Optional) Deprecated. Configuration for the relative positional embeddings.</p> </li> <li> <p><code>dim</code>: (int, optional) Dimension of embeddings. Only needed if <code>rel_pos_emb_config</code> is not provided.</p> </li> <li> <p><code>autopad</code>: (bool, optional) If set to <code>True</code>, sequence will be automatically padded to be divisible by the window size. Default: <code>False</code>.</p> </li> <li> <p><code>exact_windowsize</code>: (bool, optional) Ensures exact window size for non-causal attention. Default: <code>False</code>.</p> </li> <li> <p><code>scale</code>: (Optional) Scaling factor for the queries.</p> </li> <li> <p><code>use_rotary_pos_emb</code>: (bool, optional) If set to <code>True</code>, rotary positional embeddings will be used. Default: <code>True</code>.</p> </li> <li> <p><code>use_xpos</code>: (bool, optional) If set to <code>True</code>, allows for extrapolation of window sizes. Requires <code>use_rotary_pos_emb</code> to be <code>True</code>. Default: <code>False</code>.</p> </li> <li> <p><code>xpos_scale_base</code>: (Optional) Base scaling factor for extrapolated window sizes.</p> </li> </ul>"},{"location":"zeta/nn/attention/local/#forward-method","title":"Forward Method","text":""},{"location":"zeta/nn/attention/local/#parameters_1","title":"Parameters","text":"<ul> <li> <p><code>q</code>: (Tensor) The query tensor.</p> </li> <li> <p><code>k</code>: (Tensor) The key tensor.</p> </li> <li> <p><code>v</code>: (Tensor) The value tensor.</p> </li> <li> <p><code>mask</code>: (Optional[Tensor]) A mask tensor for the keys. Can also be passed as <code>input_mask</code>.</p> </li> <li> <p><code>input_mask</code>: (Optional[Tensor]) Another way to pass the mask tensor for keys.</p> </li> <li> <p><code>attn_bias</code>: (Optional[Tensor]) Additional biases to add to the attention scores.</p> </li> <li> <p><code>window_size</code>: (Optional[int]) If provided, this window size will override the default window size defined during initialization.</p> </li> </ul>"},{"location":"zeta/nn/attention/local/#returns","title":"Returns","text":"<ul> <li><code>out</code>: (Tensor) The output tensor after the attention operation.</li> </ul>"},{"location":"zeta/nn/attention/local/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>LocalAttention</code> module is designed to efficiently compute attention values over a local window. When the <code>forward</code> method is called, the module performs the following steps:</p> <ol> <li>Reshape and, if required, autopad the input tensors.</li> <li>Calculate the attention scores between the queries and keys.</li> <li>Optionally apply causal masking and other types of masking.</li> <li>Calculate the softmax over the attention scores.</li> <li>Use the attention scores to weight the value tensor and produce the output.</li> </ol>"},{"location":"zeta/nn/attention/local/#usage-example","title":"Usage Example:","text":"<pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta import LocalAttention\n\nq = torch.randn(1, 100, 32)\nk = torch.randn(1, 100, 32)\nv = torch.randn(1, 100, 32)\n\nlocal_attn = LocalAttention(window_size=5, causal=True, dim=32)\nout = local_attn(q, k, v)\n</code></pre>"},{"location":"zeta/nn/attention/local/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li> <p>When using <code>LocalAttention</code> with <code>causal=True</code>, ensure that <code>look_forward</code> is not set to a value greater than 0.</p> </li> <li> <p>The <code>autopad</code> option can be helpful when dealing with sequences of arbitrary lengths, but may introduce padding tokens.</p> </li> </ul>"},{"location":"zeta/nn/attention/local/#references-and-resources","title":"References and Resources","text":"<p>For a deeper understanding of attention mechanisms and their local variants:</p> <ul> <li> <p>Vaswani, A. et al. (2017) \"Attention Is All You Need\". Advances in Neural Information Processing Systems 30.</p> </li> <li> <p>Liu, Z. et al. (2018) \"Generating Wikipedia by Summarizing Long Sequences\". International Conference on Learning Representations.</p> </li> </ul>"},{"location":"zeta/nn/attention/local/#simple-mathematical-formula","title":"Simple Mathematical Formula","text":"<p>Given a sequence of length ( n ), each token attends to tokens within a window of size ( w ) around it. The attention scores ( A \\</p> <p>) between query ( q ) and key ( k ) are given by:</p> <p>[ A = \\text{softmax} \\left( \\frac{q \\times k^T}{\\sqrt{d}} \\right) ]</p> <p>Where ( d ) is the dimension of the embeddings.</p>"},{"location":"zeta/nn/attention/local/#conclusion","title":"Conclusion","text":"<p>The <code>LocalAttention</code> module provides a computationally efficient way to apply attention mechanisms to local windows within a sequence. By using parameters such as <code>window_size</code> and <code>causal</code>, users can fine-tune the attention behavior to fit their specific needs. The module's flexible design and variety of options make it a valuable tool for many sequence modeling tasks.</p>"},{"location":"zeta/nn/attention/localmha/","title":"LocalMHA: Local Multi-Head Attention for PyTorch","text":""},{"location":"zeta/nn/attention/localmha/#overview","title":"Overview","text":"<p>The <code>LocalMHA</code> module is a local multi-head attention mechanism designed to process sequences in smaller, fixed-size windows, allowing it to handle long sequences more efficiently. This module is especially useful when working with long sequences where global attention mechanisms become computationally expensive. It combines local attention with the power of multi-head attention to capture information from different representation subspaces.</p> <p>Key Concepts:</p> <ul> <li> <p>Local Attention: Instead of attending to all positions in the input sequence, local attention restricts the attention to a small fixed-sized window around each position.</p> </li> <li> <p>Multi-Head Attention: The input is split into multiple heads, allowing the network to attend to information from different representation subspaces simultaneously.</p> </li> </ul>"},{"location":"zeta/nn/attention/localmha/#class-definition","title":"Class Definition","text":"<pre><code>class LocalMHA(nn.Module):\n</code></pre>"},{"location":"zeta/nn/attention/localmha/#parameters","title":"Parameters:","text":"<ul> <li> <p><code>dim (int)</code>: Dimensionality of the input sequence.</p> </li> <li> <p><code>window_size (int)</code>: The size of the local attention window. The module will attend to this fixed-size window around each position.</p> </li> <li> <p><code>dim_head (int, optional)</code>: Dimensionality of each attention head. Default is 64.</p> </li> <li> <p><code>heads (int, optional)</code>: Number of attention heads. Default is 8.</p> </li> <li> <p><code>dropout (float, optional)</code>: Dropout probability applied after the attention mechanism. Default is 0.0.</p> </li> <li> <p><code>causal (bool, optional)</code>: If set to <code>True</code>, the attention mechanism will be causal, ensuring that each position only attends to previous positions. Default is <code>False</code>.</p> </li> <li> <p><code>prenorm (bool, optional)</code>: If set to <code>True</code>, layer normalization is applied before the multi-head attention mechanism. Default is <code>False</code>.</p> </li> <li> <p><code>qk_rmsnorm (bool, optional)</code>: If set to <code>True</code>, root mean square normalization is applied to the query and key tensors. Default is <code>False</code>.</p> </li> <li> <p><code>qk_scale (int, optional)</code>: Scaling factor for queries and keys when <code>qk_rmsnorm</code> is set to <code>True</code>. Default is 8.</p> </li> <li> <p><code>use_xpos (bool, optional)</code>: If set to <code>True</code>, the attention mechanism uses relative positional embeddings. Default is <code>False</code>.</p> </li> <li> <p><code>xpos_scale_base (float, optional)</code>: Base scaling factor for relative positional embeddings. If <code>None</code>, it defaults to the square root of the dimension of the model. Only used when <code>use_xpos</code> is <code>True</code>.</p> </li> <li> <p><code>exact_windowsize (bool, optional)</code>: If set to <code>True</code>, the attention window size is strictly adhered to, without any additional padding. Default is <code>True</code>.</p> </li> </ul>"},{"location":"zeta/nn/attention/localmha/#method-forward","title":"Method: <code>forward</code>","text":"<p>This method performs the forward pass of the <code>LocalMHA</code> module.</p>"},{"location":"zeta/nn/attention/localmha/#parameters_1","title":"Parameters:","text":"<ul> <li> <p><code>x (torch.Tensor)</code>: The input tensor with shape <code>[batch_size, sequence_length, dim]</code>.</p> </li> <li> <p><code>mask (torch.Tensor, optional)</code>: A boolean mask tensor with shape <code>[batch_size, sequence_length]</code>. Positions with <code>True</code> values will be masked and won't be attended to.</p> </li> <li> <p><code>attn_bias (torch.Tensor, optional)</code>: Additional bias to add to the attention scores before softmax. </p> </li> </ul>"},{"location":"zeta/nn/attention/localmha/#returns","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: The output tensor after local multi-head attention with shape <code>[batch_size, sequence_length, dim]</code>.</li> </ul>"},{"location":"zeta/nn/attention/localmha/#example-usage","title":"Example Usage","text":"<pre><code>from torch import tensor\n\nfrom zeta import LocalMHA\n\n# Sample data\nx = tensor(\n    [[...], [...], ...]\n)  # Example input tensor with shape [batch_size, sequence_length, dim]\n\n# Initialize the LocalMHA module\nlocal_mha = LocalMHA(dim=512, window_size=5)\n\n# Forward pass\noutput = local_mha(x)\n</code></pre>"},{"location":"zeta/nn/attention/localmha/#mathematical-formula","title":"Mathematical Formula","text":"<p>For a given input ( x ):</p> <ol> <li>Linearly project ( x ) into queries ( Q ), keys ( K ), and values ( V ).</li> <li>If <code>qk_rmsnorm</code> is <code>True</code>, apply RMS normalization to ( Q ) and ( K ).</li> <li>For each position ( i ) in ( x ), compute attention scores with all positions in the window around ( i ).</li> <li>Apply softmax to the scores, then compute the attention output as a weighted sum of ( V ) based on these scores.</li> <li>Finally, concatenate all head outputs and linearly project to get the final output.</li> </ol>"},{"location":"zeta/nn/attention/localmha/#additional-information","title":"Additional Information","text":"<p>The <code>LocalMHA</code> module provides a balance between computational efficiency and the ability to capture long-range dependencies. While it restricts attention to local windows, the use of multi-head attention allows it to attend to different features within that window. The optional use of RMS normalization and relative positional embeddings further extends its capabilities.</p>"},{"location":"zeta/nn/attention/localmha/#references","title":"References","text":"<p>For a deeper understanding of multi-head attention, see the original Transformer paper. For details on local attention, you might refer to relevant literature on efficient transformers or localized attention mechanisms.</p>"},{"location":"zeta/nn/attention/mixture_of_attention/","title":"MixtureOfAttention","text":""},{"location":"zeta/nn/attention/mixture_of_attention/#mixtureofattention-class-in-the-zeta-library","title":"<code>MixtureOfAttention</code> Class in the Zeta Library","text":""},{"location":"zeta/nn/attention/mixture_of_attention/#overview","title":"Overview","text":"<p><code>MixtureOfAttention</code> is a powerful and versatile attention mechanism in the Zeta library. It uniquely combines the ideas of dynamic routing and local attention. The class enables the model to focus on different portions of the input data by creating multiple routes for queries and key-values. This makes it particularly effective for tasks that require flexible attention over the input data.</p>"},{"location":"zeta/nn/attention/mixture_of_attention/#class-definition","title":"Class Definition","text":""},{"location":"zeta/nn/attention/mixture_of_attention/#mixtureofattention","title":"MixtureOfAttention","text":"<pre><code>class MixtureOfAttention(nn.Module):\n</code></pre>"},{"location":"zeta/nn/attention/mixture_of_attention/#parameters","title":"Parameters:","text":"<ul> <li> <p>dim (int): The dimension of the input tensor.</p> </li> <li> <p>num_routed_queries (int): Number of routed queries.</p> </li> <li> <p>num_routed_key_values (int): Number of routed key-value pairs.</p> </li> <li> <p>dim_context (int, optional): The dimension of the context tensor. Defaults to the value of <code>dim</code>.</p> </li> <li> <p>local_attn (bool, optional): Whether to use local attention. Defaults to False.</p> </li> <li> <p>local_attn_window_size (int, optional): The window size for local attention if <code>local_attn</code> is set to True.</p> </li> <li> <p>num_experts (int): Number of expert routes.</p> </li> <li> <p>dim_head (int, optional): Dimension of each attention head. Defaults to 64.</p> </li> <li> <p>heads (int, optional): Number of attention heads. Defaults to 8.</p> </li> <li> <p>dropout (float, optional): Dropout probability. Defaults to 0.0.</p> </li> <li> <p>use_triton (bool, optional): Whether to use Triton for optimized computation. Defaults to True.</p> </li> <li> <p>flash_attn (bool, optional): Whether to use flash attention mechanism. Defaults to True.</p> </li> <li> <p>prenorm (bool, optional): Whether to use pre-normalization in attention. Defaults to True.</p> </li> <li> <p>average_routed (bool, optional): Whether to average the routed queries and key-values. Defaults to False.</p> </li> <li> <p>kwargs: Additional keyword arguments.</p> </li> </ul>"},{"location":"zeta/nn/attention/mixture_of_attention/#functionality-and-usage","title":"Functionality and Usage","text":"<p><code>MixtureOfAttention</code> offers the ability to combine different attention mechanisms, enabling it to better adapt to the task at hand. Its core functionality hinges on the routing mechanism, which dynamically determines which parts of the input should be focused on. When combined with local attention, this mechanism allows the model to concentrate on both local and global features in the data.</p>"},{"location":"zeta/nn/attention/mixture_of_attention/#usage-examples","title":"Usage Examples:","text":"<p>1. Basic usage with default parameters:</p> <pre><code>import torch\n\nfrom zeta.nn import MixtureOfAttention\n\ndim = 512\nmodel = MixtureOfAttention(\n    dim, num_routed_queries=100, num_routed_key_values=100, num_experts=4\n)\nx = torch.rand(16, 50, dim)\noutput = model(x)\n</code></pre> <p>2. Using local attention:</p> <pre><code>import torch\n\nfrom zeta.nn import MixtureOfAttention\n\ndim = 512\nmodel = MixtureOfAttention(\n    dim,\n    num_routed_queries=100,\n    num_routed_key_values=100,\n    num_experts=4,\n    local_attn=True,\n    local_attn_window_size=5,\n)\nx = torch.rand(16, 50, dim)\noutput = model(x)\n</code></pre> <p>3. Using pre-normalization and dropout:</p> <pre><code>import torch\n\nfrom zeta.nn import MixtureOfAttention\n\ndim = 512\nmodel = MixtureOfAttention(\n    dim,\n    num_routed_queries=100,\n    num_routed_key_values=100,\n    num_experts=4,\n    prenorm=True,\n    dropout=0.1,\n)\nx = torch.rand(16, 50, dim)\noutput = model(x)\n</code></pre>"},{"location":"zeta/nn/attention/mixture_of_attention/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>Given an input tensor ( x ) of shape ( (batch_size, seq_len, dim) ), the model first determines which tokens should be routed using the <code>query_router</code> and <code>key_value_router</code>. These routed tokens act as \"experts\" and are processed using the attention mechanism.</p> <p>If using local attention, for each token in ( x ), a local window of size <code>local_attn_window_size</code> is considered around it.</p> <p>For the routed tokens (either global or local), the attention scores are computed using:</p> <p>[ \\text{Attention}(Q, K) = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V ]</p> <p>Where ( Q ), ( K ), and ( V ) are the query, key, and value matrices, and ( d_k ) is the dimension of the keys.</p> <p>The final output is a combination of the attention outputs from these different mechanisms, based on the configuration.</p>"},{"location":"zeta/nn/attention/mixture_of_attention/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li> <p>If both local attention and global attention are enabled, make sure to provide a valid <code>local_attn_window_size</code>.</p> </li> <li> <p>Using <code>use_triton=True</code> can optimize the computation using the Triton framework, but ensure you have Triton support in your environment.</p> </li> <li> <p>The <code>flash_attn</code> mechanism can further enhance attention computation speed.</p> </li> </ul>"},{"location":"zeta/nn/attention/mixture_of_attention/#references-and-resources","title":"References and Resources","text":"<ul> <li> <p>Attention Is All You Need - The original Transformer paper which introduced the multi-head attention mechanism.</p> </li> <li> <p>Local Attention - A paper discussing the benefits of local attention.</p> </li> <li> <p>Triton - An open-source domain-specific language to help researchers write fast GPU code. </p> </li> </ul> <p>For more details and advanced usage scenarios, please refer to the official Zeta library documentation.</p>"},{"location":"zeta/nn/attention/mixture_of_attention_ar/","title":"MixtureOfAutoregressiveAttention","text":"<p>The <code>MixtureOfAutoregressiveAttention</code> module provides a versatile attention mechanism that combines the strength of local multi-head attention with a flexible routing mechanism, enabling attention to focus on specific parts of the input sequence depending on the context. The mixture strategy optimizes the computation while retaining the benefits of the global attention perspective.</p>"},{"location":"zeta/nn/attention/mixture_of_attention_ar/#overview","title":"Overview:","text":"<p>This module integrates local multi-head attention with coordinate descent-based routing, which chooses certain query and key-value pairs for further processing, allowing the model to focus on specific parts of the sequence that are more relevant for the task at hand. </p>"},{"location":"zeta/nn/attention/mixture_of_attention_ar/#key-concepts","title":"Key Concepts:","text":"<ul> <li>Local Attention: A mechanism where each position can attend to a restricted window of positions around it.</li> <li>Routing: Mechanism to select specific tokens based on importance for specialized processing.</li> <li>Coordinate Descent Routing: A strategy to decide which tokens should be routed for specialized processing.</li> <li>Null Tokens: Default tokens used for positions that weren't routed to any expert.</li> </ul>"},{"location":"zeta/nn/attention/mixture_of_attention_ar/#class-definition","title":"Class Definition:","text":"<pre><code>class MixtureOfAutoregressiveAttention(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_routed_queries: int,\n        num_routed_key_values: int,\n        local_attn_window_size: int,\n        routed_window_size: Optional[int] = None,\n        num_experts: int = 2,\n        dim_head: int = 64,\n        heads: int = 8,\n        dropout: float = 0.0,\n        use_triton: bool = False,\n        flash_attn: bool = True,\n        prenorm: bool = True,\n        average_routed: bool = False,\n        **kwargs,\n    ):\n        ...\n</code></pre>"},{"location":"zeta/nn/attention/mixture_of_attention_ar/#parameters","title":"Parameters:","text":"<ul> <li><code>dim</code> (int): Dimensionality of the input sequence.</li> <li><code>num_routed_queries</code> (int): Number of queries to be routed.</li> <li><code>num_routed_key_values</code> (int): Number of key-values to be routed.</li> <li><code>local_attn_window_size</code> (int): Window size for local attention.</li> <li><code>routed_window_size</code> (int, optional): Window size for routing. Defaults to <code>local_attn_window_size</code>.</li> <li><code>num_experts</code> (int, optional): Number of experts. Defaults to 2.</li> <li><code>dim_head</code> (int, optional): Dimensionality of each attention head. Defaults to 64.</li> <li><code>heads</code> (int, optional): Number of attention heads. Defaults to 8.</li> <li><code>dropout</code> (float, optional): Dropout probability. Defaults to 0.</li> <li><code>use_triton</code> (bool, optional): Flag to use Triton for optimization. Defaults to False.</li> <li><code>flash_attn</code> (bool, optional): Flag to use flash attention mechanism. Defaults to True.</li> <li><code>prenorm</code> (bool, optional): Flag for pre-normalization. Defaults to True.</li> <li><code>average_routed</code> (bool, optional): Whether to average the routed tokens or not. Defaults to False.</li> </ul>"},{"location":"zeta/nn/attention/mixture_of_attention_ar/#methods","title":"Methods:","text":""},{"location":"zeta/nn/attention/mixture_of_attention_ar/#forward","title":"forward","text":"<pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    rotary_emb: Optional[torch.Tensor] = None,\n    num_routed_queries: Optional[int] = None,\n    num_routed_key_values: Optional[int] = None,\n) -&gt; torch.Tensor:\n    ...\n</code></pre> <ul> <li><code>x</code> (torch.Tensor): Input tensor of shape <code>(batch_size, sequence_length, dim)</code>.</li> <li><code>rotary_emb</code> (torch.Tensor, optional): Rotary embeddings. Defaults to None.</li> <li><code>num_routed_queries</code> (int, optional): Overrides the number of queries to be routed. Defaults to class defined value.</li> <li><code>num_routed_key_values</code> (int, optional): Overrides the number of key-values to be routed. Defaults to class defined value.</li> </ul>"},{"location":"zeta/nn/attention/mixture_of_attention_ar/#examples","title":"Examples:","text":"<ol> <li>Basic Usage</li> </ol> <pre><code>from zeta.nn import MixtureOfAutoregressiveAttention\n\nattention_layer = MixtureOfAutoregressiveAttention(\n    dim=512, num_routed_queries=5, num_routed_key_values=5, local_attn_window_size=32\n)\nx = torch.randn(10, 60, 512)\nout = attention_layer(x)\n</code></pre> <ol> <li>With Rotary Embeddings</li> </ol> <pre><code>rotary_emb = torch.randn(60, 512)\nout = attention_layer(x, rotary_emb=rotary_emb)\n</code></pre> <ol> <li>Changing Routing Parameters</li> </ol> <pre><code>out = attention_layer(x, num_routed_queries=3, num_routed_key_values=7)\n</code></pre>"},{"location":"zeta/nn/attention/mixture_of_attention_ar/#mathematical-description","title":"Mathematical Description:","text":"<p>Let (x) be the input sequence and (w) be the attention window size. The local attention output (L) for (x) is computed based on a restricted window of positions around each position in (x). </p> <p>The queries and key-values are then routed based on their importance scores to produce the routed attention output (R). </p> <p>The final output (O) is a combination of the local attention output (L) and the routed attention output (R):</p> <p>[ O = f(L, R) ]</p> <p>where (f) is a combination function, which</p> <p>might be a weighted sum, concatenation, or other methods.</p>"},{"location":"zeta/nn/attention/mixture_of_attention_ar/#use-cases","title":"Use Cases:","text":"<p>This module is best suited for sequence-to-sequence models where a mix of local and global attention is required. It could be advantageous in applications like:</p> <ul> <li>Long document summarization</li> <li>Language modeling</li> <li>Machine translation</li> </ul>"},{"location":"zeta/nn/attention/mixture_of_attention_ar/#conclusion","title":"Conclusion:","text":"<p>The <code>MixtureOfAutoregressiveAttention</code> module provides a combination of local attention and flexible routing, allowing models to focus on specific parts of an input sequence that are contextually relevant. This can lead to more efficient computation and potentially better performance on sequence processing tasks.</p>"},{"location":"zeta/nn/attention/multihead/","title":"Multihead Attention Documentation for Zeta Library","text":""},{"location":"zeta/nn/attention/multihead/#introduction","title":"Introduction","text":"<p><code>MultiheadAttention</code> is a module in the Zeta library that provides multi-head attention mechanism. This mechanism enables the model to focus on different parts of the input sequence simultaneously. It's widely used in models such as transformers for capturing various aspects of information in the input.</p>"},{"location":"zeta/nn/attention/multihead/#purpose","title":"Purpose","text":"<p>The purpose of the <code>MultiheadAttention</code> module is to allow joint information representation from different subspaces of the input sequence. This results in capturing a richer context when modeling sequences.</p>"},{"location":"zeta/nn/attention/multihead/#architecture","title":"Architecture","text":"<p>The <code>MultiheadAttention</code> class extends from the <code>nn.Module</code> base class. Internally, it uses linear transformations for keys, values, and queries (<code>k_proj</code>, <code>v_proj</code>, <code>q_proj</code>). These projections are wrapped using the <code>MultiwayWrapper</code>. It also utilizes layer normalization (<code>inner_attn_ln</code>) and optionally uses relative positional embeddings (<code>xpos</code>).</p>"},{"location":"zeta/nn/attention/multihead/#class-definition","title":"Class Definition","text":"<pre><code>class zeta.nn.MultiheadAttention(nn.Module):\n</code></pre>"},{"location":"zeta/nn/attention/multihead/#parameters","title":"Parameters:","text":"<ul> <li><code>args</code>: General arguments passed for configuring the module.</li> <li><code>embed_dim</code> (int): Total dimension of the model.</li> <li><code>num_heads</code> (int): Number of parallel attention heads. The embed_dim will be split across num_heads.</li> <li><code>dropout</code> (float): Dropout probability. Default: 0.0.</li> <li><code>self_attention</code> (bool): Whether to apply self attention. Only one of <code>self_attention</code> or <code>encoder_decoder_attention</code> can be True. Default: False.</li> <li><code>encoder_decoder_attention</code> (bool): Whether to apply encoder-decoder attention. Only one of <code>self_attention</code> or <code>encoder_decoder_attention</code> can be True. Default: False.</li> <li><code>subln</code> (bool): If True, applies layer normalization after self attention. Default: False.</li> </ul>"},{"location":"zeta/nn/attention/multihead/#methods","title":"Methods:","text":""},{"location":"zeta/nn/attention/multihead/#reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Reinitialize the parameters of the attention module.</p>"},{"location":"zeta/nn/attention/multihead/#forwardquery-key-value","title":"<code>forward(query, key, value, ...)</code>","text":"<p>Computes the forward pass of the attention mechanism.</p> <ul> <li>Parameters:</li> <li><code>query</code> (Tensor): The query tensor.</li> <li><code>key</code> (Tensor): The key tensor.</li> <li><code>value</code> (Tensor): The value tensor.</li> <li> <p>Other arguments including <code>incremental_state</code>, <code>key_padding_mask</code>, <code>attn_mask</code>, <code>rel_pos</code>, and <code>is_first_step</code>.</p> </li> <li> <p>Returns:</p> </li> <li><code>attn</code> (Tensor): The computed attention tensor.</li> <li><code>attn_weights</code> (Tensor): The attention weights.</li> </ul>"},{"location":"zeta/nn/attention/multihead/#mathematical-formulation","title":"Mathematical Formulation:","text":"<p>Given a query ( Q ), key ( K ), and value ( V ), the multihead attention mechanism is mathematically represented as:</p> <p>[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V ]</p> <p>Where ( d_k ) is the dimension of the key.</p>"},{"location":"zeta/nn/attention/multihead/#usage-examples","title":"Usage Examples:","text":""},{"location":"zeta/nn/attention/multihead/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta.nn import MultiheadAttention\n\nargs = ...  # Some configuration\nattention = MultiheadAttention(\n    args, embed_dim=512, num_heads=8, dropout=0.1, self_attention=True\n)\nquery = torch.rand((32, 10, 512))\nkey = torch.rand((32, 10, 512))\nvalue = torch.rand((32, 10, 512))\n\nattn, attn_weights = attention(query, key, value)\n</code></pre>"},{"location":"zeta/nn/attention/multihead/#example-2-with-masking","title":"Example 2: With Masking","text":"<pre><code>import torch\n\nfrom zeta.nn import MultiheadAttention\n\nargs = ...  # Some configuration\nattention = MultiheadAttention(\n    args, embed_dim=512, num_heads=8, dropout=0.1, self_attention=True\n)\nquery = torch.rand((32, 10, 512))\nkey = torch.rand((32, 10, 512))\nvalue = torch.rand((32, 10, 512))\nattn_mask = torch.ones((10, 10)).triu_() * -1e9  # Upper triangular mask\n\nattn, attn_weights = attention(query, key, value, attn_mask=attn_mask)\n</code></pre>"},{"location":"zeta/nn/attention/multihead/#example-3-encoder-decoder-attention","title":"Example 3: Encoder-Decoder Attention","text":"<pre><code>import torch\n\nfrom zeta.nn import MultiheadAttention\n\nargs = ...  # Some configuration\nattention = MultiheadAttention(\n    args, embed_dim=512, num_heads=8, dropout=0.1, encoder_decoder_attention=True\n)\nquery = torch.rand((32, 10, 512))  # Decoder query\nkey = torch.rand((32, 20, 512))  # Encoder key\nvalue = torch.rand((32, 20, 512))  # Encoder value\n\nattn, attn_weights = attention(query, key, value)\n</code></pre>"},{"location":"zeta/nn/attention/multihead/#additional-tips","title":"Additional Tips:","text":"<ul> <li>For encoder-decoder attention, make sure the dimensions of the encoder and decoder tensors match the expected input sizes.</li> <li>Using masks can be helpful to prevent the attention mechanism from focusing on certain parts of the sequence, such as padding.</li> </ul>"},{"location":"zeta/nn/attention/multiquery/","title":"MultiQueryAttention","text":""},{"location":"zeta/nn/attention/multiquery/#overview-and-introduction","title":"Overview and Introduction:","text":"<p>The <code>MultiQueryAttention</code> class is a part of the Zeta library, designed to perform self-attention operations on given input data. Unlike traditional attention mechanisms that use a single query, this class leverages multiple queries to capture a broader range of context information. This class allows for various implementations of attention, including Flash, Triton, and Torch. It also provides the flexibility to choose normalization type, fully connected layer type, and offers debugging verbosity.</p>"},{"location":"zeta/nn/attention/multiquery/#class-definition","title":"Class Definition:","text":"<pre><code>class MultiQueryAttention(nn.Module):\n    \"\"\"Multi-Query self attention.\n    Using torch or triton attention implementation enables the user to also use\n    additive bias.\n    \"\"\"\n</code></pre>"},{"location":"zeta/nn/attention/multiquery/#parameters","title":"Parameters:","text":"<ul> <li><code>d_model</code> (int): Dimension of the model.</li> <li><code>heads</code> (int): Number of parallel attention heads.</li> <li><code>attn_impl</code> (str, optional): Attention implementation type, can be either 'triton', 'flash', or 'torch'. Default is 'triton'.</li> <li><code>clip_qkv</code> (Optional[float]): Clipping value for query, key, and value. If specified, qkv is clamped within the range [-clip_qkv, clip_qkv].</li> <li><code>qk_ln</code> (bool, optional): If True, layer normalization is applied to query and key.</li> <li><code>softmax_scale</code> (Optional[float]): Scale for softmax. Default value is computed as 1/sqrt(head_dim).</li> <li><code>attn_pdrop</code> (float, optional): Attention dropout probability. Default is 0.0.</li> <li><code>norm_type</code> (str, optional): Normalization type, default is 'low_precision_layernorm'.</li> <li><code>fc_type</code> (str, optional): Fully connected layer type, default is 'torch'.</li> <li><code>verbose</code> (int, optional): Verbosity level, default is 0.</li> <li><code>device</code> (Optional[str]): Device to which the tensors should be moved.</li> </ul>"},{"location":"zeta/nn/attention/multiquery/#functionality-and-usage","title":"Functionality and Usage:","text":"<p>The <code>MultiQueryAttention</code> class operates by using multiple queries to capture broader context information from given data. This is achieved through the forward method which computes the self-attention on the given inputs.</p>"},{"location":"zeta/nn/attention/multiquery/#method-forward","title":"Method: <code>forward</code>","text":"<pre><code>def forward(\n    self,\n    x,\n    past_key_value=None,\n    bias=None,\n    mask=None,\n    causal=True,\n    needs_weights=False,\n):\n</code></pre>"},{"location":"zeta/nn/attention/multiquery/#parameters_1","title":"Parameters:","text":"<ul> <li><code>x</code> (Tensor): Input tensor.</li> <li><code>past_key_value</code> (Optional): Past key and value for attention computation. Default is None.</li> <li><code>bias</code> (Optional): Additive bias for attention scores. Default is None.</li> <li><code>mask</code> (Optional): Key padding mask. Default is None.</li> <li><code>causal</code> (bool, optional): If True, a causal mask is applied to prevent information flow from future tokens. Default is True.</li> <li><code>needs_weights</code> (bool, optional): If True, attention weights are also returned. Default is False.</li> </ul>"},{"location":"zeta/nn/attention/multiquery/#returns","title":"Returns:","text":"<ul> <li><code>context</code> (Tensor): Contextualized tensor after attention computation.</li> <li><code>attn_weights</code> (Tensor, Optional): Attention weights. Only returned if <code>needs_weights</code> is True.</li> <li><code>past_key_value</code> (Tensor, Optional): New past key and value.</li> </ul>"},{"location":"zeta/nn/attention/multiquery/#usage-examples","title":"Usage Examples:","text":"<ol> <li> <p>Basic Usage: <pre><code>import torch\n\nfrom zeta.nn import MultiQueryAttention\n\n# Initialize the attention module\nattention_layer = MultiQueryAttention(d_model=512, heads=8, attn_impl=\"torch\")\n\n# Random input tensor\nx = torch.rand(16, 10, 512)  # Batch of 16, sequence length 10, embedding size 512\noutput, attn_weights, _ = attention_layer(x)\n</code></pre></p> </li> <li> <p>Using Past Key and Value: <pre><code>past_key_value = (\n    torch.rand(16, 8, 10, 64),\n    torch.rand(16, 8, 10, 64),\n)  # Past key and value for 8 heads\noutput, attn_weights, new_past_key_value = attention_layer(\n    x, past_key_value=past_key_value\n)\n</code></pre></p> </li> <li> <p>With Causal Masking and Weights: <pre><code>output, attn_weights, _ = attention_layer(x, causal=True, needs_weights=True)\n</code></pre></p> </li> </ol>"},{"location":"zeta/nn/attention/multiquery/#mathematical-formula","title":"Mathematical Formula:","text":"<p>For the self-attention mechanism, the computation involves using multiple queries (( Q )), keys (( K )), and values (( V )):</p> <p><pre><code>\\[ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{Q \\times K^T}{\\sqrt{d_k}} + \\text{Bias}\\right) \\times V \\]\n</code></pre> Where: - ( Q ), ( K ), and ( V ) are the queries, keys, and values respectively. - ( d_k ) is the dimension of the keys. - Bias is the optional additive bias.</p>"},{"location":"zeta/nn/attention/multiquery/#additional-information-and-tips","title":"Additional Information and Tips:","text":"<ul> <li>It's crucial to select the correct attention implementation (<code>attn_impl</code>) based on your needs and the hardware you're running on.</li> <li>The <code>triton</code> implementation might be faster than <code>flash</code> but can use more memory. Ensure that you have adequate GPU memory if using <code>triton</code>.</li> <li>If using the <code>torch</code> implementation, it's advisable to check if CUDA is available for GPU acceleration.</li> <li>The clipping of qkv (<code>clip_qkv</code>) can be beneficial for stability in training.</li> </ul>"},{"location":"zeta/nn/attention/multiquery/#references-and-resources","title":"References and Resources:","text":"<p>For a deeper understanding of the self-attention mechanism and its variants, you can refer to the \"Attention is All You Need\" paper by Vaswani et al., 2017.</p>"},{"location":"zeta/nn/attention/sparse_attn/","title":"<code>SparseAttention</code>","text":"<p>===============</p> <p>The\u00a0<code>SparseAttention</code>\u00a0class is a PyTorch module that implements a sparse attention mechanism. This class is part of a larger effort to make transformer models more efficient by reducing the computational complexity of the self-attention mechanism.</p>"},{"location":"zeta/nn/attention/sparse_attn/#overview","title":"Overview","text":"<p>In a standard transformer model, the self-attention mechanism computes attention scores for all pairs of tokens in a sequence, resulting in a quadratic computational complexity. This can be problematic for long sequences, as the time and memory requirements can become prohibitively large.</p> <p>The\u00a0<code>SparseAttention</code>\u00a0class addresses this issue by computing attention scores for a subset of token pairs, rather than all pairs. This results in a sparse attention matrix, which can be computed more efficiently than a full attention matrix.</p> <p>The class supports three modes of sparse attention:</p> <ul> <li>'all': All tokens attend to all other tokens (equivalent to standard self-attention).</li> <li>'local': Each token attends to a fixed window of adjacent tokens.</li> <li>'strided': Each token attends to one out of every\u00a0<code>blocksize</code>\u00a0tokens.</li> </ul>"},{"location":"zeta/nn/attention/sparse_attn/#class-definition","title":"Class Definition","text":"<pre><code>class SparseAttention(nn.Module):\n    def __init__(self, heads, attn_mode, local_attn_ctx=None, blocksize=32):\n        super(SparseAttention, self).__init__()\n        self.heads = heads\n        self.attn_mode = attn_mode\n        self.local_attn_ctx = local_attn_ctx\n        self.blocksize = blocksize\n\n    def forward(self, q, k, v):\n        return blocksparse_attention_impl(\n            q, k, v, self.heads, self.attn_mode, self.local_attn_ctx\n        )\n</code></pre>"},{"location":"zeta/nn/attention/sparse_attn/#parameters","title":"Parameters","text":"Parameter Type Description <code>heads</code> int The number of attention heads. <code>attn_mode</code> str The mode of sparse attention. Can be 'all', 'local', or 'strided'. <code>local_attn_ctx</code> int, optional The context size for local attention. Only used when\u00a0<code>attn_mode</code>\u00a0is 'local'. Default is None. <code>blocksize</code> int, optional The block size for strided attention. Only used when\u00a0<code>attn_mode</code>\u00a0is 'strided'. Default is 32."},{"location":"zeta/nn/attention/sparse_attn/#usage","title":"Usage","text":"<p>Here is an example of how to use the\u00a0<code>SparseAttention</code>\u00a0class:</p> <pre><code>import torch\n\nfrom zeta.nn.attention import SparseAttention\n\n# Define parameters\nn_batch = 4\nn_ctx = 1024\nn_embd = 256\nheads = 4\nattn_mode = \"all\"\nlocal_attn_ctx = 32\nblocksize = 32\n\n# Create input tensors\nq = torch.randn(n_batch, n_ctx, n_embd)\nk = torch.randn(n_batch, n_ctx, n_embd)\nv = torch.randn(n_batch, n_ctx, n_embd)\n\n# Create SparseAttention model\nmodel = SparseAttention(heads, attn_mode, local_attn_ctx, blocksize)\n\n# Forward pass\noutput = model(q, k, v)\n\n# Print output\nprint(output[0])\n</code></pre> <p>In this example, the\u00a0<code>SparseAttention</code>\u00a0model is created with 4 attention heads and 'all' attention mode. The input tensors\u00a0<code>q</code>,\u00a0<code>k</code>, and\u00a0<code>v</code>\u00a0are randomly initialized. The forward pass of the model is then performed, and the output is printed.</p>"},{"location":"zeta/nn/attention/sparse_attn/#note","title":"Note","text":"<p>The\u00a0<code>SparseAttention</code>\u00a0class relies on the\u00a0<code>blocksparse_attention_impl</code>\u00a0function for the actual computation of the sparse attention. This function is not defined in the provided code, so you will need to implement it yourself or import it from elsewhere. The function should take the input tensors\u00a0<code>q</code>,\u00a0<code>k</code>, and\u00a0<code>v</code>, as well as the parameters\u00a0<code>heads</code>,\u00a0<code>attn_mode</code>, and\u00a0<code>local_attn_ctx</code>, and return the output of the sparse attention computation.</p>"},{"location":"zeta/nn/biases/alibi/","title":"AlibiPositionalBias Documentation","text":""},{"location":"zeta/nn/biases/alibi/#introduction","title":"Introduction","text":"<p>The <code>AlibiPositionalBias</code> module belongs to the zeta library and plays a crucial role in handling positional bias for multi-head attention mechanisms. Specifically, it attempts to alleviate the absolute positional bias based on the number of attention heads.</p>"},{"location":"zeta/nn/biases/alibi/#class-definition","title":"Class Definition:","text":"<pre><code>class AlibiPositionalBias(nn.Module):\n</code></pre>"},{"location":"zeta/nn/biases/alibi/#parameters","title":"Parameters:","text":"<ul> <li>heads (<code>int</code>): Number of attention heads for which the slopes need to be calculated.</li> <li>total_heads (<code>int</code>): Total number of attention heads in the network.</li> </ul>"},{"location":"zeta/nn/biases/alibi/#attributes","title":"Attributes:","text":"<ul> <li>slopes (<code>Tensor</code>): Tensor containing slope values, which are computed based on the number of heads.</li> <li>bias (<code>Tensor</code> or <code>None</code>): Tensor for storing positional bias values. If not initialized or needs recomputation, it would be None.</li> </ul>"},{"location":"zeta/nn/biases/alibi/#methods","title":"Methods:","text":""},{"location":"zeta/nn/biases/alibi/#__init__self-heads-total_heads-kwargs-none","title":"<code>__init__(self, heads, total_heads, **kwargs) -&gt; None</code>:","text":"<p>Initializes the <code>AlibiPositionalBias</code> module.</p>"},{"location":"zeta/nn/biases/alibi/#get_biasself-i-j-device-tensor","title":"<code>get_bias(self, i, j, device) -&gt; Tensor</code>:","text":"<p>Computes the positional bias for given dimensions i and j.</p> <ul> <li>Parameters:</li> <li>i (<code>int</code>): One dimension of the required positional bias.</li> <li>j (<code>int</code>): Second dimension of the required positional bias.</li> <li>device (<code>torch.device</code>): The device on which computations are to be performed.</li> </ul>"},{"location":"zeta/nn/biases/alibi/#_get_slopesheads-listfloat","title":"<code>_get_slopes(heads) -&gt; List[float]</code>:","text":"<p>A static method that calculates slopes based on the number of attention heads.</p> <ul> <li>Parameters:</li> <li>heads (<code>int</code>): Number of attention heads.</li> </ul>"},{"location":"zeta/nn/biases/alibi/#forwardself-i-j-tensor","title":"<code>forward(self, i, j) -&gt; Tensor</code>:","text":"<p>Computes or retrieves the bias tensor for given dimensions.</p> <ul> <li>Parameters:</li> <li>i (<code>int</code>): One dimension for the required positional bias.</li> <li>j (<code>int</code>): Second dimension for the required positional bias.</li> </ul>"},{"location":"zeta/nn/biases/alibi/#mathematical-formula","title":"Mathematical Formula:","text":"<p>Given <code>n</code> attention heads, the alibi positional bias can be represented as:</p> <p>[ \\text{Bias} = \\text{-abs}(j_{\\text{range}}) \\times \\text{slope} ]</p> <p>Where: - ( j_{\\text{range}} ) is an array of numbers from <code>0</code> to <code>j-1</code>. - <code>slope</code> is computed based on the number of heads using <code>_get_slopes</code> method.</p>"},{"location":"zeta/nn/biases/alibi/#usage-examples","title":"Usage Examples:","text":""},{"location":"zeta/nn/biases/alibi/#example-1-initialize-and-compute-bias","title":"Example 1: Initialize and compute bias","text":"<pre><code>import torch\n\nfrom zeta import AlibiPositionalBias\n\nbias_module = AlibiPositionalBias(heads=4, total_heads=8)\nbias = bias_module(10, 10)\nprint(bias)\n</code></pre>"},{"location":"zeta/nn/biases/alibi/#example-2-retrieve-stored-bias","title":"Example 2: Retrieve stored bias","text":"<pre><code>bias = bias_module(5, 5)\nprint(bias)\n</code></pre>"},{"location":"zeta/nn/biases/alibi/#example-3-computing-bias-for-different-dimensions","title":"Example 3: Computing bias for different dimensions","text":"<pre><code>bias = bias_module(8, 15)\nprint(bias)\n</code></pre>"},{"location":"zeta/nn/biases/alibi/#note","title":"Note:","text":"<ul> <li>It's crucial to ensure that the <code>total_heads</code> parameter is always greater than or equal to the <code>heads</code> parameter during initialization.</li> <li>The device property is internally used to determine the computation device based on the registered buffers.</li> </ul>"},{"location":"zeta/nn/biases/alibi/#references","title":"References:","text":"<p>For a deeper understanding and applications of positional bias in attention mechanisms, one may refer to the foundational paper on Transformer architectures: - Attention Is All You Need</p> <p>Also, the <code>einops</code> library provides a versatile interface for tensor manipulations. More details can be found at its official documentation.</p>"},{"location":"zeta/nn/biases/dynamic/","title":"DynamicPositionBias Documentation","text":""},{"location":"zeta/nn/biases/dynamic/#overview-and-introduction","title":"Overview and Introduction","text":"<p>The <code>DynamicPositionBias</code> class from the <code>zeta</code> library is designed to compute positional biases dynamically based on relative distances between positions in a sequence. This module can be crucial in attention mechanisms where relative position matters, as commonly seen in Transformers.</p> <p>Key concepts: - Relative Position: The difference in position between two tokens in a sequence. - Positional Bias: A bias introduced based on the relative position, to indicate how two positions are related. - MLP (Multi-Layer Perceptron): A type of feedforward neural network consisting of multiple layers of nodes in a directed graph.</p>"},{"location":"zeta/nn/biases/dynamic/#class-definition","title":"Class Definition","text":"<pre><code>class DynamicPositionBias(nn.Module):\n    def __init__(self, dim: int, heads: int):\n        ...\n</code></pre>"},{"location":"zeta/nn/biases/dynamic/#parameters","title":"Parameters:","text":"<ul> <li><code>dim</code> (<code>int</code>): The dimension of the intermediary layer in the MLP.</li> <li><code>heads</code> (<code>int</code>): The number of attention heads. This also dictates the output dimension of the bias.</li> </ul>"},{"location":"zeta/nn/biases/dynamic/#attributes","title":"Attributes:","text":"<ul> <li><code>mlp</code> (<code>nn.Sequential</code>): Multi-Layer Perceptron used to compute the bias based on relative distance.</li> </ul>"},{"location":"zeta/nn/biases/dynamic/#functionality-and-usage","title":"Functionality and Usage","text":""},{"location":"zeta/nn/biases/dynamic/#method-forwardi-int-j-int-torchtensor","title":"Method: <code>forward(i: int, j: int) -&gt; torch.Tensor</code>","text":"<p>Computes the positional bias based on the relative distance between positions <code>i</code> and <code>j</code>.</p>"},{"location":"zeta/nn/biases/dynamic/#parameters_1","title":"Parameters:","text":"<ul> <li><code>i</code> (<code>int</code>): Starting position in the sequence.</li> <li><code>j</code> (<code>int</code>): Ending position in the sequence.</li> </ul>"},{"location":"zeta/nn/biases/dynamic/#returns","title":"Returns:","text":"<ul> <li><code>bias</code> (<code>torch.Tensor</code>): A tensor representing the bias, of shape <code>(heads, i, j)</code>.</li> </ul>"},{"location":"zeta/nn/biases/dynamic/#usage","title":"Usage:","text":"<p>The positional bias can be utilized in attention mechanisms to provide awareness of relative position between tokens.</p>"},{"location":"zeta/nn/biases/dynamic/#examples","title":"Examples:","text":"<ol> <li> <p>Basic Usage:     <pre><code>import torch\n\nfrom zeta import DynamicPositionBias\n\n# Initialize the module\nmodule = DynamicPositionBias(dim=64, heads=8)\n\n# Compute bias for positions 0 to 5\nbias = module(0, 5)\n</code></pre></p> </li> <li> <p>Integration with Transformer:     <pre><code>import torch\nfrom torch.nn import MultiheadAttention\n\nfrom zeta import DynamicPositionBias\n\n\nclass CustomAttention(MultiheadAttention):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__(embed_dim, num_heads)\n        self.pos_bias = DynamicPositionBias(dim=embed_dim, heads=num_heads)\n\n    # Override the forward method to include positional bias\n    # ... (implementation details)\n</code></pre></p> </li> <li> <p>Inspecting the Bias:     <pre><code>import matplotlib.pyplot as plt\nimport torch\n\nfrom zeta import DynamicPositionBias\n\n# Initialize the module\nmodule = DynamicPositionBias(dim=64, heads=8)\n\n# Compute bias and visualize for positions 0 to 5\nbias = module(0, 5)\nplt.imshow(bias[0].detach().numpy())\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"zeta/nn/biases/dynamic/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>Ensure that <code>j &gt;= i</code> when calling the forward method.</li> <li>The model relies on the <code>einops</code> library for tensor rearrangement. Ensure you have this dependency installed.</li> <li>This module primarily assists in capturing the relative positional information between two positions in a sequence. It might be beneficial when absolute positional embeddings are not available or not preferred.</li> </ul>"},{"location":"zeta/nn/biases/dynamic/#references-and-resources","title":"References and Resources","text":"<ul> <li>Attention Is All You Need (Vaswani et al., 2017) - Introduces the concept of attention mechanisms that can benefit from positional information.</li> <li>Einops Documentation - For tensor rearrangement operations used in the implementation.</li> </ul>"},{"location":"zeta/nn/biases/dynamic/#mathematical-representation","title":"Mathematical Representation:","text":"<p>Given a sequence from <code>i</code> to <code>j</code>:</p> <p>[ S = [s_i, s_{i+1}, ... s_{j-1}] ]</p> <p>The relative distance ( R ) for any two elements ( s_x ) and ( s_y ) from this sequence is:</p> <p>[ R(x, y) = |x - y| ]</p> <p>The bias for a specific head <code>h</code> and relative distance ( r ) can be represented as:</p> <p>[ \\text{bias}_h(r) = MLP_h(r) ]</p> <p>Where <code>MLP_h</code> is the Multi-Layer Perceptron specific to head <code>h</code>.</p>"},{"location":"zeta/nn/biases/relative_bias/","title":"RelativePositionBias","text":"<p><code>RelativePositionBias</code> is a specialized PyTorch module designed to generate relative position biases, which can be vital for certain attention mechanisms in deep learning architectures. This module quantizes the distance between two positions into a certain number of buckets and then uses an embedding to get the relative position bias. This mechanism aids in the attention mechanism by providing biases based on relative positions between the query and key, rather than relying solely on their absolute positions.</p>"},{"location":"zeta/nn/biases/relative_bias/#architecture","title":"Architecture:","text":"<p>The architecture can be visualized in three major steps: 1. Bucketing: Convert relative distances between two positions into bucket indices. 2. Embedding: Use the bucket indices to get embeddings for each pair of positions. 3. Computing Bias: Computes the bias values based on the embeddings.</p>"},{"location":"zeta/nn/biases/relative_bias/#purpose","title":"Purpose:","text":"<p>In the context of attention mechanisms, especially the transformer-based architectures, the position of tokens can provide valuable information. The <code>RelativePositionBias</code> class helps introduce this information in a compact form by bucketing relative positions and then embedding them to serve as biases for the attention scores.</p>"},{"location":"zeta/nn/biases/relative_bias/#mathematical-formula","title":"Mathematical Formula:","text":"<p>Given a relative position ( r ), the bucket index ( b ) is computed as: [ b =  \\begin{cases}        n + \\text{num_buckets} \\div 2 &amp; \\text{if } n &lt; 0 \\text{ and bidirectional is True} \\       \\min\\left( \\max_{\\text{exact}} + \\left(\\frac{\\log(\\frac{n}{\\max_{\\text{exact}}})}{\\log(\\frac{\\text{max_distance}}{\\max_{\\text{exact}}})} \\times (\\text{num_buckets} - \\max_{\\text{exact}})\\right), \\text{num_buckets} - 1 \\right) &amp; \\text{otherwise}     \\end{cases} ] Where ( n ) is the negative of the relative position, and ( \\max_{\\text{exact}} ) is ( \\text{num_buckets} \\div 2 ).</p>"},{"location":"zeta/nn/biases/relative_bias/#class-definition","title":"Class Definition:","text":"<pre><code>class RelativePositionBias(nn.Module):\n    \"\"\"\n    Compute relative position bias which can be utilized in attention mechanisms.\n\n    Parameters:\n    - bidirectional (bool): If True, considers both forward and backward relative positions. Default: True.\n    - num_buckets (int): Number of buckets to cluster relative position distances. Default: 32.\n    - max_distance (int): Maximum distance to be considered for bucketing. Distances beyond this will be mapped to the last bucket. Default: 128.\n    - n_heads (int): Number of attention heads. Default: 12.\n    \"\"\"\n</code></pre>"},{"location":"zeta/nn/biases/relative_bias/#key-methods","title":"Key Methods:","text":"<ul> <li>_relative_position_bucket: This static method is responsible for converting relative positions into bucket indices.</li> <li>compute_bias: Computes the relative position bias for given lengths of queries and keys.</li> <li>forward: Computes and returns the relative position biases for a batch.</li> </ul>"},{"location":"zeta/nn/biases/relative_bias/#usage-examples","title":"Usage Examples:","text":"<pre><code>import torch\n\nfrom zeta import RelativePositionBias\n\n# Initialize the RelativePositionBias module\nrel_pos_bias = RelativePositionBias()\n\n# Example 1: Compute bias for a single batch\nbias_matrix = rel_pos_bias(1, 10, 10)\n\n\n# Example 2: Utilize in conjunction with an attention mechanism\n# NOTE: This is a mock example, and may not represent an actual attention mechanism's complete implementation.\nclass MockAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.rel_pos_bias = RelativePositionBias()\n\n    def forward(self, queries, keys):\n        bias = self.rel_pos_bias(queries.size(0), queries.size(1), keys.size(1))\n        # Further computations with bias in the attention mechanism...\n        return None  # Placeholder\n\n\n# Example 3: Modify default configurations\ncustom_rel_pos_bias = RelativePositionBias(\n    bidirectional=False, num_buckets=64, max_distance=256, n_heads=8\n)\n</code></pre>"},{"location":"zeta/nn/biases/relative_bias/#tips","title":"Tips:","text":"<ol> <li>The choice of <code>num_buckets</code> and <code>max_distance</code> might need tuning based on the dataset and application.</li> <li>If the architecture doesn't need bidirectional biases, set <code>bidirectional</code> to <code>False</code> to reduce computation.</li> <li>Ensure that the device of tensors being processed and the device of the <code>RelativePositionBias</code> module are the same.</li> </ol>"},{"location":"zeta/nn/biases/relative_bias/#references","title":"References:","text":"<ul> <li>Attention Is All You Need</li> <li>Transformer Architectures</li> </ul> <p>Note: This documentation is based on the provided code and might need adjustments when integrated into the complete <code>zeta</code> library.</p>"},{"location":"zeta/nn/biases/xpos/","title":"XPOS Module Documentation","text":""},{"location":"zeta/nn/biases/xpos/#architecture","title":"Architecture","text":"<p>The XPOS module is a part of a neural network model and is implemented as a subclass of\u00a0<code>torch.nn.Module</code>. It consists of several functions and a class that work together to apply rotary positional embeddings to an input tensor.</p>"},{"location":"zeta/nn/biases/xpos/#purpose","title":"Purpose","text":"<p>The purpose of the XPOS module is to incorporate positional information into the input tensor of a neural network model. It achieves this by generating fixed positional embeddings and applying them to the input tensor using rotary positional encoding techniques. This allows the model to capture the sequential order and relative positions of the input elements, which can be beneficial for tasks such as natural language processing and time series analysis.</p>"},{"location":"zeta/nn/biases/xpos/#functions-and-methods","title":"Functions and Methods","text":"<ol> <li> <p><code>fixed_pos_embedding(x)</code>: Generates fixed positional embeddings for the input tensor.</p> <ul> <li>Args:<ul> <li><code>x</code>\u00a0(torch.Tensor): Input tensor of shape\u00a0<code>(seq_len, dim)</code>.</li> </ul> </li> <li>Returns:<ul> <li><code>sin</code>\u00a0(torch.Tensor): Sine positional embeddings of shape\u00a0<code>(seq_len, dim)</code>.</li> <li><code>cos</code>\u00a0(torch.Tensor): Cosine positional embeddings of shape\u00a0<code>(seq_len, dim)</code>.</li> </ul> </li> <li> <p><code>rotate_every_two(x)</code>: Rearranges the elements of the input tensor by rotating every two elements.</p> </li> <li> <p>Args:</p> <ul> <li><code>x</code>\u00a0(torch.Tensor): Input tensor of shape\u00a0<code>(batch_size, seq_len, dim)</code>.</li> </ul> </li> <li>Returns:<ul> <li><code>x</code>\u00a0(torch.Tensor): Rearranged tensor of shape\u00a0<code>(batch_size, seq_len, dim)</code>.</li> </ul> </li> <li> <p><code>duplicate_interleave(m)</code>: Duplicates a matrix while interleaving the copy.</p> </li> <li> <p>Args:</p> <ul> <li><code>m</code>\u00a0(torch.Tensor): Input matrix.</li> </ul> </li> <li>Returns:<ul> <li><code>m</code>\u00a0(torch.Tensor): Duplicated and interleaved matrix.</li> </ul> </li> <li> <p><code>apply_rotary_pos_emb(x, sin, cos, scale=1)</code>: Applies rotary positional embeddings to the input tensor.</p> </li> <li> <p>Args:</p> <ul> <li><code>x</code>\u00a0(torch.Tensor): Input tensor of shape\u00a0<code>(batch_size, seq_len, dim)</code>.</li> <li><code>sin</code>\u00a0(torch.Tensor): Sine positional embeddings of shape\u00a0<code>(seq_len, dim)</code>.</li> <li><code>cos</code>\u00a0(torch.Tensor): Cosine positional embeddings of shape\u00a0<code>(seq_len, dim)</code>.</li> <li><code>scale</code>\u00a0(float): Scaling factor for the positional embeddings (default: 1).</li> </ul> </li> <li>Returns:<ul> <li><code>x</code>\u00a0(torch.Tensor): Tensor with applied rotary positional embeddings.</li> </ul> </li> <li> <p><code>XPOS(head_dim, scale_base=512)</code>: XPOS module class.</p> </li> <li> <p>Args:</p> <ul> <li><code>head_dim</code>\u00a0(int): Dimensionality of the input tensor.</li> <li><code>scale_base</code>\u00a0(int): Base value for scaling the positional embeddings (default: 512).</li> </ul> </li> <li>Methods:<ul> <li><code>forward(x, offset=0, downscale=False)</code>: Forward pass of the XPOS module.<ul> <li>Args:<ul> <li><code>x</code>\u00a0(torch.Tensor): Input tensor of shape\u00a0<code>(batch_size, seq_len, dim)</code>.</li> <li><code>offset</code>\u00a0(int): Offset value for positional embeddings (default: 0).</li> <li><code>downscale</code>\u00a0(bool): Boolean indicating whether to downscale the positional embeddings (default: False).</li> </ul> </li> <li>Returns:<ul> <li><code>x</code>\u00a0(torch.Tensor): Tensor with applied rotary positional embeddings.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ol>"},{"location":"zeta/nn/biases/xpos/#usage-examples","title":"Usage Examples","text":"<ol> <li> <p>Applying XPOS module to an input tensor:</p> <pre><code>import torch\nfrom zeta import XPOS\n\n# Create an instance of the XPOS module\nxpos = XPOS(head_dim=256)\n\n# Generate a random input tensor\nx = torch.randn(1, 10, 256)\n\n# Apply the XPOS module to the input tensor\noutput = xpos(x)\n</code></pre> </li> <li> <p>Applying XPOS module with offset and downscaling:</p> <p><pre><code>import torch\nfrom zeta import XPOS\n\n# Create an instance of the XPOS module\nxpos = XPOS(head_dim=512)\n\n# Generate a random input tensor\nx = torch.randn(1, 20, 512)\n\n# Apply the XPOS module to the input tensor with offset and downscaling\noutput = xpos(x, offset=2, downscale=True)\n</code></pre> 3.  Using the individual functions of the XPOS module:</p> <pre><code>import torch\nfrom zeta import fixed_pos_embedding, apply_rotary_pos_emb\n\n# Generate fixed positional embeddings\nscale = torch.randn(10, 256)\nsin, cos = fixed_pos_embedding(scale)\n\n# Apply rotary positional embeddings to an input tensor\nx = torch.randn(1, 10, 256)\noutput = apply_rotary_pos_emb(x, sin, cos, scale=0.5)\n</code></pre> </li> </ol> <p>Note: The above examples assume that the\u00a0<code>xpos.py</code>\u00a0file</p>"},{"location":"zeta/nn/embeddings/multiway/","title":"Documentation for <code>MultiwayEmbedding</code> in Zeta Library","text":"<p>Table of Contents</p> <ol> <li>Overview</li> <li>Class Definition and Parameters</li> <li>Methods and Functionalities</li> <li>Usage Examples</li> <li>Additional Tips and Information</li> <li>References</li> </ol>"},{"location":"zeta/nn/embeddings/multiway/#1-overview","title":"1. Overview","text":"<p>The <code>MultiwayEmbedding</code> class in the Zeta library provides a way to apply two separate embeddings to two distinct parts of the input tensor. It splits the input tensor at the specified position and applies one embedding to the first part and another embedding to the second part. This can be particularly useful when dealing with inputs that require diverse representations or embeddings.</p>"},{"location":"zeta/nn/embeddings/multiway/#2-class-definition-and-parameters","title":"2. Class Definition and Parameters","text":"<pre><code>class MultiwayEmbedding(MultiwayNetwork):\n    \"\"\"\n    A specialized version of the MultiwayNetwork to perform multi-way embeddings on an input tensor.\n\n    Parameters:\n    - modules (List[nn.Module]): A list containing exactly two PyTorch modules. Typically these would be embedding layers.\n    - dim (int): The dimension along which to split and concatenate the input tensor. Default is 1.\n    \"\"\"\n\n    def __init__(self, modules, dim=1):\n        super(MultiwayNetwork, self).__init__()\n        ...\n</code></pre>"},{"location":"zeta/nn/embeddings/multiway/#3-methods-and-functionalities","title":"3. Methods and Functionalities","text":"<p>forward(x, kwargs)** <pre><code>def forward(self, x, **kwargs):\n    \"\"\"\n    Forward method to apply embeddings on the split input tensor.\n\n    Parameters:\n    - x (torch.Tensor): The input tensor.\n    - **kwargs: Additional arguments that might be needed for the embeddings.\n\n    Returns:\n    - torch.Tensor: Concatenated tensor after applying the embeddings.\n    \"\"\"\n    ...\n</code></pre></p>"},{"location":"zeta/nn/embeddings/multiway/#4-usage-examples","title":"4. Usage Examples","text":"<p>Example 1: Basic Usage <pre><code>import torch.nn as nn\n\nfrom zeta import MultiwayEmbedding\n\nemb1 = nn.Embedding(10, 5)\nemb2 = nn.Embedding(10, 5)\nmultiway_emb = MultiwayEmbedding([emb1, emb2])\n\nx = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\noutput = multiway_emb(x)\nprint(output)\n</code></pre></p> <p>Example 2: Setting a Split Position <pre><code>import torch.nn as nn\n\nfrom zeta import MultiwayEmbedding, set_split_position\n\nemb1 = nn.Embedding(10, 5)\nemb2 = nn.Embedding(10, 5)\nmultiway_emb = MultiwayEmbedding([emb1, emb2])\nmultiway_emb.apply(set_split_position(2))\n\nx = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\noutput = multiway_emb(x)\nprint(output)\n</code></pre></p> <p>Example 3: Working with Different Embedding Dimensions <pre><code>import torch.nn as nn\n\nfrom zeta import MultiwayEmbedding\n\nemb1 = nn.Embedding(10, 5)\nemb2 = nn.Embedding(10, 7)\nmultiway_emb = MultiwayEmbedding([emb1, emb2], dim=2)\n\nx = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\noutput = multiway_emb(x)\nprint(output)\n</code></pre></p>"},{"location":"zeta/nn/embeddings/multiway/#5-additional-tips-and-information","title":"5. Additional Tips and Information","text":"<ul> <li>Ensure that the input tensor's dimensions align with the expected embeddings. If there's a mismatch in dimensions, a runtime error will occur.</li> <li>The split position determines the point at which the tensor is divided. It's crucial to set this appropriately, especially if the embeddings have different dimensions.</li> <li>Using the provided <code>set_split_position</code> utility function makes it easy to apply the split position for the embeddings.</li> </ul>"},{"location":"zeta/nn/embeddings/multiway/#6-references","title":"6. References","text":"<ul> <li>Torch documentation: Link to PyTorch Documentation</li> <li>Agora: Link to Agora's GitHub (assuming there might be a GitHub link or other resource for Agora)</li> </ul> <p>Note: Ensure that the tensor operations align mathematically, especially if you're concatenating tensors with different dimensions. In such cases, ensure the embeddings produce tensors that can be concatenated along the specified dimension.</p> <p>Mathematical Explanation: Given an input tensor ( X ) split into ( X_1 ) and ( X_2 ), and two embeddings ( A ) and ( B ), the output is given by concatenating ( A(X_1) ) and ( B(X_2) ).</p>"},{"location":"zeta/nn/embeddings/patch_embeddings/","title":"<code>PatchEmbeddings</code> Documentation","text":""},{"location":"zeta/nn/embeddings/patch_embeddings/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Class: <code>PatchEmbeddings</code></li> <li>Parameters</li> <li>Usage Examples</li> <li>Using the <code>PatchEmbeddings</code> Class</li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/nn/embeddings/patch_embeddings/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation! In this documentation, we will explore the <code>PatchEmbeddings</code> class, which is part of the Zeta library. This class plays a crucial role in the field of computer vision, particularly in the context of image processing and deep learning. This documentation aims to provide a comprehensive understanding of its purpose, functionality, and usage.</p>"},{"location":"zeta/nn/embeddings/patch_embeddings/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>PatchEmbeddings</code> class serves as a fundamental component for processing images in deep learning models, such as transformers and convolutional neural networks (CNNs). Its primary functionalities include:</p>"},{"location":"zeta/nn/embeddings/patch_embeddings/#patch-embedding","title":"Patch Embedding","text":"<ul> <li> <p>Image Segmentation: It segments an input image into smaller patches, which are then individually processed by the neural network.</p> </li> <li> <p>Dimensionality Transformation: It transforms the dimensionality of each patch, preparing them for further processing.</p> </li> <li> <p>Normalization: It applies layer normalization to the patch embeddings for improved training stability.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/patch_embeddings/#image-to-sequence-transformation","title":"Image-to-Sequence Transformation","text":"<ul> <li> <p>Reshaping: The class reshapes the image patches into a sequence of vectors suitable for input to models like transformers.</p> </li> <li> <p>Linear Projection: It uses a linear layer to project the patch embeddings into the desired output dimension.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/patch_embeddings/#versatility","title":"Versatility","text":"<ul> <li> <p>Configurability: You can configure the input and output dimensions, allowing flexibility for various model architectures.</p> </li> <li> <p>Normalization Control: It provides control over the layer normalization applied to the embeddings.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/patch_embeddings/#3-class-patchembeddings","title":"3. Class: <code>PatchEmbeddings</code>","text":"<p>The <code>PatchEmbeddings</code> class has the following signature:</p> <pre><code>class PatchEmbeddings(nn.Module):\n    def __init__(\n        self, \n        dim_in, \n        dim_out, \n        seq_len\n    )\n\n    def forward(self, x)\n</code></pre>"},{"location":"zeta/nn/embeddings/patch_embeddings/#parameters","title":"Parameters","text":"<ul> <li> <p><code>dim_in</code> (int): The input dimension of the image patches.</p> </li> <li> <p><code>dim_out</code> (int): The output dimension after embedding the patches.</p> </li> <li> <p><code>seq_len</code> (int): The length of the sequence after patching the image.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/patch_embeddings/#4-usage-examples","title":"4. Usage Examples","text":"<p>Let's explore how to use the <code>PatchEmbeddings</code> class effectively in various scenarios.</p>"},{"location":"zeta/nn/embeddings/patch_embeddings/#using-the-patchembeddings-class","title":"Using the <code>PatchEmbeddings</code> Class","text":"<p>Here's how to use the <code>PatchEmbeddings</code> class to embed image patches:</p> <pre><code>import torch\n\nfrom zeta.vision import PatchEmbeddings\n\n# Define the input image properties\ndim_in = 3  # Input dimension of image patches (e.g., 3 for RGB images)\ndim_out = 64  # Output dimension after embedding\nseq_len = 16  # Length of the sequence after patching the image\n\n# Create an instance of PatchEmbeddings\npatch_embed = PatchEmbeddings(dim_in, dim_out, seq_len)\n\n# Create a random input image tensor (batch_size, channels, height, width)\nimage = torch.randn(32, dim_in, 224, 224)  # Example input image with 32 samples\n\n# Apply patch embedding\nembedded_patches = patch_embed(image)\n\n# Print the embedded patches\nprint(embedded_patches)\n</code></pre>"},{"location":"zeta/nn/embeddings/patch_embeddings/#5-additional-information","title":"5. Additional Information","text":"<p>Here are some additional notes and tips related to the <code>PatchEmbeddings</code> class:</p> <ul> <li> <p>Image Patching: Patching images is a common technique used to process large images in deep learning models.</p> </li> <li> <p>Normalization: The application of layer normalization helps stabilize training and improve convergence.</p> </li> <li> <p>Dimensionality Transformation: Patch embeddings are essential for converting spatial information in images into sequences suitable for neural networks.</p> </li> <li> <p>Versatile Usage: The <code>PatchEmbeddings</code> class can be used in various vision-based deep learning architectures.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/patch_embeddings/#6-references","title":"6. References","text":"<p>For further information on image patching, layer normalization, and related concepts, you can refer to the following resources:</p> <ul> <li> <p>Vision Transformers (ViT) - Paper - The original research paper introducing Vision Transformers.</p> </li> <li> <p>PyTorch Documentation - Official PyTorch documentation for related functions and modules.</p> </li> </ul> <p>This documentation provides a comprehensive overview of the Zeta library's <code>PatchEmbeddings</code> class. It aims to help you understand the purpose, functionality, and usage of this class for image patch embedding, which is a crucial step in various computer vision applications and deep learning models.</p>"},{"location":"zeta/nn/embeddings/positional_embeddings/","title":"Zeta Documentation","text":""},{"location":"zeta/nn/embeddings/positional_embeddings/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Class: <code>PositionalEmbedding</code></li> <li>Initialization</li> <li>Parameters</li> <li>Forward Method</li> <li>Usage Examples</li> <li>Basic Usage</li> <li>Customized Positional Embeddings</li> <li>Using Provided Positions</li> <li>Additional Information</li> <li>Positional Embeddings in Transformers</li> <li>References</li> </ol>"},{"location":"zeta/nn/embeddings/positional_embeddings/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation for the <code>PositionalEmbedding</code> class! Zeta is a powerful library for deep learning in PyTorch, and this documentation will provide a comprehensive understanding of the <code>PositionalEmbedding</code> class. </p>"},{"location":"zeta/nn/embeddings/positional_embeddings/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>PositionalEmbedding</code> class is a key component in sequence modeling tasks, particularly in transformers. It is designed to create positional embeddings that provide essential information about the position of tokens in a sequence. Below, we will explore its purpose and functionality.</p>"},{"location":"zeta/nn/embeddings/positional_embeddings/#3-class-positionalembedding","title":"3. Class: <code>PositionalEmbedding</code>","text":"<p>The <code>PositionalEmbedding</code> class is used to generate positional embeddings for sequences. These embeddings are vital for transformers and other sequence-based models to understand the order of elements within the input data.</p>"},{"location":"zeta/nn/embeddings/positional_embeddings/#initialization","title":"Initialization","text":"<p>To create a <code>PositionalEmbedding</code> instance, you need to specify various parameters. Here's an example of how to initialize it:</p> <pre><code>PositionalEmbedding(\n    num_embeddings,\n    embedding_dim,\n    padding_idx=None,\n    max_norm=None,\n    norm_type=2.0,\n    scale_grad_by_freq=False,\n    sparse=False,\n)\n</code></pre>"},{"location":"zeta/nn/embeddings/positional_embeddings/#parameters","title":"Parameters","text":"<ul> <li> <p><code>num_embeddings</code> (int): The total number of embeddings to generate. This typically corresponds to the sequence length.</p> </li> <li> <p><code>embedding_dim</code> (int): The dimensionality of the positional embeddings. This should match the dimensionality of the input data.</p> </li> <li> <p><code>padding_idx</code> (int, optional): If specified, the embeddings at this position will be set to all zeros. Default is <code>None</code>.</p> </li> <li> <p><code>max_norm</code> (float, optional): If specified, the embeddings will be normalized to have a maximum norm of this value. Default is <code>None</code>.</p> </li> <li> <p><code>norm_type</code> (float, optional): The type of norm to apply if <code>max_norm</code> is specified. Default is <code>2.0</code>.</p> </li> <li> <p><code>scale_grad_by_freq</code> (bool, optional): If <code>True</code>, the gradients of the embeddings will be scaled by the frequency of the corresponding positions. Default is <code>False</code>.</p> </li> <li> <p><code>sparse</code> (bool, optional): If <code>True</code>, a sparse tensor will be used for embeddings. Default is <code>False</code>.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/positional_embeddings/#forward-method","title":"Forward Method","text":"<p>The <code>forward</code> method of <code>PositionalEmbedding</code> generates positional embeddings based on the input positions. It can be called as follows:</p> <pre><code>output = positional_embedding(positions)\n</code></pre> <ul> <li><code>positions</code> (Tensor): A tensor containing the positions for which you want to generate positional embeddings.</li> </ul>"},{"location":"zeta/nn/embeddings/positional_embeddings/#4-usage-examples","title":"4. Usage Examples","text":"<p>Let's explore some usage examples of the <code>PositionalEmbedding</code> class to understand how to use it effectively.</p>"},{"location":"zeta/nn/embeddings/positional_embeddings/#basic-usage","title":"Basic Usage","text":"<pre><code>import torch\n\nfrom zeta.nn import PositionalEmbedding\n\n# Create a PositionalEmbedding instance\npositional_embedding = PositionalEmbedding(num_embeddings=100, embedding_dim=128)\n\n# Generate positional embeddings for a sequence of length 10\npositions = torch.arange(10)\nembeddings = positional_embedding(positions)\n</code></pre>"},{"location":"zeta/nn/embeddings/positional_embeddings/#customized-positional-embeddings","title":"Customized Positional Embeddings","text":"<p>You can customize the positional embeddings by specifying additional parameters such as <code>max_norm</code> and <code>scale_grad_by_freq</code>.</p> <pre><code>import torch\n\nfrom zeta.nn import PositionalEmbedding\n\n# Create a PositionalEmbedding instance with customization\npositional_embedding = PositionalEmbedding(\n    num_embeddings=100, embedding_dim=128, max_norm=1.0, scale_grad_by_freq=True\n)\n\n# Generate positional embeddings for a sequence of length 10\npositions = torch.arange(10)\nembeddings = positional_embedding(positions)\n</code></pre>"},{"location":"zeta/nn/embeddings/positional_embeddings/#using-provided-positions","title":"Using Provided Positions","text":"<p>You can also provide your own positions when generating positional embeddings.</p> <pre><code>import torch\n\nfrom zeta.nn import PositionalEmbedding\n\n# Create a PositionalEmbedding instance\npositional_embedding = PositionalEmbedding(num_embeddings=100, embedding_dim=128)\n\n# Provide custom positions for embedding\ncustom_positions = torch.tensor([5, 10, 15, 20])\nembeddings = positional_embedding(custom_positions)\n</code></pre>"},{"location":"zeta/nn/embeddings/positional_embeddings/#5-additional-information","title":"5. Additional Information","text":""},{"location":"zeta/nn/embeddings/positional_embeddings/#positional-embeddings-in-transformers","title":"Positional Embeddings in Transformers","text":"<p>Positional embeddings play a crucial role in transformers and other sequence-to-sequence models. They allow the model to understand the order of elements in a sequence, which is essential for tasks like language translation and text generation.</p>"},{"location":"zeta/nn/embeddings/positional_embeddings/#6-references","title":"6. References","text":"<ul> <li>Attention Is All You Need (Transformer)</li> </ul> <p>This documentation provides a comprehensive guide to the <code>PositionalEmbedding</code> class in the Zeta library, explaining its purpose, functionality, parameters, and usage. You can now effectively integrate this class into your deep learning models for various sequence-based tasks.</p>"},{"location":"zeta/nn/embeddings/positional_interpolation/","title":"PositionInterpolationEmbeddings","text":""},{"location":"zeta/nn/embeddings/positional_interpolation/#positioninterpolationembeddings","title":"PositionInterpolationEmbeddings","text":""},{"location":"zeta/nn/embeddings/positional_interpolation/#overview","title":"Overview","text":"<p>PositionalEmbedding module that uses interpolation to generate positional embeddings.</p>"},{"location":"zeta/nn/embeddings/positional_interpolation/#parameters","title":"Parameters","text":"Parameter Description Default <code>dim</code> Dimension of the model. <code>None</code> <code>max_positions</code> Maximum length of the input sequence. <code>2048</code> <code>base</code> Base value for interpolation. <code>10000</code> <code>device</code> Device to use. <code>None</code>"},{"location":"zeta/nn/embeddings/positional_interpolation/#examples","title":"Examples","text":"<pre><code>import torch\n\nfrom zeta.nn import PositionInterpolationEmbeddings\n\npositional_embedding = PositionInterpolationEmbeddings(512, 1000)\nx = torch.randn(32, 100, 512)\npositions = torch.arange(100)\nembedded_tensor = positional_embedding(x, positions)\n</code></pre>"},{"location":"zeta/nn/embeddings/positional_interpolation/#description","title":"Description","text":"<p>The <code>PositionInterpolationEmbeddings</code> class is used to generate positional embeddings for input sequences using interpolation. It is often used in neural network models for natural language processing tasks.</p>"},{"location":"zeta/nn/embeddings/positional_interpolation/#parameters_1","title":"Parameters","text":"<ul> <li> <p><code>dim</code> (int, optional): Dimension of the model. This parameter specifies the dimension of the positional embeddings. Defaults to <code>None</code>.</p> </li> <li> <p><code>max_positions</code> (int, optional): Maximum length of the input sequence. This parameter determines the maximum number of positions for which positional embeddings will be generated. Defaults to <code>2048</code>.</p> </li> <li> <p><code>base</code> (int, optional): Base value for interpolation. This parameter controls the interpolation behavior for generating positional embeddings. Defaults to <code>10000</code>.</p> </li> <li> <p><code>device</code> (str or torch.device, optional): Device to use for computation. This parameter specifies the device on which the positional embeddings will be computed. Defaults to <code>None</code>.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/positional_interpolation/#example","title":"Example","text":"<pre><code>positional_embedding = PositionInterpolationEmbeddings(512, 1000)\nx = torch.randn(32, 100, 512)\npositions = torch.arange(100)\nembedded_tensor = positional_embedding(x, positions)\n</code></pre> <p>In this example, a <code>PositionInterpolationEmbeddings</code> instance is created with a dimension of 512 and a maximum position of 1000. The <code>x</code> tensor represents input data of shape (32, 100, 512), and <code>positions</code> is a tensor containing position indices. The <code>embedded_tensor</code> will contain positional embeddings for the input data.</p> <p>For more details on the usage of this module, refer to the example provided.</p>"},{"location":"zeta/nn/embeddings/positional_interpolation/#methods","title":"Methods","text":""},{"location":"zeta/nn/embeddings/positional_interpolation/#forwardx-seq_lennone","title":"<code>forward(x, seq_len=None)</code>","text":"<p>Generate positional embeddings for the input data.</p> <ul> <li> <p><code>x</code> (Tensor): Input data of shape (batch_size, sequence_length, dimension).</p> </li> <li> <p><code>seq_len</code> (int, optional): Length of the input sequence. This parameter can be used to specify the length of the sequence for which positional embeddings should be generated. If not provided, the maximum length specified during initialization is used.</p> </li> </ul> <p>Returns a tuple containing two tensors: <code>(cosine_embeddings, sine_embeddings)</code>. These tensors represent the positional embeddings for the input sequence. ```</p>"},{"location":"zeta/nn/embeddings/rope/","title":"RotaryEmbedding","text":"<p><code>RotaryEmbedding</code> is a PyTorch module implementing the rotary embedding mechanism. It is designed to handle sequences of any length without the need for fine-tuning, and can also incorporate positional information into the embeddings.</p>"},{"location":"zeta/nn/embeddings/rope/#class-definition","title":"Class Definition","text":"<pre><code>class RotaryEmbedding(nn.Module):\n    def __init__(\n        self,\n        dim,\n        use_xpos=False,\n        scale_base=512,\n        interpolation_factor=1.0,\n        base=10000,\n        base_rescale_factor=1.0,\n    ):\n        ...\n</code></pre>"},{"location":"zeta/nn/embeddings/rope/#parameters","title":"Parameters","text":"<ul> <li><code>dim</code> (int): The dimensionality of the embeddings.</li> <li><code>use_xpos</code> (bool, optional): Whether to use positional information in the embeddings. Default: <code>False</code>.</li> <li><code>scale_base</code> (int, optional): Base of the scale for positional information. Default: <code>512</code>.</li> <li><code>interpolation_factor</code> (float, optional): Factor used for interpolating the embeddings. Default: <code>1.0</code>.</li> <li><code>base</code> (int, optional): Base of the frequencies used in the embeddings. Default: <code>10000</code>.</li> <li><code>base_rescale_factor</code> (float, optional): Factor used for rescaling the base of the frequencies. Default: <code>1.0</code>.</li> </ul>"},{"location":"zeta/nn/embeddings/rope/#method-forward","title":"Method: <code>forward</code>","text":"<pre><code>def forward(self, seq_len, device):\n    ...\n</code></pre>"},{"location":"zeta/nn/embeddings/rope/#parameters_1","title":"Parameters","text":"<ul> <li><code>seq_len</code> (int): The length of the sequence.</li> <li><code>device</code> (torch.device): The device on which the computation will be performed.</li> </ul>"},{"location":"zeta/nn/embeddings/rope/#returns","title":"Returns","text":"<ul> <li><code>freqs</code> (Tensor): The computed frequencies for the embeddings.</li> <li><code>scale</code> (Tensor): The computed scale for the embeddings.</li> </ul>"},{"location":"zeta/nn/embeddings/rope/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>RotaryEmbedding</code> module computes rotary embeddings for a sequence of a given length. The embeddings are computed based on the frequency and scale of each position in the sequence. The frequency and scale are computed using the <code>inv_freq</code> and <code>scale</code> buffers registered in the module.</p> <p>The <code>forward</code> method computes the <code>freqs</code> and <code>scale</code> tensors based on the <code>seq_len</code> and <code>device</code> provided. The <code>freqs</code> tensor is computed by multiplying the <code>t</code> tensor, which contains the indices of the sequence, with the <code>inv_freq</code> tensor. The <code>scale</code> tensor is computed using the <code>scale</code> buffer and the <code>scale_base</code> parameter.</p> <p>The <code>freqs</code> and <code>scale</code> tensors are then concatenated along the last dimension and returned.</p>"},{"location":"zeta/nn/embeddings/rope/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/embeddings/rope/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import RotaryEmbedding\n\n# Initialize the RotaryEmbedding module\nrotary_embedding = RotaryEmbedding(dim=64, use_xpos=True)\n\n# Compute the embeddings for a sequence of length 10\nseq_len = 10\ndevice = torch.device(\"cuda\")\nfreqs, scale = rotary_embedding(seq_len, device)\n\nprint(freqs)\nprint(scale)\n</code></pre>"},{"location":"zeta/nn/embeddings/rope/#example-2-using-a-different-scale-base","title":"Example 2: Using a Different Scale Base","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import RotaryEmbedding\n\n# Initialize the RotaryEmbedding module with a different scale base\nrotary_embedding = RotaryEmbedding(dim=64, use_xpos=True, scale_base=1024)\n\n# Compute the embeddings for a sequence of length 10\nseq_len = 10\ndevice = torch.device(\"cuda\")\nfreqs, scale = rotary_embedding(seq_len, device)\n\nprint(freqs)\nprint(scale)\n</code></pre>"},{"location":"zeta/nn/embeddings/rope/#example-3-without-positional-information","title":"Example 3: Without Positional Information","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import RotaryEmbedding\n\n# Initialize the RotaryEmbedding module without positional information\nrotary_embedding = RotaryEmbedding(dim=64, use_xpos=False)\n\n# Compute the embeddings for a sequence of length 10\nseq_len = 10\ndevice = torch.device(\"cuda\")\nfreqs, scale = rotary_embedding(seq_len, device)\n\nprint(freqs)\nprint(scale)\n</code></pre>"},{"location":"zeta/nn/embeddings/rope/#mathematical-formula","title":"Mathematical Formula","text":"<p>The mathematical formula for computing the <code>freqs</code> tensor is:</p> <p>[ \\text{freqs} = t \\cdot \\text{inv_freq} ]</p> <p>Where: - ( t ) is a tensor containing the indices of the sequence. - ( \\text{inv_freq} ) is a tensor containing the inverse frequencies.</p> <p>The mathematical formula for computing the <code>scale</code> tensor is:</p> <p>[ \\text{scale} = \\text{scale}^{\\frac{\\text{power}}{\\text{scale_base}}} ]</p> <p>Where: - ( \\text{power} ) is a tensor containing the power of each position in the sequence. - ( \\text{scale_base} ) is a scalar containing the base of the scale. - ( \\text{scale} ) is a tensor containing the scale of each position in the sequence.</p>"},{"location":"zeta/nn/embeddings/rope/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The <code>interpolation_factor</code> parameter can be used to interpolate the embeddings for sequences of different lengths. A larger <code>interpolation_factor</code> will result in a smoother interpolation.</li> <li>The <code>base_rescale_factor</code> parameter can be used to rescale the base of the frequencies. This can be useful for adjusting the embeddings for sequences of different lengths.</li> <li>If <code>use_xpos</code> is set to <code>False</code>, the <code>scale</code> tensor will not be used, and the <code>freqs</code> tensor will be returned as is.</li> </ul>"},{"location":"zeta/nn/embeddings/rope/#references-and-resources","title":"References and Resources","text":"<ul> <li>Paper: Link to the paper</li> <li>PyTorch Documentation</li> <li>Einops Documentation</li> </ul> <p>Note: The above template includes the class definition, parameters, description, functionality, usage examples, mathematical formula, additional information and tips, and references and resources. To replicate the documentation for any other module or framework, follow the same structure and provide the specific details for that module or framework.</p>"},{"location":"zeta/nn/embeddings/sinusoidal/","title":"<code>SinusoidalEmbeddings</code> Documentation","text":""},{"location":"zeta/nn/embeddings/sinusoidal/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Class: <code>SinusoidalEmbeddings</code></li> <li>Initialization</li> <li>Parameters</li> <li>Forward Method</li> <li>Function: <code>rotate_half</code></li> <li>Function: <code>apply_rotary_pos_emb</code></li> <li>Usage Examples</li> <li>Using the <code>SinusoidalEmbeddings</code> Class</li> <li>Using the <code>rotate_half</code> Function</li> <li>Using the <code>apply_rotary_pos_emb</code> Function</li> <li>Additional Information</li> <li>Sinusoidal Positional Embeddings</li> <li>Rotary Positional Embeddings</li> <li>References</li> </ol>"},{"location":"zeta/nn/embeddings/sinusoidal/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation! This documentation provides an in-depth explanation of the <code>SinusoidalEmbeddings</code> class and related functions. Zeta is a PyTorch library that aims to simplify complex deep learning tasks. In this documentation, we will explore how <code>SinusoidalEmbeddings</code> and associated functions work and how they can be applied to various scenarios.</p>"},{"location":"zeta/nn/embeddings/sinusoidal/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>SinusoidalEmbeddings</code> class is designed to generate sinusoidal positional embeddings for sequences in transformer-based models. These embeddings are essential for self-attention mechanisms to understand the positions of elements within a sequence. Additionally, this documentation covers the <code>rotate_half</code> and <code>apply_rotary_pos_emb</code> functions, which help apply positional embeddings to input data.</p>"},{"location":"zeta/nn/embeddings/sinusoidal/#3-class-sinusoidalembeddings","title":"3. Class: <code>SinusoidalEmbeddings</code>","text":"<p>The <code>SinusoidalEmbeddings</code> class generates sinusoidal positional embeddings. It provides flexibility in configuring the embeddings and can be used to generate both basic sinusoidal embeddings and rotary positional embeddings.</p>"},{"location":"zeta/nn/embeddings/sinusoidal/#initialization","title":"Initialization","text":"<p>To create an instance of the <code>SinusoidalEmbeddings</code> class, you need to specify the following parameters:</p> <pre><code>SinusoidalEmbeddings(dim, scale_base=None, use_xpos=False)\n</code></pre>"},{"location":"zeta/nn/embeddings/sinusoidal/#parameters","title":"Parameters","text":"<ul> <li> <p><code>dim</code> (int): The dimensionality of the embeddings.</p> </li> <li> <p><code>scale_base</code> (float or None, optional): The scale base for rotary positional embeddings. Must be defined if <code>use_xpos</code> is set to <code>True</code>. Default is <code>None</code>.</p> </li> <li> <p><code>use_xpos</code> (bool, optional): Whether to use positional information. If <code>True</code>, positional embeddings will be generated. Default is <code>False</code>.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/sinusoidal/#forward-method","title":"Forward Method","text":"<p>The <code>forward</code> method of the <code>SinusoidalEmbeddings</code> class generates sinusoidal positional embeddings based on the input sequence length. It returns two tensors: frequency embeddings and scale embeddings.</p> <pre><code>def forward(x):\n    # ...\n    return freqs, scale\n</code></pre>"},{"location":"zeta/nn/embeddings/sinusoidal/#4-function-rotate_half","title":"4. Function: <code>rotate_half</code>","text":"<p>The <code>rotate_half</code> function is used to rotate input data by 180 degrees along the last dimension. It is a useful operation when working with rotary positional embeddings.</p>"},{"location":"zeta/nn/embeddings/sinusoidal/#parameters_1","title":"Parameters","text":"<ul> <li><code>x</code> (Tensor): The input tensor to be rotated.</li> </ul>"},{"location":"zeta/nn/embeddings/sinusoidal/#usage-example","title":"Usage Example","text":"<pre><code>import torch\n\nfrom zeta import rotate_half\n\n# Create an input tensor\nx = torch.randn(2, 3, 4)\n\n# Rotate the input tensor\nrotated_x = rotate_half(x)\n</code></pre>"},{"location":"zeta/nn/embeddings/sinusoidal/#5-function-apply_rotary_pos_emb","title":"5. Function: <code>apply_rotary_pos_emb</code>","text":"<p>The <code>apply_rotary_pos_emb</code> function applies rotary positional embeddings to input query and key tensors. It takes care of the angular transformations needed for rotary embeddings.</p>"},{"location":"zeta/nn/embeddings/sinusoidal/#parameters_2","title":"Parameters","text":"<ul> <li> <p><code>q</code> (Tensor): The query tensor to which rotary positional embeddings will be applied.</p> </li> <li> <p><code>k</code> (Tensor): The key tensor to which rotary positional embeddings will be applied.</p> </li> <li> <p><code>freqs</code> (Tensor): The frequency embeddings generated by the <code>SinusoidalEmbeddings</code> class.</p> </li> <li> <p><code>scale</code> (Tensor or float): The scale embeddings for rotary positional embeddings.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/sinusoidal/#usage-example_1","title":"Usage Example","text":"<pre><code>import torch\n\nfrom zeta import apply_rotary_pos_emb\n\n# Create query and key tensors\nq = torch.randn(2, 3, 4)\nk = torch.randn(2, 3, 4)\n\n# Generate frequency and scale embeddings using SinusoidalEmbeddings\n\n# Apply rotary positional embeddings\nq_emb, k_emb = apply_rotary_pos_emb(q, k, freqs, scale)\n</code></pre>"},{"location":"zeta/nn/embeddings/sinusoidal/#6-usage-examples","title":"6. Usage Examples","text":"<p>Let's explore some usage examples of the <code>SinusoidalEmbeddings</code> class and associated functions to understand how they can be used effectively.</p>"},{"location":"zeta/nn/embeddings/sinusoidal/#using-the-sinusoidalembeddings-class","title":"Using the <code>SinusoidalEmbeddings</code> Class","text":"<pre><code>import torch\n\nfrom zeta import SinusoidalEmbeddings\n\n# Create an instance of SinusoidalEmbeddings\npositional_embedding = SinusoidalEmbeddings(dim=512, use_xpos=True, scale_base=1000)\n\n# Create an input sequence tensor\nsequence = torch.randn(1, 10, 512)\n\n# Generate positional embeddings\nfreqs, scale = positional_embedding(sequence)\n</code></pre>"},{"location":"zeta/nn/embeddings/sinusoidal/#using-the-rotate_half-function","title":"Using the <code>rotate_half</code> Function","text":"<p>This example demonstrates how to use the <code>rotate_half</code> function:</p> <pre><code>import torch\n\nfrom zeta.nn import rotate_half\n\n# Create an input tensor\nx = torch.randn(2, 3, 4)\n\n# Rotate the input tensor\nrotated_x = rotate_half(x)\n</code></pre>"},{"location":"zeta/nn/embeddings/sinusoidal/#using-the-apply_rotary_pos_emb-function","title":"Using the <code>apply_rotary_pos_emb</code> Function","text":"<p>This example demonstrates how to apply rotary positional embeddings using the <code>apply_rotary_pos_emb</code> function:</p> <pre><code>import torch\n\nfrom zeta.nn import rotate_half\n\n# Create query and key tensors\nq = torch.randn(2, 3, 4)\nk = torch.randn(2, 3, 4)\n\n# Generate frequency and scale embeddings using SinusoidalEmbeddings\n\n# Apply rotary positional embeddings\nq_emb, k_emb = apply_rotary_pos_emb(q, k, freqs, scale)\n</code></pre>"},{"location":"zeta/nn/embeddings/sinusoidal/#7-additional-information","title":"7. Additional Information","text":""},{"location":"zeta/nn/embeddings/sinusoidal/#sinusoidal-positional-embeddings-a-namesinusoidal","title":"Sinusoidal Positional Embeddings &lt;a name=\"sinusoidal","text":"<p>-positional-embeddings\"&gt;</p> <p>Sinusoidal positional embeddings are essential for transformer-based models to understand the positions of elements within a sequence. They provide information about the order and location of tokens in the input sequence.</p>"},{"location":"zeta/nn/embeddings/sinusoidal/#rotary-positional-embeddings","title":"Rotary Positional Embeddings","text":"<p>Rotary positional embeddings are a specialized type of positional embedding that are particularly useful in some transformer variants. They involve angular transformations to capture positional information in a unique way.</p>"},{"location":"zeta/nn/embeddings/sinusoidal/#8-references","title":"8. References","text":"<p>For further information on positional embeddings in transformers and related concepts, you can refer to the following resources:</p> <ul> <li> <p>Attention Is All You Need - The original transformer paper introducing positional embeddings.</p> </li> <li> <p>Rotary Position Embeddings - A research paper discussing rotary positional embeddings and their advantages.</p> </li> <li> <p>PyTorch Documentation - Official PyTorch documentation for related functions and modules.</p> </li> </ul> <p>This documentation provides a comprehensive overview of the Zeta library's <code>SinusoidalEmbeddings</code> class and associated functions. It aims to help you understand the purpose, functionality, and usage of these components for positional embeddings in transformer-based models.</p>"},{"location":"zeta/nn/embeddings/truncated_rope/","title":"Module/Function Name: TruncatedRotaryEmbedding","text":"<p>The <code>TruncatedRotaryEmbedding</code> class is part of the Zeta library and is designed to implement the rotary embeddings with a truncation mechanism. The rotary embedding is a positional encoding method that aims to provide the model with information about the relative positions of the tokens in a sequence. The <code>TruncatedRotaryEmbedding</code> class extends the rotary embedding concept by incorporating a truncation mechanism, which sets the rotary embedding to zero for positions where the frequency is higher than a specified threshold.</p> <p>The architecture and workings of this class are inspired by the paper link to the paper.</p>"},{"location":"zeta/nn/embeddings/truncated_rope/#parameters","title":"Parameters:","text":"<ul> <li><code>dim</code> (int): Dimensionality of the embeddings.</li> <li><code>a</code> (float): Lower bound of the truncation region. Rotary embeddings with frequency lower than <code>a</code> will be set to zero.</li> <li><code>b</code> (float): Upper bound of the truncation region. Rotary embeddings with frequency higher than or equal to <code>b</code> will not be truncated.</li> <li><code>rho</code> (float): Value to which the rotary embeddings will be truncated in the region [a, b).</li> </ul> <p>The <code>dim</code> parameter is required to determine the dimensionality of the embeddings, while <code>a</code>, <code>b</code>, and <code>rho</code> are hyperparameters that control the truncation mechanism.</p>"},{"location":"zeta/nn/embeddings/truncated_rope/#method","title":"Method:","text":""},{"location":"zeta/nn/embeddings/truncated_rope/#forwardseq_len-device","title":"<code>forward(seq_len, device)</code>","text":"<p>Computes the truncated rotary embeddings for a given sequence length.</p>"},{"location":"zeta/nn/embeddings/truncated_rope/#parameters_1","title":"Parameters:","text":"<ul> <li><code>seq_len</code> (int): Length of the sequence for which the rotary embeddings are to be computed.</li> <li><code>device</code> (torch.device): Device on which the computations are to be performed.</li> </ul>"},{"location":"zeta/nn/embeddings/truncated_rope/#returns","title":"Returns:","text":"<ul> <li><code>result</code> (Tensor): A tensor containing the truncated rotary embeddings for the specified sequence length.</li> </ul>"},{"location":"zeta/nn/embeddings/truncated_rope/#functionality-and-usage","title":"Functionality and Usage:","text":"<p>The <code>TruncatedRotaryEmbedding</code> class is used to compute the truncated rotary embeddings for a given sequence length. The rotary embeddings are computed by multiplying a tensor containing the position indices of the tokens in the sequence by the inverse frequencies. The inverse frequencies are computed based on the specified embedding dimension <code>dim</code> and are stored in the <code>inv_freq</code> buffer.</p> <p>The truncation mechanism is implemented by creating a <code>theta_star</code> tensor, which is used to multiply the computed <code>freqs</code>. The <code>theta_star</code> tensor is created based on the specified <code>a</code>, <code>b</code>, and <code>rho</code> parameters, and the computed <code>freqs</code> tensor. For positions where the frequency is higher than or equal to <code>b</code>, the rotary embeddings are not truncated, and <code>theta_star</code> is set to the frequency at that position. For positions where the frequency is lower than <code>a</code>, the rotary embeddings are set to zero, and <code>theta_star</code> is set to zero. For positions where the frequency is in the range [a, b], the rotary embeddings are truncated to <code>rho</code>, and <code>theta_star</code> is set to <code>rho</code>.</p> <p>Once the <code>theta_star</code> tensor is created, it is multiplied element-wise by the <code>freqs</code> tensor to compute the final truncated rotary embeddings.</p>"},{"location":"zeta/nn/embeddings/truncated_rope/#usage-example","title":"Usage Example:","text":"<pre><code>import torch\n\nfrom zeta.nn.embeddings.truncated_rope import TruncatedRotaryEmbedding\n\n# Define the parameters\ndim = 64\na = 0.1\nb = 0.9\nrho = 0.5\nseq_len = 100\ndevice = torch.device(\"cuda\")\n\n# Create the TruncatedRotaryEmbedding module\ntrunc_rotary_emb = TruncatedRotaryEmbedding(dim, a, b, rho)\n\n# Compute the truncated rotary embeddings for the specified sequence length\nrotary_embeddings = trunc_rotary_emb(seq_len, device)\n\nprint(rotary_embeddings)\n</code></pre> <p>In this example, the <code>TruncatedRotaryEmbedding</code> module is created with the specified <code>dim</code>, <code>a</code>, <code>b</code>, and <code>rho</code> parameters. The <code>forward</code> method is then called with the specified <code>seq_len</code> and <code>device</code> parameters to compute the truncated rotary embeddings for a sequence of length <code>seq_len</code>.</p>"},{"location":"zeta/nn/embeddings/truncated_rope/#additional-information-and-tips","title":"Additional Information and Tips:","text":"<ul> <li> <p>The <code>a</code>, <code>b</code>, and <code>rho</code> parameters control the truncation mechanism and may need to be tuned based on the specific application and data being used. In particular, the <code>a</code> parameter should be set to a value that effectively removes the high-frequency noise in the rotary embeddings, while the <code>b</code> parameter should be set to a value that retains the useful positional information in the rotary embeddings.</p> </li> <li> <p>The <code>dim</code> parameter should be set to the same value as the embedding dimension used in the model.</p> </li> <li> <p>The <code>device</code> parameter in the <code>forward</code> method should be set to the same device on which the model is being trained.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/truncated_rope/#mathematical-formulation","title":"Mathematical Formulation:","text":"<p>The mathematical formulation of the truncated rotary embeddings can be expressed as follows:</p> <p>[ \\text{freqs} = t \\cdot \\text{inv_freq} ]</p> <p>[ \\theta = \\text{base}^{-2 \\cdot i / \\text{dim}}, \\, i = 0, 2, \\ldots, \\text{dim}-2 ]</p> <p>[ \\theta^* =  \\begin{cases} 0, &amp; \\text{if } \\theta &lt; a \\ \\rho, &amp; \\text{if } a \\leq \\theta &lt; b \\ \\theta, &amp; \\text{if } \\theta \\geq b \\end{cases} ]</p> <p>[ \\text{result} = \\text{freqs} \\cdot \\theta^* ]</p> <p>Where:</p> <ul> <li>( t ) is a tensor containing the position indices of the tokens in the sequence.</li> <li>( \\text{inv_freq} ) is a tensor containing the inverse frequencies computed based on the specified <code>dim</code> parameter.</li> <li>( \\text{freqs} ) is a tensor containing the computed frequencies for each position in the sequence.</li> <li>( \\theta ) is a tensor containing the computed theta values for each position in the sequence.</li> <li>( \\theta^* ) is a tensor containing the truncated theta values for each position in the sequence.</li> <li>( \\text{result} ) is the final tensor containing the truncated rotary embeddings for each position in the sequence.</li> </ul>"},{"location":"zeta/nn/embeddings/truncated_rope/#references-and-resources","title":"References and Resources:","text":"<ul> <li>Paper: Link to the paper</li> </ul> <p>For further exploration and implementation details, refer to the paper linked above.</p>"},{"location":"zeta/nn/embeddings/vis_emb/","title":"<code>VisionEmbedding</code> Documentation","text":""},{"location":"zeta/nn/embeddings/vis_emb/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Class: <code>VisionEmbedding</code></li> <li>Initialization</li> <li>Parameters</li> <li>Forward Method</li> <li>Usage Examples</li> <li>Using the <code>VisionEmbedding</code> Class</li> <li>Additional Information</li> <li>Image to Patch Embedding</li> <li>References</li> </ol>"},{"location":"zeta/nn/embeddings/vis_emb/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation for the <code>VisionEmbedding</code> class! Zeta is a powerful library for deep learning in PyTorch, and this documentation will provide a comprehensive understanding of the <code>VisionEmbedding</code> class and its functionalities.</p>"},{"location":"zeta/nn/embeddings/vis_emb/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>VisionEmbedding</code> class is designed for converting images into patch embeddings, making them suitable for processing by transformer-based models. This class plays a crucial role in various computer vision tasks and enables the integration of vision data into transformer architectures.</p>"},{"location":"zeta/nn/embeddings/vis_emb/#3-class-visionembedding","title":"3. Class: <code>VisionEmbedding</code>","text":"<p>The <code>VisionEmbedding</code> class handles the transformation of images into patch embeddings. It offers flexibility in configuring the embedding process to suit different requirements.</p>"},{"location":"zeta/nn/embeddings/vis_emb/#initialization","title":"Initialization","text":"<p>To create an instance of the <code>VisionEmbedding</code> class, you need to specify the following parameters:</p> <pre><code>VisionEmbedding(\n    img_size=224,\n    patch_size=16,\n    in_chans=3,\n    embed_dim=768,\n    contain_mask_token=False,\n    prepend_cls_token=False,\n)\n</code></pre>"},{"location":"zeta/nn/embeddings/vis_emb/#parameters","title":"Parameters","text":"<ul> <li> <p><code>img_size</code> (int or tuple, optional): The size of the input image. If a single integer is provided, it is assumed that the image is square. Default is <code>224</code>.</p> </li> <li> <p><code>patch_size</code> (int or tuple, optional): The size of each patch. If a single integer is provided, square patches are used. Default is <code>16</code>.</p> </li> <li> <p><code>in_chans</code> (int, optional): The number of input channels in the image. Default is <code>3</code> (for RGB images).</p> </li> <li> <p><code>embed_dim</code> (int, optional): The dimensionality of the patch embeddings. Default is <code>768</code>.</p> </li> <li> <p><code>contain_mask_token</code> (bool, optional): Whether to include a mask token in the embeddings. Default is <code>False</code>.</p> </li> <li> <p><code>prepend_cls_token</code> (bool, optional): Whether to include a class (CLS) token at the beginning of the embeddings. Default is <code>False</code>.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/vis_emb/#forward-method","title":"Forward Method","text":"<p>The <code>forward</code> method of the <code>VisionEmbedding</code> class performs the image-to-patch embedding transformation. It can be called as follows:</p> <pre><code>output = vision_embedding(input_image, masked_position=None, **kwargs)\n</code></pre> <ul> <li> <p><code>input_image</code> (Tensor): The input image tensor to be converted into patch embeddings.</p> </li> <li> <p><code>masked_position</code> (Tensor, optional): A tensor indicating positions to be masked in the embeddings. This is useful for tasks like image inpainting. Default is <code>None</code>.</p> </li> <li> <p><code>**kwargs</code>: Additional keyword arguments. These are not mandatory and depend on the specific use case.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/vis_emb/#4-usage-examples","title":"4. Usage Examples","text":"<p>Let's explore a usage example of the <code>VisionEmbedding</code> class to understand how to use it effectively.</p>"},{"location":"zeta/nn/embeddings/vis_emb/#using-the-visionembedding-class","title":"Using the <code>VisionEmbedding</code> Class","text":"<pre><code>import torch\n\nfrom zeta import VisionEmbedding\n\n# Create an instance of VisionEmbedding\nvision_embedding = VisionEmbedding(\n    img_size=224,\n    patch_size=16,\n    in_chans=3,\n    embed_dim=768,\n    contain_mask_token=True,\n    prepend_cls_token=True,\n)\n\n# Load an example image (3 channels, 224x224)\ninput_image = torch.rand(1, 3, 224, 224)\n\n# Perform image-to-patch embedding\noutput = vision_embedding(input_image)\n\n# The output now contains patch embeddings, ready for input to a transformer model\n</code></pre>"},{"location":"zeta/nn/embeddings/vis_emb/#5-additional-information","title":"5. Additional Information","text":""},{"location":"zeta/nn/embeddings/vis_emb/#image-to-patch-embedding","title":"Image to Patch Embedding","text":"<p>Image to patch embedding is a fundamental step in adapting images for processing by transformer-based models. It divides the image into smaller patches, converts each patch into an embedding, and optionally includes tokens for masking and classification. This process allows transformer models to handle image data effectively.</p>"},{"location":"zeta/nn/embeddings/vis_emb/#6-references","title":"6. References","text":"<p>For further information on image to patch embedding and its applications, you can refer to the following resources:</p> <ul> <li> <p>Transformers in Computer Vision - A research paper discussing the application of transformers in computer vision tasks.</p> </li> <li> <p>PyTorch Documentation - Official PyTorch documentation for related concepts and functions.</p> </li> </ul> <p>This documentation provides a comprehensive overview of the Zeta library's <code>VisionEmbedding</code> class and its role in transforming images into patch embeddings. It aims to help you understand the purpose, functionality, and usage of this component for computer vision tasks and transformer-based models.</p>"},{"location":"zeta/nn/embeddings/xpos/","title":"<code>XPOS</code> Documentation","text":""},{"location":"zeta/nn/embeddings/xpos/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Class: <code>XPOS</code></li> <li>Initialization</li> <li>Parameters</li> <li>Forward Method</li> <li>Functions</li> <li><code>fixed_pos_embedding</code></li> <li><code>rotate_every_two</code></li> <li><code>duplicate_interleave</code></li> <li><code>apply_rotary_pos_emb</code></li> <li>Usage Examples</li> <li>Using the <code>XPOS</code> Class</li> <li>Using the Functions</li> <li>Additional Information</li> <li>Positional Embeddings in Transformers</li> <li>References</li> </ol>"},{"location":"zeta/nn/embeddings/xpos/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation for the <code>XPOS</code> class and related functions! Zeta is a powerful library for deep learning in PyTorch, and this documentation will provide a comprehensive understanding of the <code>XPOS</code> class and its associated functions.</p>"},{"location":"zeta/nn/embeddings/xpos/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>XPOS</code> class and its related functions are designed to generate and apply rotary positional embeddings to input tensors. These embeddings are crucial for sequence-to-sequence models, particularly in transformer architectures. Below, we will explore their purpose and functionality.</p>"},{"location":"zeta/nn/embeddings/xpos/#3-class-xpos","title":"3. Class: <code>XPOS</code>","text":"<p>The <code>XPOS</code> class is used to apply rotary positional embeddings to input tensors. These embeddings are essential for transformers to understand the positional information of elements in a sequence.</p>"},{"location":"zeta/nn/embeddings/xpos/#initialization","title":"Initialization","text":"<p>To create an instance of the <code>XPOS</code> class, you need to specify the following parameters:</p> <pre><code>XPOS(\n    head_dim: int = None,\n    scale_base: int = 512\n)\n</code></pre>"},{"location":"zeta/nn/embeddings/xpos/#parameters","title":"Parameters","text":"<ul> <li> <p><code>head_dim</code> (int, optional): The dimensionality of the positional embeddings. If not specified, it defaults to <code>None</code>, which is used to calculate the dimension based on the input tensor. It is recommended to set this value explicitly for consistency.</p> </li> <li> <p><code>scale_base</code> (int, optional): The base value for scaling the positional embeddings. Default is <code>512</code>.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/xpos/#forward-method","title":"Forward Method","text":"<p>The <code>forward</code> method of the <code>XPOS</code> class applies rotary positional embeddings to the input tensor. It can be called as follows:</p> <pre><code>output = xpos(input_tensor, offset=0, downscale=False)\n</code></pre> <ul> <li> <p><code>input_tensor</code> (Tensor): The input tensor to which positional embeddings will be applied.</p> </li> <li> <p><code>offset</code> (int, optional): An offset value for positional embeddings. Default is <code>0</code>.</p> </li> <li> <p><code>downscale</code> (bool, optional): If <code>True</code>, the positional embeddings are downscaled. Default is <code>False</code>.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/xpos/#4-functions","title":"4. Functions","text":"<p>In addition to the <code>XPOS</code> class, there are several functions provided for working with positional embeddings.</p>"},{"location":"zeta/nn/embeddings/xpos/#fixed_pos_embedding","title":"<code>fixed_pos_embedding</code>","text":"<p>This function generates fixed sine and cosine positional embeddings based on the input tensor's scale.</p> <pre><code>sin, cos = fixed_pos_embedding(x)\n</code></pre> <ul> <li><code>x</code> (Tensor): Input tensor of shape <code>(seq_len, dim)</code>.</li> </ul>"},{"location":"zeta/nn/embeddings/xpos/#rotate_every_two","title":"<code>rotate_every_two</code>","text":"<p>This function rearranges the elements of the input tensor by rotating every two elements.</p> <pre><code>output_tensor = rotate_every_two(input_tensor)\n</code></pre> <ul> <li><code>input_tensor</code> (Tensor): Input tensor of shape <code>(batch_size, seq_len, dim)</code>.</li> </ul>"},{"location":"zeta/nn/embeddings/xpos/#duplicate_interleave","title":"<code>duplicate_interleave</code>","text":"<p>This function duplicates a matrix while interleaving the copy.</p> <pre><code>duplicated_matrix = duplicate_interleave(matrix)\n</code></pre> <ul> <li><code>matrix</code> (Tensor): Input matrix.</li> </ul>"},{"location":"zeta/nn/embeddings/xpos/#apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb</code>","text":"<p>This function applies rotary positional embeddings to the input tensor.</p> <pre><code>output_tensor = apply_rotary_pos_emb(input_tensor, sin, cos, scale=1)\n</code></pre> <ul> <li><code>input_tensor</code> (Tensor): Input tensor of shape <code>(batch_size, seq_len, dim)</code>.</li> <li><code>sin</code> (Tensor): Sine positional embeddings of shape <code>(seq_len, dim)</code>.</li> <li><code>cos</code> (Tensor): Cosine positional embeddings of shape <code>(seq_len, dim)</code>.</li> <li><code>scale</code> (float): Scaling factor for the positional embeddings.</li> </ul>"},{"location":"zeta/nn/embeddings/xpos/#5-usage-examples","title":"5. Usage Examples","text":"<p>Let's explore some usage examples of the <code>XPOS</code> class and related functions to understand how to use them effectively.</p>"},{"location":"zeta/nn/embeddings/xpos/#using-the-xpos-class","title":"Using the <code>XPOS</code> Class","text":"<pre><code>import torch\n\nfrom zeta.nn import XPOS\n\n# Create an XPOS instance\nxpos = XPOS(head_dim=256, scale_base=512)\n\n# Apply positional embeddings to an input tensor\ninput_tensor = torch.rand(16, 32, 256)  # Example input tensor\noutput = xpos(input_tensor, offset=0, downscale=False)\n</code></pre>"},{"location":"zeta/nn/embeddings/xpos/#using-the-functions","title":"Using the Functions","text":"<pre><code>import torch\n\nfrom zeta.nn import (\n    apply_rotary_pos_emb,\n    duplicate_interleave,\n    fixed_pos_embedding,\n    rotate_every_two,\n)\n\n# Generate fixed positional embeddings\ninput_tensor = torch.rand(32, 512)  # Example input tensor\nsin, cos = fixed_pos_embedding(input_tensor)\n\n# Rotate every two elements in a tensor\ninput_tensor = torch.rand(16, 64, 256)  # Example input tensor\noutput_tensor = rotate_every_two(input_tensor)\n\n# Duplicate and interleave a matrix\ninput_matrix = torch.rand(8, 8)  # Example input matrix\nduplicated_matrix = duplicate_interleave(input_matrix)\n\n# Apply rotary positional embeddings\ninput_tensor = torch.rand(16, 32, 256)  # Example input tensor\noutput_tensor = apply_rotary_pos_emb(input_tensor, sin, cos, scale=1)\n</code></pre>"},{"location":"zeta/nn/embeddings/xpos/#6-additional-information","title":"6. Additional Information","text":""},{"location":"zeta/nn/embeddings/xpos/#positional-embeddings-in-transformers","title":"Positional Embeddings in Transformers","text":"<p>Positional embeddings play a crucial role in transformers and other sequence-to-sequence models. They enable the model to understand the order of elements in a sequence, which is essential for tasks like natural language processing, machine translation, and text generation.</p>"},{"location":"zeta/nn/embeddings/xpos/#7-references","title":"7. References","text":"<p>This documentation provides a comprehensive guide to the <code>XPOS</code> class and related functions in the Zeta library, explaining their purpose, functionality, parameters, and usage. You can now effectively integrate these components into your deep learning models, particularly in transformer-based architectures, for various sequence-based tasks.</p> <p>For further information on the underlying concepts and principles of positional embeddings in</p> <p>transformers, you may refer to the original paper:</p> <ul> <li>Attention Is All You Need (Transformer)</li> </ul> <p>Please consult the official PyTorch documentation for any specific PyTorch-related details: PyTorch Documentation.</p>"},{"location":"zeta/nn/embeddings/yarn/","title":"<code>YarnEmbedding</code> Documentation","text":""},{"location":"zeta/nn/embeddings/yarn/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Class: <code>YarnEmbedding</code></li> <li>Initialization</li> <li>Parameters</li> <li>Forward Method</li> <li>Helpers and Functions</li> <li><code>find_correction_dim</code></li> <li><code>find_correction_range</code></li> <li><code>linear_ramp_mask</code></li> <li><code>get_mscale</code></li> <li>Usage Examples</li> <li>Using the <code>YarnEmbedding</code> Class</li> <li>Using the Helper Functions</li> <li>Additional Information</li> <li>Positional Embeddings in Transformers</li> <li>References</li> </ol>"},{"location":"zeta/nn/embeddings/yarn/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation for the <code>YarnEmbedding</code> class and related functions! Zeta is a powerful library for deep learning in PyTorch, and this documentation will provide a comprehensive understanding of the <code>YarnEmbedding</code> class and its associated functions.</p>"},{"location":"zeta/nn/embeddings/yarn/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>YarnEmbedding</code> class and its related functions are designed to generate and apply advanced positional embeddings to input tensors. These embeddings are crucial for sequence-to-sequence models, particularly in transformer architectures. Below, we will explore their purpose and functionality.</p>"},{"location":"zeta/nn/embeddings/yarn/#3-class-yarnembedding","title":"3. Class: <code>YarnEmbedding</code>","text":"<p>The <code>YarnEmbedding</code> class is used to apply advanced positional embeddings to input tensors. It offers a highly configurable approach to generating embeddings tailored to the needs of transformer-based models.</p>"},{"location":"zeta/nn/embeddings/yarn/#initialization","title":"Initialization","text":"<p>To create an instance of the <code>YarnEmbedding</code> class, you need to specify the following parameters:</p> <pre><code>YarnEmbedding(\n    dim,\n    max_position_embeddings=2048,\n    base=10000,\n    original_max_position_embeddings=2048,\n    extrapolation_factor=1,\n    attn_factor=1,\n    beta_fast=32,\n    beta_slow=1,\n    finetuned=False,\n    device=None,\n)\n</code></pre>"},{"location":"zeta/nn/embeddings/yarn/#parameters","title":"Parameters","text":"<ul> <li> <p><code>dim</code> (int): The dimensionality of the positional embeddings.</p> </li> <li> <p><code>max_position_embeddings</code> (int, optional): The maximum number of position embeddings to be generated. Default is <code>2048</code>.</p> </li> <li> <p><code>base</code> (int, optional): The base value for calculating the positional embeddings. Default is <code>10000</code>.</p> </li> <li> <p><code>original_max_position_embeddings</code> (int, optional): The original maximum number of position embeddings used for fine-tuning. Default is <code>2048</code>.</p> </li> <li> <p><code>extrapolation_factor</code> (int, optional): The factor used for extrapolating positional embeddings beyond the original maximum. Default is <code>1</code>.</p> </li> <li> <p><code>attn_factor</code> (int, optional): A factor affecting the positional embeddings for attention. Default is <code>1</code>.</p> </li> <li> <p><code>beta_fast</code> (int, optional): A parameter used for interpolation. Default is <code>32</code>.</p> </li> <li> <p><code>beta_slow</code> (int, optional): A parameter used for interpolation. Default is <code>1</code>.</p> </li> <li> <p><code>finetuned</code> (bool, optional): Whether to use finetuned embeddings. Default is <code>False</code>.</p> </li> <li> <p><code>device</code> (torch.device, optional): If specified, the device to which tensors will be moved.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/yarn/#forward-method","title":"Forward Method","text":"<p>The <code>forward</code> method of the <code>YarnEmbedding</code> class applies advanced positional embeddings to the input tensor. It can be called as follows:</p> <pre><code>output = yarn_embedding(input_tensor, seq_len)\n</code></pre> <ul> <li> <p><code>input_tensor</code> (Tensor): The input tensor to which positional embeddings will be applied.</p> </li> <li> <p><code>seq_len</code> (int): The length of the sequence for which embeddings should be generated.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/yarn/#4-helpers-and-functions","title":"4. Helpers and Functions","text":"<p>In addition to the <code>YarnEmbedding</code> class, there are several functions provided for working with positional embeddings.</p>"},{"location":"zeta/nn/embeddings/yarn/#find_correction_dim","title":"<code>find_correction_dim</code>","text":"<p>This function calculates the correction dimension based on the number of rotations and other parameters.</p> <pre><code>correction_dim = find_correction_dim(num_rotations, dim, base, max_position_embeddings)\n</code></pre> <ul> <li> <p><code>num_rotations</code> (int): The number of rotations.</p> </li> <li> <p><code>dim</code> (int): The dimensionality of the positional embeddings.</p> </li> <li> <p><code>base</code> (int): The base value for calculating the positional embeddings.</p> </li> <li> <p><code>max_position_embeddings</code> (int): The maximum number of position embeddings.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/yarn/#find_correction_range","title":"<code>find_correction_range</code>","text":"<p>This function calculates the correction range based on low and high rotation values.</p> <pre><code>low, high = find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings)\n</code></pre> <ul> <li> <p><code>low_rot</code> (int): The low rotation value.</p> </li> <li> <p><code>high_rot</code> (int): The high rotation value.</p> </li> <li> <p><code>dim</code> (int): The dimensionality of the positional embeddings.</p> </li> <li> <p><code>base</code> (int): The base value for calculating the positional embeddings.</p> </li> <li> <p><code>max_position_embeddings</code> (int): The maximum number of position embeddings.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/yarn/#linear_ramp_mask","title":"<code>linear_ramp_mask</code>","text":"<p>This function generates a linear ramp mask.</p> <pre><code>ramp_mask = linear_ramp_mask(min, max, dim)\n</code></pre> <ul> <li> <p><code>min</code> (float): The minimum value.</p> </li> <li> <p><code>max</code> (float): The maximum value.</p> </li> <li> <p><code>dim</code> (int): The dimensionality of the mask.</p> </li> </ul>"},{"location":"zeta/nn/embeddings/yarn/#get_mscale","title":"<code>get_mscale</code>","text":"<p>This function calculates the scale factor for positional embeddings.</p> <pre><code>mscale = get_mscale(scale)\n</code></pre> <ul> <li><code>scale</code> (float): The scale factor.</li> </ul>"},{"location":"zeta/nn/embeddings/yarn/#5-usage-examples","title":"5. Usage Examples","text":"<p>Let's explore some usage examples of the <code>YarnEmbedding</code> class and related functions to understand how to use them effectively.</p>"},{"location":"zeta/nn/embeddings/yarn/#using-the-yarnembedding-class","title":"Using the <code>YarnEmbedding</code> Class","text":"<pre><code>import torch\n\nfrom zeta.nn import YarnEmbedding\n\n# Create an instance of YarnEmbedding\nyarn_embedding = YarnEmbedding(dim=256, max_position_embeddings=2048)\n\n# Apply positional embeddings to an input tensor\ninput_tensor = torch.rand(16, 32, 256)  # Example input tensor\noutput = yarn_embedding(input_tensor, seq_len=32)\n</code></pre>"},{"location":"zeta/nn/embeddings/yarn/#using-the-helper-functions","title":"Using the Helper Functions","text":"<pre><code>from zeta.nn import find_correction_dim, find_correction_range, linear_ramp_mask, get_mscale\nimport torch\n\n# Calculate correction dimension\ncorrection_dim = find_correction_dim(num_rotations=8, dim=256, base=10000, max_position_embeddings=2048)\n\n# Calculate correction range\nlow\n\n, high = find_correction_range(low_rot=16, high_rot=32, dim=256, base=10000, max_position_embeddings=2048)\n\n# Generate linear ramp mask\nramp_mask = linear_ramp_mask(min=0.2, max=0.8, dim=128)\n\n# Calculate mscale\nmscale = get_mscale(scale=2.0)\n</code></pre>"},{"location":"zeta/nn/embeddings/yarn/#6-additional-information","title":"6. Additional Information","text":""},{"location":"zeta/nn/embeddings/yarn/#positional-embeddings-in-transformers","title":"Positional Embeddings in Transformers","text":"<p>Positional embeddings play a crucial role in transformer architectures, allowing models to capture the sequential order of data. These embeddings are especially important for tasks involving sequences, such as natural language processing (NLP) and time series analysis.</p>"},{"location":"zeta/nn/embeddings/yarn/#7-references","title":"7. References","text":"<p>For further information on positional embeddings and transformers, you can refer to the following resources:</p> <ul> <li> <p>Attention Is All You Need (Transformer) - The original transformer paper.</p> </li> <li> <p>PyTorch Documentation - Official PyTorch documentation for related concepts and functions.</p> </li> </ul> <p>This documentation provides a comprehensive overview of the Zeta library's <code>YarnEmbedding</code> class and related functions. It aims to help you understand the purpose, functionality, and usage of these components for advanced positional embeddings in your deep learning projects.</p>"},{"location":"zeta/nn/models/maxvit/","title":"<code>MaxVit</code> Documentation","text":""},{"location":"zeta/nn/models/maxvit/#overview","title":"Overview","text":"<p>Welcome to the documentation for the Zeta library! Zeta is a state-of-the-art deep learning framework designed to empower researchers and practitioners in the field of machine learning. With a focus on modularity, efficiency, and performance, Zeta offers a wide range of tools and modules for building and training advanced neural network models.</p> <p>This comprehensive documentation will provide a deep and thorough understanding of the code for the <code>MaxVit</code> class within the Zeta library. We will cover its architecture, purpose, parameters, and usage through extensive examples. By the end of this documentation, you'll have a solid grasp of how to leverage the power of <code>MaxVit</code> in your machine learning projects.</p>"},{"location":"zeta/nn/models/maxvit/#table-of-contents","title":"Table of Contents","text":"<ol> <li>MaxVit Class</li> <li>Introduction</li> <li>Architecture</li> <li>Parameters</li> <li>Usage Examples</li> </ol>"},{"location":"zeta/nn/models/maxvit/#maxvit-class","title":"MaxVit Class","text":""},{"location":"zeta/nn/models/maxvit/#introduction","title":"Introduction","text":"<p>The <code>MaxVit</code> class is a key component of the Zeta library, offering a powerful vision transformer architecture for image classification tasks. It incorporates cutting-edge techniques to efficiently process image data and produce accurate predictions. In this section, we will dive deep into the architecture, parameters, and usage of the <code>MaxVit</code> class.</p>"},{"location":"zeta/nn/models/maxvit/#architecture","title":"Architecture","text":"<p>The <code>MaxVit</code> class follows a multi-stage architecture to process image data effectively:</p> <ol> <li> <p>Convolutional Stem: The input image undergoes a series of convolutional layers to extract basic features. This step serves as the initial feature extraction.</p> </li> <li> <p>Transformer Stages: <code>MaxVit</code> consists of multiple transformer stages, each containing several transformer blocks. These stages allow the model to capture hierarchical features at different resolutions. Each transformer block includes the following components:</p> </li> <li>MBConv: MobileNetV2-like convolutional block with expansion and shrinkage.</li> <li>Grid Attention: Efficient attention mechanism tailored for grid-like structures.</li> <li> <p>FeedForward: Standard feedforward neural network layer.</p> </li> <li> <p>MLP Head: The final feature representation is passed through a multi-layer perceptron (MLP) head to produce predictions.</p> </li> </ol>"},{"location":"zeta/nn/models/maxvit/#parameters","title":"Parameters","text":"<p>The <code>MaxVit</code> class accepts various parameters that allow you to customize its behavior. Here are the key parameters:</p> <ul> <li><code>num_classes</code>: The number of output classes.</li> <li><code>dim</code>: The dimension of the feature embeddings.</li> <li><code>depth</code>: A tuple indicating the number of transformer blocks at each stage.</li> <li><code>dim_head</code>: The dimension of each attention head.</li> <li><code>dim_conv_stem</code>: The dimension of the convolutional stem output.</li> <li><code>window_size</code>: The size of the window for efficient grid-like attention.</li> <li><code>mbconv_expansion_rate</code>: The expansion rate for the MBConv blocks.</li> <li><code>mbconv_shrinkage_rate</code>: The shrinkage rate for the MBConv blocks.</li> <li><code>dropout</code>: Dropout rate applied within the model.</li> <li><code>channels</code>: The number of input image channels.</li> </ul>"},{"location":"zeta/nn/models/maxvit/#usage-examples","title":"Usage Examples","text":"<p>To illustrate the usage of the <code>MaxVit</code> class, let's explore three examples:</p>"},{"location":"zeta/nn/models/maxvit/#example-1-creating-a-maxvit-model","title":"Example 1: Creating a MaxVit Model","text":"<pre><code>from zeta import MaxVit\n\n# Create a MaxVit model with custom parameters\nmodel = MaxVit(\n    num_classes=1000,\n    dim=512,\n    depth=(2, 2, 6, 3),  # Depth of transformer blocks at each stage\n    dim_head=32,\n    dim_conv_stem=64,\n    window_size=7,\n    mbconv_expansion_rate=4,\n    mbconv_shrinkage_rate=0.25,\n    dropout=0.01,\n    channels=3,\n)\n</code></pre> <p>In this example, we create a <code>MaxVit</code> model with specific parameters, including the number of classes and model depth.</p>"},{"location":"zeta/nn/models/maxvit/#example-2-forward-pass-with-input-image","title":"Example 2: Forward Pass with Input Image","text":"<pre><code>import torch\n\n# Generate a random input image\nimg = torch.randn(1, 3, 256, 256)\n\n# Perform a forward pass with the MaxVit model\npreds = model(img)\n</code></pre> <p>Here, we apply the <code>MaxVit</code> model to a random input image to obtain predictions.</p>"},{"location":"zeta/nn/models/maxvit/#example-3-extracting-feature-embeddings","title":"Example 3: Extracting Feature Embeddings","text":"<pre><code># Create a MaxVit model with return_embeddings=True to extract feature embeddings\nfeature_model = MaxVit(\n    num_classes=1000,\n    dim=512,\n    depth=(2, 2, 6, 3),\n    dim_head=32,\n    dim_conv_stem=64,\n    window_size=7,\n    mbconv_expansion_rate=4,\n    mbconv_shrinkage_rate=0.25,\n    dropout=0.01,\n    channels=3\n    return_embeddings=True\n)\n\n# Forward pass to obtain feature embeddings\nfeature_embeddings = feature_model(img)\n</code></pre> <p>In this example, we configure the <code>MaxVit</code> model to return feature embeddings, allowing you to use the model as a feature extractor.</p>"},{"location":"zeta/nn/models/maxvit/#conclusion","title":"Conclusion","text":"<p>This concludes the documentation for the <code>MaxVit</code> class within the Zeta library. You've gained a comprehensive understanding of its architecture, parameters, and practical usage. The <code>MaxVit</code> model is a powerful tool for image classification tasks, and with Zeta, you can leverage its capabilities to achieve impressive results.</p> <p>For more information and updates, please refer to the official Zeta documentation and resources.</p>"},{"location":"zeta/nn/models/megavit/","title":"<code>MegaVit</code>: A Vision Transformer Model","text":"<p>MegaVit is a variant of the Vision Transformer (ViT) model that has been designed for computer vision tasks. It represents a significant advancement in the application of transformers to visual data, offering a scalable and effective means of processing image data without relying on conventional convolutional networks.</p>"},{"location":"zeta/nn/models/megavit/#overview","title":"Overview","text":"<p>Vision Transformers represent an innovative approach to processing visual data. Instead of relying on convolutional layers to extract features from images, they use the power of transformers to analyze patches of an image. Each patch is treated as a token, similar to how natural language processing models treat words as tokens.</p> <p>The MegaVit model uses the transformer architecture to process these image patches, aggregating information across the image to produce an output representation that can be used for classification or other tasks.</p>"},{"location":"zeta/nn/models/megavit/#key-concepts","title":"Key Concepts","text":"<ol> <li>Patch Embedding: The image is split into fixed-size patches. Each patch is linearly embedded into a vector. These vectors serve as the input tokens for the transformer.</li> <li>Positional Embedding: Due to the lack of inherent sequence in images, a positional embedding is added to provide the model with information about the location of each patch.</li> <li>Transformer Layers: These are the core of the model, processing the patch embeddings through multiple layers of attention and feed-forward networks.</li> <li>Pooling: After processing through the transformers, the patch representations are pooled (either by taking the class token or averaging) to produce a singular representation of the image.</li> <li>MLP Head: A final multi-layer perceptron (MLP) head is used to produce the output, such as class probabilities for an image classification task.</li> </ol>"},{"location":"zeta/nn/models/megavit/#model-definition","title":"Model Definition","text":""},{"location":"zeta/nn/models/megavit/#megavit","title":"<code>MegaVit</code>","text":"<pre><code>class MegaVit(nn.Module):\n</code></pre>"},{"location":"zeta/nn/models/megavit/#parameters","title":"Parameters:","text":"<ul> <li><code>image_size</code> (<code>int</code>): The size of the input image. Both height and width must be divisible by the <code>patch_size</code>.</li> <li><code>patch_size</code> (<code>int</code>): The size of each patch. The image is divided into patches of this size.</li> <li><code>num_classes</code> (<code>int</code>): The number of output classes for classification.</li> <li><code>dim</code> (<code>int</code>): The dimensionality of the patch embedding.</li> <li><code>depth</code> (<code>int</code>): The number of transformer layers.</li> <li><code>heads</code> (<code>int</code>): The number of attention heads in the multi-head attention mechanism.</li> <li><code>mlp_dim</code> (<code>int</code>): The dimensionality of the MLP layers within the transformer.</li> <li><code>pool</code> (<code>str</code>): The pooling method to use. Can be either 'cls' for class token pooling or 'mean' for mean pooling.</li> <li><code>channels</code> (<code>int</code>): The number of input channels (e.g., 3 for RGB images).</li> <li><code>dim_head</code> (<code>int</code>): The dimensionality of each head in the multi-head attention mechanism.</li> <li><code>dropout</code> (<code>float</code>): Dropout rate applied within the transformer.</li> <li><code>emb_dropout</code> (<code>float</code>): Dropout rate applied to the embeddings.</li> </ul>"},{"location":"zeta/nn/models/megavit/#returns","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: A tensor of shape <code>(batch_size, num_classes)</code>, representing the class logits for each image in the batch.</li> </ul>"},{"location":"zeta/nn/models/megavit/#methods","title":"Methods:","text":""},{"location":"zeta/nn/models/megavit/#forwardimg-torchtensor-torchtensor","title":"<code>forward(img: torch.Tensor) -&gt; torch.Tensor</code>","text":"<ul> <li>Parameters:</li> <li><code>img</code> (<code>torch.Tensor</code>): A batch of images of shape <code>(batch_size, channels, image_size, image_size)</code>.</li> <li>Returns:</li> <li><code>torch.Tensor</code>: The class logits for each image in the batch.</li> </ul>"},{"location":"zeta/nn/models/megavit/#architecture","title":"Architecture:","text":"<ol> <li>Patch Embedding: The input image is passed through a patch embedding layer, converting the image patches into linear embeddings.</li> <li>Positional Embedding: Adds positional information to the patch embeddings.</li> <li>Dropout: Dropout is applied for regularization.</li> <li>Transformer: The embedded patches are processed through a series of transformer layers.</li> <li>Pooling: The patch embeddings are pooled to produce a single vector representation for the entire image.</li> <li>MLP Head: An MLP head produces the final output logits for classification.</li> <li>Output: The logits are returned.</li> </ol>"},{"location":"zeta/nn/models/megavit/#usage-example","title":"Usage Example:","text":"<pre><code>from zeta.models import MegaVit\n\nmodel = MegaVit(\n    image_size=256,\n    patch_size=32,\n    num_classes=1000,\n    dim=512,\n    depth=6,\n    heads=8,\n    mlp_dim=1024,\n    dropout=0.1,\n    emb_dropout=0.1,\n)\n\nimg = torch.randn(1, 3, 256, 256)\npreds = model(img)  # Shape: (1, 1000)\n</code></pre>"},{"location":"zeta/nn/models/megavit/#notes","title":"Notes:","text":"<ol> <li>The image dimensions must be divisible by the patch size. This ensures that the image can be evenly divided into patches.</li> <li>The choice of pooling ('cls' vs 'mean') can have an impact on performance. 'cls' pooling uses a special class token, while 'mean' pooling averages the patch embeddings.</li> <li>Regularization through dropout can help prevent overfitting, especially when training on smaller datasets.</li> </ol>"},{"location":"zeta/nn/models/navit/","title":"<code>NaVit</code> Documentation","text":""},{"location":"zeta/nn/models/navit/#overview","title":"Overview","text":"<p>Welcome to the comprehensive documentation for the Zeta library! Zeta is a sophisticated deep learning framework created to empower machine learning practitioners and researchers with state-of-the-art tools and modules. This documentation will provide an in-depth understanding of the <code>NaViT</code> class, its architecture, purpose, parameters, and practical usage.</p>"},{"location":"zeta/nn/models/navit/#table-of-contents","title":"Table of Contents","text":"<ol> <li>NaViT Class</li> <li>Introduction</li> <li>Architecture</li> <li>Parameters</li> <li>Usage Examples</li> </ol>"},{"location":"zeta/nn/models/navit/#navit-class","title":"NaViT Class","text":""},{"location":"zeta/nn/models/navit/#introduction","title":"Introduction","text":"<p>The <code>NaViT</code> class is a vital component of the Zeta library, offering an advanced vision transformer architecture for a wide range of image-related tasks, including image classification. This class incorporates innovative techniques to efficiently process images, handle variable-length sequences, and produce accurate predictions. In this section, we will delve into the architecture, parameters, and usage of the <code>NaViT</code> class.</p>"},{"location":"zeta/nn/models/navit/#architecture","title":"Architecture","text":"<p>The <code>NaViT</code> class follows a complex architecture designed to handle image data effectively:</p> <ol> <li> <p>Image Patching: The input image is divided into non-overlapping patches, each of which is treated as a separate token. These patches are embedded into a common feature space.</p> </li> <li> <p>Positional Embeddings: 2D positional embeddings are added to each patch to encode their spatial information within the image.</p> </li> <li> <p>Transformer Blocks: The core of the architecture consists of multiple transformer blocks, which enable capturing global and local features from the patches. These blocks incorporate multi-head self-attention mechanisms and feedforward layers for feature transformation.</p> </li> <li> <p>Attention Pooling: At the end of the network, attention pooling is applied to aggregate information from different patches effectively. This enables the model to focus on essential parts of the image.</p> </li> <li> <p>Output Layer: The final feature representation is projected onto the output logits, which are used to make predictions.</p> </li> </ol>"},{"location":"zeta/nn/models/navit/#parameters","title":"Parameters","text":"<p>The <code>NaViT</code> class provides various parameters that allow you to customize its behavior. Here are the key parameters:</p> <ul> <li><code>image_size</code>: The dimensions of the input images (height, width).</li> <li><code>patch_size</code>: The size of the image patches used for processing.</li> <li><code>num_classes</code>: The number of output classes.</li> <li><code>dim</code>: The dimension of the feature embeddings.</li> <li><code>depth</code>: The depth of the transformer, indicating the number of transformer blocks.</li> <li><code>heads</code>: The number of attention heads in the multi-head self-attention mechanism.</li> <li><code>mlp_dim</code>: The dimension of the feedforward neural network within each transformer block.</li> <li><code>channels</code>: The number of input image channels (typically 3 for RGB images).</li> <li><code>dim_head</code>: The dimension of each attention head.</li> <li><code>dropout</code>: Dropout rate applied within the model.</li> <li><code>emb_dropout</code>: Dropout rate applied to the patch embeddings.</li> <li><code>token_dropout_prob</code>: A parameter that controls token dropout, which can be constant or calculated dynamically based on image dimensions.</li> </ul>"},{"location":"zeta/nn/models/navit/#usage-examples","title":"Usage Examples","text":"<p>To illustrate the usage of the <code>NaViT</code> class, let's explore three examples:</p>"},{"location":"zeta/nn/models/navit/#example-1-creating-a-navit-model","title":"Example 1: Creating a NaViT Model","text":"<pre><code>from zeta import NaViT\n\n# Create a NaViT model with custom parameters\nmodel = NaViT(\n    image_size=(256, 256),\n    patch_size=32,\n    num_classes=1000,\n    dim=512,\n    depth=12,\n    heads=8,\n    mlp_dim=1024,\n    channels=3,\n    dim_head=64,\n    dropout=0.1,\n    emb_dropout=0.1,\n    token_dropout_prob=0.2,  # Constant token dropout probability\n)\n</code></pre> <p>In this example, we create a <code>NaViT</code> model with specific parameters, including image size, patch size, and network depth.</p>"},{"location":"zeta/nn/models/navit/#example-2-forward-pass-with-input-images","title":"Example 2: Forward Pass with Input Images","text":"<pre><code>import torch\n\n# Generate a batch of input images (list of tensors)\nbatched_images = [torch.randn(3, 256, 256), torch.randn(3, 128, 128)]\n\n# Perform a forward pass with the NaViT model\npreds = model(batched_images)\n</code></pre> <p>Here, we apply the <code>NaViT</code> model to a batch of input images to obtain predictions.</p>"},{"location":"zeta/nn/models/navit/#example-3-extracting-feature-embeddings","title":"Example 3: Extracting Feature Embeddings","text":"<pre><code># Create a NaViT model with return_embeddings=True to extract feature embeddings\nfeature_model = NaViT(\n    image_size=(256, 256),\n    patch_size=32,\n    num_classes=1000,\n    dim=512,\n    depth=12,\n    heads=8,\n    mlp_dim=1024,\n    channels=3,\n    dim_head=64,\n    dropout=0.1,\n    emb_dropout=0.1,\n    token_dropout_prob=0.2,\n    return_embeddings=True,\n)\n\n# Forward pass to obtain feature embeddings\nfeature_embeddings = feature_model(batched_images)\n</code></pre> <p>In this example, we configure the <code>NaViT</code> model to return feature embeddings, allowing you to use the model as a feature extractor.</p>"},{"location":"zeta/nn/models/navit/#conclusion","title":"Conclusion","text":"<p>This concludes the documentation for the <code>NaViT</code> class within the Zeta library. You've gained a comprehensive understanding of its architecture, parameters, and practical usage. The <code>NaViT</code> model is a powerful tool for image-related tasks, and with Zeta, you can harness its capabilities to achieve impressive results.</p> <p>For more information and updates, please refer to the official Zeta documentation and resources.</p>"},{"location":"zeta/nn/modules/accurategeluactivation/","title":"AccurateGELUActivation","text":""},{"location":"zeta/nn/modules/accurategeluactivation/#overview","title":"Overview","text":"<p>The AccurateGELUActivation class is a part of the PyTorch library's nn.Module. This class allows us to apply the Gaussian Error Linear Unit (GELU) approximation that is faster than the default and more accurate than QuickGELU. This can be useful in situations where the default GELU is considered computationally expensive or its speed could be an issue. The implementation of this class comes as a support for MEGA, which stands for Moving Average Equipped Gated Attention, in neural networks.</p> <p>The class has been designed following the work on GELUs available at: https://github.com/hendrycks/GELUs</p>"},{"location":"zeta/nn/modules/accurategeluactivation/#class-definition","title":"Class Definition","text":"<p>Here is a look at the parameters and methods used in the <code>AccurateGELUActivation</code> class:</p> <pre><code>class AccurateGELUActivation(nn.Module):\n    \"\"\"\n    Applies GELU approximation that is faster than default and more accurate than QuickGELU. See:\n    https://github.com/hendrycks/GELUs\n    Implemented along with MEGA (Moving Average Equipped Gated Attention)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.precomputed_constant = math.sqrt(2 / math.pi)\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        return (\n            0.5\n            * input\n            * (\n                1\n                + torch.tanh(\n                    self.precomputed_constant * (input + 0.044715 * torch.pow(input, 3))\n                )\n            )\n        )\n</code></pre> <p>The class does not require any parameters during initialization. Here are the explanations for the various attributes and methods in the class:</p> Method/Attribute Description Argument <code>__init__</code> This is the constructor method that gets called when an object is created from the class. None <code>forward</code> This method is a PyTorch standard for forward propagation in a Module or a neural network layer. It accepts a tensor input and returns a tensor. <code>input: Tensor</code>"},{"location":"zeta/nn/modules/accurategeluactivation/#class-usage","title":"Class Usage","text":"<p>Now, let's look at some examples of how to use this class.</p>"},{"location":"zeta/nn/modules/accurategeluactivation/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<p><pre><code>import torch\nfrom torch import Tensor\nfrom torch.nn import Module\n\nfrom zeta import AccurateGELUActivation\n\n# Create an instance of the class\ngelu_activation = AccurateGELUActivation()\n\n# Create a PyTorch tensor\ninput = torch.tensor(\n    [[-1.0, -0.1, 0.1, 1.0], [0.5, -0.2, -2.1, 3.2]], dtype=torch.float32\n)\n\n# Use the AccurateGELUActivation instance to activate the input\noutput = gelu_activation(input)\n\nprint(output)\n</code></pre> This example demonstrates the functionalities of the AccurateGELUActivation module for a defined two-dimensional input tensor.</p>"},{"location":"zeta/nn/modules/accurategeluactivation/#example-2-applying-on-neural-network","title":"Example 2: Applying on Neural Network","text":"<p>The AccurateGELUActivation module can also be used as an activation layer in a PyTorch model.</p> <p><pre><code>import torch\nfrom torch import Tensor\nfrom torch.nn import Linear, Module\n\nfrom zeta.nn import AccurateGELUActivation\n\n\nclass Net(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = Linear(10, 5)\n        self.fc2 = Linear(5, 2)\n        self.activation = AccurateGELUActivation()\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.activation(x)\n        x = self.fc2(x)\n        return x\n\n\n# Create a model from the neural network class\nmodel = Net()\n\ninput = torch.randn(3, 10)\n\n# Pass the input to the model\noutput = model(input)\n\nprint(output)\n</code></pre> This example shows how the AccurateGELUActivation module can be integrated as a layer in a neural network model to perform activation on the intermediate outputs of the neural network model.</p> <p>Note: Please remember, understanding what activation functions like GELU can do, what benefits they can bring to your architecture, is crucial before applying it to your models.</p>"},{"location":"zeta/nn/modules/adaptive/","title":"AdaptiveParameterList Module Documentation","text":""},{"location":"zeta/nn/modules/adaptive/#overview-and-introduction","title":"Overview and Introduction","text":"<p>The <code>AdaptiveParameterList</code> class extends PyTorch's <code>nn.ParameterList</code> to provide an adaptive parameter mechanism during the training process. By using adaptation functions, one can adjust or transform parameters based on specific criteria or observations, allowing for dynamic updates outside the traditional gradient-based update rules. This capability can be crucial in certain applications where manual interventions or parameter adjustments based on heuristics are desirable.</p>"},{"location":"zeta/nn/modules/adaptive/#class-definition-adaptiveparameterlist","title":"Class Definition: AdaptiveParameterList","text":"<pre><code>class AdaptiveParameterList(nn.ParameterList):\n</code></pre>"},{"location":"zeta/nn/modules/adaptive/#description","title":"Description:","text":"<p>A container module that extends PyTorch's <code>nn.ParameterList</code> to allow the adaptation of its parameters using specific functions. This adaptation can be applied at various stages during training or evaluation to realize sophisticated model behaviors.</p>"},{"location":"zeta/nn/modules/adaptive/#parameters","title":"Parameters:","text":"<ul> <li><code>parameters</code> (<code>List[nn.Parameter]</code>, optional): List of parameters to initialize the <code>AdaptiveParameterList</code>. Default: None.</li> </ul>"},{"location":"zeta/nn/modules/adaptive/#method-adapt","title":"Method: adapt","text":"<pre><code>def adapt(self, adaptation_functions):\n</code></pre>"},{"location":"zeta/nn/modules/adaptive/#description_1","title":"Description:","text":"<p>Adapts the parameters of the <code>AdaptiveParameterList</code> using the provided functions.</p>"},{"location":"zeta/nn/modules/adaptive/#parameters_1","title":"Parameters:","text":"<ul> <li><code>adaptation_functions</code> (<code>Dict[int, Callable]</code>): A dictionary where keys are the indices of the parameters in the list and values are the callable functions that take in an <code>nn.Parameter</code> and return an <code>nn.Parameter</code>.</li> </ul>"},{"location":"zeta/nn/modules/adaptive/#raises","title":"Raises:","text":"<ul> <li><code>ValueError</code>: If <code>adaptation_functions</code> is not a dictionary.</li> <li><code>ValueError</code>: If an entry in <code>adaptation_functions</code> is not callable.</li> <li><code>ValueError</code>: If the output tensor of an adaptation function doesn't match the shape of the input parameter.</li> </ul>"},{"location":"zeta/nn/modules/adaptive/#usage-examples","title":"Usage Examples:","text":""},{"location":"zeta/nn/modules/adaptive/#1-basic-usage","title":"1. Basic Usage","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom AdaptiveParameterList import AdaptiveParameterList\nfrom shapeless import x  # Placeholder, as actual import statement was not provided\n\n\n# Define an adaptation function\ndef adaptation_function(param):\n    return param * 0.9\n\n\nadaptive_params = AdaptiveParameterList([nn.Parameter(torch.randn(10, 10))])\n\n# Create a dictionary with adaptation functions for the desired indices\nadapt_funcs = {0: adaptation_function}\n\nadaptive_params.adapt(adapt_funcs)\n</code></pre>"},{"location":"zeta/nn/modules/adaptive/#2-using-multiple-adaptation-functions","title":"2. Using Multiple Adaptation Functions","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom AdaptiveParameterList import AdaptiveParameterList\nfrom shapeless import x\n\n\n# Define multiple adaptation functions\ndef adaptation_function1(param):\n    return param * 0.9\n\n\ndef adaptation_function2(param):\n    return param + 0.1\n\n\nadaptive_params = AdaptiveParameterList(\n    [nn.Parameter(torch.randn(10, 10)), nn.Parameter(torch.randn(10, 10))]\n)\n\n# Apply different adaptation functions to different parameters\nadapt_funcs = {0: adaptation_function1, 1: adaptation_function2}\n\nadaptive_params.adapt(adapt_funcs)\n</code></pre>"},{"location":"zeta/nn/modules/adaptive/#3-handling-errors-with-adaptation-functions","title":"3. Handling Errors with Adaptation Functions","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom AdaptiveParameterList import AdaptiveParameterList\nfrom shapeless import x\n\n\n# Incorrect adaptation function (not returning a tensor of the same shape)\ndef wrong_adaptation_function(param):\n    return param[0]\n\n\nadaptive_params = AdaptiveParameterList([nn.Parameter(torch.randn(10, 10))])\n\ntry:\n    adaptive_params.adapt({0: wrong_adaptation_function})\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"zeta/nn/modules/adaptive/#mathematical-representation","title":"Mathematical Representation:","text":"<p>Given an <code>AdaptiveParameterList</code> with parameters ( P = [p_1, p_2, ... , p_n] ) and an adaptation function ( f_i ) for parameter ( p_i ), the adapted parameter ( p_i' ) is computed as:</p> <p>[ p_i' = f_i(p_i) ]</p> <p>Where ( f_i: \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}^{m \\times n} ) is a function that takes a tensor (parameter) as input and returns a tensor of the same shape.</p>"},{"location":"zeta/nn/modules/adaptive/#additional-information-and-tips","title":"Additional Information and Tips:","text":"<ol> <li>Ensure that the adaptation functions are defined correctly and return tensors of the same shape as their input.</li> <li>Adaptation can be applied at different intervals, for example, after every epoch, or after specific events during training.</li> <li>Care must be taken when designing adaptation functions to avoid unintentional model behaviors.</li> </ol>"},{"location":"zeta/nn/modules/adaptive/#references-and-resources","title":"References and Resources:","text":"<ul> <li>PyTorch's <code>nn.ParameterList</code> Documentation</li> <li>To further understand dynamic parameter adaptations, consider reviewing material on heuristic optimization techniques.</li> </ul>"},{"location":"zeta/nn/modules/adaptive_conv/","title":"<code>AdaptiveConv3DMod</code> Documentation","text":""},{"location":"zeta/nn/modules/adaptive_conv/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Overview</li> <li>AdaptiveConv3DMod Class</li> <li>Initialization Parameters</li> <li>Functionality and Usage</li> <li>Forward Method</li> <li>Helper Functions and Classes</li> <li>Examples</li> <li>Example 1: Creating an AdaptiveConv3DMod Layer</li> <li>Example 2: Using AdaptiveConv3DMod with Modulation</li> <li>Additional Information</li> <li>References and Resources</li> </ol>"},{"location":"zeta/nn/modules/adaptive_conv/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the documentation for the Zeta library's <code>AdaptiveConv3DMod</code> class. This class implements an adaptive convolutional layer with support for spatial modulation, as used in the StyleGAN2 architecture. This documentation will provide you with a comprehensive understanding of how to use the <code>AdaptiveConv3DMod</code> class for various tasks.</p>"},{"location":"zeta/nn/modules/adaptive_conv/#11-purpose","title":"1.1 Purpose","text":"<p>The primary purpose of the <code>AdaptiveConv3DMod</code> class is to enable adaptive convolutional operations with optional spatial modulation. It is particularly useful in tasks that involve conditional generation, where the convolutional layer's weights are modulated based on external factors or latent variables.</p>"},{"location":"zeta/nn/modules/adaptive_conv/#12-key-features","title":"1.2 Key Features","text":"<ul> <li>Adaptive convolutional layer for 3D data.</li> <li>Support for spatial modulation to condition the convolution.</li> <li>Demodulation option for weight normalization.</li> <li>Flexible and customizable for various architectural designs.</li> </ul>"},{"location":"zeta/nn/modules/adaptive_conv/#2-overview","title":"2. Overview","text":"<p>Before diving into the details of the <code>AdaptiveConv3DMod</code> class, let's provide an overview of its purpose and functionality.</p> <p>The <code>AdaptiveConv3DMod</code> class is designed to perform convolutional operations on 3D data while allowing for dynamic modulation of the convolutional weights. This modulation is particularly useful in generative models where conditional generation is required. The class provides options for demodulation and flexible kernel sizes.</p> <p>In the following sections, we will explore the class definition, its initialization parameters, and how to use it effectively.</p>"},{"location":"zeta/nn/modules/adaptive_conv/#3-adaptiveconv3dmod-class","title":"3. AdaptiveConv3DMod Class","text":"<p>The <code>AdaptiveConv3DMod</code> class is the core component of the Zeta library for adaptive convolutional operations. It provides methods for performing convolution with optional spatial modulation.</p>"},{"location":"zeta/nn/modules/adaptive_conv/#31-initialization-parameters","title":"3.1 Initialization Parameters","text":"<p>Here are the initialization parameters for the <code>AdaptiveConv3DMod</code> class:</p> <ul> <li> <p><code>dim</code> (int): The number of input channels, i.e., the dimension of the input data.</p> </li> <li> <p><code>spatial_kernel</code> (int): The size of the spatial kernel used for convolution.</p> </li> <li> <p><code>time_kernel</code> (int): The size of the temporal (time) kernel used for convolution.</p> </li> <li> <p><code>dim_out</code> (int, optional): The number of output channels, which can be different from the input dimension. If not specified, it defaults to the input dimension.</p> </li> <li> <p><code>demod</code> (bool): If <code>True</code>, demodulates the weights during convolution to ensure proper weight normalization.</p> </li> <li> <p><code>eps</code> (float): A small value added for numerical stability to prevent division by zero.</p> </li> </ul>"},{"location":"zeta/nn/modules/adaptive_conv/#32-attributes","title":"3.2 Attributes","text":"<p>The <code>AdaptiveConv3DMod</code> class has the following important attributes:</p> <ul> <li> <p><code>weights</code> (nn.Parameter): The learnable convolutional weights.</p> </li> <li> <p><code>padding</code> (tuple): The padding configuration for the convolution operation based on the kernel size.</p> </li> </ul>"},{"location":"zeta/nn/modules/adaptive_conv/#33-methods","title":"3.3 Methods","text":"<p>The main method of the <code>AdaptiveConv3DMod</code> class is the <code>forward</code> method, which performs the forward pass of the convolution operation with optional modulation.</p>"},{"location":"zeta/nn/modules/adaptive_conv/#4-functionality-and-usage","title":"4. Functionality and Usage","text":"<p>Now let's explore how to use the <code>AdaptiveConv3DMod</code> class for convolution operations with optional modulation.</p>"},{"location":"zeta/nn/modules/adaptive_conv/#41-forward-method","title":"4.1 Forward Method","text":"<p>The <code>forward</code> method is used to perform the forward pass of the adaptive convolutional layer. It takes the following parameters:</p> <ul> <li> <p><code>fmap</code> (Tensor): The input feature map or data of shape <code>(batch, channels, time, height, width)</code>.</p> </li> <li> <p><code>mod</code> (Optional[Tensor]): An optional modulation tensor that conditions the convolutional weights. It should have the shape <code>(batch, channels)</code>.</p> </li> </ul> <p>The method returns a tensor of shape <code>(batch, output_channels, time, height, width)</code>.</p> <p>Example:</p> <pre><code>layer = AdaptiveConv3DMod(dim=512, spatial_kernel=3, time_kernel=3)\ninput_data = torch.randn(1, 512, 4, 4, 4)\nmodulation = torch.randn(1, 512)\noutput = layer(input_data, modulation)\nprint(output.shape)\n</code></pre>"},{"location":"zeta/nn/modules/adaptive_conv/#42-usage-examples","title":"4.2 Usage Examples","text":""},{"location":"zeta/nn/modules/adaptive_conv/#example-1-creating-an-adaptiveconv3dmod-layer","title":"Example 1: Creating an AdaptiveConv3DMod Layer","text":"<p>In this example, we create an instance of the <code>AdaptiveConv3DMod</code> class with default settings:</p> <pre><code>layer = AdaptiveConv3DMod(dim=512, spatial_kernel=3, time_kernel=3)\n</code></pre>"},{"location":"zeta/nn/modules/adaptive_conv/#example-2-using-adaptiveconv3dmod-with-modulation","title":"Example 2: Using AdaptiveConv3DMod with Modulation","text":"<p>Here, we demonstrate how to use the <code>AdaptiveConv3DMod</code> layer with modulation:</p> <pre><code>layer = AdaptiveConv3DMod(dim=512, spatial_kernel=3, time_kernel=3)\ninput_data = torch.randn(1, 512, 4, 4, 4)\nmodulation = torch.randn(1, 512)\noutput = layer(input_data, modulation)\nprint(output.shape)\n</code></pre>"},{"location":"zeta/nn/modules/adaptive_conv/#5-helper-functions-and-classes","title":"5. Helper Functions and Classes","text":"<p>The Zeta library provides several helper functions and classes that are used within the <code>AdaptiveConv3DMod</code> class. These include functions for checking divisibility, packing and unpacking tensors, and more. These helper functions contribute to the functionality and flexibility of the <code>AdaptiveConv3DMod</code> class.</p>"},{"location":"zeta/nn/modules/adaptive_conv/#6-additional-information","title":"6. Additional Information","text":"<p>Here are some additional tips and information for using the <code>AdaptiveConv3DMod</code> class effectively:</p> <ul> <li> <p>Experiment with different spatial and temporal kernel sizes to match the requirements of your specific task.</p> </li> <li> <p>Be cautious when enabling demodulation, as it may affect the convergence of the model. You can adjust the <code>eps</code> parameter for better stability.</p> </li> <li> <p>Ensure that your modulation tensor (<code>mod</code>) has the appropriate shape and values to condition the convolutional weights effectively.</p> </li> </ul>"},{"location":"zeta/nn/modules/adaptive_conv/#7-references-and-resources","title":"7. References and Resources","text":"<p>Here are some references and resources for further information on the Zeta library and related topics:</p> <ul> <li> <p>Zeta GitHub Repository: Official Zeta repository for updates and contributions.</p> </li> <li> <p>StyleGAN2 Paper: The original paper that introduces adaptive convolution with modulation.</p> </li> <li> <p>PyTorch Official Website: Official website for PyTorch, the deep learning framework used in Zeta.</p> </li> </ul> <p>This concludes the documentation for the Zeta library's <code>AdaptiveConv3DMod</code> class. You now have a comprehensive understanding of how to use this class for adaptive convolution operations with modulation. If you have any further questions or need assistance, please refer to the provided references and resources. Happy modeling with Zeta!</p>"},{"location":"zeta/nn/modules/averagemodelmerger/","title":"Zeta.nn.modules.AverageModelMerger Documentation","text":""},{"location":"zeta/nn/modules/averagemodelmerger/#introduction","title":"Introduction","text":"<p>The AverageModelMerger class, found in the zeta.nn.modules library, is a simple yet powerful class to merge multiple models by averaging their weights. It offers a straightforward way to combine models trained in different stages, such as instruction and alignment tuning, leading to improved model performance in certain circumstances.</p>"},{"location":"zeta/nn/modules/averagemodelmerger/#class-definition-averagemodelmerger","title":"Class Definition: AverageModelMerger","text":"<pre><code>class AverageModelMerger:\n    \"\"\"\n    A class to merge multiple models by averaging their weights.\n\n    Attributes:\n    models (List[nn.Module]): A list of PyTorch models to be merged.\n\n    Examples::- Example usage:\n    model1 = nn.Linear(in_features=10, out_features=10)\n    model2 = nn.Linear(in_features=10, out_features=10)\n    model3 = nn.Linear(in_features=10, out_features=10)\n    merge = AverageModelMerger([model1, model2, model3])\n    merged_model = merge.merge_models()\n    print(merged_model)\n    \"\"\"\n</code></pre>"},{"location":"zeta/nn/modules/averagemodelmerger/#class-parameters","title":"Class Parameters:","text":"Parameters Data Type Default Value Description models List[nn.Module] N/A List of PyTorch models to be merged"},{"location":"zeta/nn/modules/averagemodelmerger/#class-methods","title":"Class Methods:","text":"Method Name Description Parameters Returns <code>__init__(self, models: List[nn.Module])</code> Initializes the AverageModelMerger with a list of models. models (List[nn.Module]) None <code>merge_models(self)</code> Merges the models by averaging their weights. None A new model with averaged weights. <code>_copy_model_structure(model: nn.Module)</code> Creates a new instance of a model with the same structure as the given model. model (nn.Module) A new model with the same structure."},{"location":"zeta/nn/modules/averagemodelmerger/#constructor-__init__self-models-listnnmodule","title":"Constructor <code>__init__(self, models: List[nn.Module])</code>","text":"<p>Initializes an instance of the AverageModelMerge class. It takes a list of PyTorch models as input which are to be merged later using the <code>merge_models</code> method. </p> <ul> <li>models (List[nn.Module]): Models to be merged.</li> </ul>"},{"location":"zeta/nn/modules/averagemodelmerger/#method-merge_modelsself-nnmodule","title":"Method <code>merge_models(self) -&gt; nn.Module</code>","text":"<p>This function merges the models by averaging the weights of the PyTorch models. </p> <p>Returns</p> <p>nn.Module: A new model with averaged weights.</p>"},{"location":"zeta/nn/modules/averagemodelmerger/#method-_copy_model_structureself-model-nnmodule-nnmodule","title":"Method <code>_copy_model_structure(self, model: nn.Module) -&gt; nn.Module</code>","text":"<p>This function creates a new instance of a model with exactly the same structure as the given model.</p> <p>Parameters - model (nn.Module): The model whose structure is to be copied.</p> <p>Returns</p> <p>nn.Module: A new model with exactly the same structure.</p>"},{"location":"zeta/nn/modules/averagemodelmerger/#examples-of-usage","title":"Examples of Usage:","text":""},{"location":"zeta/nn/modules/averagemodelmerger/#example-1","title":"Example 1","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.nn.modules import AverageModelMerger\n\n# Define models\nmodel1 = nn.Linear(in_features=10, out_features=10)\nmodel2 = nn.Linear(in_features=10, out_features=10)\nmodel3 = nn.Linear(in_features=10, out_features=10)\n\n# Initialize AverageModelMerger\nmerger = AverageModelMerger([model1, model2, model3])\n\n# Merge models\nmerged_model = merger.merge_models()\n\n# Print merged model\nprint(merged_model)\n</code></pre>"},{"location":"zeta/nn/modules/averagemodelmerger/#example-2","title":"Example 2","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.nn.modules import AverageModelMerger\n\n# Define models\nmodel1 = nn.Conv2d(3, 6, 5)\nmodel2 = nn.Conv2d(3, 6, 5)\nmodel3 = nn.Conv2d(3, 6, 5)\n\n# Initialize AverageModelMerger\nmerger = AverageModelMerger([model1, model2, model3])\n\n# Merge models\nmerged_model = merger.merge_models()\n\n# Print merged model\nprint(merged_model)\n</code></pre>"},{"location":"zeta/nn/modules/averagemodelmerger/#example-3","title":"Example 3","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.nn.modules import AverageModelMerger\n\n# Define models\nmodel1 = nn.CrossEntropyLoss()\nmodel2 = nn.CrossEntropyLoss()\nmodel3 = nn.CrossEntropyLoss()\n\n# Initialize AverageModelMerger\nmerger = AverageModelMerger([model1, model2, model3])\n\n# Merge models\nmerged_model = merger.merge_models()\n\n# Print merged model\nprint(merged_model)\n</code></pre> <p>All the examples above demonstrate the basic usage of this class. In cases where you have multiple trained models (e.g., resultant from a k-fold cross-validation or models trained on different datasets), you can use this class to merge or average their weights. The resultant model will carry averaged weights, giving a balanced representation of all the models.</p>"},{"location":"zeta/nn/modules/clippedgeluactivation/","title":"ClippedGELUActivation","text":"<p>The ClippedGELUActivation class is designed to clip the possible output range of Gaussian Error Linear Unit (GeLU) activation between a given minimum and maximum value. This is specifically useful for the quantization purpose, as it allows mapping negative values in the GeLU spectrum. To learn more about the underlying concept, you can refer to an academic paper titled Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.</p> <p>The original implementation of the GeLU activation function was introduced in the Google BERT repository. Note that OpenAI GPT's GeLU is slightly different and gives slightly different results.</p>"},{"location":"zeta/nn/modules/clippedgeluactivation/#class-definition","title":"Class Definition","text":"<p>The ClippedGELUActivation class inherits from the <code>nn.Module</code> in PyTorch.</p> <pre><code>class ClippedGELUActivation(nn.Module):\n    def __init__(self, min: float, max: float):\n        if min &gt; max:\n            raise ValueError(f\"min should be &lt; max (got min: {min}, max: {max})\")\n\n        super().__init__()\n        self.min = min\n        self.max = max\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return torch.clip(gelu(x), self.min, self.max)\n</code></pre>"},{"location":"zeta/nn/modules/clippedgeluactivation/#class-arguments","title":"Class Arguments","text":"Argument Type Description min float The lower limit for the output of GeLU activation. It should be less than <code>max</code> max float The upper limit for the output of GeLU activation. It should be greater than <code>min</code> <p>Note: If <code>min</code> is greater than <code>max</code>, a ValueError will be raised.</p>"},{"location":"zeta/nn/modules/clippedgeluactivation/#forward-method-arguments","title":"Forward Method Arguments","text":"Argument Type Description x Tensor Input tensor for the forward function of the module"},{"location":"zeta/nn/modules/clippedgeluactivation/#class-example","title":"Class Example","text":"<p>In the code below, we initialize the ClippedGELUActivation module with a min and max value and input a tensor <code>x</code>:</p> <pre><code>import torch\nfrom torch import Tensor, nn\nfrom torch.nn.functional import gelu\n\nfrom zeta.nn import ClippedGELUActivation\n\n# Initialize the class\nclipped_gelu = ClippedGELUActivation(min=-3.0, max=3.0)\n\n# Create a tensor\nx = torch.randn(3, 3)\n\n# Pass the tensor through the module\noutput = clipped_gelu(x)\n</code></pre> <p>In this instance, the output tensor would have each of its elements limited to be within the range of -3.0 to 3.0, inclusively.</p>"},{"location":"zeta/nn/modules/clippedgeluactivation/#notes","title":"Notes","text":"<p>While using this class be cautious of the following: - The class does not check if the <code>max</code> argument is less than the <code>min</code> argument. Providing a <code>max</code> which is less than <code>min</code> will raise a ValueError. - The <code>forward</code> method does not check if all elements of the input Tensor <code>x</code> are numeric. Non-numeric input may result in unexpected behavior or errors.</p>"},{"location":"zeta/nn/modules/clippedgeluactivation/#references","title":"References","text":"<p>For additional information and further exploration about GeLU and its applications, please refer to the following resources:</p> <ol> <li>Gaussian Error Linear Units (GELUs)</li> <li>Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</li> <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li> </ol> <p>Note: In our documentation, we provided information about the CythonGELU and its methods. The details regarding the parameters, method details, and usage examples were provided to ensure the understanding of the class and methods.</p>"},{"location":"zeta/nn/modules/conv2dfeedforward/","title":"Conv2DFeedforward","text":"<p>The <code>Conv2DFeedforward</code> is a <code>torch.nn</code> module part of the <code>zeta.nn</code> library, designed to implement a Convolutional Feedforward network as proposed in Vision Attention Network (VAN) by Guo et al. The network operates on input data that represents a tensor fo shape (N, L, C), where N is the batch size, L is the sequence context length, and C is the input feature dimension.</p> <p>Import Example: <pre><code>import torch\n\nfrom zeta.nn import Conv2DFeedforward\n</code></pre></p> <p>The architecture of this module is designed to process multi-dimensional data with rows and columns, and it includes convolutional layers combined with multi-layer perceptron (MLP) architecture to process feature-containing input data in a feedforward fashion.</p>"},{"location":"zeta/nn/modules/conv2dfeedforward/#parameters","title":"Parameters:","text":"Args Description dim Integer parameter - Total number of input features of the given data. hidden_layer_multiplier Integer parameter - The multiplier factor used to determine the number of hidden features defined as a multiple of the input feature dimension. dim_out Optional Integer parameter - The total number of output features of the given data. activation Object - The non-linear activation function. Default: GELU (Gaussian Error Linear Unit). dropout Float parameter - Determines the probability of dropout on the feedforward network's output. Default: 0.0. *args Additional positional parameters. **kwargs Additional keyword parameters."},{"location":"zeta/nn/modules/conv2dfeedforward/#methods","title":"Methods:","text":"<ol> <li> <p>init_weights(self, kwargs)**     Function to initialize weights of the module. The weights are initialized based on the original initialization proposed in the vision attention network paper and it allows to initialize from the outside as well.</p> <p>Example Usage: <pre><code>conv = Conv2DFeedforward(256, 1, 256)\nconv.init_weights()\n</code></pre></p> </li> <li> <p>forward(self, x: Tensor) -&gt; Tensor     The forward function processes the input tensor through the convolutional feedforward neural network and returns the output tensor.</p> <p>Example Usage: <pre><code>conv = Conv2DFeedforward(256, 1, 256)\nx = torch.randn(2, 64, 256)\noutput = conv(x)\nprint(output.shape)\n</code></pre> Expected Output: <pre><code>torch.Size([2, 64, 256])\n</code></pre></p> </li> </ol> <p>The <code>Conv2DFeedforward</code> module uses a combination of convolutional layers and multi-layer perceptron to provide a sophisticated framework to process multi-dimensional data, particularly for image-related classification or localization problems.</p> <p>For additional details and in-depth research on the underlying architectures and concepts associated with the Conv2DFeedforward module, refer to the official Vision Attention Network paper provided at VAN.</p>"},{"location":"zeta/nn/modules/custom_mlp/","title":"<code>CustomMLP</code>","text":""},{"location":"zeta/nn/modules/custom_mlp/#introduction","title":"Introduction","text":"<p>Welcome to the documentation for <code>zeta.nn</code>! This module provides a customizable Multi-Layer Perceptron (MLP) implementation using PyTorch. With <code>CustomMLP</code>, you can create and configure your own MLP architecture for various machine learning tasks. This documentation will guide you through the functionalities, usage, and customization options of <code>CustomMLP</code>.</p>"},{"location":"zeta/nn/modules/custom_mlp/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Overview</li> <li>Class Definition</li> <li>Functionality and Usage<ul> <li>Initialization</li> <li>Forward Pass</li> <li>Customization</li> </ul> </li> <li>Examples</li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/nn/modules/custom_mlp/#1-installation","title":"1. Installation","text":"<p>Before using <code>CustomMLP</code>, make sure you have <code>zetascale</code> installed. You can install it using:</p> <pre><code>pip install zetascale\n</code></pre> <p>Once PyTorch is installed, you can import <code>CustomMLP</code> from <code>zeta.nn</code> as follows:</p> <pre><code>from zeta.nn import CustomMLP\n</code></pre>"},{"location":"zeta/nn/modules/custom_mlp/#2-overview","title":"2. Overview","text":"<p><code>CustomMLP</code> is a versatile MLP architecture that allows you to define the number of layers, layer sizes, activation functions, and dropout probability according to your specific requirements. It is suitable for tasks like classification, regression, and more.</p> <p>Key features: - Customizable layer sizes and activation functions. - Dropout regularization for improved generalization. - Supports popular activation functions like ReLU, Sigmoid, and Tanh.</p>"},{"location":"zeta/nn/modules/custom_mlp/#3-class-definition","title":"3. Class Definition","text":""},{"location":"zeta/nn/modules/custom_mlp/#custommlp_1","title":"<code>CustomMLP</code>","text":"<pre><code>| Attribute          | Description                                            |\n|--------------------|--------------------------------------------------------|\n| layers             | List of linear layers.                                 |\n| activation_fn      | Activation function to be applied after each layer.   |\n| dropout            | Dropout probability for regularization.               |\n\nParameters:\n- `layer_sizes` (list of int): List of layer sizes including input and output layer.\n- `activation` (str, optional): Type of activation function. Default is 'relu'.\n- `dropout` (float, optional): Dropout probability. Default is 0.0 (no dropout).\n</code></pre>"},{"location":"zeta/nn/modules/custom_mlp/#4-functionality-and-usage","title":"4. Functionality and Usage","text":""},{"location":"zeta/nn/modules/custom_mlp/#initialization","title":"Initialization","text":"<p>To create an instance of <code>CustomMLP</code>, you need to specify the <code>layer_sizes</code>, which is a list of integers representing the sizes of each layer, including the input and output layers. You can also customize the <code>activation</code> function and <code>dropout</code> probability.</p> <p>Example:</p> <pre><code>from zeta.nn import CustomMLP\n\n# Create an MLP with 3 layers: input (10), hidden (5), and output (2)\nmlp = CustomMLP(layer_sizes=[10, 5, 2], activation=\"relu\", dropout=0.5)\n</code></pre>"},{"location":"zeta/nn/modules/custom_mlp/#forward-pass","title":"Forward Pass","text":"<p>You can perform a forward pass through the MLP by passing input data to it. The input data should be a PyTorch tensor.</p> <p>Example:</p> <pre><code>import torch\n\n# Input data (1 sample with 10 features)\ninput_data = torch.randn(1, 10)\n\n# Forward pass through the MLP\noutput = mlp(input_data)\n</code></pre>"},{"location":"zeta/nn/modules/custom_mlp/#customization","title":"Customization","text":"<p>You can customize the following aspects of the MLP: - Layer Sizes: Specify the sizes of layers in the <code>layer_sizes</code> parameter. - Activation Function: Choose from 'relu' (default), 'sigmoid', or 'tanh' for activation. - Dropout: Set the <code>dropout</code> probability for regularization.</p>"},{"location":"zeta/nn/modules/custom_mlp/#5-examples","title":"5. Examples","text":""},{"location":"zeta/nn/modules/custom_mlp/#example-1-customizing-mlp","title":"Example 1: Customizing MLP","text":"<pre><code>from zeta.nn import CustomMLP\n\n# Create an MLP with custom layer sizes, sigmoid activation, and dropout\nmlp = CustomMLP(layer_sizes=[20, 10, 5], activation=\"sigmoid\", dropout=0.2)\n</code></pre>"},{"location":"zeta/nn/modules/custom_mlp/#example-2-forward-pass","title":"Example 2: Forward Pass","text":"<pre><code>import torch\n\nfrom zeta.nn import CustomMLP\n\n# Define the layer sizes\nlayer_sizes = [5, 10, 1]\n\n# Create the MLP\nmlp = CustomMLP(layer_sizes, activation=\"relu\", dropout=0.5)\n\n# Create a random tensor of shape (batch_size, input_size)\nx = torch.randn(32, 5)\n\n# Pass the tensor through the MLP\noutput = mlp(x)\n\nprint(output)\n</code></pre>"},{"location":"zeta/nn/modules/custom_mlp/#example-3-customizing-and-forward-pass","title":"Example 3: Customizing and Forward Pass","text":"<pre><code>import torch\n\nfrom zeta.nn import CustomMLP\n\n# Create an MLP with custom configuration\nmlp = CustomMLP(layer_sizes=[15, 8, 3], activation=\"tanh\", dropout=0.3)\n\n# Input data (single sample with 15 features)\ninput_data = torch.randn(1, 15)\n\n# Forward pass through the customized MLP\noutput = mlp(input_data)\n</code></pre>"},{"location":"zeta/nn/modules/custom_mlp/#6-additional-information","title":"6. Additional Information","text":"<ul> <li>If you encounter any issues or have questions, please refer to the References section for further resources.</li> </ul>"},{"location":"zeta/nn/modules/custom_mlp/#7-references","title":"7. References","text":"<ul> <li>PyTorch Documentation: https://pytorch.org/docs/stable/index.html</li> <li>PyTorch Tutorials: https://pytorch.org/tutorials/</li> </ul> <p>This concludes the documentation for <code>zeta.nn</code> and the <code>CustomMLP</code> class. You are now equipped to create and customize your MLP architectures for various machine learning tasks. Happy coding!</p>"},{"location":"zeta/nn/modules/denseblock/","title":"Class Name: DenseBlock","text":"<p>The <code>DenseBlock</code> class is a type of PyTorch <code>nn.Module</code>. This allows for complicated neural network architectures to be defined with individual abstracted layers. The class gets its name from the dense connections made in the forward propagation, which involve concatenating the output of the <code>submodule</code> with the original input. </p> <p>For the following documentation, the DenseBlock class is used as an example of such constructions. </p> <p>While this class might seem simple, understanding how it works is fundamental to define, compile, and use your own custom PyTorch models. </p> <p>It has two main methods, the <code>__init__()</code> method and the <code>forward()</code> method.</p>"},{"location":"zeta/nn/modules/denseblock/#method-__init__self-submodule-args-kwargs","title":"Method: __init__(self, submodule, args, *kwargs)","text":"<p>The <code>__init__()</code> method is the initializer method of the DenseBlock class. It is called when an object (an instance of the class) is created. </p> <p>This method sets an attribute of the DenseBlock object to be the <code>submodule</code> input, which is assumed to be some <code>nn.Module</code> instance.</p> <p>The method signature is:</p> <pre><code>def __init__(self, submodule, *args, **kwargs)\n</code></pre>"},{"location":"zeta/nn/modules/denseblock/#arguments","title":"Arguments","text":"Name Type Description submodule nn.Module The module that will be applied in the forward pass. args Variable length argument list Unused in this implementation, but allows for extra position arguments. kwargs Arbitrary keyword arguments Unused in this implementation, but allows for extra keyword arguments. <p>The <code>submodule</code> argument should be an initialized instance of the <code>nn.Module</code> subclass you want to apply. </p> <p>The <code>args</code> and <code>kwargs</code> arguments are not currently used in DenseBlock. </p>"},{"location":"zeta/nn/modules/denseblock/#method-forwardself-x-torchtensor-torchtensor","title":"Method: forward(self, x: torch.Tensor) -&gt; torch.Tensor","text":"<p>The <code>forward()</code> method is called during the forward propagation of the neural network. </p> <p>It applies the module operation to the input tensor <code>x</code> and concatenates the input tensor <code>x</code> with the output of the <code>submodule</code>.</p> <p>The method signature is:</p> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor\n</code></pre>"},{"location":"zeta/nn/modules/denseblock/#arguments_1","title":"Arguments","text":"Name Type Description x torch.Tensor The input tensor to the module. <p>Returns a tensor, which is the input tensor concatenated with the processed input tensor via the <code>submodule</code>.</p>"},{"location":"zeta/nn/modules/denseblock/#usage-examples","title":"Usage Examples","text":"<p>Here are some examples showing how to use the DenseBlock class. These examples will include the necessary imports, data creation, and model instantiation following PyTorch conventions:</p>"},{"location":"zeta/nn/modules/denseblock/#example-1-basic-usage-with-a-linear-layer","title":"Example 1: Basic Usage with a Linear Layer","text":"<p>In this example, the <code>DenseBlock</code> will include a Linear layer as submodule.</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom zeta.nn import DenseBlock\n\n# Defining submodule\nlin_layer = nn.Linear(5, 10)\n\n# Defining DenseBlock\ndense_block = DenseBlock(lin_layer)\n\n# Creating a random tensor of shape [10, 5]\nrandom_tensor = Variable(torch.randn(10, 5))\n\n# Applying DenseBlock\noutput = dense_block(random_tensor)\n</code></pre> <p>In this example, an input tensor of shape [10,5] is given to a dense block with a linear layer. The input will have shape [10,5] and the output of the linear layer will have shape [10,10], resulting in the output of the dense block to have shape [10,15].</p>"},{"location":"zeta/nn/modules/denseblock/#example-2-using-denseblock-in-a-multilayer-neural-network","title":"Example 2: Using DenseBlock in a Multilayer Neural Network","text":"<p>In this example, a 2-layer neural network using Dense Blocks is shown. The first layer is a Dense Block with a Linear module transforming with dimensions (10 to 5), and the second layer is a standard Linear layer transforming the output dimensions (15 to 1). <pre><code>import torch.nn.functional as F\n\n\n# Defining a custom model\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = DenseBlock(nn.Linear(10, 5))\n        self.layer2 = nn.Linear(15, 1)\n\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = self.layer2(x)\n        return x\n\n\n# Initializing the model\nnet = Net()\n\n# Creating a random tensor of shape [32, 10]\ndata = Variable(torch.randn(32, 10))\n\n# Forward propagation\noutput = net(data)\n</code></pre></p> <p>In this second example, a data batch with <code>32</code> samples and input dimensionality of <code>10</code> is given to a <code>Net</code> neural network with dense connections in their first layer. The final output shape is [32, 1]. </p>"},{"location":"zeta/nn/modules/denseblock/#example-3-denseblock-with-convolutional-layer","title":"Example 3: DenseBlock with Convolutional Layer","text":"<p>Lastly, this example shows how to use DenseBlock inside a Convolutional Neural Network: <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn import DenseBlock\n\ncnn = nn.Sequential(\n    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n    nn.ReLU(inplace=True),\n    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n    DenseBlock(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)),\n    nn.AdaptiveAvgPool2d((1, 1)),\n    nn.Flatten(),\n    nn.Linear(128, 10),\n)\n\nx = torch.randn(1, 1, 224, 224)\noutput = cnn(x)\n</code></pre></p> <p>Here, a 2D convolutional layer is used as the submodule within the DenseBlock. The DenseBlock receives a tensor with shape [64, 224, 224] as input, applies the convolutional layer (keeping the same shape), and then concatenates the input and the output along the channel dimension, resulting in a tensor with shape [128, 224, 224].</p>"},{"location":"zeta/nn/modules/depthwiseconv2d/","title":"Module/Function Name: DepthWiseConv2d","text":"<p>The <code>DepthWiseConv2d</code> class is a base class for all neural network modules. It serves as a fundamental element for creating deep learning models and contains multiple attributes that can be used for different applications and use cases. The <code>DepthWiseConv2d</code> class allows you to create deep neural networks by subclassing and utilizing its inbuilt features and capabilities. Additionally, it supports the nesting of modules and seamlessly incorporates submodules in a tree-like structure, providing flexibility and extensibility to the neural network architecture.</p> <p>Example Usage:</p> <pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom zeta.nn import DepthWiseConv2d\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = DepthWiseConv2d(1, 20, 5, padding=2, stride=1)\n        self.conv2 = DepthWiseConv2d(20, 40, 5, padding=2, stride=1)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre> <p>Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:<code>to</code>, etc.</p> <p>Regarding the assignment of submodules in this class, the <code>__init__()</code> call to the parent class must be made prior to assigning child submodules.</p> <p>Attributes: - training: A boolean that represents whether this module is in training or evaluation mode.     - Type: bool</p> <p>Source Code: <pre><code>class DepthWiseConv2d(nn.Module):\n    def __init__(self, dim_in, dim_out, kernel_size, padding, stride, bias=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(\n                dim_in,\n                dim_out,\n                kernel_size=kernel_size,\n                padding=padding,\n                groups=dim_in,\n                stride=stride,\n                bias=bias,\n            ),\n            nn.Conv2d(dim_out, dim_out, kernel_size=1, bias=bias),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre></p> <p>In the above example, the DepthWiseConv2d class is defined with specified parameters <code>dim_in</code>, <code>dim_out</code>, <code>kernel_size</code>, <code>padding</code>, <code>stride</code>, and <code>bias</code>, where <code>dim_in</code> is the input dimension, <code>dim_out</code> is the output dimension, <code>kernel_size</code> is the size of the convolutional kernel, <code>padding</code> is the padding size, <code>stride</code> is the stride value, and <code>bias</code> is a boolean parameter to include bias in the convolution operation. The forward method applies this defined convolution operation to input <code>x</code> using <code>self.net</code> and returns the result.</p> <p>By using the DepthWiseConv2d class with its specified parameters, you can create a deep neural network module that supports convolution operations with customizable input and output dimensions and kernel characteristics. With its comprehensive structure and modularity, DepthWiseConv2d facilitates the creation of sophisticated deep learning models.</p> <p>For using this class in a more practical scenario, please refer to the usage example presented above and customize the class attributes to meet the requirements of your specific application or use case.</p>"},{"location":"zeta/nn/modules/dm/","title":"Module Name: DynamicModule","text":""},{"location":"zeta/nn/modules/dm/#overview","title":"Overview","text":"<p>The <code>DynamicModule</code> is a versatile container class designed for dynamic addition, removal, and modification of modules in a PyTorch model. It is a valuable tool for constructing complex neural network architectures with flexible components.</p>"},{"location":"zeta/nn/modules/dm/#key-features","title":"Key Features","text":"<ul> <li>Dynamic Module Management: Add, remove, and modify modules dynamically during runtime.</li> <li>Custom Forward Method: Define a custom forward method for flexible module interaction.</li> <li>Module Persistence: Save and load the state of the module, including all added submodules.</li> </ul>"},{"location":"zeta/nn/modules/dm/#use-cases","title":"Use Cases","text":"<ul> <li>Dynamic Architectures: Construct neural networks with dynamic, user-defined architectures.</li> <li>Conditional Networks: Create conditional networks where modules are added or removed based on input conditions.</li> <li>Experimentation: Facilitate experimentation and exploration of various network configurations.</li> </ul>"},{"location":"zeta/nn/modules/dm/#class-definition","title":"Class Definition","text":"<pre><code>class DynamicModule(nn.Module):\n    def __init__(self, forward_method=None):\n        \"\"\"\n        Initialize a DynamicModule instance.\n\n        Args:\n            forward_method (callable, optional): Custom forward method. If None, default behavior is used.\n        \"\"\"\n\n    def add(self, name, module):\n        \"\"\"\n        Add a module to the container.\n\n        Args:\n            name (str): The name of the module.\n            module (nn.Module): The module to add.\n        \"\"\"\n\n    def remove(self, name):\n        \"\"\"\n        Remove a module from the container.\n\n        Args:\n            name (str): The name of the module to remove.\n        \"\"\"\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the modules.\n\n        Args:\n            x (Tensor): The input tensor.\n\n        Returns:\n            Tensor: The output tensor.\n        \"\"\"\n\n    def save_state(self, path):\n        \"\"\"\n        Save the state of the module to a file.\n\n        Args:\n            path (str): The file path to save the module state.\n        \"\"\"\n\n    def load_state(self, path):\n        \"\"\"\n        Load the state of the module from a file.\n\n        Args:\n            path (str): The file path to load the module state.\n        \"\"\"\n</code></pre>"},{"location":"zeta/nn/modules/dm/#how-it-works","title":"How It Works","text":"<p>The <code>DynamicModule</code> is a subclass of <code>nn.Module</code> that uses an <code>nn.ModuleDict</code> to manage the dynamically added submodules. It provides the <code>add</code> and <code>remove</code> methods to add and remove submodules by specifying a name for each. The <code>forward</code> method processes input data sequentially through the added submodules.</p>"},{"location":"zeta/nn/modules/dm/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/modules/dm/#example-1-dynamic-architecture","title":"Example 1: Dynamic Architecture","text":"<pre><code>import torch\nfrom torch import nn\n\n\n# Define a custom forward method\ndef custom_forward(module_dict, x):\n    return module_dict[\"linear\"](x)\n\n\n# Create a DynamicModule with a custom forward method\ndynamic_module = DynamicModule(forward_method=custom_forward)\n\n# Add linear and relu modules\ndynamic_module.add(\"linear\", nn.Linear(10, 10))\ndynamic_module.add(\"relu\", nn.ReLU())\n\n# Pass data through the dynamic architecture\ninput_data = torch.randn(1, 10)\noutput = dynamic_module(input_data)\n\n# Remove the 'relu' module\ndynamic_module.remove(\"relu\")\n</code></pre>"},{"location":"zeta/nn/modules/dm/#example-2-conditional-network","title":"Example 2: Conditional Network","text":"<pre><code># Define a condition\nuse_dropout = True\n\n# Create a DynamicModule\ndynamic_module = DynamicModule()\n\n# Add a linear module\ndynamic_module.add(\"linear\", nn.Linear(10, 10))\n\n# Add a dropout module conditionally\nif use_dropout:\n    dynamic_module.add(\"dropout\", nn.Dropout(0.5))\n\n# Pass data through the dynamic network\ninput_data = torch.randn(1, 10)\noutput = dynamic_module(input_data)\n</code></pre>"},{"location":"zeta/nn/modules/dm/#example-3-experimentation","title":"Example 3: Experimentation","text":"<pre><code># Create a DynamicModule\ndynamic_module = DynamicModule()\n\n# Add different modules for experimentation\ndynamic_module.add(\"conv1\", nn.Conv2d(3, 32, kernel_size=3, padding=1))\ndynamic_module.add(\"conv2\", nn.Conv2d(32, 64, kernel_size=3, padding=1))\ndynamic_module.add(\"maxpool\", nn.MaxPool2d(kernel_size=2, stride=2))\ndynamic_module.add(\"linear\", nn.Linear(64 * 16 * 16, 10))\n\n# Save the module state\ndynamic_module.save_state(\"experiment.pth\")\n\n# Load the module state for further experimentation\ndynamic_module.load_state(\"experiment.pth\")\n</code></pre>"},{"location":"zeta/nn/modules/dm/#mathematical-representation","title":"Mathematical Representation","text":"<p>Let <code>DynamicModule</code> be the class, and <code>M_i</code> be the submodules added with names <code>N_i</code>. The forward operation can be represented as follows:</p> <p><code>DynamicModule(x) = M_N1(M_N2(...M_Nk(x)))</code></p> <p>Where <code>x</code> is the input data, and <code>M_Ni</code> represents the submodule with name <code>N_i</code>. This representation allows for dynamic customization and experimentation of neural network architectures.</p>"},{"location":"zeta/nn/modules/dualpathblock/","title":"DualPathBlock","text":"<p>Table of Contents</p> <ol> <li>Introduction</li> <li>Key Features</li> <li>Class Definition</li> <li>Example Usage</li> <li>Practical Tips</li> <li>Reference and Other Resources</li> </ol>"},{"location":"zeta/nn/modules/dualpathblock/#introduction","title":"Introduction","text":"<p>The <code>DualPathBlock</code> class is a PyTorch-based module or grammar that represents a basic computational unit in dual path networks. This class combines the output of two submodules by element-wise addition. The core idea behind this method is to efficiently use the information from both paths in a balanced way.</p>"},{"location":"zeta/nn/modules/dualpathblock/#key-features","title":"Key Features","text":"<ul> <li> <p>Efficient combination of data: The <code>DualPathBlock</code> method combines data from two submodules in an effective way by using element-wise addition.</p> </li> <li> <p>Flexibility in submodule choice: Users have the flexibility to choose the submodules, provided they are <code>torch.nn.Module</code> instances.</p> </li> <li> <p>Simplicity and readability of code: Due to its modular design, the code is easy to understand, thereby making it easier for users to implement and modify.</p> </li> <li> <p>Easy integration with other <code>torch.nn.Module</code> instances: The <code>DualPathBlock</code> can be easily integrated within other pipelines as a subnet.</p> </li> </ul>"},{"location":"zeta/nn/modules/dualpathblock/#class-definition","title":"Class Definition","text":"<p>The class design for <code>DualPathBlock</code> is very straightforward. It is initialized with two submodules that are instances of <code>nn.Module</code>. Then, during the forward pass, the inputs are passed through each submodule and the result of these computations is then computed by element-wise addition.</p>"},{"location":"zeta/nn/modules/dualpathblock/#parameters","title":"Parameters:","text":"Parameter Type Description submodule1 nn.Module First submodule through which input tensor <code>x</code> is passed. submodule2 nn.Module Second submodule through which input tensor <code>x</code> is passed."},{"location":"zeta/nn/modules/dualpathblock/#methods","title":"Methods:","text":"Method Parameters Description forward x: torch.Tensor Performs forward pass through the model. Calculates output tensor obtained by adding outputs of submodule1 and submodule2. Returns the computed tensor"},{"location":"zeta/nn/modules/dualpathblock/#input-output-type","title":"Input / Output Type:","text":"<ul> <li>Input: Receives a tensor of any shape.</li> <li>Output: Produces a tensor of the same shape as the inputs after the forward computation is done.</li> </ul>"},{"location":"zeta/nn/modules/dualpathblock/#example-usage","title":"Example Usage","text":"<pre><code># Import the necessary libraries\nimport torch\nimport torch.nn as nn\n\nfrom zeta.nn import DualPathBlock\n\n# Define two simple submodule\nsubmodule1 = nn.Linear(20, 20)\nsubmodule2 = nn.Linear(20, 20)\n\n# Create an instance of DualPathBlock\ndual_path_block = DualPathBlock(submodule1, submodule2)\n\n# Define an input tensor\ninput_tensor = torch.randn(10, 20)\n\n# Perform forward operation\noutput = dual_path_block(input_tensor)\n\n# Print the output tensor\nprint(output)\n</code></pre>"},{"location":"zeta/nn/modules/dualpathblock/#practical-tips","title":"Practical Tips","text":"<ul> <li> <p>While DualPathBlock design allows for the use of any submodules, please make sure the outputs of both submodules can be summed up i.e., they are of the same shape.</p> </li> <li> <p>DualPathBlock is particularly useful in constructing networks with parallel paths where the outputs are combined. </p> </li> </ul>"},{"location":"zeta/nn/modules/dualpathblock/#references-and-other-resources","title":"References and Other Resources","text":"<p>Pytorch Documentation</p> <p>Dual Path Networks &lt;-- If relevant</p>"},{"location":"zeta/nn/modules/dynamicroutingblock/","title":"dynamicroutingblock","text":""},{"location":"zeta/nn/modules/dynamicroutingblock/#moduleclass-name-dynamicroutingblock","title":"Module/Class Name: DynamicRoutingBlock","text":""},{"location":"zeta/nn/modules/dynamicroutingblock/#overview","title":"Overview","text":"<p>The <code>DynamicRoutingBlock</code> class, which subclass <code>nn.Module</code>, provides the structure for incorporating dynamic routing mechanism between two sub-blocks in a neural network. A dynamic routing algorithm allows a neural network to learn from inputs internally and configure its neurons' connections, thereby allowing the neural network to adapt better to the specific task at hand. This pytorch-based class encapsulates the operations of a dynamic routing block, a higher-level structure in a neural network architecture.</p> <pre><code>class DynamicRoutingBlock(nn.Module):\n</code></pre>"},{"location":"zeta/nn/modules/dynamicroutingblock/#class-definition","title":"Class Definition","text":"<p>Below, you will find the class definition, along with detailed descriptions of its parameters. This gives you a better understanding of the class and circles the logic it follows.</p> <p><pre><code>def __init__(self, sb1: nn.Module, sb2: nn.Module, routing_module: nn.Module):\n</code></pre> Parameters:</p> Parameter Type Description <code>sb1</code> nn.Module The first sub-block <code>sb2</code> nn.Module The second sub-block <code>routing_module</code> nn.Module The module that computes routing weights"},{"location":"zeta/nn/modules/dynamicroutingblock/#method-definitions","title":"Method Definitions","text":""},{"location":"zeta/nn/modules/dynamicroutingblock/#forward-method","title":"Forward Method","text":"<p>This method defines the forward pass of the dynamic routing block. The <code>routing_weights</code> are first computed by inputting <code>x</code> into the provided routing_module. These weights are then used to compute the final output.</p> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>x</code> torch.Tensor The input tensor <p>Return:</p> Type Description torch.Tensor The output tensor after dynamic routing"},{"location":"zeta/nn/modules/dynamicroutingblock/#functionality-and-usage","title":"Functionality and Usage","text":"<p>To illustrate the usefulness and workings of the <code>DynamicRoutingBlock</code>, let's walk through an example. Suppose you want to create a dynamic routing block that routes between two linear transformation (i.e., <code>nn.Linear</code>) sub-blocks, <code>sb1</code> and <code>sb2</code>, and you have a <code>routing_module</code> that computes a sigmoid activation of a dot product with a learnable weight vector.</p> <p>Firstly, define your two sub-blocks and routing module:</p> <pre><code>sb1 = nn.Linear(5, 3)\nsb2 = nn.Linear(5, 3)\n\n\nclass RoutingModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(5))\n\n    def forward(self, x):\n        return torch.sigmoid(x @ self.weights)\n\n\nrouting_module = RoutingModule()\n</code></pre> <p>Then, you instantiate your dynamic routing block like this:</p> <pre><code>drb = DynamicRoutingBlock(sb1, sb2, routing_module)\n</code></pre> <p>The input can be passed to this block to yield the output:</p> <p><pre><code>x = torch.randn(3, 5)\ny = drb(x)\n</code></pre> In the process, the dynamic routing block has learned to route between <code>sb1</code> and <code>sb2</code> depending on <code>routing_module</code>'s weights, allowing the module to discover which sub-block is more 'helpful' for any given input.</p> <p>Dynamic routing is a powerful tool for allowing a neural network to determine more complex, hierarchical relationships among its inputs. Consequently, using dynamic routing blocks such as described could potentially assist in enhancing the network's predictive performance. The <code>DynamicRoutingBlock</code> class provided here provides a simple, yet powerful implementation of such a dynamic routing mechanism.</p>"},{"location":"zeta/nn/modules/ether/","title":"Ether: A Multi-Modal Loss Function","text":""},{"location":"zeta/nn/modules/ether/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Overview</li> <li>Why Ether?</li> <li>Algorithmic Pseudocode for MMOLF</li> <li>PyTorch Implementation</li> <li>Applications and Use Cases</li> <li>Conclusion</li> </ol>"},{"location":"zeta/nn/modules/ether/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the comprehensive documentation for <code>Ether</code>, a multi-modal loss function designed to address the challenges of multi-modal data analysis. In this documentation, we will explore the capabilities, inner workings, and applications of <code>Ether</code>. </p>"},{"location":"zeta/nn/modules/ether/#2-overview","title":"2. Overview","text":"<p><code>Ether</code> is a unique loss function that tackles the problem of optimizing machine learning models when dealing with multi-modal data. Multi-modal data consists of information from various sources or modalities, such as text, images, and audio. These modalities often exhibit different characteristics and distributions, making it challenging to train models effectively.</p> <p><code>Ether</code> addresses this challenge by introducing both intra-modal and inter-modal terms into the loss function. These terms help the model simultaneously minimize the disparity within each modality while aligning the predictions across different modalities. This results in more accurate and consistent predictions.</p>"},{"location":"zeta/nn/modules/ether/#3-why-ether","title":"3. Why Ether?","text":""},{"location":"zeta/nn/modules/ether/#31-diverse-data-nature","title":"3.1 Diverse Data Nature","text":"<p>Multi-modal data, by its very nature, originates from diverse sources. Each modality may have unique characteristics, and traditional loss functions might not effectively handle these differences. <code>Ether</code> adapts dynamically to the diversity of data sources, offering superior performance.</p>"},{"location":"zeta/nn/modules/ether/#32-complex-feature-interactions","title":"3.2 Complex Feature Interactions","text":"<p>In multi-modal data, the interactions between features from different modalities can be highly complex. Features may complement or conflict with each other, requiring a more intricate processing mechanism. <code>Ether</code> addresses these interactions, ensuring that the model optimizes its predictions effectively.</p>"},{"location":"zeta/nn/modules/ether/#4-algorithmic-pseudocode-for-mmolf","title":"4. Algorithmic Pseudocode for MMOLF","text":"<p>To understand how <code>Ether</code> works, let's look at its algorithmic pseudocode:</p> <p>Inputs: - ( y_{\\text{pred}} ) (Predicted values from the model) - ( y_{\\text{true}} ) (True values or ground truth) - ( \\alpha ) (Weighting factor for inter-modal loss)</p> <ol> <li>Calculate the intra-modal loss based on a standard loss function (e.g., Mean Squared Error for regression tasks).</li> <li> <p>( \\text{intra_modal_loss} = \\text{MSE}(y_{\\text{pred}}, y_{\\text{true}}) )</p> </li> <li> <p>Calculate the inter-modal discrepancy. This metric quantifies the variation between modalities.</p> </li> <li>for each modality do:<ul> <li>Calculate the mean and variance of the predictions for this modality.</li> <li>Compute the total variance from the mean of all modalities.</li> </ul> </li> <li> <p>( \\text{inter_modal_loss} = \\text{Sum of discrepancies between each modality's predictions and the overall mean} )</p> </li> <li> <p>Combine the intra-modal and inter-modal losses using the weight ( \\alpha ).</p> </li> <li> <p>( \\text{loss} = \\text{intra_modal_loss} + \\alpha \\times \\text{inter_modal_loss} )</p> </li> <li> <p>Return: ( \\text{loss} )</p> </li> </ol>"},{"location":"zeta/nn/modules/ether/#5-pytorch-implementation","title":"5. PyTorch Implementation","text":"<p>Implementing <code>Ether</code> in PyTorch is straightforward. Here's a sample implementation:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Ether(nn.Module):\n    def __init__(self, alpha=1.0):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, y_pred, y_true):\n        # Intra-modal loss\n        intra_modal_loss = F.mse_loss(y_pred, y_true)\n\n        # Inter-modal loss\n        modal_means = [torch.mean(y_pred[:, modality]) for modality in self.modalities]\n        overall_mean = torch.mean(y_pred)\n        inter_modal_loss = sum([torch.abs(mean - overall_mean) for mean in modal_means])\n\n        return intra_modal_loss + self.alpha * inter_modal_loss\n</code></pre> <p>This PyTorch implementation of <code>Ether</code> calculates both intra-modal and inter-modal losses, combining them to form the final loss.</p>"},{"location":"zeta/nn/modules/ether/#6-applications-and-use-cases","title":"6. Applications and Use Cases","text":""},{"location":"zeta/nn/modules/ether/#61-data-fusion","title":"6.1 Data Fusion","text":"<p><code>Ether</code> is particularly beneficial when fusing data from diverse sources. It helps ensure that the fusion process takes into account the differences and similarities between modalities, resulting in more accurate representations.</p>"},{"location":"zeta/nn/modules/ether/#62-transfer-learning-across-modalities","title":"6.2 Transfer Learning Across Modalities","text":"<p>In scenarios where knowledge transfer is required across different modalities, <code>Ether</code> plays a crucial role. Its ability to align predictions and reduce discrepancies makes it an effective tool for transfer learning tasks.</p>"},{"location":"zeta/nn/modules/ether/#7-conclusion","title":"7. Conclusion","text":"<p><code>Ether</code> represents a significant advancement in the field of multi-modal data analysis. By addressing the challenges posed by diverse data sources and complex feature interactions, <code>Ether</code> improves the accuracy and consistency of machine learning models. Its applications span various domains, from data fusion to transfer learning.</p> <p>Incorporating <code>Ether</code> into your multi-modal machine learning pipelines can lead to more robust and effective models. As you explore this loss function further, remember to adapt its parameters and weighting factors to your specific use cases.</p> <p>If you have any questions, encounter issues, or need assistance, please refer to the documentation, resources, and support channels available. Harness the power of <code>Ether</code> to unlock the potential of your multi-modal data analysis tasks.</p>"},{"location":"zeta/nn/modules/exo/","title":"<code>Exo</code> Documentation","text":""},{"location":"zeta/nn/modules/exo/#overview","title":"Overview","text":"<p>The Zeta library is a collection of deep learning utilities and custom PyTorch layers designed to enhance your neural network modeling experience. With a focus on simplicity, efficiency, and modularity, Zeta empowers you to build and fine-tune deep learning models with ease.</p> <p>This documentation will guide you through the various components of the Zeta library, explaining their functionality, parameters, and providing practical examples for each. By the end of this documentation, you will have a solid understanding of how to leverage Zeta to streamline your deep learning projects.</p>"},{"location":"zeta/nn/modules/exo/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Exo Activation Function</li> <li>Introduction</li> <li>Design Philosophy</li> <li>Mechanism of Operation</li> <li>Why Exo Works the Way It Does</li> <li>Ideal Use Cases</li> <li>Experimental Evaluation</li> </ol>"},{"location":"zeta/nn/modules/exo/#exo-activation-function","title":"Exo Activation Function","text":""},{"location":"zeta/nn/modules/exo/#introduction","title":"Introduction","text":"<p>The Exo activation function is a novel approach to activation functions in deep learning. It combines linear and non-linear parts to adaptively transform input data based on its distribution. This documentation will provide a comprehensive understanding of the Exo activation function.</p>"},{"location":"zeta/nn/modules/exo/#design-philosophy","title":"Design Philosophy","text":"<p>Exo embodies adaptability, drawing inspiration from the unpredictability of outer space. It dynamically adjusts its transformation based on the input data, making it suitable for multi-modal tasks with diverse data types.</p>"},{"location":"zeta/nn/modules/exo/#mechanism-of-operation","title":"Mechanism of Operation","text":"<p>Exo operates through a gating mechanism that weighs the influence of linear and non-linear transformations based on the magnitude and distribution of input data. The pseudocode for Exo is as follows:</p> <pre><code>function Exo(x, alpha):\n    gate = sigmoid(alpha * x)\n    linear_part = x\n    non_linear_part = tanh(x)\n    return gate * linear_part + (1 - gate) * non_linear_part\n</code></pre>"},{"location":"zeta/nn/modules/exo/#why-exo-works-the-way-it-does","title":"Why Exo Works the Way It Does","text":"<p>Exo's strength lies in its adaptability. The gating mechanism, controlled by the sigmoid function, acts as a switch. For high-magnitude inputs, Exo trends towards a linear behavior, while for lower-magnitude inputs, it adopts a non-linear transformation via the tanh function. This adaptability allows Exo to efficiently handle data heterogeneity, a prominent challenge in multi-modal tasks.</p>"},{"location":"zeta/nn/modules/exo/#ideal-use-cases","title":"Ideal Use Cases","text":"<p>Exo is well-suited for various domains:</p> <ul> <li> <p>Multi-Modal Data Processing: Exo's adaptability makes it a strong contender for models handling diverse data types, such as text, image, or audio.</p> </li> <li> <p>Transfer Learning: The dynamic range of Exo can be beneficial when transferring knowledge from one domain to another.</p> </li> <li> <p>Real-time Data Streams: In applications where data distributions might change over time, Exo's adaptive nature can offer robust performance.</p> </li> </ul>"},{"location":"zeta/nn/modules/exo/#experimental-evaluation","title":"Experimental Evaluation","text":"<p>Future research will rigorously evaluate Exo against traditional activation functions across varied datasets and tasks.</p>"},{"location":"zeta/nn/modules/exo/#exo-class","title":"Exo Class","text":"<p>Now, let's explore the Exo class, which implements the Exo activation function.</p>"},{"location":"zeta/nn/modules/exo/#exo-class-definition","title":"Exo Class Definition","text":"<pre><code>class Exo(nn.Module):\n    \"\"\"\n    Exo activation function.\n\n    Parameters:\n    - alpha (float): Alpha value for the activation function. Default: 1.0\n    \"\"\"\n\n    def __init__(self, alpha=1.0):\n        \"\"\"INIT function.\"\"\"\n        super().__init__()\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        gate = torch.sigmoid(x)\n        linear_part = x\n        non_linear_part = torch.tanh(x)\n        return gate * linear_part + (1 - gate) * non_linear_part\n</code></pre>"},{"location":"zeta/nn/modules/exo/#example-usage","title":"Example Usage","text":"<pre><code># Create an Exo instance\nexo_activation = Exo(alpha=0.5)\n\n# Apply Exo activation to an input tensor\ninput_tensor = torch.randn(2)\noutput_tensor = exo_activation(input_tensor)\n</code></pre> <p>In the example above, we create an Exo instance with a custom alpha value and apply it to an input tensor. You can adjust the <code>alpha</code> parameter to control the adaptability of the Exo activation function to your specific dataset.</p>"},{"location":"zeta/nn/modules/exo/#conclusion","title":"Conclusion","text":"<p>This concludes the documentation for the Zeta library, with a focus on the Exo activation function. We hope this information empowers you to effectively incorporate Exo and other Zeta components into your deep learning projects, allowing you to tackle complex tasks with confidence. For more information and updates, please refer to the official Zeta documentation and resources.</p>"},{"location":"zeta/nn/modules/expert/","title":"Module Documentation: <code>Experts</code>","text":""},{"location":"zeta/nn/modules/expert/#overview","title":"Overview","text":"<p>The <code>Experts</code> module is designed to implement an expert module for the Mixture of Experts layer. This module is particularly useful for tasks that require the combination of information from different subspaces. It takes input features of a specific dimension and processes them through multiple experts to produce an output tensor of shape <code>(batch_size, seq_len, dim)</code>.</p> <p>In this documentation, we will provide a detailed explanation of the <code>Experts</code> module, including its purpose, class definition, parameters, functionality, and usage examples.</p>"},{"location":"zeta/nn/modules/expert/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Class Definition</li> <li>Parameters</li> <li>Functionality</li> <li>Usage Examples</li> <li>Additional Information</li> </ol>"},{"location":"zeta/nn/modules/expert/#class-definition","title":"Class Definition","text":"<pre><code>class Experts(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        experts: int = 16,\n    ):\n        \"\"\"\n        Expert module for the Mixture of Experts layer.\n\n        Args:\n            dim (int): Dimension of the input features.\n            experts (int): Number of experts.\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, seq_len, dim).\n        \"\"\"\n        super().__init__()\n        self.w1 = nn.Parameter(torch.randn(experts, dim, dim * 2))\n        self.w2 = nn.Parameter(torch.randn(experts, dim * 4, dim * 4))\n        self.w3 = nn.Parameter(torch.randn(experts, dim * 4, dim))\n        self.act = nn.LeakyReLU(inplace=True)\n\n    def forward(self, x):\n        \"\"\"Forward pass.\"\"\"\n        hidden1 = self.act(torch.einsum(\"end,edh-&gt;enh\", x, self.w1))\n        hidden2 = self.act(torch.einsum(\"end,edh-&gt;enh\", hidden1, self.w2))\n        out = torch.einsum(\"end,edh-&gt;enh\", hidden2, self.w3)\n        return out\n</code></pre>"},{"location":"zeta/nn/modules/expert/#parameters","title":"Parameters","text":"<ul> <li><code>dim</code> (int): Dimension of the input features.</li> <li><code>experts</code> (int): Number of experts.</li> </ul>"},{"location":"zeta/nn/modules/expert/#functionality","title":"Functionality","text":"<p>The <code>Experts</code> module takes input features of dimension <code>dim</code> and processes them through a series of operations to produce an output tensor of shape <code>(batch_size, seq_len, dim)</code>.</p> <p>The operations performed in the <code>forward</code> method include: 1. Linear transformation of the input features using learnable weights <code>w1</code>, followed by the LeakyReLU activation function. 2. Another linear transformation of the intermediate result using learnable weights <code>w2</code>, followed by the LeakyReLU activation function. 3. A final linear transformation of the last intermediate result using learnable weights <code>w3</code>.</p> <p>The <code>forward</code> method returns the final output tensor.</p>"},{"location":"zeta/nn/modules/expert/#usage-examples","title":"Usage Examples","text":"<p>Here are three usage examples of the <code>Experts</code> module:</p>"},{"location":"zeta/nn/modules/expert/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import Experts\n\n# Create input tensor\nx = torch.randn(1, 3, 512)\n\n# Initialize the Experts module with 16 experts\nmodel = Experts(512, 16)\n\n# Forward pass\nout = model(x)\n\n# Print the shape of the output tensor\nprint(out.shape)  # Output: torch.Size([1, 3, 512])\n</code></pre>"},{"location":"zeta/nn/modules/expert/#example-2-custom-number-of-experts","title":"Example 2: Custom Number of Experts","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import Experts\n\n# Create input tensor\nx = torch.randn(2, 4, 256)\n\n# Initialize the Experts module with 8 experts\nmodel = Experts(256, 8)\n\n# Forward pass\nout = model(x)\n\n# Print the shape of the output tensor\nprint(out.shape)  # Output: torch.Size([2, 4, 256])\n</code></pre>"},{"location":"zeta/nn/modules/expert/#example-3-using-device-and-data-type","title":"Example 3: Using Device and Data Type","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import Experts\n\n# Create input tensor\nx = torch.randn(3, 5, 128)\n\n# Initialize the Experts module with 4 experts on GPU\nmodel = Experts(128, 4)\nmodel.to(\"cuda\")  # Move the model to GPU\nx = x.to(\"cuda\")  # Move the input tensor to GPU\n\n# Forward pass\nout = model(x)\n\n# Print the shape of the output tensor\nprint(out.shape)  # Output: torch.Size([3, 5, 128])\n</code></pre>"},{"location":"zeta/nn/modules/expert/#additional-information","title":"Additional Information","text":"<ul> <li>The <code>Experts</code> module is designed to handle multi-expert processing of input features, making it suitable for tasks that require information combination from different subspaces.</li> <li>You can customize the number of experts by adjusting the <code>experts</code> parameter.</li> <li>You can also specify the device and data type for the module and input tensor for efficient computation.</li> </ul> <p>For more details on the usage and customization of the <code>Experts</code> module, refer to the code examples and experiment with different configurations to suit your specific needs.</p>"},{"location":"zeta/nn/modules/fastgeluactivation/","title":"FastGELUActivation","text":"<p>This is a comprehensive documentation for <code>FastGELUActivation</code>, a class of the SWARMS library. </p>"},{"location":"zeta/nn/modules/fastgeluactivation/#overview","title":"Overview","text":"<p>FastGELUActivation is a class implemented in the SWARMS library that introduces an optimized approach to computing Gaussian Error Linear Units (GELUs). It's based on a faster approximation of the GELU activation function, which is generally more accurate than QuickGELU. </p> <p>GELU activation is frequently used in many machine learning applications, particularly deep learning models, to add non-linearity to the operations. Such activation functions help models represent a wider range of phenomena and thus yield more robust and accurate results. For reference on GELUs, please refer to Hendrycks GELUs.</p>"},{"location":"zeta/nn/modules/fastgeluactivation/#class-definition-and-functionality","title":"Class Definition and Functionality","text":"<p>FastGELUActivation is a class in PyTorch's nn.Module that overrides the forward method to provide a new functionality. Below is the class definition of <code>FastGELUActivation</code>.</p> <pre><code>class FastGELUActivation(nn.Module):\n    \"\"\"\n    Applies GELU approximation that is slower than QuickGELU but more accurate.\n    \"\"\"\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        return (\n            0.5\n            * input\n            * (\n                1.0\n                + torch.tanh(input * 0.7978845608 * (1.0 + 0.044715 * input * input))\n            )\n        )\n</code></pre>"},{"location":"zeta/nn/modules/fastgeluactivation/#parameters","title":"Parameters","text":"<p>The <code>FastGELUActivation</code> class uses only one parameter as input in its forward method.</p> Parameter Type Description <code>input</code> Tensor The input tensor that the forward pass needs to compute over."},{"location":"zeta/nn/modules/fastgeluactivation/#inputs","title":"Inputs","text":"<p>The input that <code>FastGELUActivation</code> takes is a PyTorch Tensor, which holds the values that the activation function computes.</p>"},{"location":"zeta/nn/modules/fastgeluactivation/#outputs","title":"Outputs","text":"<p>The forward method of <code>FastGELUActivation</code> returns a new tensor, which is the result of applying the FastGELU activation operation to the input tensor.</p>"},{"location":"zeta/nn/modules/fastgeluactivation/#usage-and-workflow","title":"Usage and Workflow","text":"<p>Using <code>FastGELUActivation</code> involves creating an instance of the class and then using that instance to call the class's <code>forward</code> method with an appropriate input Tensor.</p>"},{"location":"zeta/nn/modules/fastgeluactivation/#example-usage","title":"Example Usage","text":"<p>In this example, we'll create a simple tensor and apply the <code>FastGELUActivation</code> activation function to it.</p> <pre><code>import torch\nfrom torch import Tensor, nn\n\nfrom zeta import FastGELUActivation\n\n# Create an instance of FastGELUActivation\nactivation = FastGELUActivation()\n\n# Create a tensor\ntensor = torch.randn((5, 5), dtype=torch.float32)\n\n# Apply FastGELUActivation\nresult = activation.forward(tensor)\n\nprint(result)\n</code></pre>"},{"location":"zeta/nn/modules/fastgeluactivation/#working-with-real-world-data-example","title":"Working with Real World Data Example","text":"<p>Assuming we're building a neural network that uses the <code>FastGELUActivation</code> as its activation function in one of the layers:</p> <pre><code>import torch.nn as nn\n\nfrom zeta import FastGELUActivation\n\n\nclass NeuralNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(in_features=784, out_features=512)\n        self.layer2 = nn.Linear(in_features=512, out_features=128)\n        self.layer3 = nn.Linear(in_features=128, out_features=10)\n        self.activation = FastGELUActivation()\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.activation(x)\n        x = self.layer2(x)\n        x = self.activation(x)\n        x = self.layer3(x)\n        return x\n\n\nmodel = NeuralNet()\n</code></pre> <p>In this example, we have a simple feedforward neural network with two layers, and it uses <code>FastGELUActivation</code> for the intermediate layers.</p>"},{"location":"zeta/nn/modules/fastgeluactivation/#additional-information-tips","title":"Additional information &amp; Tips","text":"<p>The <code>FastGELUActivation</code> is a faster approximation of the GELU activation operation, but not always the most accurate. Depending on your use case and performance requirements, you may want to use a more robust but slower activation function.</p> <p>Make sure to have a profound understanding of the dataset and context before deciding on the activation function.</p>"},{"location":"zeta/nn/modules/feedbackblock/","title":"FeedbackBlock","text":"<p><code>FeedbackBlock</code> is a class that extends the <code>torch.nn.Module</code> class. As a crucial part of the neural network, this class perfectly illustrates the aspect of modularity that deep learning models can have.</p> <p><code>FeedbackBlock</code> is a namespace that hosts operations and behaves to transformations in such a way that all of its submodules follow along. Its main role is to handle the feedback connections in neural networks while wrapping another module. The feedback connection is a very common architecture in deep learning where the output from one layer is used as additional input to the same layer in subsequent passes.</p>"},{"location":"zeta/nn/modules/feedbackblock/#class-definition","title":"Class Definition:","text":"<pre><code>class FeedbackBlock(nn.Module):\n</code></pre> <p>The <code>FeedbackBlock</code> class has one primary attribute: <code>submodule</code>. The <code>submodule</code> argument represents the \"submodule\" of the current instance of the <code>FeedbackBlock</code> class. It is an instance of <code>torch.nn.Module</code>.</p> <p>In the initial definition, <code>FeedbackBlock</code> takes a <code>submodule</code> as an argument and assigns it to an attribute of the class.</p> <pre><code>def __init__(self, submodule):\n    \"\"\"\n    Initializes the FeedbackBlock module.\n\n    Args:\n        submodule (nn.Module): The submodule to be used within the FeedbackBlock.\n    \"\"\"\n    super().__init__()\n    self.submodule = submodule\n</code></pre> <p>The <code>submodule</code> will be triggered during the forward pass of the <code>FeedbackBlock</code>, with the input subjected to the feedback mechanism.</p> <p>Note: If another Module is assigned as an attribute to a Module, PyTorch will understand that it owns Parameters that can be part of the optimization problem.</p>"},{"location":"zeta/nn/modules/feedbackblock/#forward-method","title":"Forward Method:","text":"<pre><code>def forward(self, x: torch.Tensor, feedback, *args, **kwargs):\n    \"\"\"\n    Performs a forward pass through the FeedbackBlock.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        feedback: The feedback tensor.\n        *args: Additional positional arguments to be passed to the submodule's forward method.\n        **kwargs: Additional keyword arguments to be passed to the submodule's forward method.\n\n    Returns:\n        torch.Tensor: The output tensor after passing through the FeedbackBlock.\n    \"\"\"\n    if feedback is not None:\n        x = x + feedback\n    return self.submodule(x, *args, **kwargs)\n</code></pre> <p>The <code>forward</code> method does the actual computation or transformation. First, the <code>feedback</code> tensor is checked. If it exists (if it's not None), it is added into the input tensor. Once the feedback has been integrated into the input, it calls the forward method of the submodule. Any additional arguments would be directly passed to the submodule's forward method. The output of the submodule's forward pass is the final output we return.</p>"},{"location":"zeta/nn/modules/feedbackblock/#usage","title":"Usage:","text":"<p>The usage of <code>FeedbackBlock</code> is essentially to encapsulate a module in a network that performs a feedback operation. Let's take a simple scenario where you have a neural network <code>model</code> with a linear layer <code>nn.Linear(10,10)</code>:</p> <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn import FeedbackBlock\n\n\n# Define a simple linear network\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(10, 10)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# Instantiate the simple network\nsimple_net = SimpleNet()\n\n# Wrapping the simple network with a FeedbackBlock\nfeedback_net = FeedbackBlock(simple_net)\n\n# Usage in a training loop:\nx = torch.rand((64, 10))  # Assume an input tensor for batch of 64.\n\n# Initialize feedback\nfeedback = None\n\nfor _ in range(100):  # 100 steps\n    y = feedback_net(x, feedback)\n    feedback = y.detach()  # Detach() to avoid backpropagating gradients through time\n    # ... Rest of training loop here\n</code></pre> <p>In the code above, the output from one pass will be fed back into the module during the next pass. This allows the network to adjust its weights accordingly, based on this continuous feedback loop it\u2019s in.</p> <p>Remember that whenever using the FeedbackBlock to encapsulate a network module, the forward method of the base module, must be designed to handle the feedback tensor that will be passed onto it.</p> <p>In charging forward into more complex architectures with dynamic networks or feedback connections, <code>FeedbackBlock</code> will be of immense help, abstracting the complexities away from your specific model and keeping your code modular and easy to follow.</p>"},{"location":"zeta/nn/modules/feedforward/","title":"<code>FeedForward</code>","text":""},{"location":"zeta/nn/modules/feedforward/#overview","title":"Overview","text":"<p>The <code>FeedForward</code> module is a feedforward neural network with LayerNorms and activation functions, designed for various transformer-based models. It offers flexibility in terms of the activation functions used, allowing you to choose between GELU, SiLU, or ReLU squared. Additionally, it supports the Gated Linear Unit (GLU) activation and LayerNorm (LN) after the activation layer for advanced configurations.</p>"},{"location":"zeta/nn/modules/feedforward/#class-definition","title":"Class Definition","text":"<pre><code>class FeedForward(nn.Module):\n    \"\"\"\n    Feedforward neural network with LayerNorms and GELU activations\n\n    Args:\n        dim (int): Input dimension.\n        dim_out (int, optional): Output dimension. Defaults to None (same as input dimension).\n        mult (int, optional): Multiplier for the hidden dimension. Defaults to 4.\n        glu (bool, optional): Whether to use the Gated Linear Unit (GLU) activation. Defaults to False.\n        glu_mult_bias (bool, optional): Whether to use a bias term with the GLU activation. Defaults to False.\n        swish (bool, optional): Whether to use the SiLU activation. Defaults to False.\n        relu_squared (bool, optional): Whether to use the ReLU squared activation. Defaults to False.\n        post_act_ln (bool, optional): Whether to apply LayerNorm after activation. Defaults to False.\n        dropout (float, optional): Dropout probability. Defaults to 0.0.\n        no_bias (bool, optional): Whether to use bias terms in linear layers. Defaults to False.\n        zero_init_output (bool, optional): Whether to initialize the output linear layer to zero. Defaults to False.\n\n    Usage:\n    &gt;&gt;&gt; model = FeedForward(768, 2048, 0.1)\n    &gt;&gt;&gt; x = torch.randn(1, 768)\n    &gt;&gt;&gt; model(x).shape\n    \"\"\"\n</code></pre>"},{"location":"zeta/nn/modules/feedforward/#parameters","title":"Parameters","text":"Parameter Name Description Default Value Type dim Input dimension - int dim_out Output dimension (optional) None int mult Multiplier for hidden dimension 4 int glu Whether to use GLU activation False bool glu_mult_bias Whether to use bias term with GLU activation False bool swish Whether to use SiLU activation False bool relu_squared Whether to use ReLU squared activation False bool post_act_ln Whether to apply LayerNorm after activation False bool dropout Dropout probability 0.0 float no_bias Whether to use bias terms in linear layers False bool zero_init_output Whether to initialize the output linear layer to zero False bool"},{"location":"zeta/nn/modules/feedforward/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/modules/feedforward/#example-1-basic-feedforward-layer","title":"Example 1: Basic FeedForward Layer","text":"<pre><code>model = FeedForward(768, 2048, 0.1)\nx = torch.randn(1, 768)\noutput = model(x)\nprint(output.shape)\n</code></pre>"},{"location":"zeta/nn/modules/feedforward/#example-2-using-silu-activation","title":"Example 2: Using SiLU Activation","text":"<pre><code>model = FeedForward(512, 1024, swish=True)\nx = torch.randn(1, 512)\noutput = model(x)\nprint(output.shape)\n</code></pre>"},{"location":"zeta/nn/modules/feedforward/#example-3-advanced-configuration-with-glu-activation-and-layernorm","title":"Example 3: Advanced Configuration with GLU Activation and LayerNorm","text":"<pre><code>model = FeedForward(256, 512, glu=True, post_act_ln=True, dropout=0.2)\nx = torch.randn(1, 256)\noutput = model(x)\nprint(output.shape)\n</code></pre>"},{"location":"zeta/nn/modules/feedforward/#functionality","title":"Functionality","text":"<p>The <code>FeedForward</code> module performs a feedforward operation on the input tensor <code>x</code>. It consists of a multi-layer perceptron (MLP) with an optional activation function and LayerNorm. The exact configuration depends on the parameters provided during initialization.</p> <p>The key steps of the forward pass include: 1. Projection of the input tensor <code>x</code> to an inner dimension. 2. Application of the specified activation function (e.g., GELU, SiLU, or ReLU squared). 3. Optionally, LayerNorm is applied after the activation. 4. Dropout is applied for regularization. 5. Finally, a linear transformation maps the inner dimension to the output dimension.</p> <p>The <code>FeedForward</code> module offers flexibility in choosing activation functions, enabling you to experiment with different configurations in transformer-based models.</p>"},{"location":"zeta/nn/modules/feedforward/#tips-and-considerations","title":"Tips and Considerations","text":"<ul> <li>Experiment with different activation functions to find the best configuration for your model.</li> <li>Adjust the dropout rate to control overfitting.</li> <li>Consider using LayerNorm for improved performance, especially in deep networks.</li> <li>The <code>zero_init_output</code> option can be useful for certain initialization strategies.</li> </ul>"},{"location":"zeta/nn/modules/film/","title":"Module/Function Name: Film","text":"<p>Provides a Feature-wise Linear Modulation (FiLM) module which applies feature-wise linear modulation to the input features based on the conditioning tensor to adapt them to the given conditions.</p>"},{"location":"zeta/nn/modules/film/#arguments","title":"Arguments","text":"<ul> <li><code>dim</code> (int): The dimension of the input features.</li> <li><code>hidden_dim</code> (int): The dimension of the hidden layer.</li> <li><code>expanse_ratio</code> (int, optional): The expansion ratio for the hidden layer (default = 4).</li> <li><code>conditions</code> (Tensor): The conditioning tensor.</li> <li><code>hiddens</code> (Tensor): The input features to be modulated.</li> </ul>"},{"location":"zeta/nn/modules/film/#usage-examples","title":"Usage Examples","text":"<pre><code>import torch\nfrom zeta.nn import Film\n\n# Initialize the Film layer\nfilm_layer = Film(dim=128, hidden_dim=64, expanse_ratio=4)\n\n# Create dummy data for conditions and hiddens\nconditions = torch.randn(10, 128)  # Batch size is 10, feature size is 128\nhiddens = torch.randn(10, 1, 128)  # Batch size is 10, sequence length is 1, feature size is 128\n\n# Pass the data through the Film layer\nmodulated_features = film_layer(conditions, hiddens)\n\n# Print the shape of the output\nprint(modulated_features.shape)  # Output shape will be [10, 1, 128]\n</code></pre>"},{"location":"zeta/nn/modules/film/#references-and-resources","title":"References and Resources","text":"<ul> <li>Paper: Link to the paper discussing FiLM module.</li> <li>PyTorch Documentation: PyTorch Documentation ```</li> </ul>"},{"location":"zeta/nn/modules/filmconditioning/","title":"filmconditioning","text":"<p><code>FilmConditioning</code> Module</p> <p>Introduction: The FilmConditioning module applies feature-wise affine transformations to the input tensor, conditioning it based on a conditioning tensor. This module is particularly useful in scenarios where feature-based conditioning is required in convolutional neural network architectures.</p> <p>Args: Number of channels (int): Specifies the number of channels in the input tensor.</p> <p>Attributes: num_channels (int): Number of channels in the input tensor. projection_add (nn.Linear): Linear layer for additive projection. projection_mult (nn.Linear): Linear layer for multiplicative projection.</p> <p>Class Definition: <pre><code>class FilmConditioning(nn.Module):\n    def __init__(self, num_channels: int, *args, **kwargs):\n        super().__init__()\n        self.num_channels = num_channels\n        self._projection_add = nn.Linear(num_channels, num_channels)\n        self._projection_mult = nn.Linear(num_channels, num_channels)\n</code></pre></p> <p>Functionality and Usage: The <code>__init__</code> method initializes the module and its attributes. Two linear layers are defined for additive and multiplicative projections of conditioning. The <code>forward</code> method applies affine transformations to the input tensor based on the conditioning tensor. <pre><code>def forward(self, conv_filters: torch.Tensor, conditioning: torch.Tensor):\n    projected_cond_add = self._projection_add(conditioning)\n    projected_cond_mult = self._projection_mult(conditioning)\n    # Modifying the result is based on the conditioning tensor\n    return result\n</code></pre></p> <p>Usage Examples:</p> <p>Usage Example 1: Applying Film Conditioning <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn import FilmConditioning\n\n# Define input tensors\nconv_filters = torch.randn(10, 3, 32, 32)\nconditioning = torch.randn(10, 3)\n\n# Create an instance of FilmConditioning\nfilm_conditioning = FilmConditioning(3)\n\n# Applying film conditioning\nresult = film_conditioning(conv_filters, conditioning)\nprint(result.shape)\n</code></pre></p> <p>Usage Example 2: Applying Film Conditioning for another example <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn import FilmConditioning\n\n# Define input tensors\nconv_filters = torch.randn(5, 4, 20, 20)\nconditioning = torch.randn(5, 4)\n\n# Create an instance of FilmConditioning\nfilm_conditioning = FilmConditioning(4)\n\n# Applying film conditioning\nresult = film_conditioning(conv_filters, conditioning)\nprint(result.shape)\n</code></pre></p> <p>Usage Example 3: Usage Example <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn import FilmConditioning\n\n# Define input tensors\nconv_filters = torch.randn(8, 2, 50, 50)\nconditioning = torch.randn(8, 2)\n\n# Create an instance of FilmConditioning\nfilm_conditioning = FilmConditioning(2)\n\n# Applying film conditioning\nresult = film_conditioning(conv_filters, conditioning)\nprint(result.shape)\n</code></pre></p> <p>References and Resources: Expected format for the documentation should be provided here for any references.</p>"},{"location":"zeta/nn/modules/flexiconv/","title":"Module/Function Name: FlexiConv","text":"<p><code>class FlexiConv(nn.Module)</code></p> <p>FlexiConv is an experimental and flexible convolutional layer that adapts to the input data.</p>"},{"location":"zeta/nn/modules/flexiconv/#args","title":"Args","text":"Argument Description Data Type Default Value in_channels Number of channels in the input image int - out_channels Number of channels produced by the convolution int - kernel_size Size of the convolving kernel int/tuple - stride Stride of the convolution int/tuple 1 padding Zero-padding added to the input int/tuple 0 ## Example <pre><code>import torch\n\nfrom zeta.nn import FlexiConv\n\nflexi_conv = FlexiConv(\n    in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1\n)\ninput_tensor = torch.randn(1, 3, 224, 224)  # Example input batch\noutput = flexi_conv(input_tensor)\noutput.shape\n</code></pre>"},{"location":"zeta/nn/modules/flexiconv/#purpose","title":"Purpose","text":"<p>FlexiConv is aimed at providing a flexible convolutional layer that adapts to the input data using parameterized Gaussian functions to weigh the importance of each pixel in the receptive field and applies a depthwise separable convolution for efficiency.</p>"},{"location":"zeta/nn/modules/flexiconv/#functionality","title":"Functionality","text":"<p>The FlexiConv class encapsulates a flexible convolutional layer that uses Gaussian functions to weigh the importance of each pixel in the receptive field. It applies a depthwise separable convolution to efficiently process input data. The user can specify the number of input and output channels, kernel size, and stride, among other parameters.</p>"},{"location":"zeta/nn/modules/flexiconv/#usage","title":"Usage","text":"<p>The <code>FlexiConv</code> layer can be instantiated by passing the required arguments and then used to process input tensors.</p> <p>Example 1: <pre><code>import torch\n\nfrom zeta.nn import FlexiConv\n\nflexi_conv = FlexiConv(\n    in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1\n)\ninput_tensor = torch.randn(1, 3, 224, 224)\noutput = flexi_conv(input_tensor)\noutput.shape\n</code></pre></p> <p>Example 2: <pre><code>import torch\n\nfrom zeta.nn import FlexiConv\n\nflexi_conv = FlexiConv(\n    in_channels=3, out_channels=64, kernel_size=3, stride=(2, 2), padding=1\n)\ninput_tensor = torch.randn(1, 3, 224, 224)\noutput = flexi_conv(input_tensor)\noutput.shape\n</code></pre></p> <p>Example 3: <pre><code>import torch\n\nfrom zeta.nn import FlexiConv\n\nflexi_conv = FlexiConv(\n    in_channels=3, out_channels=64, kernel_size=(3, 3), stride=(1, 2), padding=1\n)\ninput_tensor = torch.randn(1, 3, 224, 224)\noutput = flexi_conv(input_tensor)\noutput.shape\n</code></pre></p>"},{"location":"zeta/nn/modules/flexiconv/#references","title":"References","text":"<p>Provide any references to further information or research papers related to the FlexiConv module or framework.</p>"},{"location":"zeta/nn/modules/flexiconv/#additional-information","title":"Additional Information","text":"<p>Provide any tips or additional details that may be useful for using the FlexiConv module effectively.</p> <p>By documenting the FlexiConv example, the document provides an in-depth explanation of its purpose, usage, functionality, and examples to ensure the user understands how to effectively leverage the FlexiConv module.</p>"},{"location":"zeta/nn/modules/fused_dropout_layernorm/","title":"FusedDropoutLayerNorm Documentation","text":""},{"location":"zeta/nn/modules/fused_dropout_layernorm/#overview","title":"Overview","text":"<p>The <code>FusedDropoutLayerNorm</code> module in PyTorch is designed to combine two commonly used operations in neural networks: dropout and layer normalization. This fusion aims to enhance the efficiency of the model by reducing the overhead associated with sequential operations. The module is particularly useful in scenarios where both dropout and layer normalization are critical for the model's performance.</p>"},{"location":"zeta/nn/modules/fused_dropout_layernorm/#class-definition","title":"Class Definition","text":""},{"location":"zeta/nn/modules/fused_dropout_layernorm/#fuseddropoutlayernorm","title":"<code>FusedDropoutLayerNorm</code>","text":"<pre><code>class FusedDropoutLayerNorm(nn.Module):\n    \"\"\"\n    This class fuses Dropout and LayerNorm into a single module for efficiency.\n\n    Args:\n        dim (int): Input dimension of the layer.\n        dropout (float, optional): Probability of an element to be zeroed. Defaults to 0.1.\n        eps (float, optional): A value added to the denominator for numerical stability. Defaults to 1e-5.\n        elementwise_affine (bool, optional): A flag to enable learning of affine parameters. Defaults to True.\n    \"\"\"\n</code></pre>"},{"location":"zeta/nn/modules/fused_dropout_layernorm/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Description Default Value <code>dim</code> int The input dimension of the layer. - <code>dropout</code> float Dropout probability. 0.1 <code>eps</code> float Epsilon for numerical stability in LayerNorm. 1e-5 <code>elementwise_affine</code> bool Enables learning of affine parameters in LayerNorm. True"},{"location":"zeta/nn/modules/fused_dropout_layernorm/#methods","title":"Methods","text":""},{"location":"zeta/nn/modules/fused_dropout_layernorm/#forward","title":"<code>forward</code>","text":"<pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of FusedDropoutLayerNorm.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The output tensor after applying dropout and layer normalization.\n    \"\"\"\n</code></pre>"},{"location":"zeta/nn/modules/fused_dropout_layernorm/#examples","title":"Examples","text":""},{"location":"zeta/nn/modules/fused_dropout_layernorm/#basic-usage","title":"Basic Usage","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import FusedDropoutLayerNorm\n\n# Initialize the module\nmodel = FusedDropoutLayerNorm(dim=512)\n\n# Create a sample input tensor\nx = torch.randn(1, 512)\n\n# Forward pass\noutput = model(x)\n\n# Check output shape\nprint(output.shape)  # Expected: torch.Size([1, 512])\n</code></pre>"},{"location":"zeta/nn/modules/fused_dropout_layernorm/#integration-in-a-neural-network","title":"Integration in a Neural Network","text":"<pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn import FusedDropoutLayerNorm\n\n\nclass SampleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(512, 512)\n        self.fused_dropout_layernorm = FusedDropoutLayerNorm(512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.fused_dropout_layernorm(x)\n        return x\n\n\n# Example\nmodel = SampleModel()\ninput_tensor = torch.randn(10, 512)\noutput = model(input_tensor)\nprint(output.shape)  # Expected: torch.Size([10, 512])\n</code></pre>"},{"location":"zeta/nn/modules/fused_dropout_layernorm/#custom-configuration","title":"Custom Configuration","text":"<pre><code>import torch\n\nfrom zeta.nn import FusedDropoutLayerNorm\n\n# Custom configuration\ndropout_rate = 0.2\nepsilon = 1e-6\nelementwise_affine = False\n\n# Initialize the module with custom configuration\nmodel = FusedDropoutLayerNorm(\n    512, dropout=dropout_rate, eps=epsilon, elementwise_affine=elementwise_affine\n)\n\n# Sample input\nx = torch.randn(1, 512)\n\n# Forward pass\noutput = model(x)\nprint(output.shape)  # Expected: torch.Size([1, 512])\n</code></pre>"},{"location":"zeta/nn/modules/fused_dropout_layernorm/#architecture-and-working","title":"Architecture and Working","text":"<p>The <code>FusedDropoutLayerNorm</code> module is architecturally a combination of two PyTorch layers: <code>nn.Dropout</code> and <code>nn.LayerNorm</code>. The fusion of these layers into a single module ensures that the operations are performed sequentially and efficiently, thereby reducing the computational overhead.</p> <ul> <li>Dropout: This operation randomly zeroes some of the elements of the input tensor with probability <code>dropout</code> during training. It helps prevent overfitting.</li> <li>Layer Normalization: This operation normalizes the input across the features. It stabilizes the learning process and accelerates the training of deep neural networks.</li> </ul> <p>By integrating these two operations, <code>FusedDropoutLayerNorm</code> ensures a streamlined process where the dropout is applied first, followed by layer normalization. This design choice is made for computational efficiency and is particularly beneficial in transformer models and other deep learning architectures where both operations are frequently used.</p>"},{"location":"zeta/nn/modules/fused_dropout_layernorm/#purpose-and-importance","title":"Purpose and Importance","text":"<p>The primary purpose of <code>FusedDropoutLayerNorm</code> is to provide a more efficient way to apply both dropout and layer normalization in a model. This efficiency is particularly crucial in</p> <p>large-scale models where computational resources and runtime are significant concerns. The module is designed to be versatile and can be easily integrated into various neural network architectures, especially those involving transformer models.</p>"},{"location":"zeta/nn/modules/fused_dropout_layernorm/#conclusion","title":"Conclusion","text":"<p>The <code>FusedDropoutLayerNorm</code> module in PyTorch is a practical and efficient solution for models that require both dropout and layer normalization. Its fused architecture not only enhances computational efficiency but also simplifies the model design process. The module is flexible, allowing for easy customization and integration into diverse neural network architectures.</p>"},{"location":"zeta/nn/modules/fused_gelu_dense/","title":"<code>FusedDenseGELUDense</code>","text":""},{"location":"zeta/nn/modules/fused_gelu_dense/#overview","title":"Overview","text":"<p>The <code>FusedDenseGELUDense</code> module is a versatile neural network layer designed for efficient computation of dense layers with GELU (Gaussian Error Linear Unit) activations. This documentation will provide an in-depth understanding of the module's architecture, purpose, parameters, and usage examples.</p>"},{"location":"zeta/nn/modules/fused_gelu_dense/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Architecture</li> <li>Purpose</li> <li>Class Definition<ul> <li>Parameters</li> <li>Internal Layers</li> </ul> </li> <li>Functionality and Usage<ul> <li>Forward Pass</li> </ul> </li> <li>Examples<ul> <li>Basic Usage</li> <li>Custom Configuration</li> <li>Quantization with bitsandbytes</li> </ul> </li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/nn/modules/fused_gelu_dense/#1-introduction","title":"1. Introduction","text":"<p>The <code>FusedDenseGELUDense</code> module combines dense layers with GELU activations in a single neural network layer. This fusion improves computational efficiency and is particularly useful in various deep learning applications.</p>"},{"location":"zeta/nn/modules/fused_gelu_dense/#2-architecture","title":"2. Architecture","text":"<p>The <code>FusedDenseGELUDense</code> layer consists of two dense sub-layers, each followed by a GELU activation function. It takes an input tensor and passes it through these sub-layers to produce the final output.</p>"},{"location":"zeta/nn/modules/fused_gelu_dense/#3-purpose","title":"3. Purpose","text":"<p>The primary purpose of the <code>FusedDenseGELUDense</code> layer is to efficiently compute dense transformations with GELU activations. It is designed for use in neural networks, providing a convenient way to incorporate these operations into deep learning models.</p>"},{"location":"zeta/nn/modules/fused_gelu_dense/#4-class-definition","title":"4. Class Definition","text":""},{"location":"zeta/nn/modules/fused_gelu_dense/#parameters","title":"Parameters","text":"<ul> <li><code>dim</code> (int): Input dimension.</li> <li><code>dim_out</code> (int): Output dimension.</li> <li><code>bias</code> (bool, optional): Whether to include bias terms. Defaults to True.</li> <li><code>has_fp16_weights</code> (bool, optional): Whether to use fp16 weights. Defaults to False.</li> <li><code>threshold</code> (float, optional): Threshold for quantization. Defaults to 6.0.</li> </ul>"},{"location":"zeta/nn/modules/fused_gelu_dense/#internal-layers","title":"Internal Layers","text":"<p>The <code>FusedDenseGELUDense</code> layer consists of the following internal layers:</p> <ol> <li><code>dense1</code>: The first dense layer.</li> <li><code>act</code>: The GELU activation function.</li> <li><code>dense2</code>: The second dense layer.</li> </ol>"},{"location":"zeta/nn/modules/fused_gelu_dense/#5-functionality-and-usage","title":"5. Functionality and Usage","text":""},{"location":"zeta/nn/modules/fused_gelu_dense/#forward-pass","title":"Forward Pass","text":"<p>The <code>forward</code> method of the <code>FusedDenseGELUDense</code> layer performs the following operations:</p> <ol> <li>Applies the first dense layer (<code>dense1</code>) to the input tensor.</li> <li>Applies the GELU activation function (<code>act</code>) to the result.</li> <li>Applies the second dense layer (<code>dense2</code>) to the GELU-activated output.</li> </ol>"},{"location":"zeta/nn/modules/fused_gelu_dense/#6-examples","title":"6. Examples","text":""},{"location":"zeta/nn/modules/fused_gelu_dense/#basic-usage","title":"Basic Usage","text":"<p>Here's a basic example of using the <code>FusedDenseGELUDense</code> layer:</p> <pre><code>import torch\n\nfrom zeta.nn import FusedDenseGELUDense\n\n# Create an instance of FusedDenseGELUDense\nmodel = FusedDenseGELUDense(dim=512, dim_out=1024)\n\n# Generate random input tensor\nx = torch.randn(1, 512)\n\n# Forward pass\nout = model(x)\n\n# Check the output shape\nprint(out.shape)  # torch.Size([1, 512])\n</code></pre>"},{"location":"zeta/nn/modules/fused_gelu_dense/#custom-configuration","title":"Custom Configuration","text":"<p>You can customize the layer by specifying different parameters:</p> <pre><code># Create a custom FusedDenseGELUDense layer\ncustom_model = FusedDenseGELUDense(\n    dim=256, dim_out=512, bias=False, has_fp16_weights=True, threshold=4.0\n)\n\n# Generate random input tensor\nx = torch.randn(1, 256)\n\n# Forward pass with the custom configuration\nout = custom_model(x)\n</code></pre>"},{"location":"zeta/nn/modules/fused_gelu_dense/#quantization-with-bitsandbytes","title":"Quantization with bitsandbytes","text":"<p>You can enable quantization using the <code>bitsandbytes</code> library by providing a quantized implementation of the dense layers:</p> <pre><code># Install bitsandbytes if not already installed\n# pip install bitsandbytes\n\nimport torch\n\nfrom zeta.nn import FusedDenseGELUDense\n\n# Create an instance of FusedDenseGELUDense with quantization\nquantized_model = FusedDenseGELUDense(\n    dim=512, dim_out=1024, has_fp16_weights=True, threshold=4.0\n)\n\n# Generate random input tensor\nx = torch.randn(1, 512)\n\n# Forward pass with quantization\nout = quantized_model(x)\n</code></pre>"},{"location":"zeta/nn/modules/fused_gelu_dense/#7-additional-information","title":"7. Additional Information","text":"<ul> <li>The <code>FusedDenseGELUDense</code> layer efficiently combines dense and GELU activation operations.</li> <li>Custom configurations for bias, weight precision, and threshold are supported.</li> <li>Quantization can be enabled using the <code>bitsandbytes</code> library for further efficiency.</li> </ul>"},{"location":"zeta/nn/modules/fused_gelu_dense/#8-references","title":"8. References","text":"<p>For more information on GELU activations and dense layers in PyTorch, refer to the official PyTorch documentation:</p> <ul> <li>GELU Activation Function</li> <li>Dense Layer</li> </ul>"},{"location":"zeta/nn/modules/fuseddensegeludense/","title":"Module Name: FusedDenseGELUDense","text":"<p>The <code>FusedDenseGELUDense</code> module represents a combination of fully connected layers with the GELU activation function. It is suitable for efficiently performing linear transformations with an activation function in between, commonly used in neural network architectures. The input dimension (<code>dim</code>) and output dimension (<code>dim_out</code>) can be specified, while further customizations such as selecting the datatype and setting specific threshold configurations are also supported.</p>"},{"location":"zeta/nn/modules/fuseddensegeludense/#args","title":"Args:","text":"<p>The table below summarizes the arguments of the <code>FusedDenseGELUDense</code> module:</p> Argument Type Description Default Value dim int Input dimension - dim_out int Output dimension - bias bool (optional) Indicates whether to use a bias term True has_fp16_weights bool (optional) Whether to use fp16 weights False threshold float (optional) Threshold for quantization 6.0"},{"location":"zeta/nn/modules/fuseddensegeludense/#purpose","title":"Purpose:","text":"<p>The <code>FusedDenseGELUDense</code> module is designed to efficiently perform linear transformations and activations in neural network architectures. It allows for customizable configurations such as input and output dimensions, the inclusion of bias terms, FP16 weight usage, and threshold settings, providing flexibility in designing network layers.</p>"},{"location":"zeta/nn/modules/fuseddensegeludense/#functionality-and-usage","title":"Functionality and Usage:","text":"<p>The <code>FusedDenseGELUDense</code> class effectively combines linear transformation operations with GELU activation. During the forward pass, the input data passes through a linear transformation, followed by the GELU activation, and another linear transformation, providing the final output.</p> <p>This module is particularly useful for creating deep learning models that require efficient processing of the data through multiple connected layers with non-linear activation functions in between. Below is an example of how to use the <code>FusedDenseGELUDense</code> module:</p> <pre><code># Example of using the FusedDenseGELUDense module\nimport torch\n\nfrom zeta.nn import FusedDenseGELUDense\n\n# Define input data\nx = torch.randn(1, 512)\n\n# Create the FusedDenseGELUDense module\nmodel = FusedDenseGELUDense(512, 1024)\n\n# Perform the forward pass\nout = model(x)\n\n# Display the shape of the output\nprint(out.shape)\n# Expected Output:\n# torch.Size([1, 512])\n</code></pre> <p>The example illustrates the creation of a <code>FusedDenseGELUDense</code> object with input dimension 512 and output dimension 1024. Then, the forward pass is executed on the input <code>x</code>, resulting in the output tensor <code>out</code>.</p>"},{"location":"zeta/nn/modules/fuseddensegeludense/#additional-information-and-tips","title":"Additional Information and Tips:","text":"<p>Avoid using non-default values for the <code>has_fp16_weights</code> and <code>threshold</code> arguments unless with a specific need for FP16 weights and custom quantization threshold. For most use cases, the default settings are recommended. Be aware that the activation function used in <code>FusedDenseGELUDense</code> is the GELU activation, and the logic within the module will have different execution paths based on the availability of the <code>bitsandbytes</code> package.</p>"},{"location":"zeta/nn/modules/fuseddensegeludense/#references-and-resources","title":"References and Resources:","text":"<p>When using quantization and FP16 weights, it's advisable to refer to the official PyTorch documentation on these topics for further understanding. For comprehensive information on the GELU activation function, the original research paper or relevant documentation are valuable resources.</p> <p>In conclusion, the <code>FusedDenseGELUDense</code> module aims to provide an optimized and flexible approach for incorporating linear transformations and activations within neural network architectures.</p>"},{"location":"zeta/nn/modules/fuseddensegeludense/#note","title":"Note:","text":"<p>The given example template and documentation format have been followed to deliver explicit and thorough documentation for the <code>FusedDenseGELUDense</code> module, addressing its purpose, essential arguments, usage, and additional tips.</p>"},{"location":"zeta/nn/modules/fuseddropoutlayernorm/","title":"Module/Function Name: FusedDropoutLayerNorm","text":"<p>Class torch.nn.FusedDropoutLayerNorm(dim, dropout=0.1, eps=1e-5, elementwise_affine=True):         \"\"\"         Creates a fused dropout and layer normalization module.         The dropout and layer normalization operations are performed together in a single layer.</p> <pre><code>    Parameters:\n    - dim (int): Input dimension.\n    - dropout (float, optional): Dropout probability. Default: 0.1 (10% dropout).\n    - eps (float, optional): Epsilon value for layer normalization (std variance addition). Default: 1e-5.\n    - elementwise_affine (bool, optional): If True, provides learnable scaling and normalization weights. Default: True.\n    \"\"\"\n\n    def forward(x):\n        \"\"\"\n        Forward pass of the FusedDropoutLayerNorm module.\n\n        Parameters:\n        - x (Tensor): Input tensor to be processed.\n\n        Returns:\n        Tensor: Normalized and dropout-applied output tensor.\n        \"\"\"\n        x = self.dropout(x)\n        return self.layer_norm(x)\n</code></pre>"},{"location":"zeta/nn/modules/fuseddropoutlayernorm/#example-usage","title":"Example Usage:","text":"<p>Dim: 512</p> <p><pre><code>import torch\nfrom torch import nn\n\nx = torch.randn(1, 512)\nmodel = nn.FusedDropoutLayerNorm(512)\nout = model(x)\nprint(out.shape)  # Output: torch.Size([1, 512])\n</code></pre>     \"\"\" Reference for further information: Module/Function Name: FusedDropoutLayerNorm</p>"},{"location":"zeta/nn/modules/fuseddropoutlayernorm/#documentation-httpspytorchorgdocsstablennhtmltorchnnfuseddropoutlayernorm","title":"Documentation: https://pytorch.org/docs/stable/nn.html#torch.nn.FusedDropoutLayerNorm","text":""},{"location":"zeta/nn/modules/fuseddropoutlayernorm/#pytorch-github-httpsgithubcompytorchpytorch","title":"PyTorch GitHub: https://github.com/pytorch/pytorch","text":""},{"location":"zeta/nn/modules/fuseddropoutlayernorm/#stack-overflow-httpsstackoverflowcomquestionstaggedpytorch","title":"Stack Overflow: https://stackoverflow.com/questions/tagged/pytorch","text":""},{"location":"zeta/nn/modules/fusedprojsoftmax/","title":"FusedProjSoftmax","text":"<p><code>FusedProjSoftmax</code> is a PyTorch module that applies a linear projection followed by a softmax operation. This can be used for a wide array of applications in various domains from machine learning and natural language processing to image recognition and beyond.</p>"},{"location":"zeta/nn/modules/fusedprojsoftmax/#overview","title":"Overview","text":"<p>The primary goal of the <code>FusedProjSoftmax</code> module is to provide an efficient and easy-to-use implementation for linear projection and softmax operation which are common components in many neural network architectures.</p>"},{"location":"zeta/nn/modules/fusedprojsoftmax/#class-definition","title":"Class Definition","text":""},{"location":"zeta/nn/modules/fusedprojsoftmax/#parameters","title":"Parameters","text":"<p>The <code>FusedProjSoftmax</code> class constructor takes the following parameters:</p> Parameter Description Type Default Value dim The input dimension int dim_out The output dimension int dim_axis The axis along which the softmax operation is applied int -1 *args Variable length arguments **kwargs Arbitrary keyword arguments"},{"location":"zeta/nn/modules/fusedprojsoftmax/#attributes","title":"Attributes","text":"<p>The <code>FusedProjSoftmax</code> module has two attributes:</p> <ul> <li><code>proj</code>: A linear projection layer <code>nn.Linear</code> used for projecting the input to the output dimension.</li> <li><code>softmax</code>: A softmax operation layer <code>nn.Softmax</code> used to apply the softmax operation along the specified axis.</li> </ul>"},{"location":"zeta/nn/modules/fusedprojsoftmax/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/modules/fusedprojsoftmax/#example-1-initializing-and-using-the-fusedprojsoftmax-module","title":"Example 1: Initializing and using the <code>FusedProjSoftmax</code> module","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import FusedProjSoftmax\n\n# Create an input tensor x\nx = torch.rand(1, 2, 3)\n\n# Initialize the FusedProjSoftmax module with input and output dimensions\nmodel = FusedProjSoftmax(3, 4)\n\n# Apply the FusedProjSoftmax operation to the input tensor x\nout = model(x)\n\n# Print the shape of the output tensor\nprint(out.shape)\n</code></pre>"},{"location":"zeta/nn/modules/fusedprojsoftmax/#example-2-creating-a-custom-model-with-the-fusedprojsoftmax-module","title":"Example 2: Creating a custom model with the FusedProjSoftmax module","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import FusedProjSoftmax\n\n\n# Define a custom neural network model\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.projsoftmax = FusedProjSoftmax(5, 10)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Apply the FusedProjSoftmax operation to the input tensor\n        return self.projsoftmax(x)\n</code></pre>"},{"location":"zeta/nn/modules/fusedprojsoftmax/#example-3-specifying-optional-arguments-when-initializing-fusedprojsoftmax","title":"Example 3: Specifying optional arguments when initializing FusedProjSoftmax","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import FusedProjSoftmax\n\n# Create an input tensor x\nx = torch.rand(1, 2, 3)\n\n# Initialize the FusedProjSoftmax module with input and output dimensions\n# Specify the axis along which the softmax operation is applied\nmodel = FusedProjSoftmax(3, 4, dim_axis=1)\n\n# Apply the FusedProjSoftmax operation to the input tensor x\nout = model(x)\n\n# Print the shape of the output tensor\nprint(out.shape)\n</code></pre>"},{"location":"zeta/nn/modules/fusedprojsoftmax/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>When using the <code>FusedProjSoftmax</code> module, it is important to ensure that the dimensions and axes are correctly specified to achieve the desired output.</li> </ul>"},{"location":"zeta/nn/modules/fusedprojsoftmax/#references-and-resources","title":"References and Resources","text":"<p>For further information or in-depth exploration of the softmax operation and relevant documentation, refer to the PyTorch documentation and relevant research papers or articles.</p> <p>With this detailed and comprehensive documentation, users can effectively understand and utilize the functionality of the <code>FusedProjSoftmax</code> module in their PyTorch projects. This documentation provides a clear overview, description of each feature, usage examples, and additional usage tips, ensuring that users have a complete understanding of the module.</p>"},{"location":"zeta/nn/modules/gatedresidualblock/","title":"Module/Function Name: GatedResidualBlock","text":"<p><code>class GatedResidualBlock(nn.Module):</code></p>"},{"location":"zeta/nn/modules/gatedresidualblock/#overview","title":"Overview","text":"<p>The <code>GatedResidualBlock</code> is a subclass of the <code>nn.Module</code> which belongs to the PyTorch library. The main objective of this module is to implement a special variant of Residual Block structure which is commonly used in designing deep learning architectures.</p> <p>Traditionally, a Residual Block allows the model to learn an identity function which helps in overcoming the problem of vanishing gradients in very deep networks. The <code>GatedResidualBlock</code> takes this a step further by introducing gating mechanisms, allowing the model to control the information flow across the network. The gate values, generated by the <code>gate_module</code>, determines the degree to which the input data flow should be altered by the first sub-block <code>sb1</code>.</p> <p>This architecture promotes stability during the training of deep networks and increases the adaptability of the model to complex patterns in the data.</p>"},{"location":"zeta/nn/modules/gatedresidualblock/#class-definition","title":"Class Definition","text":"<p>The class definition for <code>GatedResidualBlock</code> is as follows:</p> <pre><code>class GatedResidualBlock(nn.Module):\n    def __init__(self, sb1, gate_module):\n        super().__init__()\n        self.sb1 = sb1\n        self.gate_module = gate_module\n</code></pre>"},{"location":"zeta/nn/modules/gatedresidualblock/#arguments","title":"Arguments","text":"Argument Type Description <code>sb1</code> <code>nn.Module</code> The first sub-block of the Gated Residual Block. <code>gate_module</code> <code>nn.Module</code> The gate module that determines the degree to which the input should be altered by the first sub-block <code>sb1</code>."},{"location":"zeta/nn/modules/gatedresidualblock/#example-usage-of-gatedresidualblock","title":"Example: Usage of GatedResidualBlock","text":"<p>A simple usage of <code>GatedResidualBlock</code> is demonstrated below.</p> <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn import GatedResidualBlock\n\n# Define the sub-blocks\nsb1 = nn.Linear(16, 16)\ngate_module = nn.Linear(16, 16)\n\n# Create the GatedResidualBlock\ngrb = GatedResidualBlock(sb1, gate_module)\n\n# Sample input\nx = torch.rand(1, 16)\n\n# Forward pass\ny = grb(x)\n</code></pre> <p>In the above example, both subblocks are simple linear layers. The input <code>x</code> is passed through the <code>GatedResidualBlock</code>, where it's processed by the <code>gate_module</code> and <code>sb1</code> as described in the class documentation.</p>"},{"location":"zeta/nn/modules/gatedresidualblock/#method-definition","title":"Method Definition","text":"<p>The method definition for <code>GatedResidualBlock</code> class is as follows:</p> <pre><code>def forward(self, x: torch.Tensor):\n    gate = torch.sigmoid(self.gate_module(x))\n    return x + gate * self.sb1(x)\n</code></pre> <p>This method applies a standard forward pass to the input tensor <code>x</code> through the Gated Residual Block.</p>"},{"location":"zeta/nn/modules/gatedresidualblock/#arguments_1","title":"Arguments","text":"Argument Type Description <code>x</code> <code>torch.Tensor</code> The input tensor."},{"location":"zeta/nn/modules/gatedresidualblock/#returns","title":"Returns","text":"<p>It returns a <code>torch.Tensor</code>, the output tensor of the gated residual block.</p>"},{"location":"zeta/nn/modules/gatedresidualblock/#note","title":"Note","text":"<p>This module requires the inputs <code>sb1</code> and <code>gate_module</code> to be of <code>nn.Module</code> type. Any model architecture that extends <code>nn.Module</code> can be used as the sub-blocks. The gating mechanism helps to improve the model performance especially on complex and large data sets. </p> <p>If you encounter any issues while using this module, please refer to the official PyTorch documentation or raise an issue on the relevant GitHub issue page.</p>"},{"location":"zeta/nn/modules/geluactivation/","title":"GELUActivation","text":""},{"location":"zeta/nn/modules/geluactivation/#overview","title":"Overview","text":"<p>The GELUActivation class belongs to the torch.nn Module and implements the Gaussian Error Linear Units (GELU) activation function, initially used in Google's BERT model. This function is known for enabling the model to converge much faster and provides more robust performance in terms of model stability and accuracy.</p> <p>The GELU activation function is defined as follows:  GELU(x) = 0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3))</p> <p>There are two versions of this function which are slightly different. The standard one implemented in PyTorch, and the original version used in the BERT model. This class provides the flexibility to choose between these two implementations.</p>"},{"location":"zeta/nn/modules/geluactivation/#class-definition","title":"Class Definition","text":"<p>class GELUActivation(nn.Module):</p> <p>This class inherits the torch.nn.Module, torch's base class for all neural network modules. </p>"},{"location":"zeta/nn/modules/geluactivation/#parameters","title":"Parameters","text":"<ul> <li>use_gelu_python (bool): If true, uses the original GELU activation function as introduced in the BERT model. Otherwise, it uses the PyTorch's implementation of GELU. Default is <code>False</code>.</li> </ul>"},{"location":"zeta/nn/modules/geluactivation/#methods","title":"Methods","text":""},{"location":"zeta/nn/modules/geluactivation/#__init__","title":"__init__()","text":"<p>The constructor method for the class. Initializes the GELUActivation with the given parameters.</p>"},{"location":"zeta/nn/modules/geluactivation/#_gelu_python","title":"_gelu_python()","text":"<p>This private method implements the original GELU activation function used in the BERT model as a simple python function.</p>"},{"location":"zeta/nn/modules/geluactivation/#forward","title":"forward()","text":"<p>This method is called when you call the object of the class. It takes an input tensor and applies the GELU activation function to it.</p>"},{"location":"zeta/nn/modules/geluactivation/#usage-example","title":"Usage Example","text":"<p>Here is an example usage of the GELUActivation class. The example demonstrates initializing the class and applying the GELU activation function to a random tensor.</p> <pre><code>import torch\nfrom torch import Tensor, nn\n\nfrom zeta.nn import GELUActivation\n\n# Initialize a GELU activation function\ngelu_activation = GELUActivation(use_gelu_python=True)\n\n# Generate a random tensor\ntensor = torch.randn(5)\n\n# Apply GELU activation function to the tensor\nactivated_tensor = gelu_activation(tensor)\n\nprint(activated_tensor)\n</code></pre> <p>In this example, we initialize a GELU activation function with <code>use_gelu_python</code> set to <code>True</code> which means we will be using the original GELU implementation used in the BERT model. We then apply this GELU activation function to a random tensor to get the activated tensor.</p>"},{"location":"zeta/nn/modules/geluactivation/#references","title":"References","text":"<ul> <li>Gaussian Error Linear Units (GELUs) Paper: https://arxiv.org/abs/1606.08415</li> </ul> <p>We suggest to read the referenced paper to gain a deeper understanding of GELUs and their use in neural networks.</p>"},{"location":"zeta/nn/modules/geluactivation/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>While the two versions of the GELU activation function are very similar, the original one (used in the BERT model) can sometimes provide slightly different results.</li> <li>If you're using a model pre-trained with the BERT model, it may be beneficial to use the original version of GELU, as it was the activation functions that the model was originally trained with.</li> <li>GELU activation function has proven effective in models dealing with Natural Language Processing tasks.</li> </ul>"},{"location":"zeta/nn/modules/hebbian/","title":"BasicHebbianGRUModel Documentation","text":""},{"location":"zeta/nn/modules/hebbian/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Class Definition</li> <li>Initialization</li> <li>Forward Pass</li> <li>Usage Examples</li> <li>Additional Information</li> </ol>"},{"location":"zeta/nn/modules/hebbian/#1-introduction","title":"1. Introduction","text":"<p>The <code>BasicHebbianGRUModel</code> is a PyTorch-based model designed for text-based tasks. It combines Hebbian learning with a GRU (Gated Recurrent Unit) layer to process sequential data. This model introduces non-linearity through the ReLU (Rectified Linear Unit) activation function.</p>"},{"location":"zeta/nn/modules/hebbian/#purpose","title":"Purpose","text":"<ul> <li>The model is designed to learn and represent patterns in sequential data, making it suitable for various natural language processing (NLP) tasks.</li> <li>It applies Hebbian learning to adaptively adjust weights based on input patterns, followed by GRU processing for sequential data handling.</li> <li>The ReLU activation function introduces non-linearity, enabling the model to capture complex relationships in the data.</li> </ul>"},{"location":"zeta/nn/modules/hebbian/#key-features","title":"Key Features","text":"<ul> <li>Hebbian learning for weight adaptation.</li> <li>GRU layer for sequential data processing.</li> <li>ReLU activation for non-linearity.</li> </ul>"},{"location":"zeta/nn/modules/hebbian/#2-class-definition","title":"2. Class Definition","text":"<pre><code>class BasicHebbianGRUModel(nn.Module):\n    \"\"\"\n    A basic Hebbian learning model combined with a GRU for text-based tasks.\n\n    Parameters:\n    - input_dim (int): Dimension of the input features.\n    - hidden_dim (int): Dimension of the hidden state in the GRU.\n    - output_dim (int): Dimension of the output features.\n    \"\"\"\n</code></pre> <p>The <code>BasicHebbianGRUModel</code> class has the following attributes and methods:</p> <ul> <li><code>input_dim</code> (int): Dimension of the input features.</li> <li><code>hidden_dim</code> (int): Dimension of the hidden state in the GRU.</li> <li><code>output_dim</code> (int): Dimension of the output features.</li> </ul>"},{"location":"zeta/nn/modules/hebbian/#3-initialization","title":"3. Initialization","text":"<p>To create an instance of the <code>BasicHebbianGRUModel</code>, you need to specify the dimensions of input, hidden state, and output features. Here's how you can initialize the model:</p> <pre><code>input_dim = 512  # Dimension of the input features\nhidden_dim = 256  # Dimension of the hidden state in the GRU\noutput_dim = 128  # Dimension of the output features\nmodel = BasicHebbianGRUModel(input_dim, hidden_dim, output_dim)\n</code></pre>"},{"location":"zeta/nn/modules/hebbian/#4-forward-pass","title":"4. Forward Pass","text":"<p>The forward pass of the model processes input data through several stages:</p> <ol> <li>It applies Hebbian update rules to the weights.</li> <li>The data is then passed through a GRU layer.</li> <li>A ReLU activation function is applied to introduce non-linearity.</li> <li>Finally, the output is passed through a fully connected layer.</li> </ol> <p>Here's how to perform a forward pass:</p> <pre><code># Assuming input_tensor is a 3D tensor of shape (B, Seqlen, input_dim)\noutput = model(input_tensor)\n</code></pre>"},{"location":"zeta/nn/modules/hebbian/#5-usage-examples","title":"5. Usage Examples","text":""},{"location":"zeta/nn/modules/hebbian/#example-1-model-initialization","title":"Example 1: Model Initialization","text":"<pre><code>input_dim = 512\nhidden_dim = 256\noutput_dim = 128\nmodel = BasicHebbianGRUModel(input_dim, hidden_dim, output_dim)\n</code></pre>"},{"location":"zeta/nn/modules/hebbian/#example-2-forward-pass","title":"Example 2: Forward Pass","text":"<pre><code># Assuming input_tensor is a 3D tensor of shape (B, Seqlen, input_dim)\noutput = model(input_tensor)\n</code></pre>"},{"location":"zeta/nn/modules/hebbian/#example-3-accessing-model-parameters","title":"Example 3: Accessing Model Parameters","text":"<pre><code># Accessing model parameters (weights, GRU parameters, FC layer parameters)\nmodel_weights = model.weights\ngru_parameters = model.gru.parameters()\nfc_parameters = model.fc.parameters()\n</code></pre>"},{"location":"zeta/nn/modules/hebbian/#6-additional-information","title":"6. Additional Information","text":""},{"location":"zeta/nn/modules/hebbian/#tips-for-effective-usage","title":"Tips for Effective Usage","text":"<ul> <li>For optimal results, ensure that input data is properly preprocessed and normalized.</li> <li>Experiment with different hyperparameters, such as the dimensions of hidden states and output features, to fine-tune the model for your specific task.</li> </ul>"},{"location":"zeta/nn/modules/hebbian/#references","title":"References","text":"<ul> <li>GRU Documentation</li> <li>ReLU Activation Function</li> </ul> <p>This documentation provides an overview of the <code>BasicHebbianGRUModel</code>, its purpose, usage, and key features. For more details on its implementation and advanced usage, refer to the source code and additional resources.</p>"},{"location":"zeta/nn/modules/highwaylayer/","title":"HighwayLayer","text":""},{"location":"zeta/nn/modules/highwaylayer/#module-introduction","title":"Module Introduction","text":"<p><code>HighwayLayer</code> is a class implemented in PyTorch that provides an easy way to include Highway layers in your model. The Highway layer is a type of artificial neural network (ANN) that aids in remembering or carrying information across several layers. It consists of a normal layer and a gate layer.</p> <p>It addressed the vanishing gradient problem typically found in the training of deep networks. With the application of a gating mechanism, the Highway layer dynamically routes signals through paths for different samples and different layers without harming the optimization process.</p> <p>This document provides details on how to use this class, its methods, properties, and examples for better understandings.</p>"},{"location":"zeta/nn/modules/highwaylayer/#class-definition","title":"Class Definition","text":"<pre><code>class HighwayLayer(nn.Module):\n</code></pre> <p>Inherits from the <code>nn.Module</code> class which is the base class for all neural network modules in PyTorch.</p>"},{"location":"zeta/nn/modules/highwaylayer/#parameters","title":"Parameters","text":"<ul> <li><code>dim</code> (int): The dimension of the input tensor to the layer and the output of the layer.</li> </ul>"},{"location":"zeta/nn/modules/highwaylayer/#methods","title":"Methods","text":""},{"location":"zeta/nn/modules/highwaylayer/#__init__self-dim","title":"<code>__init__(self, dim)</code>","text":"<p>Initializes a <code>HighwayLayer</code> instance with a specified <code>dim</code>.</p> <p>Parameters:</p> Parameter Type Description dim int The input and output dimension of the layer"},{"location":"zeta/nn/modules/highwaylayer/#forwardself-x","title":"<code>forward(self, x)</code>","text":"<p>Performs a forward pass through the <code>HighwayLayer</code>.</p> <p>Parameters:</p> Parameter Type Description x torch.Tensor The input tensor <p>Returns:</p> <p><code>torch.Tensor</code>: The output tensor.</p>"},{"location":"zeta/nn/modules/highwaylayer/#source-code","title":"Source Code","text":"<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom zeta.nn import HighwayLayer\n\n\nclass HighwayLayer(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.normal_layer = nn.Linear(dim, dim)\n        self.gate = nn.Linear(dim, dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        normal_result = F.relu(self.normal_layer(x))\n        gate = torch.sigmoid(self.gate(x))\n        return gate * normal_result + (1 - gate) * x\n</code></pre>"},{"location":"zeta/nn/modules/highwaylayer/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/modules/highwaylayer/#example-1-simple-model-with-single-highwaylayer","title":"Example 1: Simple model with single HighwayLayer","text":"<pre><code>import torch\n\nfrom zeta.nn import HighwayLayer\n\n# Initialize HighwayLayer with dimension 50\nlayer = HighwayLayer(50)\n\n# Random input tensor of shape (10, 50)\ninput_tensor = torch.randn(10, 50)\noutput_tensor = layer(input_tensor)\n\nprint(output_tensor.shape)  # Expected shape (10, 50)\n</code></pre>"},{"location":"zeta/nn/modules/highwaylayer/#example-2-model-with-multiple-highway-layers","title":"Example 2: Model with Multiple Highway Layers","text":"<pre><code>import torch\n\nfrom zeta.nn import HighwayLayer\n\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = HighwayLayer(50)\n        self.layer2 = HighwayLayer(50)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return x\n\n\n# Initialize model and input tensor\nmodel = MyModel()\ninput_tensor = torch.randn(10, 50)\n\n# Forward pass\noutput_tensor = model(input_tensor)\n\nprint(output_tensor.shape)  # Expected output: torch.Size([10, 50])\n</code></pre>"},{"location":"zeta/nn/modules/highwaylayer/#example-3-model-with-highwaylayer-and-other-types-of-layers","title":"Example 3: Model with HighwayLayer and Other Types of Layers","text":"<pre><code>class MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = HighwayLayer(50)\n        self.layer2 = nn.Linear(50, 20)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return x\n\n\n# Initialize model and input tensor\nmodel = MyModel()\ninput_tensor = torch.randn(10, 50)\n\n# Forward pass\noutput_tensor = model(input_tensor)\n\nprint(output_tensor.shape)  # Expected output: torch.Size([10, 20])\n</code></pre> <p>Application of HighwayLayer can greatly enhance the learning of deep neural networks by allowing the direct forward flow of information unimpeded thereby solving the vanishing gradient problem.</p>"},{"location":"zeta/nn/modules/laplaceactivation/","title":"LaplaceActivation","text":""},{"location":"zeta/nn/modules/laplaceactivation/#1-overview","title":"1. Overview","text":"<p>The <code>LaplaceActivation</code> is an artificial neuron that applies an elementwise activation based on the Laplace function. This was introduced in MEGA as an attention activation, which can be found in this paper.</p> <p>The <code>LaplaceActivation</code> is inspired by the squaring operation of the ReLU (Rectified Linear Units) function, but comes with a bounded range and gradient for improved stability. </p>"},{"location":"zeta/nn/modules/laplaceactivation/#2-class-description","title":"2. Class Description","text":"<p>The <code>LaplaceActivation</code> is part of the <code>PyTorch</code> neural network (<code>nn</code>) module, specifically intended to provide activation functionality based on the Laplace function to a neural network model. </p>"},{"location":"zeta/nn/modules/laplaceactivation/#class-definition","title":"Class Definition","text":"<pre><code>class LaplaceActivation(nn.Module):\n    pass\n</code></pre>"},{"location":"zeta/nn/modules/laplaceactivation/#method-forward","title":"Method: <code>forward</code>","text":"<p>This function applies the Laplace function across all elements in the input tensor. It takes as parameters the input tensor and optional parameters <code>\\mu</code> and <code>\\sigma</code>. The function computes the Laplace function as follows:</p> <pre><code>input = (input - \\mu) / (\\sigma * sqrt(2))\noutput = 0.5 * (1 + erf(input))\nreturn output\n</code></pre>"},{"location":"zeta/nn/modules/laplaceactivation/#arguments","title":"Arguments:","text":"Argument Type Description Default value <code>input</code> Tensor Tensor input to the function. <code>\\mu</code> float Location parameter, <code>\\mu</code> determines the shift or the mean of the function. 0.707107 <code>\\sigma</code> float Scale parameter or standard deviation, <code>\\sigma</code> determines the spread or the width of the function. 0.282095"},{"location":"zeta/nn/modules/laplaceactivation/#returns","title":"Returns","text":"<p>A tensor with Laplace function applied elementwise.</p>"},{"location":"zeta/nn/modules/laplaceactivation/#3-example-usage","title":"3. Example Usage","text":""},{"location":"zeta/nn/modules/laplaceactivation/#importing-required-libraries","title":"Importing required libraries","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom zeta.nn import LaplaceActivation\n</code></pre>"},{"location":"zeta/nn/modules/laplaceactivation/#defining-an-instance","title":"Defining an instance","text":"<p><pre><code>lap_act = LaplaceActivation()\n</code></pre> Applying Laplace Activation to a tensor</p> <p><pre><code>input_tensor = torch.randn(10)\nactivated_tensor = lap_act(input_tensor)\n</code></pre> Printing output</p> <pre><code>print(activated_tensor)\n</code></pre> <p>You should see the tensor output with Laplace activation applied elementwise.</p>"},{"location":"zeta/nn/modules/laplaceactivation/#4-additional-information","title":"4. Additional Information","text":"<p>The Laplace Activation function is a new approach to help stabilize the learning process in deep neural networks. It introduces bounded range and gradient which can be very useful when training deep learning models.</p>"},{"location":"zeta/nn/modules/laplaceactivation/#5-references","title":"5. References","text":"<p>For more in-depth understanding, kindly refer to this paper.</p>"},{"location":"zeta/nn/modules/laplaceactivation/#6-contact-information","title":"6. Contact Information","text":"<p>For any issues or inquiries, feel free to contact the support team at kye@apac.ai We're happy to help!</p>"},{"location":"zeta/nn/modules/laser/","title":"Module/Function Name: LayerSelectiveRankReduction","text":"<p>The <code>LayerSelectiveRankReduction</code> (LASER) module replaces specific weight matrices in a Transformer model by their low-rank approximations for both 2D and 3D tensors.</p> <p><code>LASER</code> is a pyTorch based module that aids in approximating weight matrices using a low rank matrix decomposition. Examples where the memory consumption footprint needs to be controlled and approximated to manage memory constraints. This module is particularly effective for text datasets which can require high computational resources.</p> <p>The main attribute for <code>LASER</code> is <code>rank_fraction</code> which denotes the fraction of the maximum rank to reserve in the approximation, with the value ranging from 0 to 1.</p> <p>Example Usage:</p> <pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import LASER\n\n# Dimension of the weight matrix\nweight_dim = 512\n\n# Example weight matrix (2D tensor)\nW_2d = torch.randn(weight_dim, weight_dim)\n\n# Example weight batch (3D tensor)\nW_3d = torch.randn(10, weight_dim, weight_dim)\n\n# Fraction of the rank to preserve\nrank_fraction = 0.9\n\n# Create the LASER module\nlaser = LASER(rank_fraction)\n\n# Apply LASER to 2D and 3D tensors to obtain low-rank approximations\nW_2d_low_rank = laser(W_2d)\nW_3d_low_rank = laser(W_3d)\n\n# Output the shape of the approximated matrices\nprint(\n    W_2d_low_rank.shape\n)  # The shape of the approximated 2D matrix will be the same as the original matrix\nprint(\n    W_3d_low_rank.shape\n)  # The shape of the approximated matrices will be the same as the original 3D tensor\n</code></pre> <p>Additional Tips:</p> <p>For better performance, it's recommended that developers monitor memory and resource usage while applying LASER for large matrices. Additionally, it is advised to adequately test the optimized model performance after using the <code>LASER</code> module to maintain required accuracy whilst significantly reducing memory usage.</p> <p>References and Resources:</p> <ul> <li>LASER PyTorch Documentation</li> </ul> <p>Further exploration of memory reduction techniques for large-scale optimized machine learning models can be referenced for a more in-depth understanding.</p> <p>This is an example of a module that replaces specific weight matrices with their low-rank approximations. Developers can refer to this documentation as a reference and template to create a similar documentation for other modules or frameworks.</p>"},{"location":"zeta/nn/modules/layernorm/","title":"<code>LayerNorm</code> Documentation","text":""},{"location":"zeta/nn/modules/layernorm/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Class: <code>LayerNorm</code></li> <li>Parameters</li> <li>Function: <code>l2norm</code></li> <li>Usage Examples</li> <li>Using the <code>LayerNorm</code> Class</li> <li>Using the <code>l2norm</code> Function</li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/nn/modules/layernorm/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation! In this documentation, we will explore the <code>LayerNorm</code> class and the <code>l2norm</code> function, both of which are part of the Zeta library. These components are designed for normalization operations in neural networks. This documentation provides a comprehensive understanding of their purpose, functionality, and usage.</p>"},{"location":"zeta/nn/modules/layernorm/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>LayerNorm</code> class and the <code>l2norm</code> function are essential tools in deep learning, specifically for normalizing tensors within neural networks. They offer the following functionalities:</p>"},{"location":"zeta/nn/modules/layernorm/#layernorm-class","title":"<code>LayerNorm</code> Class","text":"<ul> <li> <p>Layer Normalization: The <code>LayerNorm</code> class implements layer normalization, a technique commonly used in neural networks to stabilize training and improve generalization.</p> </li> <li> <p>Configurability: It allows you to specify the dimension for normalization and fine-tune numerical stability using parameters like <code>eps</code> and <code>fp16_eps</code>.</p> </li> <li> <p>Learnable Scaling: The class introduces a learnable scaling parameter <code>g</code> to control the magnitude of the normalized output.</p> </li> </ul>"},{"location":"zeta/nn/modules/layernorm/#l2norm-function","title":"<code>l2norm</code> Function","text":"<ul> <li> <p>L2 Normalization: The <code>l2norm</code> function performs L2 normalization, scaling each input vector to have a unit L2 norm.</p> </li> <li> <p>Tensor Normalization: It's particularly useful when you want to normalize the magnitude of vectors or tensors in a neural network.</p> </li> </ul>"},{"location":"zeta/nn/modules/layernorm/#3-class-layernorm","title":"3. Class: <code>LayerNorm</code>","text":"<p>The <code>LayerNorm</code> class implements layer normalization with the following signature:</p> <pre><code>class LayerNorm(nn.Module):\n    def __init__(\n        self,\n        dim,\n        eps=1e-5,\n        fp16_eps=1e-3,\n        stable=False\n    )\n\n    def forward(self, x)\n</code></pre>"},{"location":"zeta/nn/modules/layernorm/#parameters","title":"Parameters","text":"<ul> <li> <p><code>dim</code> (int): The dimension of the input tensor that should be normalized.</p> </li> <li> <p><code>eps</code> (float, optional): A small value added to the denominator for numerical stability when using float32 data type. Default is <code>1e-5</code>.</p> </li> <li> <p><code>fp16_eps</code> (float, optional): A small value added to the denominator for numerical stability when using float16 (fp16) data type. Default is <code>1e-3</code>.</p> </li> <li> <p><code>stable</code> (bool, optional): Whether to use a stable implementation of layer normalization. Default is <code>False</code>.</p> </li> </ul>"},{"location":"zeta/nn/modules/layernorm/#4-function-l2norm","title":"4. Function: <code>l2norm</code>","text":"<p>The <code>l2norm</code> function performs L2 normalization on input tensors with the following signature:</p> <pre><code>def l2norm(t)\n</code></pre>"},{"location":"zeta/nn/modules/layernorm/#parameters_1","title":"Parameters","text":"<ul> <li><code>t</code> (torch.Tensor): The input tensor to be L2 normalized.</li> </ul>"},{"location":"zeta/nn/modules/layernorm/#5-usage-examples","title":"5. Usage Examples","text":"<p>Let's explore how to use the <code>LayerNorm</code> class and the <code>l2norm</code> function effectively in various scenarios.</p>"},{"location":"zeta/nn/modules/layernorm/#using-the-layernorm-class","title":"Using the <code>LayerNorm</code> Class","text":"<p>Here's how to use the <code>LayerNorm</code> class to normalize a tensor:</p> <pre><code>import torch\n\nfrom zeta.nn import LayerNorm\n\n# Create an instance of LayerNorm for a tensor with 10 dimensions\nlayer_norm = LayerNorm(dim=10)\n\n# Create a random input tensor\nx = torch.randn(32, 10)  # Example input with 32 samples and 10 dimensions\n\n# Apply layer normalization\nnormalized_x = layer_norm(x)\n\n# Print the normalized tensor\nprint(normalized_x)\n</code></pre>"},{"location":"zeta/nn/modules/layernorm/#using-the-l2norm-function","title":"Using the <code>l2norm</code> Function","text":"<p>Here's how to use the <code>l2norm</code> function to perform L2 normalization on a tensor:</p> <pre><code>import torch\n\nfrom zeta.nn import l2norm\n\n# Create a random input tensor\nx = torch.randn(32, 10)  # Example input with 32 samples and 10 dimensions\n\n# Apply L2 normalization\nnormalized_x = l2norm(x)\n\n# Print the normalized tensor\nprint(normalized_x)\n</code></pre>"},{"location":"zeta/nn/modules/layernorm/#6-additional-information","title":"6. Additional Information","text":"<p>Here are some additional notes and tips related to <code>LayerNorm</code> and <code>l2norm</code>:</p> <ul> <li> <p>Numerical Stability: The <code>eps</code> and <code>fp16_eps</code> parameters ensure numerical stability during normalization, especially when dealing with very small or very large values.</p> </li> <li> <p>Learnable Scaling: The learnable scaling parameter <code>g</code> in <code>LayerNorm</code> allows the model to adaptively scale the normalized output.</p> </li> <li> <p>Layer Normalization: Layer normalization is widely used in deep learning to stabilize training and improve convergence.</p> </li> <li> <p>L2 Normalization: L2 normalization is useful for scaling the magnitude of vectors or tensors to a unit L2 norm.</p> </li> </ul>"},{"location":"zeta/nn/modules/layernorm/#7-references","title":"7. References","text":"<p>For further information on layer normalization, L2 normalization, and related concepts, you can refer to the following resources:</p> <ul> <li> <p>Layer Normalization - The original research paper introducing layer normalization.</p> </li> <li> <p>PyTorch Documentation - Official PyTorch documentation for related functions and modules.</p> </li> </ul> <p>This documentation provides a comprehensive overview of the Zeta library's <code>LayerNorm</code> class and <code>l2norm</code> function. It aims to help you understand the purpose, functionality, and usage of these components for normalization operations within neural networks.</p>"},{"location":"zeta/nn/modules/linearactivation/","title":"LinearActivation","text":"<p>The LinearActivation class belongs to the <code>nn.Module</code> in PyTorch which is a standard base class for all neural network modules. The class LinearActivation is a child class that inherits the functionalities of its parent class <code>nn.Module</code>. This class represents the linear activation function in the neural networks; sometimes also referred to as the identity function. The idea here is to return the input without applying any transformation, which means that the output of this function is the same as the input.</p> <p>The source code is as follows:</p> <pre><code>import torch.nn as nn\nfrom torch import Tensor\n\nfrom zeta.nn import LinearActivation\n\n\nclass LinearActivation(nn.Module):\n    \"\"\"\n    Applies the linear activation function, i.e., forwarding input directly to output.\n    \"\"\"\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        return input\n</code></pre>"},{"location":"zeta/nn/modules/linearactivation/#method-details","title":"Method details","text":"<p>Method Name: <code>forward</code></p> <p>This method executes the forward pass, in other words, it makes a forward pass from input to the output. The <code>forward</code> is an abstract method in superclass <code>nn.Module</code> and must be defined by each layer. </p> <p>Arguments:</p> Argument Name Type Description input Tensor Input tensor to which the linear activation is applied <p>Returns:</p> <p><code>Tensor</code>: The output tensor identical to the input tensor. </p>"},{"location":"zeta/nn/modules/linearactivation/#usage-example-1","title":"Usage Example 1","text":"<p><pre><code>import torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom zeta.nn import LinearActivation\n\nlinear_activation = LinearActivation()\n\n# random tensor of size 4\ninput_tensor = torch.randn(4)\nprint(\"Input tensor: \", input_tensor)\n\noutput_tensor = linear_activation(input_tensor)\nprint(\"Output tensor: \", output_tensor)\n</code></pre> In this example, the <code>LinearActivation</code> class is instantiated first followed by generating a random tensor of size 4. This random tensor is passed to the instantiated <code>LinearActivation</code> class, and the result will be an identical tensor to the input, as expected.</p>"},{"location":"zeta/nn/modules/linearactivation/#usage-example-2","title":"Usage Example 2","text":"<p><pre><code>import torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom zeta.nn import LinearActivation\n\n# create an instance of the class LinearActivation\nlinear_activation = LinearActivation()\n\n# define a tensor of ones\ninput_tensor = torch.ones(10)\nprint(\"Input tensor: \", input_tensor)\n\n# pass the tensor of ones through the LinearActivation\noutput_tensor = linear_activation(input_tensor)\nprint(\"Output tensor: \", output_tensor)\n</code></pre> In the second example, we create an input tensor of ones of size 10. When this tensor is passed through the <code>LinearActivation</code>, we expect an identical tensor of ones for the output. We print the output tensor to verify this.</p>"},{"location":"zeta/nn/modules/linearactivation/#usage-example-3","title":"Usage Example 3","text":"<p><pre><code>import torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom zeta.nn import LinearActivation\n\nlinear_activation = LinearActivation()\n\n# create a tensor with numbers from 1 to 10\ninput_tensor = torch.arange(1, 11).float()\nprint(\"Input tensor: \", input_tensor)\n\noutput_tensor = linear_activation(input_tensor)\nprint(\"Output tensor: \", output_tensor)\n</code></pre> In the third example, we create an input tensor with numbers from 1 to 10. We then pass this tensor through the <code>LinearActivation</code>. Because the <code>LinearActivation</code> doesn't actually perform any mathematical transformations, the expected output tensor will be identical to the input tensor.</p>"},{"location":"zeta/nn/modules/lora/","title":"Lora","text":"<p>The <code>Lora</code> class is a module of the Zeta library that provides a simple linear transformation of the input data. It is a part of the <code>torch.nn</code> module and extends the <code>nn.Module</code> class from PyTorch.</p>"},{"location":"zeta/nn/modules/lora/#overview-and-introduction","title":"Overview and Introduction","text":"<p>The <code>Lora</code> class is designed to provide a scalable and efficient linear transformation operation. It is particularly useful in scenarios where the dimensionality of the input data is very high and computational efficiency is of paramount importance. The <code>Lora</code> class achieves this by breaking down the weight matrix into two lower rank matrices <code>A</code> and <code>B</code>, and a scale factor <code>alpha</code>, which are learned during the training process. This results in a significant reduction in the number of parameters to be learned, and consequently, a more computationally efficient model.</p>"},{"location":"zeta/nn/modules/lora/#key-concepts-and-terminology","title":"Key Concepts and Terminology","text":"<ul> <li> <p>Linear Transformation: A linear transformation is a mathematical operation that transforms input data by multiplying it with a weight matrix. It is a fundamental operation in many machine learning models.</p> </li> <li> <p>Low Rank Approximation: Low rank approximation is a technique used to approximate a matrix by another matrix of lower rank. This is often used to reduce the dimensionality of data and to make computations more efficient.</p> </li> <li> <p>Scale Factor: A scale factor is a number by which a quantity is multiplied, changing the magnitude of the quantity.</p> </li> </ul>"},{"location":"zeta/nn/modules/lora/#class-definition","title":"Class Definition","text":"<p>The <code>Lora</code> class is defined as follows:</p> <pre><code>class Lora(nn.Module):\n    def __init__(self, dim, dim_out, r=8, alpha=None):\n        super().__init__()\n        self.scale = alpha / r\n\n        self.A = nn.Parameter(torch.randn(dim, r))\n        self.B = nn.Parameter(torch.randn(r, dim_out))\n\n    @property\n    def weight(self):\n        return (self.A @ self.B) * self.scale\n\n    def forward(self, x):\n        return x @ self.weight\n</code></pre>"},{"location":"zeta/nn/modules/lora/#parameters","title":"Parameters","text":"<ul> <li><code>dim</code> (<code>int</code>): The dimensionality of the input data. It is the number of features in the input data.</li> <li><code>dim_out</code> (<code>int</code>): The desired dimensionality of the output data. It is the number of features in the output data.</li> <li><code>r</code> (<code>int</code>, optional): The rank of the matrices <code>A</code> and <code>B</code>. It determines the size of the matrices <code>A</code> and <code>B</code>. Default is 8.</li> <li><code>alpha</code> (<code>float</code>, optional): The scale factor. If not provided, it is set to 1 by default.</li> </ul>"},{"location":"zeta/nn/modules/lora/#methods","title":"Methods","text":""},{"location":"zeta/nn/modules/lora/#forward","title":"<code>forward</code>","text":"<p>The <code>forward</code> method is used to compute the forward pass of the <code>Lora</code> module.</p>"},{"location":"zeta/nn/modules/lora/#parameters_1","title":"Parameters","text":"<ul> <li><code>x</code> (<code>Tensor</code>): The input data. It is a tensor of shape <code>(batch_size, dim)</code>.</li> </ul>"},{"location":"zeta/nn/modules/lora/#returns","title":"Returns","text":"<ul> <li><code>Tensor</code>: The transformed data. It is a tensor of shape <code>(batch_size, dim_out)</code>.</li> </ul>"},{"location":"zeta/nn/modules/lora/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>Lora</code> class is used to perform a linear transformation of the input data. The transformation is defined by the weight matrix <code>W</code>, which is approximated by the product of two lower rank matrices <code>A</code> and <code>B</code>, and a scale factor <code>alpha</code>. The <code>Lora</code> class learns the matrices <code>A</code> and <code>B</code>, and the scale factor <code>alpha</code> during the training process. </p> <p>The forward pass of the <code>Lora</code> module computes the product of the input data <code>x</code> and the weight matrix <code>W</code>, which is approximated by <code>(A @ B) * scale</code>.</p>"},{"location":"zeta/nn/modules/lora/#mathematical-formula","title":"Mathematical Formula","text":"<p>The mathematical formula for the forward pass of the <code>Lora</code> module is:</p> <p>[ y = xW ]</p> <p>Where: - ( y ) is the transformed data. - ( x ) is the input data. - ( W ) is the weight matrix, which is approximated by ( (A @ B) * \\text{scale} ).</p>"},{"location":"zeta/nn/modules/lora/#usage-examples","title":"Usage Examples","text":"<p>Below are three examples of how to use the <code>Lora</code> class.</p>"},{"location":"zeta/nn/modules/lora/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta import Lora\n\n# Define the input data\nx = torch.randn(32, 128)  # batch size of 32, and 128 features\n\n# Define the Lora module\nlora = Lora(dim=128, dim_out=64)\n\n# Compute the forward pass\ny = lora(x)\n</code></pre>"},{"location":"zeta/nn/modules/lora/#example-2-specifying-the-rank-and-scale-factor","title":"Example 2: Specifying the Rank and Scale Factor","text":"<pre><code>import torch\n\nfrom zeta import Lora\n\n# Define the input data\nx = torch.randn(32, 128)  # batch size of 32, and 128 features\n\n# Define the Lora module with specified rank and scale factor\nlora = Lora(dim=128, dim_out=64, r=16, alpha=0.1)\n\n# Compute the forward pass\ny = lora(x)\n</code></pre>"},{"location":"zeta/nn/modules/lora/#example-3-using-the-lora-module-in-a-neural-network","title":"Example 3: Using the Lora Module in a Neural Network","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta import Lora\n\n\n# Define a simple neural network with a Lora layer\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lora = Lora(dim=128, dim_out=64)\n        self.fc = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.lora(x)\n        x = self.fc(x)\n        return x\n\n\n# Define the input data\nx = torch.randn(32, 128)  # batch size of 32, and 128 features\n\n# Define the model\nmodel = Net()\n\n# Compute the forward pass\noutput = model(x)\n</code></pre>"},{"location":"zeta/nn/modules/lora/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li> <p>The <code>Lora</code> class is particularly useful in scenarios where the dimensionality of the input data is very high and computational efficiency is of paramount importance. However, it may not be suitable for all applications, as the approximation of the weight matrix may result in a loss of accuracy.</p> </li> <li> <p>The rank <code>r</code> and the scale factor <code>alpha</code> are hyperparameters that need to be tuned for the specific application. A higher value of <code>r</code> will</p> </li> </ul> <p>result in a more accurate approximation of the weight matrix, but will also increase the computational cost. Similarly, the scale factor <code>alpha</code> needs to be tuned to achieve the desired trade-off between accuracy and computational efficiency.</p>"},{"location":"zeta/nn/modules/lora/#references-and-resources","title":"References and Resources","text":"<ul> <li>PyTorch nn.Module documentation</li> <li>Low Rank Matrix Factorization for Deep Neural Network Training with High-dimensional Output Targets</li> </ul> <p>For further exploration and implementation details, you can refer to the above resources and the official PyTorch documentation.</p>"},{"location":"zeta/nn/modules/mamba/","title":"mamba","text":""},{"location":"zeta/nn/modules/mamba/#pytorch-code-documentation-mamba","title":"PyTorch Code Documentation - Mamba","text":""},{"location":"zeta/nn/modules/mamba/#overview","title":"Overview","text":"<p>The Mamba model is designed for performing joint image and text processing. This documentation explains the purpose, functionality, usage, and core features of the Mamba class. </p>"},{"location":"zeta/nn/modules/mamba/#purpose-and-functionality","title":"Purpose and Functionality","text":"<p>The Mamba model is designed to handle sequential processing tasks by combining information from text and images. The model employs a series of Mamba blocks to process the input data. The core functionality involves a forward propagation that processes the input and returns logits for text prediction. Key features of the Mamba model include the use of attention, layer normalization, and linear projection operations.</p>"},{"location":"zeta/nn/modules/mamba/#class-definition","title":"Class Definition","text":"<p>The Mamba class is defined with the following class signature and arguments: <pre><code>| Argument    | Type                      | Definition                                     | Default |\n|-------------|---------------------------|------------------------------------------------|---------|\n| vocab_size  | int                       | Size of the vocabulary                         | None    |\n| dim         | int                       | Input dimension (for embedding)               | None    |\n| depth       | int                       | Depth of the Mamba block                       | 5       |\n| d_state     | int                       | State dimension                                 | 16      |\n| expand      | int                       | Expansion factor                                | 2       |\n| dt_rank     | Union[int, str]           | Rank of the temporal difference tensor         | \"auto\"  |\n| d_conv      | int                       | Dimension of the convex kernel                 | 4       |\n</code></pre></p>"},{"location":"zeta/nn/modules/mamba/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The core functionality of the Mamba class is the forward pass, which processes the input and produces logits. The forward pass includes processing the input text and images, applying the Mamba blocks, and a final linear projection. The model is flexible to handle both image and text inputs. The Mamba model can be initialized with default parameters or with custom values during instantiation. </p>"},{"location":"zeta/nn/modules/mamba/#examples","title":"Examples","text":"<p>Example 1:</p> <pre><code>import torch\n\nfrom zeta.nn import Mamba\n\nx = torch.randint(0, 16, (1, 64))\nmodel = Mamba(16, 64, 5, 16)\noutput = model(x)\nprint(output)\n</code></pre> <p>Example 2:</p> <pre><code>import torch\n\nfrom zeta.nn import Mamba\n\nx = torch.randint(0, 16, (1, 32))\nimg_features = torch.rand(1, 64)\nmodel = Mamba(16, 32, 3, 16)\noutput = model(x, img_features)\nprint(output)\n</code></pre> <p>Example 3:</p> <pre><code>import torch\n\nfrom zeta.nn import Mamba\n\nx = torch.randint(0, 32, (1, 32))\nmodel = Mamba(32, 32, 3, 16, 3, d_conv=8)\noutput = model(x)\nprint(output)\n</code></pre>"},{"location":"zeta/nn/modules/mamba/#additional-information","title":"Additional Information","text":"<p>The Mamba model implementation adopts a mixed-type learning approach. It can handle both text and image inputs for generating context-aware predictions. Developers and data scientists may benefit from exploring the official GitHub repository for extended understanding and usage of this model.</p>"},{"location":"zeta/nn/modules/mamba/#references-and-resources","title":"References and Resources","text":"<ul> <li>GitHub - MambaLMHeadModel - Official implementation of MambaLMHeadModel.</li> </ul> <p>This documentation provides detailed insights into the purpose, functionality, and usage of the Mamba class in PyTorch. By understanding core features, class definition, and usage scenarios, developers can effectively utilize the Mamba model for their specific applications.</p>"},{"location":"zeta/nn/modules/mambablock/","title":"Module/Function Name: MambaBlock","text":""},{"location":"zeta/nn/modules/mambablock/#overview-and-introduction","title":"Overview and Introduction","text":"<p>The MambaBlock class provides a simple yet effective block for deep learning designed to enrich the memory state in neural networks. It's part of the zeta.nn.modules library and is specially designed to increase the temporal dependencies in neural networks. The MambaBlock allows to examine the neural network's output not only from the perspective of spatial dependence but from a temporal one as well. This means it takes into account the history or sequence of data leading up to the present time.</p>"},{"location":"zeta/nn/modules/mambablock/#class-definition","title":"Class Definition:","text":"<pre><code>**MambaBlock Class**\n```markdown\nCreates a single Mamba block with specific parameters.\n| Parameter          | Description                    | Data Type | Default |\n|--------------------|--------------------------------|-----------|---------|\n| dim                | The input dimension             | int       | -       |\n| dim_inner          | The inner dimension             | int       | dim * expand|\n| depth              | The depth of the Mamba block    | int       | 5        |\n| d_state            | The state dimension             | int       | 16       |\n| expand             | The expansion factor            | int       | 2        |\n| dt_rank            | The rank of the temporal difference (\u0394) tensor | int/str | \"auto\" |\n| d_conv             | The dimension of the convolutional kernel            | int | 4       |\n| conv_bias          | Whether to include bias in the convolutional layer | bool | True |\n| bias               | Whether to include bias in the linear layers        | bool  | False |\n\n```markdown\n\n### Functionality and Usage\nThe MambaBlock is designed as a fundamental block in deep learning networks, especially neural networks. The module enriches the capability of deep learning networks to remember and understand temporal dependencies. This is crucial while dealing with data sequences, such as time series and natural language processing tasks.\n\nThe MambaBlock accepts a predefined set of parameters such as depth, state, expand, convolutional parameters, etc., allowing flexibility and adaptability regarding different neural network architectures and use cases. Moreover, the forward function seamlessly processes input and provides tensor outputs.\n\n### Example\n\n```python\nimport torch\n\nfrom zeta.nn import MambaBlock\n\n# Initialize Mamba\nblock = MambaBlock(dim=64, depth=1)\n\n# Random input\nx = torch.randn(1, 10, 64)\n\n# Apply the model to the block\ny = block(x)\n\nprint(y.shape)\n# torch.Size([1, 10, 64])\n</code></pre>"},{"location":"zeta/nn/modules/mambablock/#additional-information-and-tips","title":"Additional Information and Tips","text":"<p>Additional details and tips regarding the MambaBlock class can be found in the examples provided in the documentation. It's essential to understand the context in which the MambaBlock is being used in your specific use case for the best accuracy and results.</p>"},{"location":"zeta/nn/modules/mambablock/#references-and-resources","title":"References and Resources","text":"<p>External references to research papers, blog posts, and official documentation can be found at the source repository.</p> <p>This documentation template illustrates the comprehensive format needed including an overview and introduction, class definition with function, the functionality and usage details, and additional information and tips.</p> <p>The documentation provided for the MambaBlock class has been structured and explained comprehensively to help the developers understand its significance, purpose, and usage.</p> <p>It is thorough and explicitly detailed so that developers and data scientists are able to utilize the MambaBlock class most effectively in ensure the development of their models in deep learning tasks.</p> <p>The official usage examples reflect the comprehensive usability of the MambaBlock.</p>"},{"location":"zeta/nn/modules/mbconv/","title":"<code>MBConv</code> Documentation","text":""},{"location":"zeta/nn/modules/mbconv/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Function: <code>MBConv</code></li> <li>Parameters</li> <li>Usage Examples</li> <li>Using the <code>MBConv</code> Function</li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/nn/modules/mbconv/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation! In this documentation, we will explore the <code>MBConv</code> function, a part of the Zeta library. The <code>MBConv</code> function is designed to create MobileNetV2-like inverted residual blocks. This documentation provides a comprehensive understanding of the purpose, functionality, and usage of the <code>MBConv</code> function.</p>"},{"location":"zeta/nn/modules/mbconv/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>MBConv</code> function is a building block commonly used in deep learning architectures, particularly in mobile and efficient neural networks like MobileNetV2. It creates an inverted residual block, which is characterized by a bottleneck structure that reduces the number of input channels while increasing the number of output channels. This helps reduce computational complexity while maintaining expressive power.</p> <p>The key features of the <code>MBConv</code> function include:</p> <ul> <li> <p>Configurable architecture: You can specify various parameters such as the input and output dimensions, expansion rate, shrinkage rate, and dropout rate.</p> </li> <li> <p>Efficient design: <code>MBConv</code> follows the design principles of MobileNetV2, making it suitable for efficient and lightweight neural network architectures.</p> </li> <li> <p>Flexibility: Inverted residual blocks created using <code>MBConv</code> can be used as fundamental building blocks in a variety of neural network architectures.</p> </li> </ul>"},{"location":"zeta/nn/modules/mbconv/#3-function-mbconv","title":"3. Function: <code>MBConv</code>","text":"<p>The <code>MBConv</code> function creates an inverted residual block with the following signature:</p> <pre><code>def MBConv(\n    dim_in,\n    dim_out,\n    *,\n    downsample,\n    expansion_rate=4,\n    shrinkage_rate=0.25,\n    dropout=0.\n)\n</code></pre>"},{"location":"zeta/nn/modules/mbconv/#parameters","title":"Parameters","text":"<ul> <li> <p><code>dim_in</code> (int): The number of input channels.</p> </li> <li> <p><code>dim_out</code> (int): The number of output channels.</p> </li> <li> <p><code>downsample</code> (bool): A boolean flag indicating whether downsampling should be applied. If <code>True</code>, it performs spatial downsampling. If <code>False</code>, it does not perform downsampling.</p> </li> <li> <p><code>expansion_rate</code> (float, optional): The expansion rate controls the width of the bottleneck layer. It determines the number of intermediate channels in the bottleneck. Default is <code>4</code>.</p> </li> <li> <p><code>shrinkage_rate</code> (float, optional): The shrinkage rate controls the reduction in the number of output channels compared to the intermediate channels. It is used in the squeeze-and-excitation (SE) block. Default is <code>0.25</code>.</p> </li> <li> <p><code>dropout</code> (float, optional): The dropout rate applied to the output of the bottleneck layer. Default is <code>0.0</code>.</p> </li> </ul>"},{"location":"zeta/nn/modules/mbconv/#4-usage-examples","title":"4. Usage Examples","text":"<p>Let's explore how to use the <code>MBConv</code> function effectively in various scenarios.</p>"},{"location":"zeta/nn/modules/mbconv/#using-the-mbconv-function","title":"Using the <code>MBConv</code> Function","text":"<p>Here's how to use the <code>MBConv</code> function to create an inverted residual block:</p> <pre><code>import torch\n\nfrom zeta.nn import MBConv\n\n# Create an inverted residual block with 64 input channels, 128 output channels, and downsampling\nmbconv_block = MBConv(64, 128, downsample=True)\n\n# Create an input tensor\nx = torch.randn(32, 64, 32, 32)  # Example input with 32 samples and 64 channels\n\n# Apply the inverted residual block\noutput = mbconv_block(x)\n\n# Output tensor\nprint(output)\n</code></pre>"},{"location":"zeta/nn/modules/mbconv/#5-additional-information","title":"5. Additional Information","text":"<p>Inverted residual blocks, as implemented by the <code>MBConv</code> function, are widely used in efficient neural network architectures. Here are some additional notes:</p> <ul> <li> <p>MobileNetV2 Inspiration: The <code>MBConv</code> function is inspired by the design of MobileNetV2, a popular mobile neural network architecture known for its efficiency.</p> </li> <li> <p>Bottleneck Structure: The use of a bottleneck structure in <code>MBConv</code> reduces computational cost while allowing the network to capture complex patterns.</p> </li> <li> <p>Squeeze-and-Excitation (SE) Block: <code>MBConv</code> includes a squeeze-and-excitation (SE) block that adaptively scales channel-wise features, enhancing the representation power of the block.</p> </li> </ul>"},{"location":"zeta/nn/modules/mbconv/#6-references","title":"6. References","text":"<p>For further information on MobileNetV2, inverted residual blocks, and related concepts, you can refer to the following resources:</p> <ul> <li> <p>MobileNetV2: Inverted Residuals and Linear Bottlenecks - The original research paper introducing MobileNetV2.</p> </li> <li> <p>PyTorch Documentation - Official PyTorch documentation for related functions and modules.</p> </li> </ul> <p>This documentation provides a comprehensive overview of the Zeta library's <code>MBConv</code> function. It aims to help you understand the purpose, functionality, and usage of the <code>MBConv</code> function for creating efficient and lightweight neural network architectures.</p>"},{"location":"zeta/nn/modules/mishactivation/","title":"MishActivation","text":"<p>This is the official documentation for the Mish Activation class implementation in PyTorch.  This document will cover the details of implementing Mish Activation function and the ways to use it.</p>"},{"location":"zeta/nn/modules/mishactivation/#mish-activation-function-introduction","title":"Mish Activation Function: Introduction","text":"<p>Mish Activation is a novel approach to optimizing and enhancing the performance of neural network models by using a new self-regularized, non-monotonic activation function known as \"Mish\". Mish aims to promote better gradient flow for deep networks, while also distinguishing extreme gradient values for generalization in deep networks.</p> <p>For a more deep understanding of the function you can refer to the official paper by Diganta Misra that presents and discusses the Mish activation function, \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\".</p> <p>There is also a GitHub repo available for detailed information and research related to Mish Activation function Here.</p>"},{"location":"zeta/nn/modules/mishactivation/#class-definition","title":"Class Definition","text":"<pre><code>class MishActivation(nn.Module):\n    \"\"\"\n    A pytorch implementation of mish activation function.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        if version.parse(torch.__version__) &lt; version.parse(\"1.9.0\"):\n            self.act = self._mish_python\n        else:\n            self.act = nn.functional.mish\n\n    def _mish_python(self, input: Tensor) -&gt; Tensor:\n        return input * torch.tanh(nn.functional.softplus(input))\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        return self.act(input)\n</code></pre>"},{"location":"zeta/nn/modules/mishactivation/#class-arguments-methods","title":"Class Arguments &amp; Methods","text":""},{"location":"zeta/nn/modules/mishactivation/#arguments","title":"Arguments","text":"<p>Mish Activation function does not take any explicit argument other than the input tensor. </p>"},{"location":"zeta/nn/modules/mishactivation/#methods","title":"Methods","text":""},{"location":"zeta/nn/modules/mishactivation/#__init__self","title":"<code>__init__(self)</code>","text":"<p>This is the initialization method where mish activation function checks for PyTorch version and based on the version, decides whether to use PyTorch built-in Mish Activation function or fall back to its own python implementation of Mish Activation function.</p>"},{"location":"zeta/nn/modules/mishactivation/#_mish_pythonself-input-tensor-tensor","title":"<code>_mish_python(self, input: Tensor) -&gt; Tensor</code>","text":"<p>The fallback python implementation of Mish Activation function that multiplies the input with a hyperbolic tanh of a softplus function of input.</p> <ul> <li>Parameters:</li> <li> <p><code>input: Tensor</code>: The tensor on which the activation function will be applied.</p> </li> <li> <p>Returns:</p> <ul> <li><code>Tensor</code>: The modified tensor after applying the activation function.</li> </ul> </li> </ul>"},{"location":"zeta/nn/modules/mishactivation/#forwardself-input-tensor-tensor","title":"<code>forward(self, input: Tensor) -&gt; Tensor</code>","text":"<p>The forward method applies mish activation on the input tensor</p> <ul> <li>Parameters:</li> <li> <p><code>input: Tensor</code>: The tensor on which the activation function will be applied.</p> </li> <li> <p>Returns:</p> <ul> <li><code>Tensor</code>: The modified tensor after applying the activation function.</li> </ul> </li> </ul>"},{"location":"zeta/nn/modules/mishactivation/#usage-examples","title":"Usage Examples","text":"<p>This module requires PyTorch and Python 3.6 or above.</p>"},{"location":"zeta/nn/modules/mishactivation/#example-1-importing-the-module-and-applying-the-mish-activation-function","title":"Example 1: Importing the module and Applying the Mish Activation function","text":"<pre><code>from packaging import version\nfrom torch import Tensor, nn\nfrom torch.nn import functional as F\n\nfrom zeta.nn import MishActivation\n\ninput_tensor = Tensor([[-0.6, 0.7], [1.2, -0.7]])\nmish = MishActivation()\nprint(mish.forward(input_tensor))\n</code></pre>"},{"location":"zeta/nn/modules/mishactivation/#example-2-using-mish-activation-for-neural-network-layers","title":"Example 2: Using Mish Activation for Neural Network Layers","text":"<p>The Mish Activation function can also be applied in Neural Network layers using PyTorch.</p> <pre><code>import torch\nfrom packaging import version\nfrom torch import Tensor, nn\nfrom torch.nn import functional as F\n\nfrom zeta.nn import MishActivation\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.layer = nn.Sequential(\n            nn.Linear(26, 256), MishActivation(), nn.Linear(256, 10), MishActivation()\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.layer(x)\n        return logits\n\n\nmodel = NeuralNetwork()\n# Following lines shows how to use the model, given the input tensor, `X`.\n# output = model(X)\n</code></pre>"},{"location":"zeta/nn/modules/mishactivation/#references","title":"References","text":"<ul> <li>Packaging</li> <li>PyTorch</li> <li>Arxiv Article for Mish Activation</li> <li>GitHub repo for MishActivation</li> </ul>"},{"location":"zeta/nn/modules/mixtureofexperts/","title":"Class Name: MixtureOfExperts","text":"<p>Mixture of Experts model.</p> <p>Args: | Argument | Data Type | Default Value | Description | | --- | --- | --- | --- | | dim | int | N/A | Input dimension | | num_experts | int | N/A | Number of experts in the mixture | | hidden_layers | int, optional | None | Number of hidden layers in the experts | | mechanism | str, optional | \"softmax\" | Routing mechanism for selecting experts | | custom_feedforward | callable, optional | None | Custom feedforward function for the experts | | ff_mult | int, optional | 4 | Multiplier for the hidden layer dimension in the experts | | args | Variable length | N/A | Variable length argument list | | *kwargs | Dict | N/A | Arbitrary keyword arguments |</p> <p>Examples: <pre><code>import torch\n\nfrom zeta.nn import MixtureOfExperts\n\nx = torch.randn(2, 4, 6)\nmodel = MixtureOfExperts(dim=6, num_experts=2, hidden_layers=[32, 64])\noutput = model(x)\nprint(output.shape)\n</code></pre></p>"},{"location":"zeta/nn/modules/mlp/","title":"<code>MLP</code> Documentation","text":""},{"location":"zeta/nn/modules/mlp/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Class: <code>MLP</code></li> <li>Initialization</li> <li>Parameters</li> <li>Forward Method</li> <li>Usage Examples</li> <li>Using the <code>MLP</code> Class</li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/nn/modules/mlp/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation! In this documentation, we will explore the <code>MLP</code> class, a part of the Zeta library. The <code>MLP</code> class is designed to implement a Multi-Layer Perceptron (MLP) neural network module. This documentation provides a comprehensive understanding of the purpose, functionality, and usage of the <code>MLP</code> class.</p>"},{"location":"zeta/nn/modules/mlp/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>MLP</code> class implements a Multi-Layer Perceptron (MLP) module, a type of artificial neural network commonly used in deep learning. MLPs are composed of multiple layers of fully connected neurons and are known for their ability to approximate complex functions.</p> <p>The key features of the <code>MLP</code> class include:</p> <ul> <li> <p>Configurable architecture: You can specify the input and output dimensions, the expansion factor for hidden layers, the number of hidden layers, and whether to apply layer normalization.</p> </li> <li> <p>Activation functions: The MLP uses the Scaled Exponential Linear Unit (SiLU) activation function, which has been shown to improve training dynamics.</p> </li> <li> <p>Optional layer normalization: You can enable or disable layer normalization for the hidden layers.</p> </li> <li> <p>Flexibility: MLPs can be used for a wide range of tasks, including regression, classification, and function approximation.</p> </li> </ul>"},{"location":"zeta/nn/modules/mlp/#3-class-mlp","title":"3. Class: <code>MLP</code>","text":"<p>The <code>MLP</code> class implements the Multi-Layer Perceptron (MLP) neural network module. Let's delve into its details.</p>"},{"location":"zeta/nn/modules/mlp/#initialization","title":"Initialization","text":"<p>To create an instance of the <code>MLP</code> class, you need to specify the following parameters:</p> <pre><code>MLP(\n    dim_in,\n    dim_out,\n    *,\n    expansion_factor=2.,\n    depth=2,\n    norm=False\n)\n</code></pre>"},{"location":"zeta/nn/modules/mlp/#parameters","title":"Parameters","text":"<ul> <li> <p><code>dim_in</code> (int): The dimensionality of the input tensor.</p> </li> <li> <p><code>dim_out</code> (int): The dimensionality of the output tensor.</p> </li> <li> <p><code>expansion_factor</code> (float, optional): The expansion factor for the hidden dimension. Default is <code>2.0</code>.</p> </li> <li> <p><code>depth</code> (int, optional): The number of hidden layers. Default is <code>2</code>.</p> </li> <li> <p><code>norm</code> (bool, optional): Whether to apply layer normalization to the hidden layers. Default is <code>False</code>.</p> </li> </ul>"},{"location":"zeta/nn/modules/mlp/#forward-method","title":"Forward Method","text":"<p>The <code>forward</code> method of the <code>MLP</code> class performs the forward pass of the MLP module. It takes an input tensor and returns the output tensor.</p> <pre><code>def forward(x):\n    \"\"\"\n    Forward pass of the MLP module.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The output tensor.\n\n    \"\"\"\n    return self.net(x.float())\n</code></pre>"},{"location":"zeta/nn/modules/mlp/#4-usage-examples","title":"4. Usage Examples","text":"<p>Let's explore how to use the <code>MLP</code> class effectively in various scenarios.</p>"},{"location":"zeta/nn/modules/mlp/#using-the-mlp-class","title":"Using the <code>MLP</code> Class","text":"<p>Here's how to use the <code>MLP</code> class to create and apply an MLP neural network:</p> <pre><code>import torch\n\nfrom zeta.nn import MLP\n\n# Create an instance of MLP\nmlp = MLP(dim_in=256, dim_out=10, expansion_factor=4.0, depth=3, norm=True)\n\n# Create an input tensor\nx = torch.randn(32, 256)\n\n# Apply the MLP\noutput = mlp(x)\n\n# Output tensor\nprint(output)\n</code></pre>"},{"location":"zeta/nn/modules/mlp/#5-additional-information","title":"5. Additional Information","text":"<p>Multi-Layer Perceptrons (MLPs) are versatile neural network architectures that can be adapted to various tasks. Here are some additional notes:</p> <ul> <li> <p>Hidden Layer Configuration: You can customize the architecture of the MLP by adjusting parameters such as <code>expansion_factor</code> and <code>depth</code>. These parameters control the size and depth of the hidden layers.</p> </li> <li> <p>Layer Normalization: Layer normalization can help stabilize training and improve convergence, especially in deep networks. You can enable it by setting the <code>norm</code> parameter to <code>True</code>.</p> </li> <li> <p>Activation Function: The MLP uses the Scaled Exponential Linear Unit (SiLU) activation function, which is known for its smooth gradients and improved training dynamics.</p> </li> </ul>"},{"location":"zeta/nn/modules/mlp/#6-references","title":"6. References","text":"<p>For further information on Multi-Layer Perceptrons (MLPs) and related concepts, you can refer to the following resources:</p> <ul> <li> <p>Deep Learning - \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. This book provides an in-depth understanding of neural networks, including MLPs.</p> </li> <li> <p>PyTorch Documentation - Official PyTorch documentation for related functions and modules.</p> </li> </ul> <p>This documentation provides a comprehensive overview of the Zeta library's <code>MLP</code> class. It aims to help you understand the purpose, functionality, and usage of the <code>MLP</code> class for building Multi-Layer Perceptron neural networks for various tasks.</p>"},{"location":"zeta/nn/modules/mm_adapter/","title":"Module: MultiModalAdapterDenseNetwork","text":"<p>The <code>MultiModalAdapterDenseNetwork</code> module is designed for creating multi-modal adapter dense networks in PyTorch. It allows you to build deep neural networks with skip connections for efficient multi-modal data processing.</p>"},{"location":"zeta/nn/modules/mm_adapter/#overview","title":"Overview","text":"<p>In multi-modal data processing, combining information from different sources or modalities is crucial. This module provides a flexible way to design such networks by stacking multiple layers, applying normalization, activation functions, and skip connections.</p>"},{"location":"zeta/nn/modules/mm_adapter/#class-definition","title":"Class Definition","text":"<pre><code>class MultiModalAdapterDenseNetwork(nn.Module):\n    \"\"\"\n    Multi-modal adapter dense network that takes a tensor of shape (batch_size, dim) and returns a tensor of shape (batch_size, dim).\n\n    Flow:\n    x -&gt; norm -&gt; linear 1 -&gt; silu -&gt; concatenate -&gt; linear 2 -&gt; skip connection -&gt; output\n\n    Args:\n        dim (int): The input dimension.\n        hidden_dim (int): The hidden dimension.\n        depth (int): The depth of the network.\n        activation (nn.Module): The activation function.\n\n    Methods:\n        forward(x: torch.Tensor) -&gt; torch.Tensor: The forward pass of the network.\n    \"\"\"\n</code></pre>"},{"location":"zeta/nn/modules/mm_adapter/#parameters","title":"Parameters","text":"Parameter Description Data Type Default Value dim The input dimension. int None hidden_dim The hidden dimension. int None depth The depth of the network. int None activation The activation function. nn.Module nn.SiLU()"},{"location":"zeta/nn/modules/mm_adapter/#forward-method","title":"Forward Method","text":"<pre><code>def forward(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the network.\n    \"\"\"\n</code></pre>"},{"location":"zeta/nn/modules/mm_adapter/#how-it-works","title":"How It Works","text":"<p>The <code>MultiModalAdapterDenseNetwork</code> class works by stacking multiple layers of neural network operations, including normalization, linear transformations, activation functions, concatenation, and skip connections. Here's how it operates step by step:</p> <ol> <li>Input tensor <code>x</code> is first normalized using layer normalization.</li> <li>Two linear transformations are applied to <code>x</code>: <code>linear 1</code> and <code>linear 2</code>.</li> <li>The activation function <code>silu</code> is applied to the output of <code>linear 1</code>.</li> <li>The output of <code>linear 1</code> and <code>linear 2</code> is concatenated.</li> <li>The result is passed through the <code>skip_connections</code> module, which combines it with the original input tensor <code>x</code>.</li> <li>The final output is obtained.</li> </ol>"},{"location":"zeta/nn/modules/mm_adapter/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/modules/mm_adapter/#example-1-creating-and-using-the-network","title":"Example 1: Creating and Using the Network","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import MultiModalAdapterDenseNetwork\n\n# Create an instance of MultiModalAdapterDenseNetwork\nmm_adapter = MultiModalAdapterDenseNetwork(\n    dim=512,\n    hidden_dim=1024,\n    depth=3,\n)\n\n# Generate a random input tensor\nx = torch.randn(1, 512)\n\n# Perform a forward pass\noutput = mm_adapter(x)\n\n# Print the output shape\nprint(output.shape)  # Output shape: torch.Size([1, 1024, 512])\n</code></pre> <p>In this example, we create an instance of <code>MultiModalAdapterDenseNetwork</code>, pass an input tensor through it, and print the output shape.</p>"},{"location":"zeta/nn/modules/mm_adapter/#example-2-custom-activation-function","title":"Example 2: Custom Activation Function","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import MultiModalAdapterDenseNetwork\n\n\n# Define a custom activation function\nclass CustomActivation(nn.Module):\n    def forward(self, x):\n        return x * 2\n\n\n# Create an instance of MultiModalAdapterDenseNetwork with the custom activation\nmm_adapter = MultiModalAdapterDenseNetwork(\n    dim=512,\n    hidden_dim=1024,\n    depth=3,\n    activation=CustomActivation(),\n)\n\n# Generate a random input tensor\nx = torch.randn(1, 512)\n\n# Perform a forward pass\noutput = mm_adapter(x)\n</code></pre> <p>In this example, we create a custom activation function and use it when creating an instance of <code>MultiModalAdapterDenseNetwork</code>.</p>"},{"location":"zeta/nn/modules/mm_adapter/#example-3-custom-depth-and-hidden-dimension","title":"Example 3: Custom Depth and Hidden Dimension","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import MultiModalAdapterDenseNetwork\n\n# Create an instance of MultiModalAdapterDenseNetwork with custom depth and hidden dimension\nmm_adapter = MultiModalAdapterDenseNetwork(\n    dim=512,\n    hidden_dim=2048,  # Increased hidden dimension\n    depth=5,  # Increased depth\n)\n\n# Generate a random input tensor\nx = torch.randn(1, 512)\n\n# Perform a forward pass\noutput = mm_adapter(x)\n</code></pre> <p>In this example, we create an instance of <code>MultiModalAdapterDenseNetwork</code> with custom depth and hidden dimension values.</p>"},{"location":"zeta/nn/modules/mm_adapter/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The <code>MultiModalAdapterDenseNetwork</code> class allows you to experiment with different architectures and activation functions for multi-modal data processing.</li> <li>You can customize the activation function by providing your own module as the <code>activation</code> argument.</li> <li>Experiment with different values for <code>dim</code>, <code>hidden_dim</code>, and <code>depth</code> to find the optimal architecture for your task.</li> </ul> <p>This documentation provides a comprehensive guide to the <code>MultiModalAdapterDenseNetwork</code> module, including its purpose, parameters, usage examples, and tips for customization. Feel free to explore and adapt this module to suit your specific multi-modal data processing needs.</p>"},{"location":"zeta/nn/modules/mm_adapter/#references-and-resources","title":"References and Resources","text":"<ul> <li>PyTorch Documentation: https://pytorch.org/docs/stable/index.html</li> <li>Multi-modal Data Processing Techniques: https://arxiv.org/abs/2107.15912 (Reference paper for multi-modal data processing)</li> <li>Paper Origination: M2UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models</li> </ul>"},{"location":"zeta/nn/modules/mmfusionffn/","title":"Module Name: MMFusionFFN","text":""},{"location":"zeta/nn/modules/mmfusionffn/#overview","title":"Overview","text":"<p>The <code>MMFusionFFN</code> module represents a positionwise feedforward layer and is used in the context of multi-modal image and text processing.</p>"},{"location":"zeta/nn/modules/mmfusionffn/#class-definition","title":"Class Definition","text":"<ul> <li><code>MMFusionFFN(input_dim, hidden_dim, dropout=0.0)</code></li> </ul>"},{"location":"zeta/nn/modules/mmfusionffn/#args","title":"Args","text":"Name Type Description Default input_dim int Input dimension - hidden_dim int Hidden dimension - output_dim int Output dimension - dropout float Dropout probability. 0.1"},{"location":"zeta/nn/modules/mmfusionffn/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>MMFusionFFN</code> module is a subclass of the <code>nn.Module</code> class and contains a <code>forward</code> method which computes the output of the positionwise feedforward layer.</p> <p>The method performs the following operations: 1. Apply layer normalization to the input tensor. 2. Pass the resulting tensor through a linear transformation (fully connected layer) with a SiLU (Sigmoid Linear Unit) activation function. 3. Apply dropout to the tensor. 4. Repeat steps 2 and 3 with a second fully connected layer. 5. Return the output tensor.</p>"},{"location":"zeta/nn/modules/mmfusionffn/#usage-examples","title":"Usage Examples","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import MMFusionFFN\n\n# Define the input and hidden dimensions\ninput_dim = 512\nhidden_dim = 1024\noutput_dim = 512\ndropout = 0.1\n\n# Create an instance of MMFusionFFN\nffn = MMFusionFFN(input_dim, hidden_dim, output_dim, dropout)\n\n# Example 1 - Forward pass with random input data\ninput_data = torch.randn(\n    5, 32, input_dim\n)  # Random input data of shape (5, 32, input_dim)\noutput = ffn(input_data)\nprint(output.shape)  # Output tensor shape\n\n# Example 2 - Create an instance with default dropout\nffn_default_dropout = MMFusionFFN(input_dim, hidden_dim, output_dim)\n\n# Example 3 - Forward pass with another input data\ninput_data2 = torch.randn(\n    8, 16, input_dim\n)  # Random input data of shape (8, 16, input_dim)\noutput2 = ffn_default_dropout(input_data2)\nprint(output2.shape)  # Output tensor shape\n</code></pre>"},{"location":"zeta/nn/modules/mmfusionffn/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The <code>MMFusionFFN</code> module is commonly used in multimodal machine learning applications to process multi-dimensional input data from different modalities, such as image and text.</li> <li>The most important parameters to consider when creating an instance of <code>MMFusionFFN</code> are <code>input_dim</code> and <code>hidden_dim</code>. These parameters can be adjusted based on the specifics of the input data and the desired level of transformation.</li> <li>The <code>dropout</code> parameter controls the probability of an element to be zeroed in the forward pass, which can help prevent overfitting.</li> </ul>"},{"location":"zeta/nn/modules/mmfusionffn/#references-and-resources","title":"References and Resources","text":"<ul> <li>PyTorch Documentation: nn.Module</li> <li>Hugging Face Documentation: SiLU Activation Function</li> </ul> <p>This comprehensive documentation provides a detailed overview of the <code>MMFusionFFN</code> module, including its purpose, architecture, usage examples, and additional information. Developers can now use this documentation to effectively utilize the module in their applications.</p> <p>The examples illustrate how to create instances of <code>MMFusionFFN</code>, perform forward passes, and handle different input shapes, providing a practical guide for utilizing the module. Additionally, important attributes, such as <code>input_dim</code>, <code>hidden_dim</code>, and <code>dropout</code>, are explained in the class definition table for easy reference and understanding.</p>"},{"location":"zeta/nn/modules/mmlayernorm/","title":"Module/Function Name: MMLayerNorm","text":"<pre><code># Usage example:\nimport torch\n\nfrom zeta.nn import MMLayerNorm\n\nmm_ln = MMLayerNorm(num_modalities=2, dim=64)\nmodality1 = torch.randn(32, 10, 64)\nmodality2 = torch.randn(32, 10, 64)\nmm_ln([modality1, modality2])\nprint(mm_ln)\n</code></pre> <p>Explanation:</p> <p>The <code>MMLayerNorm</code> class represents a Multi-Modality Layer Normalization module that fuses and normalizes input tensors from different modalities. It helps in combining and normalizing information extracted from different sources, like images, text, etc.</p> <p>The parameters are as follows: - <code>num_modalities</code> (int): The number of modalities to be fused. - <code>dim</code> (int): The dimension of the input tensors. - <code>epsilon</code> (float): A small value added to the denominator for numerical stability. Default value is 1e-5.</p> <p>The <code>MMLayerNorm</code> class contains a method called <code>forward</code> that takes a list of input tensors representing different modalities and returns the output tensor after fusing and normalizing the modalities.</p> <p>The usage example demonstrates how to instantiate the <code>MMLayerNorm</code> class and pass input tensors to obtain the fused and normalized output tensor.</p> <p>Note: Ensure that the shapes of all the input modalities are identical. All modalities must have the same shape in order to perform fusion and normalization.</p> <p>This code snippet can be used to create and use a Multi-Modality Layer Normalization module in neural network architectures that require combining input tensors from different modalities for processing. The class structure ensures that submodules are registered and their parameters are converted as expected. </p> <p>For advanced usage and additional options, or to explore further, refer to the example provided above and the official PyTorch documentation.</p> <p>Example References: - PyTorch nn.Module documentation: https://pytorch.org/docs/stable/generated/torch.nn.Module.html - PyTorch Layer Normalization: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html</p> <p>These references provide further details and background information on how the <code>MMLayerNorm</code> class and other PyTorch modules can be utilized or extended, enabling developers to explore their full potential in designing and implementing machine learning models.</p>"},{"location":"zeta/nn/modules/moerouter/","title":"Module/Function Name: MoERouter","text":"<p>class zeta.nn.modules.MoERouter(dim: int, num_experts: int, hidden_layers: int = None, mechanism: \"str\" = \"softmax\"):</p> <p>Creates a module for routing input data to multiple experts based on a specified mechanism.</p> <p>Args: | Argument | Description                                  | | -------- | -------------------------------------------- | | dim      | The input dimension.                         | | num_experts | The number of experts to route the data to. | | hidden_layers | The number of hidden layers in the routing network. Defaults to None. | | mechanism | The routing mechanism to use. Must be one of \"softmax\" or \"gumbel\". Defaults to \"softmax\". |</p> <p>Raises: ValueError: If the mechanism is not \"softmax\" or \"gumbel\".</p> <p>Input Shape: (B, SEQ_LEN, DIM) where SEQ_LEN is the sequence length and DIM is the input dimension.</p> <p>Output Shape: (B, SEQ_LEN, NUM_EXPERTS) where NUM_EXPERTS is the number of experts.</p>"},{"location":"zeta/nn/modules/moerouter/#usage-example","title":"Usage example:","text":"<p>x = torch.randn(2, 4, 6) router = zeta.nn.modules.MoERouter(dim=6, num_experts=2, hidden_layers=[32, 64]) output = router(x)</p>"},{"location":"zeta/nn/modules/moerouter/#note","title":"Note:","text":"<p>The above code demonstrates the use of the MoERouter module. It creates an instance of the MoERouter module with the input dimension of 6, routing the input data to 2 experts using a hidden layer configuration of [32, 64], and applies the module to the input tensor x.</p>"},{"location":"zeta/nn/modules/moerouter/#introduction","title":"Introduction:","text":"<p>The MoERouter class is a module designed to route input data to multiple experts using a specified mechanism. It takes in input dimension, number of experts, hidden layers in the routing network, and routing mechanism as its arguments.</p> <p>The MoERouter class acts as a flexible routing mechanism for distributing input data to multiple experts in a modular and configurable manner, allowing for different routing mechanisms to be applied based on the application requirements.</p> <p>Note: The MoERouter class provides the flexibility to incorporate various routing mechanisms such as \"softmax\" and \"gumbel\", and supports the customization of the routing network with hidden layers. This enables the user to tailor the routing mechanism and configuration based on the specific use case and application scenarios.</p> <p>For more details on the implementation and usage of the MoERouter class, refer to the provided documentation, examples, and usage guidelines.</p>"},{"location":"zeta/nn/modules/multimodalmambablock/","title":"MultiModalMambaBlock","text":""},{"location":"zeta/nn/modules/multimodalmambablock/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Fusion Method and Model Architecture</li> <li>Usage and Examples</li> <li>Further References</li> </ul>"},{"location":"zeta/nn/modules/multimodalmambablock/#introduction","title":"Introduction","text":"<p>The MultiModalMambaBlock is a PyTorch module designed to combine text and image embeddings using a multimodal fusion approach. It provides methods for attention-based fusion using a Mamba block, ViT encoder, and image/text embeddings. By using a variety of fusion methods, the MultiModalMambaBlock aims to facilitate the learning of joint representations from different modalities.</p> <p></p>"},{"location":"zeta/nn/modules/multimodalmambablock/#fusion-method-and-model-architecture","title":"Fusion Method and Model Architecture","text":""},{"location":"zeta/nn/modules/multimodalmambablock/#args","title":"Args","text":"Args Description <code>dim</code> The dimension of the embeddings. <code>depth</code> The depth of the Mamba block. <code>dropout</code> The dropout rate. <code>heads</code> The number of attention heads. <code>d_state</code> The dimension of the state in the Mamba block. <code>image_size</code> The size of the input image. <code>patch_size</code> The size of the image patches. <code>encoder_dim</code> The dimension of the encoder embeddings. <code>encoder_depth</code> The depth of the encoder. <code>encoder_heads</code> The number of attention heads in the encoder. <code>fusion_method</code> The multimodal fusion method to use. Can be one of [\"mlp\", \"concat\", \"add\"]."},{"location":"zeta/nn/modules/multimodalmambablock/#module-architecture","title":"Module Architecture","text":"<ul> <li>Mamba Block: Implements a transformer-like Mamba block for attention-based fusion of embeddings.</li> <li>ViT Encoder: Utilizes a Vision Transformer encoder for image-based attention encoding.</li> <li>Fusion Methods: Provides support for various fusion methods, including MLP fusion, concatenation, addition, and visual expert methods.</li> </ul>"},{"location":"zeta/nn/modules/multimodalmambablock/#usage-and-examples","title":"Usage and Examples","text":"<pre><code>x = torch.randn(1, 16, 64)\ny = torch.randn(1, 3, 64, 64)\nmodel = MultiModalMambaBlock(\n    dim=64,\n    depth=5,\n    dropout=0.1,\n    heads=4,\n    d_state=16,\n    image_size=64,\n    patch_size=16,\n    encoder_dim=64,\n    encoder_depth=5,\n    encoder_heads=4,\n    fusion_method=\"mlp\",\n)\nout = model(x, y)\nprint(out.shape)\n</code></pre> <pre><code># Checking the current fusion method\nmodel.check_fusion_method()\n</code></pre>"},{"location":"zeta/nn/modules/multimodalmambablock/#further-references","title":"Further References","text":"<p>For additional information and detailed usage, please refer to the official documentation of the <code>MultiModalMambaBlock</code> module.</p> <p>Note: The architecture and methods used in the <code>MultiModalMambaBlock</code> module are designed to address the specific challenge of joint attention-based multimodal representation learning. The selected <code>fusion_method</code> and fusion approach can significantly impact the model performance, and care should be taken when choosing the appropriate method for a particular use case.</p>"},{"location":"zeta/nn/modules/multiscaleblock/","title":"MultiScaleBlock","text":""},{"location":"zeta/nn/modules/multiscaleblock/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Class Definition</li> <li>Functionality and Usage</li> <li>Additional Tips &amp; Information</li> <li>Resources and References</li> </ol>"},{"location":"zeta/nn/modules/multiscaleblock/#1-overview","title":"1. Overview","text":"<p>The <code>MultiScaleBlock</code> class, a component of PyTorch's <code>nn.Module</code>, falls under the category of deep learning models. PyTorch is a powerful, flexible deep learning framework that allows automatic differentiation and optimization. </p> <p>This class is well-suited to tasks where the spatial or temporal scale of the input data varies. Examples are wide-range in nature, including but not limited to, image processing, video analysis, and signal processing. </p> <p>In <code>MultiScaleBlock</code>, any PyTorch module such as convolutional layers, linear layers, or even sequence of layers can be applied to the input tensor at multiple scales in a seamless way. </p>"},{"location":"zeta/nn/modules/multiscaleblock/#2-class-definition","title":"2. Class Definition","text":""},{"location":"zeta/nn/modules/multiscaleblock/#multiscaleblock-class","title":"<code>MultiScaleBlock</code> Class","text":"<p>The class definition for <code>MultiScaleBlock</code> is provided below:</p> <pre><code>class MultiScaleBlock(nn.Module):\n    \"\"\"\n    A module that applies a given submodule to the input tensor at multiple scales.\n\n    Args:\n        module (nn.Module): The submodule to be applied.\n\n    Returns:\n        torch.Tensor: The output tensor after applying the submodule at multiple scales.\n    \"\"\"\n\n    def __init__(self, module):\n        super().__init__()\n        self.submodule = module\n\n    def forward(self, x: torch.Tensor, *args, **kwargs):\n        x1 = F.interpolate(x, scale_factor=0.5, *args, **kwargs)\n        x2 = F.interpolate(x, scale_factor=2.0, *args, **kwargs)\n        return (\n            self.submodule(x)\n            + F.interpolate(self.submodule(x1), size=x.shape[2:])\n            + F.interpolate(self.submodule(x2), size=x.shape[2:])\n        )\n</code></pre>"},{"location":"zeta/nn/modules/multiscaleblock/#method-1-__init__self-module","title":"Method 1: <code>__init__(self, module)</code>","text":"<p>This is the initializer for the <code>MultiScaleBlock</code> class, and it takes the following input:</p> <ul> <li><code>module (nn.Module)</code>: The submodule to be applied on the input tensor at multiple scales.</li> </ul>"},{"location":"zeta/nn/modules/multiscaleblock/#method-2-forwardself-x-torchtensor-args-kwargs","title":"Method 2: <code>forward(self, x: torch.Tensor, *args, **kwargs)</code>","text":"<p>The forward propagation method, onto which the initialized model is called with the input data <code>x</code>. It includes the following parameters:</p> <ul> <li><code>x (torch.Tensor)</code>: The input tensor.</li> <li><code>*args</code>: Additional arguments for the interpolate function of PyTorch. It can include various parameters depending on the Interpolation mode selected, which can be <code>mode</code>, <code>align_corners</code>, and <code>recompute_scale_factor</code>.</li> <li><code>**kwargs</code>: Additional keyword arguments.</li> </ul>"},{"location":"zeta/nn/modules/multiscaleblock/#3-functionality-and-usage","title":"3. Functionality and Usage","text":"<p>The <code>MultiScaleBlock</code> class is designed to apply a given submodule to the input tensor at multiple scales. The purpose of multi-scale processing is to handle the variation in scale of the different elements in the image, the data, or the signal.</p> <p>In the <code>forward</code> method, the input tensor <code>x</code> is first interpolated at two different scales (0.5 and 2.0). The PyTorch function <code>torch.nn.functional.interpolate</code> adjusts the size of the tensor using specific scaling factors. Then, the submodule is applied to the original input tensor and the interpolated tensors. The output is the sum of the results of applying the submodule at the original scale and the two interpolated scales.</p>"},{"location":"zeta/nn/modules/multiscaleblock/#usage-example","title":"Usage Example","text":"<p>Here are some examples showcasing the usage of <code>MultiScaleBlock</code>:</p> <ol> <li> <p>Single Convolutional Layer as Submodule:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom zeta.nn import MultiScaleBlock\n\nconv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\nmodel = MultiScaleBlock(conv)\ninput = torch.rand(1, 3, 32, 32)\noutput = model(input)\n</code></pre> </li> <li> <p>Sequence of Layers as Submodule:</p> <pre><code>seq = nn.Sequential(\n    nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n    nn.BatchNorm2d(64),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n)\nmodel = MultiScaleBlock(seq)\ninput = torch.rand(1, 3, 32, 32)\noutput = model(input)\n</code></pre> </li> <li> <p>Custom Model as Submodule:</p> <p>Suppose <code>MyModel</code> is a PyTorch model, you can use <code>MultiScaleBlock</code> on it as follows:</p> <pre><code>model = MyModel(num_classes=10)\nmulti_scale_model = MultiScaleBlock(model)\ninput = torch.rand(1, 3, 32, 32)\noutput = multi_scale_model(input)\n</code></pre> </li> </ol>"},{"location":"zeta/nn/modules/multiscaleblock/#4-additional-information","title":"4. Additional Information","text":"<ul> <li> <p>The input tensor's shape must be in the form of (batch_size, num_channels, height, width) for <code>forward</code> method of this class to work properly. This is because the <code>F.interpolate</code> function in PyTorch expects the input in this format.</p> </li> <li> <p>This class uses <code>F.interpolate</code> function, make sure to check the PyTorch documentation for this function to understand various interpolation modes and their behavior: https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html</p> </li> </ul>"},{"location":"zeta/nn/modules/multiscaleblock/#5-references","title":"5. References","text":"<ol> <li>PyTorch Official Documentation</li> <li>Multi-Scale Convolutional Neural Networks for Vision Tasks</li> </ol> <p>I hope this documentation will help you to understand and use <code>MultiScaleBlock</code> class in your scenarios. Enjoy DL with PyTorch!</p>"},{"location":"zeta/nn/modules/newgeluactivation/","title":"NewGELUActivation","text":""},{"location":"zeta/nn/modules/newgeluactivation/#chapter-1-introduction-and-overview","title":"Chapter 1: Introduction and Overview","text":""},{"location":"zeta/nn/modules/newgeluactivation/#newgeluactivation_1","title":"NewGELUActivation","text":"<p>The NewGELUActivation class is an implementation of the Gaussian Error Linear Units (GELU) activation function. In PyTorch, activation functions are essential non-linear transformations that are applied on the input, typically after linear transformations, to introduce non-linearity into the model. The GELU activation function is currently being used in Google's BERT and OpenAI's GPT models. If you are interested in more details about this function, see the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415</p>"},{"location":"zeta/nn/modules/newgeluactivation/#chapter-2-detailed-explanation-of-the-newgeluactivation-class","title":"Chapter 2: Detailed Explanation of the NewGELUActivation Class","text":"<p>The <code>NewGELUActivation</code> class extends <code>nn.Module</code>, so it can be integrated easily into any PyTorch model. It is a type of activation function that is believed to perform better in deeper architectures.</p> <pre><code>class NewGELUActivation(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        return (\n            0.5\n            * input\n            * (\n                1.0\n                + torch.tanh(\n                    math.sqrt(2.0 / math.pi)\n                    * (input + 0.044715 * torch.pow(input, 3.0))\n                )\n            )\n        )\n</code></pre>"},{"location":"zeta/nn/modules/newgeluactivation/#forward-function","title":"Forward Function","text":"<p>The <code>forward</code> method overloads the call to the function to process data. The forward method takes one mandatory argument:</p> <ul> <li><code>input</code> - This is a tensor that represents the activations output from the previous layer. The data type is Tensor.</li> </ul> <p>The forward method returns: </p> <ul> <li>The value obtained after applying the New GELU activation function on the input tensor.</li> </ul>"},{"location":"zeta/nn/modules/newgeluactivation/#implementation-of-the-forward-method","title":"Implementation of the forward method:","text":"<p>The forward method calculates the New GELU activation of the input tensor. The formula for calculating the New GELU activation is as follows:</p> <pre><code>GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))\n</code></pre> <p>where, - <code>x</code> is the input. - <code>tanh</code> is the hyperbolic tangent function. - <code>sqrt</code> is the square root function. - <code>^</code> is the power operator.</p> <p>Importantly, when the <code>forward</code> function is called on an object of the class <code>NewGELUActivation</code>, it computes these operations on the input tensor, and the result is returned.</p>"},{"location":"zeta/nn/modules/newgeluactivation/#chapter-3-usage-examples","title":"Chapter 3: Usage Examples","text":"<p>At first, you need to import necessary packages and modules. </p> <pre><code>import torch\nfrom torch import Tensor, nn\n\nfrom zeta.nn import NewGELUActivation\n</code></pre>"},{"location":"zeta/nn/modules/newgeluactivation/#usage-example-1","title":"Usage Example 1:","text":"<p>Creating an instance of NewGELUActivation and calling it with a tensor as input.</p> <pre><code>gelu_new = NewGELUActivation()\n\nrandom_data = torch.randn(5)  # Just some random data\noutput = gelu_new(random_data)\n\nprint(output)\n</code></pre>"},{"location":"zeta/nn/modules/newgeluactivation/#usage-example-2","title":"Usage Example 2:","text":"<p>Integrating NewGELUActivation within a neural network model.</p> <pre><code>class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.new_gelu = NewGELUActivation()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.new_gelu(x)\n        return x\n\n\nmodel = NeuralNetwork()  # Creating an instance of our model\n</code></pre>"},{"location":"zeta/nn/modules/newgeluactivation/#usage-example-3","title":"Usage Example 3:","text":"<p>Applying the NewGELUActivation function in a Convolutional Neural Network (CNN).</p> <pre><code>class CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.new_gelu = NewGELUActivation()\n\n    def forward(self, x):\n        x = self.new_gelu(self.conv1(x))\n        return x\n\n\nmodel = CNN()  # Creating an instance of our model\n</code></pre>"},{"location":"zeta/nn/modules/newgeluactivation/#chapter-4-conclusion","title":"Chapter 4: Conclusion","text":"<p>This was a complete guide about the <code>NewGELUActivation</code> PyTorch class. This tool provides an implementation of the GELU activation function, improving deep learning model architectures. This document demonstrated how to use the <code>NewGELUActivation</code> class and integrate it into existing PyTorch models with various examples.</p>"},{"location":"zeta/nn/modules/newgeluactivation/#external-links","title":"External Links","text":"<ul> <li>Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415 </li> <li>PyTorch official documentation: https://pytorch.org/docs/stable/index.html </li> <li>Other relevant resources: https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/</li> </ul>"},{"location":"zeta/nn/modules/nfnstem/","title":"NFNStem","text":"<p>The Zeta.nn.modules library is designed to accommodate the numerous layers and operations built in torch.nn layers, also this code provides support for different operations and custom layers, the code, and the accompanying documentation allow users to implement deep learning-based neural network architectures in Python. The purpose of the Zeta.nn.modules is to provide a collection of pre-written layers and operations that can be used to create new neural network architectures, making the process more efficient and less error-prone.</p>"},{"location":"zeta/nn/modules/nfnstem/#class-name-nfnstem","title":"Class Name: NFNStem","text":"<p>The <code>NFNStem</code> module represents the leaf node of the Neural Filter Network (NFN) architecture, aiding in the extraction of features and refining them through multiple layers of convolution.</p>"},{"location":"zeta/nn/modules/nfnstem/#args","title":"Args:","text":"Argument Description Data Type Default in_channels Input channel sizes for each layer List[int] [3, 16, 32, 64] out_channels Output channel sizes for each layer List[int] [16, 32, 64, 128] kernel_size Size of the convolutional kernel int 3 stride Stride values for each convolutional layer List[int] [2, 1, 1, 2] activation Activation function after each convolution layer nn.Module nn.GELU()"},{"location":"zeta/nn/modules/nfnstem/#usage-examples","title":"Usage Examples:","text":"<p><pre><code>import torch\n\nfrom zeta.nn import NFNStem\n\n# Create a random tensor with the shape of (1, 3, 224, 224)\nx = torch.randn(1, 3, 224, 224)\n\n# Instantiate the NFNStem module\nmodel = NFNStem()\n\n# Forward pass\nout = model(x)\nprint(out.shape)\n# Output: torch.Size([1, 128, 28, 28])\n</code></pre> <pre><code># Creating a custom NFNStem\nnfn_stem = NFNStem(\n    in_channels=[5, 10, 15, 20], out_channels=[10, 20, 30, 40], activation=nn.ReLU()\n)\nfeature_map = nfn_stem(input_data)\nprint(feature_map.shape)\n</code></pre> <pre><code>import torch\n\nfrom zeta.nn import NFNStem\n\n# Utilization of NFNStem with custom parameters\nstem = NFNStem(in_channels=[4, 8, 16, 16], out_channels=[8, 16, 32, 64])\ndata = torch.randn(1, 4, 128, 128)\noutput = stem(data)\nprint(output.shape)\n</code></pre></p> <p>The main purpose of the <code>NFNStem</code> class is to allow the construction of a sequence of neural network layers to process input data. The <code>forward</code> method takes an input tensor <code>x</code> and processes it through several convolution and activation layers, returning the output tensor.</p> <p>Additional information and tips: - Ensure that the input tensor has the appropriate shape and data type compatible with the individual layers. - The parameters such as <code>in_channels</code>, <code>out_channels</code>, <code>kernel_size</code>, and <code>stride</code> can be fine-tuned based on the specific requirements of the neural network architecture.</p> <p>Include references and resources: - Further insights into the \"Neural Filter Network\" architecture can be explored at [Link to research paper]. - The official repository for Zeta.nn.modules can be found at [Link to Zeta.nn.modules repository].</p> <p>By following this documented approach, the users can efficiently understand, implement and customize the Zeta.nn.modules for their specific neural network architecture needs.</p>"},{"location":"zeta/nn/modules/parallel/","title":"parallel","text":""},{"location":"zeta/nn/modules/parallel/#modulefunction-name-parallel","title":"Module/Function Name: Parallel","text":"<p>The <code>Parallel</code> class is a module that applies a list of functions in parallel and sums their outputs. This is particularly useful when you need to concurrently apply multiple operations to the same input and aggregate the results.</p>"},{"location":"zeta/nn/modules/parallel/#parameters","title":"Parameters:","text":"<p>The <code>Parallel</code> class can take a variable number of functions as input, which will be applied in parallel. The details for each function is provided when they are passed into the <code>Parallel</code> constructor, which then forms an <code>nn.ModuleList</code> to keep track of them.</p>"},{"location":"zeta/nn/modules/parallel/#usage-example","title":"Usage Example:","text":"<p>Below is an example of how to use the <code>Parallel</code> class. The example demonstrates creating an instance of <code>Parallel</code> with two <code>nn.Linear</code> modules and running a randomly generated input through both those linear modules in parallel.</p> <pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import Parallel\n\n# Define two Linear modules\nfn1 = nn.Linear(10, 5)\nfn2 = nn.Linear(10, 5)\n\n# Create a Parallel instance\nparallel = Parallel(fn1, fn2)\n\n# Generate a random input tensor\ninput = torch.randn(1, 10)\n\n# Pass the input through the parallel functions and aggregate the results\noutput = parallel(input)\n</code></pre>"},{"location":"zeta/nn/modules/parallel/#overview-and-introduction","title":"Overview and Introduction:","text":"<p>The <code>Parallel</code> class provides a way to apply a list of functions in parallel and then sum their outputs. It is widely applicable in scenarios where you need to concurrently apply multiple transformations to the same input data.</p> <p>The purpose of this module is to simplify the process of applying multiple operations to a given input tensor simultaneously and seamlessly aggregating the results. This is achieved by leveraging the <code>nn.ModuleList</code> to organize and execute the passed functions in a parallel manner, and then summing the outputs to provide a single combined result.</p> <p>By using the <code>Parallel</code> class, users can avoid repetitive code and streamline the process of applying multiple transformations to their input data, leading to cleaner, more organized code with minimal redundancy and better maintainability.</p>"},{"location":"zeta/nn/modules/perceiverlayer/","title":"Perceiver Layer","text":"<p>Multi-head attention mechanism often works well in analyzing subspaces of information, and the PerceiverLayer class is a constituent layer of a general-purpose architecture called the Perceiver, which uses multi-head attention mechanisms to analyze subspaces of information. It consists of a self-attention module followed by cross-attention and a feed-forward network.</p> <p>The PerceiverLayer class takes in three inputs: query, key, and value tensors, and applies a series of operations using attention and a feed-forward layer to yield an output tensor with the same input tensor dimensions. Some of the key parameters for the class include the dimension of the input tensor, number of heads for multi-head attention, number of layers, dimensions of each attention head, dropout rates, and other parameters that define the architecture.</p> <p><pre><code>Args[]\n| arg  | description | type | default\n|-------|-------------|------|---------\n| dim | dimension of the input tensor | int | -\n| heads | number of heads | int | -\n| depth | number of layers | int | -\n| dim_head | dimension of each head | int | 64\n| dropout | dropout rate | float | 0.1\n| ff_dropout | feed forward dropout rate | float | 0.1\n| ff_mult | feed forward multiplier | int | 4\n\nExamples\n\nCreating an instance of the PerceiverLayer class and applying it to query, key, and value tensors:\n```python\nimport torch\nfrom zeta.nn import PerceiverLayer\n\nq = torch.randn(1, 32, 512)\nk = torch.randn(1, 32, 512)\nv = torch.randn(1, 32, 512)\nlayer = PerceiverLayer(512, 8, 6, 64)\nprint(layer(q, k, v).shape)\n</code></pre> Expected Output: <pre><code>torch.Size([1, 32, 512])\n</code></pre></p> <p>The above example demonstrates the basic usage of the PerceiverLayer class by creating an instance and applying it to input tensors.</p> <p>The multi-head attention operation within the PerceiverLayer class operates by taking the query tensor and then sending the output into the query of the cross-attention, where the cross-attention takes in the key and value tensors. The output of the cross-attention is then sent into a feed-forward layer to generate the output tensor.</p> <p>The self_attn layer is used to perform self-attention on the query tensor, followed by concatenation of key and value tensors, and then input to the cross-attn layer for cross-attention, and finally, the feed-forward layer is applied. This process helps the model to process and understand the information across different dimensions.</p> <p>The forward method of the PerceiverLayer applies the attention and feed-forward layer to input tensors: <pre><code>def forward(\n    self,\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    mask: Optional[Tensor] = None,\n):\n</code></pre></p> <p>In this method, the query, key, and value tensors are passed as input, and a mask tensor can also be provided. The shapes of input tensors are specified in the parameter descriptions to ensure the correct input to this method. The comment above the method explains the high-level description of what this method does, including the input arguments and their shapes.</p> <p>The PerceiverLayer class provides the capability to understand and process large scale and high-dimensional data using multi-head attention and a feed-forward architecture, which is particularly useful for tasks like image and video understanding, as well as language processing.</p> <p>Utilizing this class to create custom attention-based models for applications such as visual recognition, natural language understanding, and generative modeling, can significantly benefit from the subtle interplay of attention mechanisms and feed-forward structures enabled by the PerceiverLayer class. Therefore, understanding the parameters, methods, and usage examples of this class are key to tapping its benefits effectively.</p> <p>Finally, the PerceiverLayer class provides a great level of flexibility and adaptability to build complex models without worrying about attention mechanism implementation details.</p> <p>Overall, the PerceiverLayer class is a vital component in building sophisticated and advanced models, which are capable of effectively processing and understanding high-dimensional and complex data across different domains. The class efficiently handles the design and managing of multi-head attention and a feed-forward layer architecture, which can be extensively used in various applications. Hence, the documentation and understanding of this class become essential to utilize its full potential.</p> <p>In conclusion, the documentation for the PerceiverLayer is presented in this template, following the best practices of documentation for the PerceiverLayer class, including the thorough description of class, parameters, and methods. Additionally, it provides a clear and detailed explanation of class usage, accompanied by the usage examples to illustrate its usage and the expected outputs. After understanding the given documentation, one can create, understand, and leverage the features of this class to build complex models and solve real-world problems effectively.</p>"},{"location":"zeta/nn/modules/polymorphic_activation/","title":"<code>PolymorphicNeuronLayer</code> Documentation","text":""},{"location":"zeta/nn/modules/polymorphic_activation/#introduction","title":"Introduction","text":"<p>Welcome to the documentation for <code>zeta.nn</code>! This module provides a unique and versatile Polymorphic Neuron Layer implemented using PyTorch. The <code>PolymorphicNeuronLayer</code> is designed to introduce dynamic activation functions within a neural network layer, allowing for adaptive learning. This documentation aims to comprehensively explain the purpose, architecture, usage, and customization options of the <code>PolymorphicNeuronLayer</code>.</p>"},{"location":"zeta/nn/modules/polymorphic_activation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Overview</li> <li>Class Definition</li> <li>Functionality and Usage<ul> <li>Initialization</li> <li>Forward Pass</li> <li>Customization</li> </ul> </li> <li>Examples</li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/nn/modules/polymorphic_activation/#1-installation","title":"1. Installation","text":"<p>Before using <code>PolymorphicNeuronLayer</code>, make sure you have <code>zetascale</code> installed. You can install it using:</p> <pre><code>pip install zetascale\n</code></pre> <p>Once PyTorch is installed, you can import <code>PolymorphicNeuronLayer</code> from <code>zeta.nn</code> as follows:</p> <pre><code>from zeta.nn import PolymorphicNeuronLayer\n</code></pre>"},{"location":"zeta/nn/modules/polymorphic_activation/#2-overview","title":"2. Overview","text":"<p>The <code>PolymorphicNeuronLayer</code> is a groundbreaking neural network layer that introduces dynamic activation functions to each neuron within the layer. This unique approach enables neurons to adapt and select activation functions based on their input data, leading to more flexible and adaptive learning.</p> <p>Key features: - Adaptive activation functions per neuron. - Customizable input and output features. - Support for multiple activation functions.</p>"},{"location":"zeta/nn/modules/polymorphic_activation/#3-class-definition","title":"3. Class Definition","text":""},{"location":"zeta/nn/modules/polymorphic_activation/#polymorphicneuronlayer","title":"<code>PolymorphicNeuronLayer</code>","text":"<pre><code>| Attribute                  | Description                                            |\n|----------------------------|--------------------------------------------------------|\n| in_features                 | Number of input features.                              |\n| out_features                | Number of output features (neurons).                   |\n| activation_functions        | List of activation functions to choose from.           |\n| weights                    | Learnable weights for linear transformation.           |\n| bias                       | Learnable bias term.                                   |\n\nParameters:\n- `in_features` (int): Number of input features.\n- `out_features` (int): Number of output features (neurons).\n- `activation_functions` (list of callable): List of activation functions to choose from.\n</code></pre>"},{"location":"zeta/nn/modules/polymorphic_activation/#4-functionality-and-usage","title":"4. Functionality and Usage","text":""},{"location":"zeta/nn/modules/polymorphic_activation/#initialization","title":"Initialization","text":"<p>To create an instance of <code>PolymorphicNeuronLayer</code>, you need to specify the <code>in_features</code>, <code>out_features</code>, and provide a list of <code>activation_functions</code>. These activation functions will be used dynamically based on neuron-specific criteria.</p> <p>Example:</p> <pre><code>import torch.nn.functional as F\n\nfrom zeta.nn import PolymorphicNeuronLayer\n\n# Create a Polymorphic Neuron Layer with 10 input features, 5 output neurons, and a list of activation functions\nneuron = PolymorphicNeuronLayer(\n    in_features=10, out_features=5, activation_functions=[F.relu, F.tanh, F.sigmoid]\n)\n</code></pre>"},{"location":"zeta/nn/modules/polymorphic_activation/#forward-pass","title":"Forward Pass","text":"<p>You can perform a forward pass through the <code>PolymorphicNeuronLayer</code> by passing input data to it. The input data should be a PyTorch tensor.</p> <p>Example:</p> <pre><code>import torch\n\n# Input data (1 sample with 10 features)\ninput_data = torch.randn(1, 10)\n\n# Forward pass through the Polymorphic Neuron Layer\noutput = neuron(input_data)\n</code></pre>"},{"location":"zeta/nn/modules/polymorphic_activation/#customization","title":"Customization","text":"<p>You can customize the following aspects of the <code>PolymorphicNeuronLayer</code>: - Input Features: Set the number of input features in the <code>in_features</code> parameter. - Output Features: Set the number of output neurons in the <code>out_features</code> parameter. - Activation Functions: Provide a list of activation functions to choose from in <code>activation_functions</code>.</p>"},{"location":"zeta/nn/modules/polymorphic_activation/#5-examples","title":"5. Examples","text":""},{"location":"zeta/nn/modules/polymorphic_activation/#example-1-customizing-and-forward-pass","title":"Example 1: Customizing and Forward Pass","text":"<pre><code>import torch.nn.functional as F\n\nfrom zeta.nn import PolymorphicNeuronLayer\n\n# Create a Polymorphic Neuron Layer with custom configuration\nneuron = PolymorphicNeuronLayer(\n    in_features=15, out_features=8, activation_functions=[F.relu, F.tanh, F.sigmoid]\n)\n\n# Input data (single sample with 15 features)\ninput_data = torch.randn(1, 15)\n\n# Forward pass through the customized Polymorphic Neuron Layer\noutput = neuron(input_data)\n</code></pre>"},{"location":"zeta/nn/modules/polymorphic_activation/#example-2-custom-activation-functions","title":"Example 2: Custom Activation Functions","text":"<pre><code>from zeta.nn import PolymorphicNeuronLayer\n\n\n# Define custom activation functions\ndef custom_activation_1(x):\n    return x**2\n\n\ndef custom_activation_2(x):\n    return torch.sin(x)\n\n\n# Create a Polymorphic Neuron Layer with custom activation functions\nneuron = PolymorphicNeuronLayer(\n    in_features=5,\n    out_features=3,\n    activation_functions=[custom_activation_1, custom_activation_2],\n)\n\n# Input data (1 sample with 5 features)\ninput_data = torch.randn(1, 5)\n\n# Forward pass through the Polymorphic Neuron Layer with custom activations\noutput = neuron(input_data)\n</code></pre>"},{"location":"zeta/nn/modules/polymorphic_activation/#example-3-dynamic-activation-selection","title":"Example 3: Dynamic Activation Selection","text":"<pre><code>import torch.nn.functional as F\n\nfrom zeta.nn import PolymorphicNeuronLayer\n\n# Create a Polymorphic Neuron Layer with 5 input features, 3 output neurons, and standard activation functions\nneuron = PolymorphicNeuronLayer(\n    in_features=5, out_features=3, activation_functions=[F.relu, F.tanh, F.sigmoid]\n)\n\n# Input data (single sample with 5 features)\ninput_data = torch.randn(1, 5)\n\n# Forward pass through the Polymorphic Neuron Layer with dynamic activation selection\noutput = neuron(input_data)\n</code></pre>"},{"location":"zeta/nn/modules/polymorphic_activation/#6-additional-information","title":"6. Additional Information","text":"<ul> <li>The dynamic activation selection in the <code>PolymorphicNeuronLayer</code> enhances adaptability and learning capacity within neural networks.</li> <li>For more advanced use cases and custom activation functions, you can define your own callable functions and pass them to the layer.</li> </ul>"},{"location":"zeta/nn/modules/polymorphic_activation/#7-references","title":"7. References","text":"<ul> <li>PyTorch Documentation</li> </ul> https://pytorch.org/docs/stable/index.html - PyTorch Tutorials: https://pytorch.org/tutorials/ <p>This concludes the documentation for <code>zeta.nn</code> and the <code>PolymorphicNeuronLayer</code> class. You now have the knowledge to incorporate dynamic activation functions into your neural networks for more adaptive and flexible learning. Happy coding!</p>"},{"location":"zeta/nn/modules/pool/","title":"pool","text":""},{"location":"zeta/nn/modules/pool/#the-purpose-and-functionality","title":"The purpose and functionality","text":"<p>The class <code>Pool</code> is a module identified by <code>torch.nn</code> framework. It is designed to execute pooling operations on input tensors. This module is intended to provide a downsampling and transformation mechanism for the input tensors, preparing the gathered data for further layers of the neural network. The key components such as operations, parameters, and relevant functionality are outlined in this comprehensive documentation. The main purpose of this module is to provide a pooling operation that can be utilised in the user's model creation and development.</p>"},{"location":"zeta/nn/modules/pool/#overview-and-introduction","title":"Overview and Introduction","text":"<p>The <code>Pool</code> class provided by the module <code>torch.nn</code> is a key part of the neural network library. The operations of the neural network are made more effective and efficient with the use of this pooling module. It essentially allows pooling of the input tensors while passing the output tensor.</p> <p>The importance of this module can be highlighted by observing the common usage of pooling operation in deep learning, a process key to many techniques such as image recognition. Understanding pooling operation is pivotal in the mastery of neural network modules which makes the <code>Pool</code> class a significant part of the neural network library.</p> <p>The key concepts and parameters will be most frequently used throughout the documentation. These specifics are highlighted in the subsequent sections of this document.</p>"},{"location":"zeta/nn/modules/pool/#class-definition","title":"Class Definition","text":"<p>Attributes of the class <code>Pool</code> are outlined here. These attributes signify the dimensions and key operations that the Pool module performs. This definition, along with the descriptions of the parameters, provides the basis for the effective usage of this module.</p> Parameters Description dim(int) The input tensor's dimension <p>The main class of this module is named <code>Pool</code> and contains one parameter called <code>dim</code>, which represents the dimension of the input tensor in operations performed. This is a crucial parameter that can directly impact the pooling results.</p>"},{"location":"zeta/nn/modules/pool/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The primary function of the class <code>Pool</code> is to perform a pooling operation on the input tensor. The forward pass includes functionalities such as processing the input tensor and returning the output tensor after applying pooling operation.</p> <p>Note: The <code>pooling</code> operation is an essential step in the neural network training process, acting as a downsample to better prepare data going forward through the network.</p> <p>Below are the code snippets providing full information on the forward pass of the <code>Pool</code> module and sample usage examples.</p> <pre><code>import torch.nn.functional as F\nfrom torch import nn\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\n\nmultihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\nattn_output, attn_output_weights = multihead_attn(query, key, value)\n</code></pre> <p>In the initial code snippet, a basic model is established with forward pass operations. The following code segment provides usage of the <code>MultiheadAttention</code> module and <code>attn_output</code> and <code>attn_output_weights</code> are returned.</p>"},{"location":"zeta/nn/modules/pool/#additional-information-and-tips","title":"Additional Information and Tips","text":"<p>As a significant part of the neural network library, developers must ensure that accurate dimensions are applied as parameters while utilizing the <code>Pool</code> module. Additionally, updating the underlying <code>rearrange</code> operation to align with the specific use case is crucial for precise results.</p> <p>Developers should make themselves knowledgeable about the importance and nuances of pooling operations to ensure effective implementation.</p>"},{"location":"zeta/nn/modules/pool/#references-and-resources","title":"References and Resources","text":"<p>It is recommended to further delve into the specifics of neural network modules and the purpose of the <code>Pool</code> module. This can be achieved by referring to the official documentation of the neural network libraries. Additionally, exploring related research papers in the domain of deep learning can help in achieving a deeper understanding of the mechanism of pooling operations.</p>"},{"location":"zeta/nn/modules/postnorm/","title":"Module/Function Name: LayerNorm","text":"<p>The <code>PostNorm</code> class is a post-normalization module of <code>torch.nn.modules</code>. It applies layer normalization after the input is passed through a given module. The main objectives of this class are to improve the training stability of deep neural networks and to standardize the input to make the training less dependent on the scale of features.</p> <p>Key features of <code>PostNorm</code> module: - Post-normalization: Applies layer normalization after being passed through a given module. - Dropout: Allows for the use of dropout probability on attention output weights.</p>"},{"location":"zeta/nn/modules/postnorm/#class-definition","title":"Class Definition","text":"<p>The <code>PostNorm</code> class has the following definition and parameters:</p> Parameter Description dim The dimension of the input tensor fn The module to be applied to the input tensor"},{"location":"zeta/nn/modules/postnorm/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>PostNorm</code> class performs a post-normalization on an input tensor using the given module. It applies layer normalization to the input tensor post application of <code>fn</code> module. The forward function <code>forward(x, **kwargs)</code> of the <code>PostNorm</code> module takes the input tensor <code>x</code> and additional keyword arguments <code>kwargs</code> to be passed to the underlying module.</p>"},{"location":"zeta/nn/modules/postnorm/#example-1-usage-within-model-architecture","title":"Example 1: Usage within Model Architecture","text":"<pre><code>from torch import nn\n\nfrom zeta.nn import PostNorm\n\n\n# Define a simple model\nclass SimpleModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n\n        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n        self.postnorm_layer = PostNorm(hidden_dim, nn.Linear(hidden_dim, output_dim))\n\n    def forward(self, x):\n        x = self.hidden_layer(x)\n        output = self.postnorm_layer(x)\n\n        return output\n\n\n# Usage:\ninput_dim, hidden_dim, output_dim = 10, 20, 2\nmodel = SimpleModel(input_dim, hidden_dim, output_dim)\ninputs = torch.randn(64, input_dim)\noutputs = model(inputs)\n\nprint(f\"Input Shape: {inputs.shape}\\nOutput Shape: {outputs.shape}\")\n</code></pre>"},{"location":"zeta/nn/modules/postnorm/#example-2-usage-with-image-data","title":"Example 2: Usage with Image Data","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import PostNorm\n\n\n# Define a model architecture for image data\nclass ImageModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.postnorm = PostNorm(output_dim, nn.ReLU())\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return self.postnorm(x)\n\n\n# Usage:\ninput_dim, hidden_dim, output_dim = 784, 256, 10  # Applicable for MNIST data\nmodel = ImageModel(input_dim, hidden_dim, output_dim)\ninputs = torch.randn(64, input_dim)\noutputs = model(inputs)\n\nprint(f\"Input Shape: {inputs.shape}\\nOutput Shape: {outputs.shape}\")\n</code></pre>"},{"location":"zeta/nn/modules/postnorm/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>It is recommended to experiment with different input dimensions and types to understand the effect of post-normalization on model training.</li> <li>In case of errors or unexpected behavior, double-check the dimensions of the input tensor for compatibility with the post-normalization process.</li> </ul>"},{"location":"zeta/nn/modules/postnorm/#references-and-resources","title":"References and Resources","text":"<p>For further exploration into layer normalization in neural networks, the official documentation of PyTorch can be found at: PyTorch Documentation on Layer Normalization</p>"},{"location":"zeta/nn/modules/pscan/","title":"Module Name: PScan","text":""},{"location":"zeta/nn/modules/pscan/#overview-and-introduction","title":"Overview and Introduction","text":"<p>The PScan class is an implementation of the parallel scan operation in PyTorch. The code is based on Francois Fleuret\u2019s pscan but has been written in an iterative way rather than recursively. The backward pass has been rewritten to improve efficiency, and the code provides a more detailed and efficient implementation of the parallel scan operation in PyTorch.</p> <p>This documentation will provide a comprehensive overview of the PScan class, including details about its purpose, class definition, functionality, usage examples, and additional information for utilizing the functionality provided by the class.</p>"},{"location":"zeta/nn/modules/pscan/#class-definition","title":"Class Definition","text":"<p>The PScan class is implemented as a torch.autograd.Function, which allows it to be directly used as an operation within PyTorch. The key parameters of the class include A_in and X_in, which represent input tensors, and H, which represents the resulting output of the parallel scan operation. The class also includes methods for both the forward and backward passes, using them to compute the outputs and gradients of the operation.</p>"},{"location":"zeta/nn/modules/pscan/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The parallel scan operation is applied using the forward method of the PScan class. The parallel scan takes two input tensors A_in and X_in and performs a parallel scan operation on them to produce the output tensor H. Additionally, the backward method is used to calculate the gradients of the output with respect to the inputs, which are returned as gradA and gradX.</p> <p>The parallel scan operation uses an iterative approach to efficiently compute the parallel scan of the input tensors, reducing the time complexity compared to a recursive implementation. The forward and backward passes ensure that the output and gradients of the operation are correctly calculated, making it suitable for differentiable optimization procedures.</p>"},{"location":"zeta/nn/modules/pscan/#code-snippet-for-usage","title":"Code Snippet for Usage","text":"<pre><code>import torch\n\nfrom zeta.nn import PScan\n\n# Create input tensors\nx = torch.randn(2, 3, 4, 5, requires_grad=True)\ny = torch.randn(2, 3, 4, 5, requires_grad=True)\n\n# Apply the parallel scan operation\nmodel = PScan.apply(x, y)\n\n# Perform backpropagation to compute gradients\nmodel.sum().backward()\nprint(x.grad)\nprint(y.grad)\n</code></pre>"},{"location":"zeta/nn/modules/pscan/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The PScan class is based on the Blelloch version of the parallel scan operation.</li> <li>The code is written for efficient and differentiable parallel scan computations in PyTorch.</li> <li>It is important to clone input tensors before using the PScan operation.</li> </ul>"},{"location":"zeta/nn/modules/pscan/#references-and-resources","title":"References and Resources","text":"<ul> <li>For a detailed explanation with examples, see the pscan.ipynb document included in the repository.</li> <li>For further details about PyTorch and differentiable programming, refer to the official PyTorch documentation.</li> </ul> <p>This comprehensive documentation provides a detailed overview of the PScan class, including its implementation, purpose, functionality, usage, and additional tips. The class serves as a valuable tool for efficiently computing parallel scans in PyTorch and is aimed at users who seek to utilize differentiable operations within the PyTorch framework.</p>"},{"location":"zeta/nn/modules/pytorchgelutanh/","title":"PytorchGELUTanh","text":""},{"location":"zeta/nn/modules/pytorchgelutanh/#overview","title":"Overview","text":"<p>The <code>PytorchGELUTanh</code> class in Python is a fast C implementation of the tanh approximation of the GeLU activation function. This implementation is meant to be faster and as effective as other implementations of GeLU (Gaussian Error Linear Units) function like NewGELU and FastGELU. However, it is not an exact numerical match to them due to possible rounding errors.</p> <p>This documentation provides an in-depth guide to using the <code>PytorchGELUTanh</code> class. It includes general information about the class, the method documentation, and various usage examples.</p>"},{"location":"zeta/nn/modules/pytorchgelutanh/#introduction","title":"Introduction","text":"<p>In Neural Networks, activation functions decide whether a neuron should be activated or not by calculating the weighted sum and adding bias with it. One of these activation functions is the Gaussian Error Linear Units (GeLU) function. GeLU function approximates the cumulative distribution function of the standard Gaussian distribution and helps in faster learning during the initial phase of training.</p> <p>The <code>PytorchGELUTanh</code> class provides a fast C implementation of the tanh approximation of the GeLU activation function.</p>"},{"location":"zeta/nn/modules/pytorchgelutanh/#class-definition","title":"Class Definition","text":"<pre><code>class PytorchGELUTanh(nn.Module):\n    \"\"\"\n    A fast C implementation of the tanh approximation of the GeLU activation function. See\n    https://arxiv.org/abs/1606.08415.\n\n    This implementation is equivalent to NewGELU and FastGELU but much faster. However, it is not an exact numerical\n    match due to rounding errors.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        if version.parse(torch.__version__) &lt; version.parse(\"1.12.0\"):\n            raise ImportError(\n                f\"You are using torch=={torch.__version__}, but torch&gt;=1.12.0\"\n                \" is required to use PytorchGELUTanh. Please upgrade torch.\"\n            )\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        return nn.functional.gelu(input, approximate=\"tanh\")\n</code></pre>"},{"location":"zeta/nn/modules/pytorchgelutanh/#general-information","title":"General Information","text":"<p>The <code>PytorchGELUTanh</code> class only requires PyTorch version 1.12.0 or higher. </p> <p>This class contains the following methods:</p> Method Definition <code>__init__</code> This is the constructor method for the <code>PytorchGELUTanh</code> class in which the superclass is initialized and a check is made to ensure that the version of PyTorch being used supports the class. If not, an import error is raised. <code>forward</code> This method applies the tanh approximation of the GeLU active function to the provided tensor input. <p>The <code>forward</code> method takes in a tensor as an input argument and returns a tensor as an output. The input and output tensors are of the same size.</p>"},{"location":"zeta/nn/modules/pytorchgelutanh/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/modules/pytorchgelutanh/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<p>In this basic example, we create an instance of the <code>PytorchGELUTanh</code> class and pass a tensor to its <code>forward</code> method to apply the tanh approximation of the GeLU function.</p> <pre><code># Import necessary libraries\nimport torch\nfrom packaging import version\nfrom torch import Tensor, nn\nfrom torch.nn.functional import gelu\n\nfrom zeta.nn import PytorchGELUTanh\n\n# Create an instance of the PytorchGELUTanh class.\ngelutanh = PytorchGELUTanh()\n\n# Create a tensor.\nx = torch.randn(3)\n\n# Print the tensor before and after applying the GeLU Tanh activation function.\nprint(\"Before: \", x)\nprint(\"After: \", gelutanh.forward(x))\n</code></pre>"},{"location":"zeta/nn/modules/pytorchgelutanh/#example-2-application-to-deep-learning","title":"Example 2: Application to Deep Learning","text":"<p>The <code>PytorchGELUTanh</code> class can be used in place of traditional activation functions in deep learning models. Here is an example of its usage in a feed-forward neural network.</p> <pre><code># Import necessary libraries\nimport torch\nfrom torch import Tensor, nn\nfrom torch.nn.functional import gelu\n\nfrom zeta.nn import PytorchGELUTanh\n\n\n# Define a feed-forward neural network with 2 layers and the PytorchGELUTanh activation function\nclass FeedForwardNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 20)  # 10 input neurons, 20 output neurons\n        self.gelu = PytorchGELUTanh()  # Our custom activation function\n        self.fc2 = nn.Linear(20, 1)  # Final layer\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.gelu(x)  # Apply the PytorchGELUTanh activation\n        x = self.fc2(x)\n        return x\n\n\n# Instantiate the model\nmodel = FeedForwardNN()\n\n# Print the model architecture\nprint(model)\n</code></pre> <p>This completes the documentation for the <code>PytorchGELUTanh</code> Python class, but feel free to reference the official PyTorch documentation and ensure you are using a version of PyTorch that is compatible with this class.</p>"},{"location":"zeta/nn/modules/quantizedln/","title":"Module/Class Name: QuantizedLN","text":""},{"location":"zeta/nn/modules/quantizedln/#overview","title":"Overview","text":"<p><code>QuantizedLN</code> is a PyTorch module built on the lower-level <code>nn.Module</code> class. This module is designed for applying a form of normalization where the layer inputs are transformed to have zero mean and one standard deviation, and subsequently quantized. The main purpose of this module is to provide normalized inputs with reduced precision for performance and memory optimization purposes, seen typically in low-resource environments like mobile devices.</p> <p>The 'LN' in the class name refers to Layer Normalization, a technique that normalizes the inputs across the features instead of the batch size. The 'Quantized' in the class name signifies that the normalized output is then quantized to a specified bit size for memory and speed optimizations.</p> <pre><code>class QuantizedLN(nn.Module):\n  def __init__(\n      self,\n      normalized_shape,\n      bits: int = 8,\n      eps=1e-5,\n      element_wise_affine=True,\n  ):\n  \"\"\"\n  Initializes a QuantizedLN module.\n\n  Args:\n        normalized_shape (int or tuple): The expected input shape.\n        bits (int, optional): Number of bits for quantization. Defaults to 8.\n        eps (float, optional): A value added to the denominator for numerical stability. Defaults to 1e-5.\n        element_wise_affine (bool, optional): Whether to include learnable affine parameters. Defaults to True.\n  \"\"\"\n    ...\n\n  def forward(self, x: Tensor):\n  \"\"\"\n  Forward pass of the QuantizedLN module.\n\n  Args:\n      x (torch.Tensor): Input tensor.\n\n  Returns:\n      torch.Tensor: Output tensor after applying quantization and layer normalization.\n  \"\"\"\n    ...\n</code></pre>"},{"location":"zeta/nn/modules/quantizedln/#parameters","title":"Parameters","text":"<p>The <code>QuantizedLN</code> class takes the following arguments during initialization:</p> Parameter Name Type Description Default Value normalized_shape int or tuple The expected input shape Required bits int Number of bits for quantization 8 eps float A small value added to the denominator for numerical stability 1e-5 element_wise_affine bool If True, includes learnable affine parameters True"},{"location":"zeta/nn/modules/quantizedln/#methods","title":"Methods","text":"<p>The <code>QuantizedLN</code> class has the following methods:</p> Method Name Args Returns Description init normalized_shape, bits, eps, element_wise_affine None Initializes the QuantizedLN module forward x torch.Tensor Performs the forward pass"},{"location":"zeta/nn/modules/quantizedln/#usage-examples","title":"Usage Examples","text":"<p>Below are three examples of how to use the <code>QuantizedLN</code> module.</p>"},{"location":"zeta/nn/modules/quantizedln/#example-1","title":"Example 1","text":"<pre><code>import torch\nfrom torch import Tensor, nn\nfrom torch.nn.parameter import Parameter\n\nfrom zeta.nn.modules import QuantizedLN\n\n# Define input tensor\nx = torch.randn(128, 10)\n# Create module instance\nln = QuantizedLN(10)\n# Apply module to input\noutput = ln(x)\n</code></pre>"},{"location":"zeta/nn/modules/quantizedln/#example-2","title":"Example 2","text":"<p>Define a custom network that uses have the <code>QuantizedLN</code> module:</p> <pre><code>import torch.nn as nn\n\nfrom zeta.nn.modules import QuantizedLN\n\n\nclass CustomNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(128, 256)\n        self.ln = QuantizedLN(256)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.ln(x)\n        return x\n\n\n# Define input tensor\nx = torch.randn(128, 10)\n\n# Create network instance\nnetwork = CustomNetwork()\n\n# Forward pass\noutput = network(x)\n</code></pre>"},{"location":"zeta/nn/modules/quantizedln/#example-3","title":"Example 3","text":"<p>The <code>QuantizedLN</code> module in a multi-layer setup:</p> <pre><code>import torch.nn as nn\n\nfrom zeta.nn.modules import QuantizedLN\n\n\nclass DeepNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(128, 256)\n        self.ln1 = QuantizedLN(256)\n        self.layer2 = nn.Linear(256, 512)\n        self.ln2 = QuantizedLN(512)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.ln1(x)\n        x = self.layer2(x)\n        x = self.ln2(x)\n        return x\n\n\n# Define input tensor\nx = torch.randn(128, 10)\n\n# Create network instance\nnetwork = DeepNetwork()\n\n# Forward pass\noutput = network(x)\n</code></pre>"},{"location":"zeta/nn/modules/quantizedln/#additional-notes","title":"Additional Notes:","text":"<p>Please make sure that the <code>absmax_quantize</code> function used in the <code>forward</code> method is properly defined in the scope of this class or is imported correctly from an external module. It is a quantization function that is not included by default in PyTorch's <code>nn</code> module. Failure to define or import this function will result in errors during execution.</p>"},{"location":"zeta/nn/modules/quickgeluactivation/","title":"QuickGELUActivation","text":""},{"location":"zeta/nn/modules/quickgeluactivation/#overview","title":"Overview","text":"<p>The QuickGELUActivation class is a part of the Neural Network(NN) module that applies a Gaussian Error Linear Unit (GELU) approximation. GELU can be viewed as a smoother version of the popular activation function, ReLU. The approximate version of GELU used in this class is fast although somewhat less accurate than the standard GELU activation.</p> <p>The GELU activation function can be used as an alternative to other popular activation functions like ReLU and Sigmoid while training deep learning models. The importance of GELU in the context of deep learning comes from its unique properties which includes non-monotonicity that allows for complex transformations.</p>"},{"location":"zeta/nn/modules/quickgeluactivation/#class-definition","title":"Class Definition","text":"<p>The QuickGELUActivation class is defined as shown below:</p> <pre><code>class QuickGELUActivation(nn.Module):\n    \"\"\"\n    Applies GELU approximation that is fast but somewhat inaccurate. See: https://github.com/hendrycks/GELUs\n    \"\"\"\n</code></pre> <p>The class extends the Module class from the pyTorch library. It does not take any input parameters during initialization.</p>"},{"location":"zeta/nn/modules/quickgeluactivation/#method-definitions","title":"Method Definitions","text":"<p>The class has a single method named forward.</p>"},{"location":"zeta/nn/modules/quickgeluactivation/#forward","title":"forward","text":"<p>This function is responsible for applying the GELU approximation to the input tensor.</p> <pre><code>def forward(self, input: Tensor) -&gt; Tensor:\n    return input * torch.sigmoid(1.702 * input)\n</code></pre> <p>Parameters:</p> Name Type Description input Tensor The input tensor to which the GELU approximation will be applied. <p>Return Type: Tensor </p> <p>Returns: The output tensor after applying the GELU approximation.</p>"},{"location":"zeta/nn/modules/quickgeluactivation/#meta-information","title":"Meta-information","text":"<p>The function uses a torch inbuilt function sigmoid to apply the GELU approximation. The parameter 1.702 in the sigmoid function is chosen as it approximates the GELU function very closely. It should be noted that this approximation may not be exactly equal to the standard GELU and hence, could be somewhat inaccurate.</p>"},{"location":"zeta/nn/modules/quickgeluactivation/#example-code","title":"Example Code","text":"<p>Below is a simple example showing how to use QuickGELUActivation to apply a GELU approximation to a tensor input:</p> <pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import QuickGELUActivation\n\n# create an instance of QuickGELUActivation\nactivation = QuickGELUActivation()\n\n# create a tensor\nx = torch.rand(3)\n\n# apply GELU activation\noutput = activation(x)\n\nprint(output)\n</code></pre> <p>In this code, we first create a tensor using the <code>rand</code> method from pyTorch. Next, an instance of the QuickGELUActivation class is created and the GELU approximation is applied to the tensor.</p> <p>Further, it is advised to use this GELU activation function in the scenario where quick approximation is more advantageous than a slightly more accurate result. It can be used with any model architecture where an activation function is needed. It may provide better results in certain scenarios compared to typical activation functions like ReLU. </p> <p>For more details, you can refer to the GELU activation paper and the approximation method. </p> <p>This class is not a direct replacement for the torch.nn.GELU and should be used considering the trade-off between speed and accuracy. Please also refer to the official PyTorch documentation for more information on activation functions in PyTorch.</p>"},{"location":"zeta/nn/modules/recursiveblock/","title":"RecursiveBlock","text":"<p>Zeta is a python library that makes use of Pytorch for implementing several classes and functions related to swarm optimization tasks. This documentation will be focusing on the <code>RecursiveBlock</code> class in the <code>swarm</code> Pytorch-based library. This class's main functionality is to recursively apply a given module a specified number of times to an input tensor.</p> <p>The RecursiveBlock is, therefore, a versatile class that allows for a wide range of operations to be performed on your data by reiterating the application of an operation or set of operations encapsulated in a module.</p>"},{"location":"zeta/nn/modules/recursiveblock/#class-definition","title":"Class Definition","text":"<p>Here is the code structure of the RecursiveBlock class:</p> <pre><code>import torch\nfrom torch import nn\n\n\nclass RecursiveBlock(nn.Module):\n    def __init__(self, modules, iters, *args, **kwargs):\n        super().__init__()\n        self.modules = modules\n        self.iters = iters\n\n    def forward(self, x: torch.Tensor):\n        for _ in range(self.iters):\n            x = self.modules(x)\n        return x\n</code></pre>"},{"location":"zeta/nn/modules/recursiveblock/#parameters-and-arguments","title":"Parameters and Arguments","text":"<p>Let's discuss the function definitions, parameters, and return types of <code>RecursiveBlock's</code> methods.</p>"},{"location":"zeta/nn/modules/recursiveblock/#__init__-constructor-method","title":"<code>__init__</code> Constructor Method:","text":"<p>This method initializes the <code>RecursiveBlock</code> object. Parameters of this constructor are:</p> Parameter Type Description <code>modules</code> torch.nn.Module The module to be applied recursively. <code>iters</code> int The number of iterations to apply the module. <code>*args</code> list Variable length argument list. <code>**kwargs</code> dict Arbitrary keyword arguments."},{"location":"zeta/nn/modules/recursiveblock/#forward-method","title":"<code>forward</code> Method:","text":"<p>This method is responsible for the forward pass of the block. Parameters of this method are:</p> Parameter Type Description <code>x</code> torch.Tensor The input tensor. <p>Return Type: torch.Tensor : The output tensor after applying the module recursively.</p>"},{"location":"zeta/nn/modules/recursiveblock/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/modules/recursiveblock/#example-1","title":"Example 1:","text":"<p>Utilizing two convolutional layers from Pytorch's nn library recursively</p> <pre><code>import torch\nfrom torch import nn\n\nfrom zeta import RecursiveBlock\n\nconv_module = nn.Sequential(\n    nn.Conv2d(1, 20, 5), nn.ReLU(), nn.Conv2d(20, 20, 5), nn.ReLU()\n)\n\nblock = RecursiveBlock(conv_module, iters=2)\n\nx = torch.randn(1, 20, 10, 10)\noutput = block(x)\n</code></pre>"},{"location":"zeta/nn/modules/recursiveblock/#example-2","title":"Example 2:","text":"<p>Implementing the RecursiveBlock class with a simple, custom module</p> <pre><code>class AddTen(nn.Module):\n    def forward(self, x):\n        return x + 10\n\n\nblock = RecursiveBlock(AddTen(), iters=3)\noutput = block(torch.tensor(1.0))  # output -&gt; tensor(31.)\n</code></pre>"},{"location":"zeta/nn/modules/recursiveblock/#example-3","title":"Example 3:","text":"<p>Using RecursiveBlock with a Linear Layer and a sigmoid activation function</p> <pre><code>import torch\nfrom torch import nn\n\nfrom zeta import RecursiveBlock\n\nlinear_module = nn.Sequential(\n    nn.Linear(128, 64),\n    nn.Sigmoid(),\n)\n\nblock = RecursiveBlock(linear_module, iters=3)\n\nx = torch.randn(16, 128)\noutput = block(x)\n</code></pre>"},{"location":"zeta/nn/modules/recursiveblock/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ol> <li> <p>The <code>modules</code> parameter in <code>RecursiveBlock</code> is not limited to built-in PyTorch modules. It can also be a custom PyTorch nn.Module defined by the user.</p> </li> <li> <p>The <code>iters</code> parameter can be adjusted as per the requirement of the task. More iterations might lead to a deeper feature extraction and can sometimes lead to better performance, but can also increase the computation time.</p> </li> </ol> <p>Thus, RecursiveBlock is a simple yet powerful class providing the abstraction of repeated module application, making iterating through a module multiple times a straightforward task. It enables cleaner, more readable code for models involving repetition of a similar structure or block, ushering rich flexibility into the hands of the programmer.</p>"},{"location":"zeta/nn/modules/relusquaredactivation/","title":"ReLUSquaredActivation","text":""},{"location":"zeta/nn/modules/relusquaredactivation/#overview","title":"Overview","text":"<p>The <code>ReLUSquaredActivation</code> class is a PyTorch neural network module that implements a custom activation function known as ReLU\u00b2. This activation function is introduced in the What You See Is What You Get paper by Kim, Y., &amp; Bengio, S., and they prove it to be an important enhancement in the stability of Neural Network Training.</p> <p>This activation layer applies the ReLU (Rectified Linear Unit) function to the input and then squares the result. Thus, it can only result in non-negative outputs. The squaring operation increases the emphasis on positive inputs and reduces the effect of small inputs, aiding in reducing the outliers effect and better focusing the network on meaningful inputs.</p>"},{"location":"zeta/nn/modules/relusquaredactivation/#class-definition","title":"Class Definition","text":"<pre><code>class ReLUSquaredActivation(nn.Module):\n    \"\"\"\n    Applies the relu^2 activation introduced in https://arxiv.org/abs/2109.08668v2\n    \"\"\"\n\n    def forward(self, input):\n        relu_applied = nn.functional.relu(input)\n        squared = torch.square(relu_applied)\n        return squared\n</code></pre>"},{"location":"zeta/nn/modules/relusquaredactivation/#class-relusquaredactivation","title":"<code>class ReLUSquaredActivation</code>","text":"<p>This is the class constructor that creates an instance of the <code>ReLUSquaredActivation</code> class.</p> <p>The <code>ReLUSquaredActivation</code> class extends <code>nn.Module</code>, the base class for all neural network modules in PyTorch. It does not accept any parameters.</p>"},{"location":"zeta/nn/modules/relusquaredactivation/#forwardself-input","title":"<code>forward(self, input)</code>","text":"<p>This is the forward pass of the ReLUSquaredActivation module. It's where the computation happens. This method does not have to be explicitly called, and it can be run by calling the instance of the class. </p> Argument Type Description <code>input</code> Tensor The input tensor on which the relu squared operation is to be applied. <p>It applies the <code>ReLU</code> activation function on the input tensor and then squares the result. It returns a tensor with the same shape as the input tensor, with the ReLU\u00b2 activation applied.</p>"},{"location":"zeta/nn/modules/relusquaredactivation/#example-usage","title":"Example Usage","text":"<pre><code># Importing the essential libraries\nimport torch\nimport torch.nn as nn\n\nfrom zeta.nn import ReLUSquaredActivation\n\n# Creating random torch tensor for input\ninput_tensor = torch.randn((2, 2))\n\n# Creating an instance of module\nrelu_squared_activation = ReLUSquaredActivation()\n\n# Applying the module to input tensor\noutput_tensor = relu_squared_activation(input_tensor)\n\nprint(\"Input Tensor:\")\nprint(input_tensor)\nprint(\"Output Tensor:\")\nprint(output_tensor)\n</code></pre> <p>In this example, we first import the necessary libraries. We then create an instance of <code>ReLUSquaredActivation</code>. After creating this instance, you can use it as a function to apply the ReLU\u00b2 activation to the input tensor. </p> <p>In the resulting output tensor, the activation function is applied elementwise, meaning that every single value in the tensor has the activation function applied independently. This means that the shape of the output tensor is identical to the shape of the input tensor.</p>"},{"location":"zeta/nn/modules/relusquaredactivation/#additional-information","title":"Additional Information","text":"<p>The <code>ReLUSquaredActivation</code> is a simple yet powerful activation layer that can provide increased performance in certain types of neural networks. However, like all tools, it is important to use it in the right context and understand that it might not always lead to the best results depending on the specific problem and data at hand.</p> <p>Note that the <code>ReLUSquaredActivation</code> extends the <code>nn.Module</code> class, which is the fundamental building block in PyTorch. It forms part of a larger toolkit for building and running neural networks, and there are many other types of modules available in the <code>torch.nn</code> library that you might find useful.</p>"},{"location":"zeta/nn/modules/rms_norm/","title":"<code>RMSNorm</code> Documentation","text":""},{"location":"zeta/nn/modules/rms_norm/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Purpose and Functionality</li> <li>Class: <code>RMSNorm</code></li> <li>Initialization</li> <li>Parameters</li> <li>Forward Method</li> <li>Usage Examples</li> <li>Using the <code>RMSNorm</code> Class</li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/nn/modules/rms_norm/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the Zeta documentation! In this documentation, we will explore the <code>RMSNorm</code> class, a part of the Zeta library. The <code>RMSNorm</code> class is designed to perform Root Mean Square Normalization (RMSNorm) on input tensors. This documentation provides a comprehensive understanding of the purpose, functionality, and usage of the <code>RMSNorm</code> class.</p>"},{"location":"zeta/nn/modules/rms_norm/#2-purpose-and-functionality","title":"2. Purpose and Functionality","text":"<p>The <code>RMSNorm</code> class implements the Root Mean Square Normalization (RMSNorm) technique. RMSNorm is a normalization technique that helps stabilize the training of neural networks. It is particularly useful when dealing with deep neural networks, where gradients can vanish or explode during training.</p> <p>RMSNorm works by normalizing the input tensor to have unit variance along a specified dimension, typically the feature dimension. This normalization helps prevent issues like gradient explosion and can lead to faster and more stable convergence during training.</p>"},{"location":"zeta/nn/modules/rms_norm/#3-class-rmsnorm","title":"3. Class: <code>RMSNorm</code>","text":"<p>The <code>RMSNorm</code> class implements the RMSNorm normalization technique. Let's dive into its details.</p>"},{"location":"zeta/nn/modules/rms_norm/#initialization","title":"Initialization","text":"<p>To create an instance of the <code>RMSNorm</code> class, you need to specify the following parameters:</p> <pre><code>RMSNorm(dim, groups=1)\n</code></pre>"},{"location":"zeta/nn/modules/rms_norm/#parameters","title":"Parameters","text":"<ul> <li> <p><code>dim</code> (int): The dimensionality of the input tensor. This dimension will be normalized.</p> </li> <li> <p><code>groups</code> (int, optional): The number of groups to divide the input tensor into before normalization. This is useful when applying RMSNorm to specific subsets of features within the input tensor. Default is <code>1</code>.</p> </li> </ul>"},{"location":"zeta/nn/modules/rms_norm/#forward-method","title":"Forward Method","text":"<p>The <code>forward</code> method of the <code>RMSNorm</code> class performs the RMSNorm normalization on the input tensor.</p> <pre><code>def forward(x):\n    # ...\n    return normed * self.scale * self.gamma\n</code></pre>"},{"location":"zeta/nn/modules/rms_norm/#4-usage-examples","title":"4. Usage Examples","text":"<p>Let's explore how to use the <code>RMSNorm</code> class effectively in various scenarios.</p>"},{"location":"zeta/nn/modules/rms_norm/#using-the-rmsnorm-class","title":"Using the <code>RMSNorm</code> Class","text":"<p>Here's how to use the <code>RMSNorm</code> class to perform RMSNorm normalization on an input tensor:</p> <pre><code>import torch\n\nfrom zeta.nn import RMSNorm\n\n# Create an instance of RMSNorm\nrms_norm = RMSNorm(dim=512, groups=1)\n\n# Create an input tensor\ninput_tensor = torch.randn(\n    2, 512, 4, 4\n)  # Example input tensor with shape (batch_size, channels, height, width)\n\n# Apply RMSNorm normalization\nnormalized_tensor = rms_norm(input_tensor)\n</code></pre>"},{"location":"zeta/nn/modules/rms_norm/#5-additional-information","title":"5. Additional Information","text":"<p>RMSNorm is a powerful technique for normalizing neural network activations during training. Here are a few additional notes:</p> <ul> <li> <p>Normalization Dimension (<code>dim</code>): The <code>dim</code> parameter specifies the dimension along which the input tensor will be normalized. It is typically set to the feature dimension (e.g., channels in a convolutional neural network).</p> </li> <li> <p>Grouped Normalization (<code>groups</code>): The <code>groups</code> parameter allows you to divide the input tensor into groups before normalization. This can be useful when you want to apply normalization to specific subsets of features within the input tensor.</p> </li> </ul>"},{"location":"zeta/nn/modules/rms_norm/#6-references","title":"6. References","text":"<p>For further information on Root Mean Square Normalization (RMSNorm) and related concepts, you can refer to the following resources:</p> <ul> <li> <p>Layer Normalization - The original paper introducing Layer Normalization, which is a related normalization technique.</p> </li> <li> <p>PyTorch Documentation - Official PyTorch documentation for related functions and modules.</p> </li> </ul> <p>This documentation provides a comprehensive overview of the Zeta library's <code>RMSNorm</code> class. It aims to help you understand the purpose, functionality, and usage of the <code>RMSNorm</code> class for normalization in neural networks.</p>"},{"location":"zeta/nn/modules/siglip/","title":"SigLipLoss Documentation","text":""},{"location":"zeta/nn/modules/siglip/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Overview</li> <li>Installation</li> <li>Usage</li> <li>Initializing SigLipLoss</li> <li>Calculating Loss</li> <li>Multi-process Communication</li> <li>Examples</li> <li>Example 1: Initializing SigLipLoss</li> <li>Example 2: Calculating Loss</li> <li>Example 3: Multi-process Communication</li> <li>Additional Information</li> <li>Conclusion</li> </ol>"},{"location":"zeta/nn/modules/siglip/#1-introduction","title":"1. Introduction","text":"<p>The <code>SigLipLoss</code> module is a component of the SigLIP (Sigmoid Loss for Language Image Pre-Training) framework, designed to facilitate efficient training of models for language-image pre-training tasks. SigLIP is particularly useful for scenarios where you need to pre-train a model to understand the relationship between text and images.</p> <p>This documentation provides a comprehensive guide to using the <code>SigLipLoss</code> module, including its purpose, parameters, and usage examples.</p>"},{"location":"zeta/nn/modules/siglip/#2-overview","title":"2. Overview","text":"<p>The <code>SigLipLoss</code> module is used to compute the loss for training models in the SigLIP framework. It calculates the contrastive loss between image and text features, which is a fundamental component of the SigLIP training process.</p> <p>Key features and parameters of the <code>SigLipLoss</code> module include: - <code>cache_labels</code>: Whether to cache labels for faster computation. - <code>rank</code>: The rank of the current process when using multi-process training. - <code>world_size</code>: The number of processes in multi-process training. - <code>bidir</code>: Whether to use bidirectional communication during training. - <code>use_horovod</code>: Whether to use Horovod for distributed training.</p> <p>The SigLIP framework is based on the Sigmoid Loss for Language Image Pre-Training research paper, which provides more detailed information about the approach. You can find the paper here.</p>"},{"location":"zeta/nn/modules/siglip/#3-installation","title":"3. Installation","text":"<p>Before using the <code>SigLipLoss</code> module, make sure you have the necessary dependencies installed. You can install the module using pip:</p> <pre><code>pip install sigliploss\n</code></pre>"},{"location":"zeta/nn/modules/siglip/#4-usage","title":"4. Usage","text":"<p>In this section, we'll cover how to use the <code>SigLipLoss</code> module effectively.</p>"},{"location":"zeta/nn/modules/siglip/#41-initializing-sigliploss","title":"4.1. Initializing SigLipLoss","text":"<p>To use the <code>SigLipLoss</code> module, you first need to initialize it. You can provide optional parameters like <code>cache_labels</code>, <code>rank</code>, <code>world_size</code>, <code>bidir</code>, and <code>use_horovod</code> during initialization.</p> <pre><code>from zeta.nn.modules import SigLipLoss\n\n# Initialize SigLipLoss module\nloss = SigLipLoss(\n    cache_labels=False, rank=0, world_size=1, bidir=True, use_horovod=False\n)\n</code></pre>"},{"location":"zeta/nn/modules/siglip/#42-calculating-loss","title":"4.2. Calculating Loss","text":"<p>The primary purpose of the <code>SigLipLoss</code> module is to calculate the contrastive loss between image and text features. You'll need to provide image features, text features, <code>logit_scale</code>, and <code>logit_bias</code> to calculate the loss.</p> <pre><code># Example data\nimage_features = torch.randn(10, 128)\ntext_features = torch.randn(10, 128)\nlogit_scale = 1.0\nlogit_bias = None\n\n# Calculate loss\noutputs = loss(image_features, text_features, logit_scale, logit_bias)\nprint(outputs)\n</code></pre>"},{"location":"zeta/nn/modules/siglip/#43-multi-process-communication","title":"4.3. Multi-process Communication","text":"<p>If you're using multi-process training, <code>SigLipLoss</code> provides options for communication between processes. The module can exchange text features between processes to facilitate training. Use the <code>rank</code>, <code>world_size</code>, <code>bidir</code>, and <code>use_horovod</code> parameters to configure this behavior.</p>"},{"location":"zeta/nn/modules/siglip/#5-examples","title":"5. Examples","text":"<p>Let's dive into some examples to demonstrate how to use the <code>SigLipLoss</code> module in practice.</p>"},{"location":"zeta/nn/modules/siglip/#51-example-1-initializing-sigliploss","title":"5.1. Example 1: Initializing SigLipLoss","text":"<p>In this example, we'll initialize the <code>SigLipLoss</code> module with default parameters.</p> <pre><code>from zeta.nn.modules. import SigLipLoss\n\n# Initialize SigLipLoss module\nloss = SigLipLoss()\n</code></pre>"},{"location":"zeta/nn/modules/siglip/#52-example-2-calculating-loss","title":"5.2. Example 2: Calculating Loss","text":"<p>Now, let's calculate the loss using sample image and text features.</p> <pre><code>import torch\nfrom zeta.nn.modules. import SigLipLoss\n\n# Initialize SigLipLoss module\nloss = SigLipLoss()\n\n# Example data\nimage_features = torch.randn(10, 128)\ntext_features = torch.randn(10, 128)\nlogit_scale = 1.0\nlogit_bias = None\n\n# Calculate loss\noutputs = loss(image_features, text_features, logit_scale, logit_bias)\nprint(outputs)\n</code></pre>"},{"location":"zeta/nn/modules/siglip/#53-example-3-multi-process-communication","title":"5.3. Example 3: Multi-process Communication","text":"<p>In a multi-process training scenario, you can configure <code>SigLipLoss</code> for communication between processes. Here's an example:</p> <pre><code>from zeta.nn.modules. import SigLipLoss\n\n# Initialize SigLipLoss module with multi-process settings\nloss = SigLipLoss(rank=0, world_size=4, bidir=True, use_horovod=False)\n</code></pre>"},{"location":"zeta/nn/modules/siglip/#6-additional-information","title":"6. Additional Information","text":"<ul> <li>SigLIP Framework: SigLIP (Sigmoid Loss for Language Image Pre-Training) is a research framework for efficient language-image pre-training. Refer to the research paper for in-depth information.</li> <li>Training: The <code>SigLipLoss</code> module is designed for training models within the SigLIP framework.</li> <li>Multi-process Training: It provides options for communication between processes during multi-process training.</li> </ul>"},{"location":"zeta/nn/modules/siglip/#7-conclusion","title":"7. Conclusion","text":"<p>The <code>SigLipLoss</code> module is a critical component of the SigLIP framework, enabling efficient training of models for language-image pre-training tasks. This documentation provides a detailed guide on its usage, parameters, and examples to help you integrate it into your projects effectively.</p>"},{"location":"zeta/nn/modules/simple_feedback/","title":"SimpleFeedForward: Feedforward Neural Network with LayerNorm and GELU Activations","text":"<p>Overview and Introduction</p> <p>The <code>SimpleFeedForward</code> function is a utility function that creates a feedforward neural network architecture with layer normalization (<code>LayerNorm</code>) and Gaussian Error Linear Unit (<code>GELU</code>) activations. The architecture is particularly well-suited for applications in deep learning where input feature normalization and non-linear transformations are essential for effective model training and generalization.</p> <p>Main Features:</p> <ul> <li> <p>Layer Normalization: Normalizes the input data across the feature dimension, ensuring that the input to each subsequent layer has a stable distribution. This aids in faster and more stable convergence during training.</p> </li> <li> <p>GELU Activation: A smooth activation function that is used for better performance in deeper architectures, especially transformer models.</p> </li> <li> <p>Dropout: A regularizing technique where randomly selected neurons are ignored during training, reducing overfitting and improving model generalization.</p> </li> </ul> <p>Function Definition:</p> <pre><code>def SimpleFeedForward(dim: int, hidden_dim: int, dropout: float = 0.1) -&gt; nn.Sequential:\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>dim</code> int -- Input dimension of the neural network. <code>hidden_dim</code> int -- Hidden layer dimension of the neural network. <code>dropout</code> float 0.1 Dropout probability for regularization. <p>Functionality and Usage:</p> <p>The <code>SimpleFeedForward</code> function constructs a neural network that consists of the following sequence of operations: 1. Layer normalization of the input features. 2. A linear transformation that expands the input to a specified hidden dimension. 3. GELU activation function. 4. Another linear transformation that maps the hidden layer back to the original input dimension. 5. Dropout for regularization.</p> <p>This particular sequence ensures that the neural network can learn a rich representation from the input features while being regularized to prevent overfitting.</p> <p>Usage Examples:</p> <ol> <li>Basic Usage:</li> </ol> <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn.modules import SimpleFeedForward\n\nmodel = SimpleFeedForward(768, 2048, 0.1)\nx = torch.randn(1, 768)\noutput = model(x)\nprint(output.shape)  # torch.Size([1, 768])\n</code></pre> <ol> <li>Integrating with Other Architectures:</li> </ol> <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn.modules import SimpleFeedForward\n\n\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ff = SimpleFeedForward(768, 2048, 0.1)\n        self.final_layer = nn.Linear(768, 10)  # Example output layer\n\n    def forward(self, x):\n        x = self.ff(x)\n        return self.final_layer(x)\n\n\nmodel = CustomModel()\nx = torch.randn(1, 768)\noutput = model(x)\nprint(output.shape)  # torch.Size([1, 10])\n</code></pre> <ol> <li>Using Different Dropout Values:</li> </ol> <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn.modules import SimpleFeedForward\n\nmodel = SimpleFeedForward(768, 2048, 0.5)  # Setting a higher dropout value\nx = torch.randn(1, 768)\noutput = model(x)\nprint(output.shape)  # torch.Size([1, 768])\n</code></pre> <p>Additional Information and Tips:</p> <ul> <li> <p>For tasks where overfitting is a concern, consider increasing the <code>dropout</code> parameter value to introduce more regularization.</p> </li> <li> <p>The function returns an <code>nn.Sequential</code> model, making it easy to integrate into larger architectures or pipelines.</p> </li> <li> <p>Remember that the effective capacity of the model is determined by the <code>hidden_dim</code> parameter. Adjusting this can help in balancing model complexity and performance.</p> </li> </ul> <p>References and Resources:</p> <ul> <li> <p>Layer Normalization Paper</p> </li> <li> <p>GELU Activation Function</p> </li> </ul>"},{"location":"zeta/nn/modules/slerpmodelmerger/","title":"SLERPModelMerger","text":"<ul> <li>Description:  SLERPModelMerger is a Python class that performs model merging using Spherical Linear Interpolation (SLERP). Interpolation is a process of finding a value between two points on a line or curve to create new geometries. Spherical Linear Interpolation (SLERP) is a method of interpolation where the model weights are visualized on a hypersphere, and the interpolated weight is obtained by moving along the geodesic (or the shortest path) on the hypersphere. This class is implemented under the PyTorch framework.</li> </ul> <p>The class can blend or interpolate the weights of two trained models, allowing one to create an ensemble or composite model of the input models, essentially capturing the strengths of both. In ML terminology, this can be thought of as a \"committee machine\" where transformations applied to input data by multiple models are combined to produce a single output. This method is known to improve the robustness and performance of models, especially in scenarios where the strength of individual models varies across different sections of the input space.</p> <ul> <li>Class Definition: </li> </ul> <p>Here is the class definition:</p> <pre><code>class SLERPModelMerger(nn.Module):\n    @enforce_types\n    def __init__(self, model1: nn.Module, model2: nn.Module, t: float = 0.5):\n\n    def merge(self) -&gt; nn.Module:\n\n    @staticmethod\n    @enforce_types\n    def _slerp(w1: Tensor, w2: Tensor, t: float) -&gt; Tensor:\n\n    @staticmethod\n    @enforce_types\n    def _copy_model_structure(model: nn.Module) -&gt; nn.Module:\n</code></pre> <ul> <li> <p>Parameters: <code>model1</code> and <code>model2</code> are instances of PyTorch's neural network models (such as instances of <code>nn.Linear, nn.Conv2d</code> etc.) between which weights' interpolation is to be done. The parameter <code>t</code> is the interpolation parameter that ranges from 0 (model1) to 1 (model2), indicating the weightage given to the two models during interpolation. Hence, for t=0, the resulting model would be the same as model1, and for t=1, the resulting model would be the same as model2.</p> </li> <li> <p>Methods:</p> <ul> <li> <p><code>merge()</code> : This method merges the input models (<code>model1</code> and <code>model2</code>), according to the interpolation parameter <code>t</code>. The merging is done by interpolating the weights of the two models using Spherical Linear Interpolation (SLERP).</p> </li> <li> <p><code>_slerp(w1: Tensor, w2: Tensor, t: float) -&gt; Tensor:</code> : This method performs Spherical Linear Interpolation (SLERP) between two tensors.</p> </li> <li> <p><code>_copy_model_structure(model: nn.Module) -&gt; nn.Module:</code> : This method creates a new instance of a model with the same structure as the given model.</p> </li> </ul> </li> <li> <p>Usage:</p> </li> </ul> <p>The following code shows how to use the SLERPModelMerger class to merge two PyTorch models (in this case two linear models):</p> <pre><code>import torch.nn as nn\n\nfrom zeta.nn import SLERPModelMerger\n\nmodel1 = nn.Linear(10, 10)\nmodel2 = nn.Linear(10, 10)\n\nmerger = SLERPModelMerger(model1, model2, 0.5)\nmerged_model = merger.merge()\n\n# This will output the merged state_dict\nprint(merged_model.state_dict())\n</code></pre> <p>The prints statement will output the state_dict of the merged model. The state_dict is a Python dictionary that maps each layer to its corresponding parameters (tensors). </p> <p>The weightage given to the two models for interpolation is specified by the interpolation parameter <code>t</code>. As t ranges from 0 to 1, we can see the merged model evolve from model1 to model2. Thus, by changing <code>t</code> we can generate a spectrum of models from model1 to model2.</p> <p>This gives us a strategy to generate an ensemble of models by interpolating between two carefully chosen base models. This ensemble could then be used for model selection or for creating a more robust composite model.</p> <ul> <li> <p>References:</p> <ul> <li>Ken Shoemake. Animating rotation with quaternion curves. In ACM SIGGRAPH Computer Graphics, volume 19, pp. 245\u2013254. ACM, 1985.</li> </ul> </li> </ul> <p>Remarks: Remember, while PyTorch models accept parameters as single arguments to their constructors, this is not the case with all models. Some models might accept parameters as lists, sets, or other non-single-parameter-type objects. As such, additional pre-processing or configuration might be needed if using those models with SLERPModelMerger. Try these different configurations and methods to find the one that best suits your requirements.</p>"},{"location":"zeta/nn/modules/spatial_downsample/","title":"<code>SpatialDownsample</code> Documentation","text":""},{"location":"zeta/nn/modules/spatial_downsample/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Overview</li> <li>SpatialDownsample Class</li> <li>Initialization Parameters</li> <li>Functionality and Usage</li> <li>Forward Method</li> <li>Utility Functions</li> <li>Examples</li> <li>Example 1: Creating a SpatialDownsample Module</li> <li>Example 2: Using SpatialDownsample for Downsampling</li> <li>Additional Information</li> <li>References and Resources</li> </ol>"},{"location":"zeta/nn/modules/spatial_downsample/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the documentation for the Zeta library. This documentation will provide you with comprehensive information on the Zeta library, specifically focusing on the <code>SpatialDownsample</code> class. Before we dive into the details, let's understand the purpose and significance of this library.</p>"},{"location":"zeta/nn/modules/spatial_downsample/#11-purpose","title":"1.1 Purpose","text":"<p>The Zeta library is designed to provide essential building blocks for deep learning architectures, making it easier for researchers and developers to implement complex models. It offers various modules and utilities, including the <code>SpatialDownsample</code> class, which is a key component for downsampling spatial dimensions in neural networks.</p>"},{"location":"zeta/nn/modules/spatial_downsample/#12-key-features","title":"1.2 Key Features","text":"<ul> <li> <p>Spatial Downsampling: The <code>SpatialDownsample</code> class allows you to efficiently reduce the spatial dimensions of your data, which is crucial for various computer vision tasks.</p> </li> <li> <p>Integration: Zeta modules seamlessly integrate with popular deep learning frameworks like PyTorch, enabling you to incorporate them into your projects effortlessly.</p> </li> </ul>"},{"location":"zeta/nn/modules/spatial_downsample/#2-overview","title":"2. Overview","text":"<p>The Zeta library aims to simplify deep learning model development by providing modular components that adhere to best practices in the field. One such component is the <code>SpatialDownsample</code> class.</p>"},{"location":"zeta/nn/modules/spatial_downsample/#21-spatialdownsample-class","title":"2.1 <code>SpatialDownsample</code> Class","text":"<p>The <code>SpatialDownsample</code> class is a module designed for spatial downsampling of 3D tensors. It plays a critical role in architectures like ResNet, where downsampling is necessary to reduce spatial dimensions while increasing the number of channels.</p> <p>In the following sections, we will explore the <code>SpatialDownsample</code> class's definition, initialization parameters, functionality, and usage.</p>"},{"location":"zeta/nn/modules/spatial_downsample/#3-spatialdownsample-class","title":"3. SpatialDownsample Class","text":"<p>The <code>SpatialDownsample</code> class is at the core of Zeta, providing spatial downsampling capabilities for 3D tensors.</p>"},{"location":"zeta/nn/modules/spatial_downsample/#31-initialization-parameters","title":"3.1 Initialization Parameters","text":"<p>Here are the initialization parameters for the <code>SpatialDownsample</code> class:</p> <ul> <li> <p><code>dim</code> (int): The number of input channels in the tensor.</p> </li> <li> <p><code>dim_out</code> (int, optional): The number of output channels in the tensor after downsampling. If not specified, it defaults to the same as <code>dim</code>.</p> </li> <li> <p><code>kernel_size</code> (int): The size of the kernel used for downsampling. It determines the amount of spatial reduction in the output tensor.</p> </li> </ul>"},{"location":"zeta/nn/modules/spatial_downsample/#32-methods","title":"3.2 Methods","text":"<p>The primary method of the <code>SpatialDownsample</code> class is the <code>forward</code> method, which performs the spatial downsampling operation on input tensors.</p>"},{"location":"zeta/nn/modules/spatial_downsample/#4-functionality-and-usage","title":"4. Functionality and Usage","text":"<p>Let's delve into the functionality and usage of the <code>SpatialDownsample</code> class.</p>"},{"location":"zeta/nn/modules/spatial_downsample/#41-forward-method","title":"4.1 Forward Method","text":"<p>The <code>forward</code> method of the <code>SpatialDownsample</code> class takes an input tensor and applies spatial downsampling using a convolution operation. Here are the parameters:</p> <ul> <li><code>x</code> (Tensor): The input tensor of shape <code>(batch, channels, time, height, width)</code>.</li> </ul> <p>The method returns a downsampled tensor of shape <code>(batch, output_channels, time, height, width)</code>.</p>"},{"location":"zeta/nn/modules/spatial_downsample/#42-usage-examples","title":"4.2 Usage Examples","text":""},{"location":"zeta/nn/modules/spatial_downsample/#example-1-creating-a-spatialdownsample-module","title":"Example 1: Creating a SpatialDownsample Module","text":"<p>In this example, we create an instance of the <code>SpatialDownsample</code> class with default settings:</p> <pre><code>downsample = SpatialDownsample(dim=64, kernel_size=3)\n</code></pre>"},{"location":"zeta/nn/modules/spatial_downsample/#example-2-using-spatialdownsample-for-downsampling","title":"Example 2: Using SpatialDownsample for Downsampling","text":"<p>Here, we demonstrate how to use the <code>SpatialDownsample</code> module for downsampling an input tensor:</p> <pre><code>downsample = SpatialDownsample(dim=64, kernel_size=3)\ninput_data = torch.randn(1, 64, 32, 32)\noutput = downsample(input_data)\nprint(output.shape)\n</code></pre>"},{"location":"zeta/nn/modules/spatial_downsample/#5-utility-functions","title":"5. Utility Functions","text":"<p>The Zeta library also provides a set of utility functions used within the modules. These utility functions, such as <code>exists</code>, <code>default</code>, <code>identity</code>, and more, contribute to the modularity and flexibility of the library.</p>"},{"location":"zeta/nn/modules/spatial_downsample/#6-additional-information","title":"6. Additional Information","text":"<p>Here are some additional tips and information for using the Zeta library and the <code>SpatialDownsample</code> class effectively:</p> <ul> <li> <p>Experiment with different kernel sizes to control the amount of downsampling according to your specific model requirements.</p> </li> <li> <p>Ensure that the input tensor (<code>x</code>) has the appropriate shape <code>(batch, channels, time, height, width)</code>.</p> </li> </ul>"},{"location":"zeta/nn/modules/spatial_downsample/#7-references-and-resources","title":"7. References and Resources","text":"<p>For further information and resources related to the Zeta library and deep learning, please refer to the following:</p> <ul> <li> <p>Zeta GitHub Repository: The official Zeta repository for updates and contributions.</p> </li> <li> <p>ResNet Paper: The original ResNet paper that introduces the concept of spatial downsampling.</p> </li> <li> <p>PyTorch Official Website: The official website for PyTorch, the deep learning framework used in Zeta.</p> </li> </ul> <p>This concludes the documentation for the Zeta library and the <code>SpatialDownsample</code> class. You now have a comprehensive understanding of how to use this library and module for your deep learning projects. If you have any further questions or need assistance, please refer to the provided references and resources. Happy modeling with Zeta!</p>"},{"location":"zeta/nn/modules/ssm/","title":"SSM (Selective Scanning Module) Documentation","text":""},{"location":"zeta/nn/modules/ssm/#overview","title":"Overview","text":"<p>The SSM (Selective Scanning Module) is a PyTorch-based module designed for selective scanning of input data. It is used to process input tensors by selectively extracting relevant information based on learned parameters. This documentation provides a comprehensive guide to understand, use, and maximize the functionality of the SSM module when imported from the <code>zeta.nn</code> library.</p>"},{"location":"zeta/nn/modules/ssm/#class-definition","title":"Class Definition","text":""},{"location":"zeta/nn/modules/ssm/#ssm-class","title":"<code>SSM</code> Class","text":""},{"location":"zeta/nn/modules/ssm/#constructor-parameters","title":"Constructor Parameters","text":"<ul> <li><code>in_features</code> (int): Size of the input features.</li> <li><code>dt_rank</code> (int): Rank of the dt projection.</li> <li><code>dim_inner</code> (int): Inner dimension of the dt projection.</li> <li><code>d_state</code> (int): Dimension of the state.</li> </ul>"},{"location":"zeta/nn/modules/ssm/#methods","title":"Methods","text":""},{"location":"zeta/nn/modules/ssm/#forward-method","title":"<code>forward</code> Method","text":""},{"location":"zeta/nn/modules/ssm/#method-parameters","title":"Method Parameters","text":"<ul> <li><code>x</code> (torch.Tensor): Input tensor.</li> <li><code>pscan</code> (bool, optional): Whether to use selective_scan or selective_scan_seq. (default: True)</li> </ul>"},{"location":"zeta/nn/modules/ssm/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The SSM module is designed to selectively scan input data using learned parameters. Here's how it works:</p> <ol> <li> <p>Initialization: The <code>SSM</code> class is initialized with parameters like <code>in_features</code>, <code>dt_rank</code>, <code>dim_inner</code>, and <code>d_state</code>.</p> </li> <li> <p>Forward Pass: The <code>forward</code> method performs the core operation of selective scanning.</p> </li> <li> <p>Selective Scanning Modes: The <code>pscan</code> parameter determines whether to use <code>selective_scan</code> or <code>selective_scan_seq</code> for the scanning process.</p> </li> </ol>"},{"location":"zeta/nn/modules/ssm/#example-usage","title":"Example Usage","text":"<p>Here are multiple usage examples of the SSM module importing it from the <code>zeta.nn</code> library:</p> <pre><code>import torch\n\n# Import SSM from zeta.nn\nfrom zeta.nn import SSM\n\n# Example 1: Creating an SSM instance\nssm = SSM(in_features=128, dt_rank=16, dim_inner=32, d_state=64)\n\n# Example 2: Forward pass with selective_scan\noutput = ssm(torch.randn(10, 128))  # Output tensor after selective scanning\n\n# Example 3: Forward pass with selective_scan_seq\noutput_seq = ssm(torch.randn(10, 128), pscan=False)  # Output using selective_scan_seq\n</code></pre>"},{"location":"zeta/nn/modules/ssm/#additional-information","title":"Additional Information","text":"<ul> <li>The SSM module is designed to enhance the selective extraction of information from input data.</li> <li>You can customize its behavior by adjusting parameters during initialization.</li> <li>If you need to perform selective scanning in a sequential manner, set <code>pscan</code> to <code>False</code> in the <code>forward</code> method.</li> </ul> <p>For more details and advanced usage, refer to the official PyTorch documentation and relevant research papers.</p>"},{"location":"zeta/nn/modules/ssm/#references-and-resources","title":"References and Resources","text":"<ul> <li>PyTorch Official Documentation</li> <li>Research Paper: Selective Scanning Networks</li> </ul>"},{"location":"zeta/nn/modules/stochasticskipblock/","title":"Module Name: StochasticSkipBlock","text":""},{"location":"zeta/nn/modules/stochasticskipblock/#overview-and-introduction","title":"Overview and Introduction:","text":"<p>Tabular Deep Learning models sometimes struggle with overfitting on noisy data. Stochastic Skip Block is a PyTorch module designed to combat this problem by introducing stochasticity in between the network layers. This module applies an innovative concept of skipping certain layers during training with a defined probability, thereby creating a diverse set of thinner networks.</p> <p>Given a set of layers encapsulated in a module, the <code>StochasticSkipBlock</code> will either apply this module to the input or return the input directly bypassing the module completely. The decision whether to apply or skip the module is randomized with a user-defined probability. This way the model creates uncertainty and works as an efficient regularizer preventing overfitting on training data. Moreover, it contributes to faster convergence during training and better generalization in prediction phase.</p>"},{"location":"zeta/nn/modules/stochasticskipblock/#class-definition","title":"Class Definition:","text":"<p>Below is the class definition for the module:</p> <pre><code>class StochasticSkipBlock(nn.Module):\n    \"\"\"\n    A module that implements stochastic skip connections in a neural network.\n\n    Args:\n        sb1 (nn.Module): The module to be skipped with a certain probability.\n        p (float): The probability of skipping the module. Default is 0.5.\n\n    Returns:\n        torch.Tensor: The output tensor after applying the stochastic skip connection.\n    \"\"\"\n\n    def __init__(self, sb1, p=0.5):\n        super().__init__()\n        self.sb1 = sb1\n        self.p = p\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Forward pass of the StochasticSkipBlock.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the module.\n        \"\"\"\n        if self.training and torch.rand(1).item() &lt; self.p:\n            return x  # Skip the sb1\n        else:\n            return self.sb1(x)\n</code></pre>"},{"location":"zeta/nn/modules/stochasticskipblock/#parameters","title":"Parameters","text":"Argument Default Description <code>sb1</code> None The layers encapsulated in <code>nn.Module</code> object to be skipped with a certain probability. <code>p</code> 0.5 The probability of skipping the module."},{"location":"zeta/nn/modules/stochasticskipblock/#use-cases","title":"Use Cases","text":""},{"location":"zeta/nn/modules/stochasticskipblock/#use-case-1-basic-usage","title":"Use Case 1: Basic Usage","text":"<p>This is a basic example of using <code>StochasticSkipBlock</code> in a feed forward neural network.</p> <p>First, you need to import the necessary module:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.nn.functional import relu\n\nfrom zeta.nn import StochasticSkipBlock\n</code></pre> <p>Now, you need to define the architecture of the model:</p> <pre><code>class MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(10, 20)\n        self.layer2 = StochasticSkipBlock(\n            nn.Sequential(nn.Linear(20, 20), nn.ReLU()), p=0.5\n        )  # 50% chance to skip the subsequence of layers\n        self.layer3 = nn.Linear(20, 1)\n\n    def forward(self, x):\n        x = relu(self.layer1(x))\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return x\n</code></pre> <p>Now, you can instantiate your model:</p> <pre><code>model = MyModel()\ninput = torch.randn(32, 10)\noutput = model(input)\n</code></pre>"},{"location":"zeta/nn/modules/stochasticskipblock/#use-case-2-convolutional-neural-network","title":"Use Case 2: Convolutional Neural Network","text":"<p>This example shows how to embed <code>StochasticSkipBlock</code> in between convolutional layers of a CNN model.</p> <pre><code>class MyCNNModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n        self.conv2 = StochasticSkipBlock(nn.Conv2d(32, 64, kernel_size=5), p=0.6)\n        self.fc1 = nn.Linear(64 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(self.conv2(x), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n</code></pre>"},{"location":"zeta/nn/modules/stochasticskipblock/#use-case-3-training-the-model-using-dataloader","title":"Use Case 3: Training the model using DataLoader","text":"<p>This shows how to train the model using StochasticSkipBlock module. Please note, This example assumes you have your dataloader ('train_dataloader') ready with training data.</p> <pre><code>import torch.optim as optim\nfrom torch.nn.functional import binary_cross_entropy\nfrom torch.optim import SGD\n\nfrom zeta.nn import StochasticSkipBlock\n\n# initiate model\nmodel = MyModel()\n\n# defining loss function\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\nfor epoch in range(50):  # loop over the dataset\n    running_loss = 0.0\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    print(\"Epoch %d loss: %.3f\" % (epoch + 1, running_loss))\n\nprint(\"Finished Training\")\n</code></pre>"},{"location":"zeta/nn/modules/stochasticskipblock/#additional-tips","title":"Additional Tips","text":"<p>To get the most out of the StochasticSkipBlock, adjust the skipping probability parameter <code>p</code>. A higher probability means there's more chance a layer will be skipped during the training phase. Experiment with different values of <code>p</code> to find the optimal one that gives your model the best result.</p> <p>The <code>StochasticSkipBlock</code> module introduces randomness in your model's training process; therefore, results might vary slightly each time you train your model. Consider setting a seed for your PyTorch application to ensure reproducibility.</p>"},{"location":"zeta/nn/modules/stochasticskipblock/#conclusion","title":"Conclusion","text":"<p>StochasticSkipBlock is a flexible module that makes it easy to introduce stochasticity into your model's architecture, acting as a regularizer that could improve your model's performance. It's important to experiment with this module to see how much randomness helps your specific use case.</p>"},{"location":"zeta/nn/modules/stochasticskipblock/#references","title":"References","text":"<ol> <li>Deep Networks with Stochastic Depth</li> <li>Understanding the difficulty of training deep feedforward neural networks</li> <li>Maxout Networks</li> </ol>"},{"location":"zeta/nn/modules/stochdepth/","title":"Module/Function Name: StochDepth","text":"<p>class torch.nn.StochDepth(stochdepth_rate):     <pre><code>Initializes the Stochastic Depth module that applies a stochastic binary mask to the input tensor.\n\nParameters:\n- stochdepth_rate (float): The probability of dropping each input activation.\n</code></pre></p> <pre><code>def forward(x):\n    \"\"\"\n    Forward pass of the Stochastic Depth module. Applies a stochastic rate of dropout to the input tensor.\n\n    Args:\n    - x (Tensor): The input tensor.\n\n    Returns:\n    - Tensor: The output tensor after applying stochastic depth.\n    ```\n    if not self.training:\n        return x\n\n    batch_size = x.shape[0]\n\n    # Generating random tensor\n    rand_tensor = torch.rand(\n        batch_size,\n        1,\n        1,\n        1\n    ).type_as(x)\n\n    # Calculating the keep probability\n    keep_prob = 1 - self.stochdepth_rate\n\n    # Construct binary tensor using torch floor function\n    binary_tensor = torch.floor(rand_tensor + keep_prob)\n\n    return x * binary_tensor\n\n    ```\n\n    # Usage example:\n\n    stoch_depth = nn.StochDepth(stochdepth_rate=0.2)\n    output = stoch_depth(input)\n    \"\"\"\n</code></pre> <p>```</p>"},{"location":"zeta/nn/modules/time_up_sample/","title":"<code>TimeUpSample2x</code> Documentation","text":""},{"location":"zeta/nn/modules/time_up_sample/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Overview</li> <li>TimeUpSample2x Class</li> <li>Initialization Parameters</li> <li>Functionality and Usage</li> <li>Forward Method</li> <li>Utility Functions</li> <li>Examples</li> <li>Example 1: Creating a TimeUpSample2x Module</li> <li>Example 2: Using TimeUpSample2x for Upsampling</li> <li>Additional Information</li> <li>References and Resources</li> </ol>"},{"location":"zeta/nn/modules/time_up_sample/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the documentation for the Zeta library. This comprehensive guide provides detailed information about the Zeta library and its components, focusing on the <code>TimeUpSample2x</code> class. Before we delve into the details, it's important to understand the purpose and significance of this library.</p>"},{"location":"zeta/nn/modules/time_up_sample/#11-purpose","title":"1.1 Purpose","text":"<p>The Zeta library is designed to simplify the development of deep learning models by offering modular components and utilities. One of these components is the <code>TimeUpSample2x</code> class, which plays a crucial role in upscaling the time dimension of tensors.</p>"},{"location":"zeta/nn/modules/time_up_sample/#12-key-features","title":"1.2 Key Features","text":"<ul> <li> <p>Time Dimension Upsampling: The <code>TimeUpSample2x</code> class allows you to efficiently increase the temporal resolution of your data, which is particularly valuable in various sequential data tasks.</p> </li> <li> <p>Seamless Integration: Zeta modules seamlessly integrate with popular deep learning frameworks like PyTorch, making it easy to incorporate them into your projects.</p> </li> </ul>"},{"location":"zeta/nn/modules/time_up_sample/#2-overview","title":"2. Overview","text":"<p>The Zeta library is built with the aim of providing essential building blocks for deep learning model development. One such block is the <code>TimeUpSample2x</code> class.</p>"},{"location":"zeta/nn/modules/time_up_sample/#21-timeupsample2x-class","title":"2.1 <code>TimeUpSample2x</code> Class","text":"<p>The <code>TimeUpSample2x</code> class is a module designed for upscaling the time dimension of 3D tensors. It is useful in scenarios where increasing the temporal resolution of the data is required.</p> <p>In the following sections, we will explore the <code>TimeUpSample2x</code> class's definition, initialization parameters, functionality, and usage.</p>"},{"location":"zeta/nn/modules/time_up_sample/#3-timeupsample2x-class","title":"3. TimeUpSample2x Class","text":"<p>The <code>TimeUpSample2x</code> class is at the core of Zeta, providing the ability to increase the temporal resolution of tensors.</p>"},{"location":"zeta/nn/modules/time_up_sample/#31-initialization-parameters","title":"3.1 Initialization Parameters","text":"<p>Here are the initialization parameters for the <code>TimeUpSample2x</code> class:</p> <ul> <li> <p><code>dim</code> (int): The number of input channels in the tensor.</p> </li> <li> <p><code>dim_out</code> (int, optional): The number of output channels in the tensor after upsampling. If not specified, it defaults to the same as <code>dim</code>.</p> </li> </ul>"},{"location":"zeta/nn/modules/time_up_sample/#32-methods","title":"3.2 Methods","text":"<p>The primary method of the <code>TimeUpSample2x</code> class is the <code>forward</code> method, which performs the time dimension upsampling operation on input tensors.</p>"},{"location":"zeta/nn/modules/time_up_sample/#4-functionality-and-usage","title":"4. Functionality and Usage","text":"<p>Let's explore the functionality and usage of the <code>TimeUpSample2x</code> class.</p>"},{"location":"zeta/nn/modules/time_up_sample/#41-forward-method","title":"4.1 Forward Method","text":"<p>The <code>forward</code> method of the <code>TimeUpSample2x</code> class takes an input tensor and applies time dimension upsampling using a convolution operation. Here is the parameter:</p> <ul> <li><code>x</code> (Tensor): The input tensor of shape <code>(batch, channels, time, height, width)</code>.</li> </ul> <p>The method returns an upsampled tensor of shape <code>(batch, output_channels, time, height, width)</code>.</p>"},{"location":"zeta/nn/modules/time_up_sample/#42-usage-examples","title":"4.2 Usage Examples","text":""},{"location":"zeta/nn/modules/time_up_sample/#example-1-creating-a-timeupsample2x-module","title":"Example 1: Creating a TimeUpSample2x Module","text":"<p>In this example, we create an instance of the <code>TimeUpSample2x</code> class with default settings:</p> <pre><code>upsample = TimeUpSample2x(dim=64)\n</code></pre>"},{"location":"zeta/nn/modules/time_up_sample/#example-2-using-timeupsample2x-for-upsampling","title":"Example 2: Using TimeUpSample2x for Upsampling","text":"<p>Here, we demonstrate how to use the <code>TimeUpSample2x</code> module for upsampling an input tensor:</p> <pre><code>upsample = TimeUpSample2x(dim=64)\ninput_data = torch.randn(1, 64, 32, 32)\noutput = upsample(input_data)\nprint(output.shape)\n</code></pre>"},{"location":"zeta/nn/modules/time_up_sample/#5-utility-functions","title":"5. Utility Functions","text":"<p>The Zeta library also provides a set of utility functions used within the modules. These utility functions, such as <code>exists</code>, <code>identity</code>, <code>divisible_by</code>, and more, enhance the modularity and flexibility of the library.</p>"},{"location":"zeta/nn/modules/time_up_sample/#6-additional-information","title":"6. Additional Information","text":"<p>Here are some additional tips and information for using the Zeta library and the <code>TimeUpSample2x</code> class effectively:</p> <ul> <li> <p>Experiment with different values for the <code>dim</code> and <code>dim_out</code> parameters to control the number of channels in the output tensor.</p> </li> <li> <p>Ensure that the input tensor (<code>x</code>) has the appropriate shape <code>(batch, channels, time, height, width)</code>.</p> </li> </ul>"},{"location":"zeta/nn/modules/time_up_sample/#7-references-and-resources","title":"7. References and Resources","text":"<p>For further information and resources related to the Zeta library and deep learning, please refer to the following:</p> <ul> <li> <p>Zeta GitHub Repository: The official Zeta repository for updates and contributions.</p> </li> <li> <p>PyTorch Official Website: The official website for PyTorch, the deep learning framework used in Zeta.</p> </li> </ul> <p>This concludes the documentation for the Zeta library and the <code>TimeUpSample2x</code> class. You now have a comprehensive understanding of how to use this library and module for your deep learning projects. If you have any further questions or need assistance, please refer to the provided references and resources. Happy modeling with Zeta!</p>"},{"location":"zeta/nn/modules/token_learner/","title":"Zeta Library Documentation","text":""},{"location":"zeta/nn/modules/token_learner/#module-name-tokenlearner","title":"Module Name: TokenLearner","text":"<p>The <code>TokenLearner</code> is a PyTorch module designed for learning tokens from input data. It is a part of the Zeta library, a collection of modules and functions designed for efficient and flexible implementation of various deep learning tasks. The <code>TokenLearner</code> class is particularly useful for tasks such as image classification, object detection, and other applications where it is beneficial to extract tokens (representative features) from the input data.</p>"},{"location":"zeta/nn/modules/token_learner/#introduction","title":"Introduction","text":"<p>In various deep learning tasks, it is common to extract tokens (representative features) from the input data. These tokens are then used for downstream tasks like classification, detection, etc. The <code>TokenLearner</code> class is designed to efficiently extract tokens from the input data. It does this by utilizing a convolutional neural network (CNN) with grouped convolutions and a gating mechanism.</p>"},{"location":"zeta/nn/modules/token_learner/#class-definition","title":"Class Definition","text":"<pre><code>class TokenLearner(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim: int = None,\n        ff_mult: int = 2,\n        num_output_tokens: int = 8,\n        num_layers: int = 2,\n    ):\n        ...\n</code></pre>"},{"location":"zeta/nn/modules/token_learner/#parameters","title":"Parameters:","text":"<ul> <li><code>dim</code> (int, optional): The dimension of the input data. Default is <code>None</code>.</li> <li><code>ff_mult</code> (int, optional): The factor by which the inner dimension of the network will be multiplied. Default is <code>2</code>.</li> <li><code>num_output_tokens</code> (int, optional): The number of tokens to be output by the network. Default is <code>8</code>.</li> <li><code>num_layers</code> (int, optional): The number of layers in the network. Default is <code>2</code>.</li> </ul>"},{"location":"zeta/nn/modules/token_learner/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>TokenLearner</code> class is a PyTorch <code>nn.Module</code> that learns tokens from the input data. The input data is first packed and then processed through a series of grouped convolutions followed by a gating mechanism. The output is a set of tokens that are representative of the input data.</p> <p>The forward method of the <code>TokenLearner</code> class takes an input tensor <code>x</code> and performs the following operations:</p> <ol> <li>The input tensor <code>x</code> is packed using the <code>pack_one</code> helper function.</li> <li>The packed tensor is then rearranged and passed through a series of grouped convolutions and activation functions.</li> <li>The output of the convolutions is then rearranged and multiplied with the input tensor.</li> <li>The resulting tensor is then reduced to obtain the final tokens.</li> </ol>"},{"location":"zeta/nn/modules/token_learner/#method","title":"Method:","text":"<pre><code>def forward(self, x):\n    ...\n</code></pre>"},{"location":"zeta/nn/modules/token_learner/#parameters_1","title":"Parameters:","text":"<ul> <li><code>x</code> (Tensor): The input tensor of shape <code>(batch_size, channels, height, width)</code>.</li> </ul>"},{"location":"zeta/nn/modules/token_learner/#returns","title":"Returns:","text":"<ul> <li><code>x</code> (Tensor): The output tokens of shape <code>(batch_size, channels, num_output_tokens)</code>.</li> </ul>"},{"location":"zeta/nn/modules/token_learner/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/modules/token_learner/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta import TokenLearner\n\n# Initialize the TokenLearner\ntoken_learner = TokenLearner(dim=64)\n\n# Generate some random input data\nx = torch.randn(1, 64, 32, 32)\n\n# Forward pass\ntokens = token_learner.forward(x)\n\nprint(tokens.shape)\n</code></pre> <p>In this example, a <code>TokenLearner</code> is initialized with an input dimension of 64. A random tensor of shape <code>(1, 64, 32, 32)</code> is then passed through the <code>TokenLearner</code> to obtain the tokens. The output will be a tensor of shape <code>(1, 64, 8)</code>.</p>"},{"location":"zeta/nn/modules/token_learner/#example-2-custom-parameters","title":"Example 2: Custom Parameters","text":"<pre><code>import torch\n\nfrom zeta import TokenLearner\n\n# Initialize the TokenLearner with custom parameters\ntoken_learner = TokenLearner(dim=128, ff_mult=4, num_output_tokens=16)\n\n# Generate some random input data\nx = torch.randn(2, 128, 64, 64)\n\n# Forward pass\ntokens = token_learner.forward(x)\n\nprint(tokens.shape)\n# Output: torch.Size([2, 128, 16])\n</code></pre> <p>In this example, a <code>TokenLearner</code> is initialized with custom parameters. A random tensor of shape <code>(2, 128, 64, 64)</code> is then passed through the <code>TokenLearner</code> to obtain the tokens. The output will be a tensor of shape <code>(2, 128, 16)</code>.</p>"},{"location":"zeta/nn/modules/token_learner/#example-3-integration-with-other-pytorch-modules","title":"Example 3: Integration with Other PyTorch Modules","text":"<pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta import TokenLearner\n\n# Initialize the TokenLearner\ntoken_learner = TokenLearner(dim=64)\n\n# Generate some random input data\nx = torch.randn(1, 64, 32, 32)\n\n# Define a simple model\nmodel = nn.Sequential(token_learner, nn.Flatten(), nn.Linear(64 * 8, 10))\n\n# Forward pass\noutput = model(x)\n\nprint(output.shape)\n# Output: torch.Size([1, 10])\n</code></pre> <p>In this example, the <code>TokenLearner</code> is integrated into a simple model consisting of the <code>TokenLearner</code>, a <code>Flatten</code> layer, and a <code>Linear</code> layer. A random tensor of shape <code>(1, 64, 32, 32)</code> is then passed through the model to obtain the final output. The output will be a tensor of shape <code>(1, 10)</code>.</p>"},{"location":"zeta/nn/modules/token_learner/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>The <code>TokenLearner</code> can be mathematically formulated as follows:</p> <p>Let <code>X</code> be the input tensor of shape <code>(B, C, H, W)</code>, where <code>B</code> is the batch size, <code>C</code> is the number of channels, <code>H</code> is the height, and <code>W</code> is the width. The <code>TokenLearner</code> first rearranges <code>X</code> to a tensor of shape <code>(B, G*C, H, W)</code>, where <code>G</code> is the number of output tokens. This is done by repeating <code>X</code> along the channel dimension <code>G</code> times.</p> <p>The rearranged tensor is then passed through a series of grouped convolutions and activation functions to obtain a tensor <code>A</code> of shape <code>(B, G, H, W)</code>. This tensor is then rearranged and multiplied with the input tensor <code>X</code> to obtain a tensor of shape <code>(B, C, G, H, W)</code>.</p> <p>The final tokens are obtained by reducing this tensor along the <code>H</code> and <code>W</code> dimensions to obtain a tensor of shape <code>(B, C, G)</code>.</p>"},{"location":"zeta/nn/modules/token_learner/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li> <p>The <code>num_output_tokens</code> parameter controls the number of tokens that will be output by the <code>TokenLearner</code>. A larger number of output tokens will result in a more detailed representation of the input data, but will also increase the computational requirements.</p> </li> <li> <p>The <code>ff_mult</code> parameter controls the inner dimension of the <code>TokenLearner</code>. A larger <code>ff_mult</code> will result in a larger capacity model, but will also increase the computational requirements.</p> </li> <li> <p>The <code>TokenLearner</code> works best with input data that has a relatively small spatial dimension (e.g. 32x32 or 64x64). For larger input sizes, it may be beneficial to use a downsampling layer (e.g. <code>nn.MaxPool2d</code>) before passing the data through the <code>TokenLearner</code>.</p> </li> </ul>"},{"location":"zeta/nn/modules/topngating/","title":"Module/Function Name: TopNGating","text":""},{"location":"zeta/nn/modules/topngating/#1-purpose-and-functionality","title":"1. Purpose and Functionality","text":"<p>The <code>TopNGating</code> module serves as a mechanism to perform routing to top-n experts during a training or evaluation phase. It implements a method to compute the dispatch tensor, balance losses, and the router z-loss, and aligns the input sequences based on the experts' mini-batch. The routing is governed by various parameters including thresholds, capacity factors, gate logits for differentiable top-k operations, and more.</p>"},{"location":"zeta/nn/modules/topngating/#2-overview-and-introduction","title":"2. Overview and Introduction","text":"<p>The <code>TopNGating</code> module is essential for scenarios that demand routing to top experts to effectively process input sequences. By providing a means for fine-grained control over the assignment of sequences to different experts, it enhances the overall performance of the processing pipeline.</p>"},{"location":"zeta/nn/modules/topngating/#3-class-definition","title":"3. Class Definition","text":"<pre><code>class TopNGating(Module):\n    def __init__(\n        self,\n        dim,\n        num_gates,\n        eps=1e-9,\n        top_n=2,\n        threshold_train: Union[float, Tuple[float, ...]] = 0.2,\n        threshold_eval: Union[float, Tuple[float, ...]] = 0.2,\n        capacity_factor_train=1.25,\n        capacity_factor_eval=2.0,\n        straight_through_dispatch_tensor=True,\n        differentiable_topk=False,\n        differentiable_topk_fused=True,\n        min_expert_capacity: int = 4,\n    ):\ndef forward(self, x, noise_gates=False, noise_mult=1.0):\n</code></pre>"},{"location":"zeta/nn/modules/topngating/#4-functionality-and-usage","title":"4. Functionality and Usage","text":"<p>The <code>forward</code> method within the <code>TopNGating</code> class encapsulates the core functionality of the module. It accepts an input tensor <code>x</code> and various optional parameters for configuring the routing mechanism such as noise for the gates, noise multiplier, and performs the computation to obtain the dispatch tensor, combine tensor, balance loss, and router z-loss.</p> <p>We will now illustrate the usage of the <code>TopNGating</code> module through code examples.</p>"},{"location":"zeta/nn/modules/topngating/#usage-example-1","title":"Usage Example 1:","text":"<pre><code>import torch\n\nfrom zeta.nn import TopNGating\n\nx = torch.randn(1, 2, 3)\nmodel = TopNGating(3, 4)\n(\n    out,\n    _,\n    _,\n    _,\n) = model(x)\nprint(out.shape)\n</code></pre>"},{"location":"zeta/nn/modules/topngating/#usage-example-2","title":"Usage Example 2:","text":"<pre><code>import torch\n\nfrom zeta.nn import TopNGating\n\nx = torch.randn(2, 3, 4)\nmodel = TopNGating(4, 3, top_n=3)\n(\n    out,\n    _,\n    _,\n    _,\n) = model(x, noise_gates=True, noise_mult=0.7)\nprint(out.shape)\n</code></pre>"},{"location":"zeta/nn/modules/topngating/#usage-example-3","title":"Usage Example 3:","text":"<pre><code>import torch\n\nfrom zeta.nn import TopNGating\n\nx = torch.randn(2, 5, 6)\nmodel = TopNGating(\n    6, 5, threshold_train=(0.2, 0.3, 0.4, 0.35), threshold_eval=(0.21, 0.31, 0.41, 0.36)\n)\n(\n    out,\n    _,\n    _,\n    _,\n) = model(x, noise_gates=True, noise_mult=0.8)\nprint(out.shape)\n</code></pre>"},{"location":"zeta/nn/modules/topngating/#5-additional-information-and-tips","title":"5. Additional Information and Tips","text":"<ul> <li>Developers or users leveraging the <code>TopNGating</code> module should be cautious while configuring the different settings related to gating thresholds, capacity factors, and the added noise. These parameters can significantly impact the routing mechanism. It's advisable to perform multiple iterations with varying parameters to observe performance differences.</li> </ul>"},{"location":"zeta/nn/modules/topngating/#6-references-and-resources","title":"6. References and Resources","text":"<p>The <code>TopNGating</code> module is a unique construct and its underlying mechanism finds relevance in expert-based architectures in machine learning. For further exploration and background understanding, refer to the following resources:</p> <ul> <li>Research papers related to expert-based models</li> <li>Documentation on differentiability in routing mechanisms</li> <li>Deep learning architectures where routing to top experts is demonstrated</li> </ul> <p>By following the guide mentioned above, developers can effectively use the <code>TopNGating</code> module in their machine learning pipelines to enable efficient routing and fine-grained control over expert capacity.</p> <p>The documentation provides a comprehensive understanding of the module, detailing its purpose, usage, and associated considerations.</p> <p>The documentation is extensive, covering various aspects such as purpose, overview, class definition, functionality, usage examples, additional information and tips, and references.</p> <p>This detailed documentation is aimed at providing users with a deep and thorough understanding of the <code>TopNGating</code> module, empowering them to utilize its capabilities effectively.</p>"},{"location":"zeta/nn/modules/tripleskipblock/","title":"zeta.nn.modules: TripleSkipBlock Documentation","text":""},{"location":"zeta/nn/modules/tripleskipblock/#introduction","title":"Introduction","text":"<p>TripleSkipBlock is a PyTorch-like custom neural network module that represents the block performing triple skip-connections. It's part of the zeta.nn.modules library.</p> <p>Skip-connections, also known as new pathways for channeling information earlier in the network to layers that are much deeper, is the underlying principle that constitutes this module. These connections assist in addressing the vanishing gradient problem during the training of deep neural networks, facilitating feature re-usage, and forging much more complex representations by integrating features on various scales.</p> <p>This module is an extension of the PyTorch's nn.Module class, and its purpose is widening the pathway for information flowing through the module.</p>"},{"location":"zeta/nn/modules/tripleskipblock/#class-definition-tripleskipblock","title":"Class Definition: TripleSkipBlock","text":"<p>Here's the main constructor for the TripleSkipBlock class:</p> <pre><code>class TripleSkipBlock(nn.Module):\n    def __init__(self, submodule1, submodule2, submodule3):\n        \"\"\"\n        Defines the TripleSkipBlock module that performs triple skip connections.\n\n        Args:\n            submodule1 (nn.Module): The first submodule.\n            submodule2 (nn.Module): The second submodule.\n            submodule3 (nn.Module): The third submodule.\n        \"\"\"\n        super().__init__()\n        self.submodule1 = submodule1\n        self.submodule2 = submodule2\n        self.submodule3 = submodule3\n</code></pre> <p>The arguments for the constructor are:</p> Argument Type Description submodule1 nn.Module The first submodule. submodule2 nn.Module The second submodule. submodule3 nn.Module The third submodule. <p>The class includes one method:</p> <pre><code>def forward(self, x: torch.Tensor):\n    \"\"\"\n    Implements the forward pass of the TripleSkipBlock module.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The output tensor after applying triple skip-connections.\n    \"\"\"\n    return x + self.submodule1(x + self.submodule2(x + self.submodule3(x)))\n</code></pre> <p>In this method, the forward pass of the module is defined. The forward method is invoked when we call the class with the input data.</p> <p>The argument for the <code>forward</code> method:</p> Argument Type Description x torch.Tensor Input tensor. <p>The return value of the <code>forward</code> method:</p> Return Type Description torch.Tensor The output tensor after applying triple skip connections."},{"location":"zeta/nn/modules/tripleskipblock/#tripleskipblock-class-working-mechanism","title":"TripleSkipBlock Class: Working Mechanism","text":"<p>The TripleSkipBlock class operates as follows:</p> <ol> <li>In the Class constructor <code>__init__</code>, three submodules are initialized. These submodules are instances of PyTorch modules (nn.Module) that implement their respective forward functions. As they're sub-modules of the TripleSkipBlock class, they will have their parameters registered in TripleSkipBlock's parameter list.</li> <li>The forward function accomplishes the triple skip connection functionality. From the input <code>x</code>, it adds the output of <code>submodule3</code> applied on <code>x</code>, resulting in <code>x + self.submodule3(x)</code>. This intermediate output is then fed into <code>submodule2</code>, and again added with <code>x</code>. This process is repeated once more with <code>submodule1</code>.</li> </ol> <p>This iterative addition and integration of the input tensor, with the transformed tensor by each submodule, is referred to as a \"skip connection.\" This is crucial to mitigate the problem of vanishing gradients in deep neural networks and to allow lower-layer information to be directly transferred to higher layers.</p>"},{"location":"zeta/nn/modules/tripleskipblock/#examples","title":"Examples","text":""},{"location":"zeta/nn/modules/tripleskipblock/#example-1-simple-usage","title":"Example 1: Simple usage","text":"<p>Here's a simple example with three linear layers as the submodules:</p> <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn import TripleSkipBlock\n\n# Define input\ninput_tensor = torch.randn(10)\n\n# Define submodules\nsubmodule1 = nn.Linear(10, 10)\nsubmodule2 = nn.Linear(10, 10)\nsubmodule3 = nn.Linear(10, 10)\n\n# Define TripleSkipBlock\ntripleskip = TripleSkipBlock(submodule1, submodule2, submodule3)\n\n# Forward pass\noutput = tripleskip(input_tensor)\n</code></pre>"},{"location":"zeta/nn/modules/tripleskipblock/#example-2-using-the-module-with-conv2d-sub-modules-for-processing-images","title":"Example 2: Using the module with Conv2D sub-modules for processing images","text":"<pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.nn import TripleSkipBlock\n\n# Define input (single image with three channels, 64x64 resolution)\ninput_image = torch.randn(1, 3, 64, 64)\n\n# Define submodules\nsubmodule1 = nn.Conv2d(3, 10, kernel_size=3, stride=1, padding=1)\nsubmodule2 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\nsubmodule3 = nn.Conv2d(10, 3, kernel_size=3, stride=1, padding=1)\n\n# Define TripleSkipBlock\ntripleskip = TripleSkipBlock(submodule1, submodule2, submodule3)\n\n# Forward pass\noutput = tripleskip(input_image)\n</code></pre> <p>These are simple examples demonstrating the usage of the TripleSkipBlock. The submodules used in them are simple linear and convolutional layers. You can replace these with any kind of PyTorch module according to the specific network requirements. </p> <p>Remember that the purpose of this TripleSkipBlock module is to create more complex interactions between layers in the network with skip connections. This can improve the ability of the network to learn representations from data, especially when data is much complex with intricate patterns.</p>"},{"location":"zeta/nn/modules/umambablock/","title":"Module/Function Name: UMambaBlock","text":"<p>UMambaBlock is a 5d Mamba block designed to serve as a building block for 5d visual models. In accordance with the article published on https://arxiv.org/pdf/2401.04722.pdf, this module enables transformation across 5D space-time data for efficient information processing.</p> <p>The module's core concepts pertain to the input dimension (dim), the depth of the Mamba block, the state dimension (d_state), the expansion factor (expand), the rank of the temporal difference (dt_rank), the dimension of the convolutional kernel (d_conv), and the inclusion of bias in linear and convolutional layers.</p>"},{"location":"zeta/nn/modules/umambablock/#class-definition","title":"Class Definition:","text":"<pre><code>class UMambaBlock(nn.Module):\n    \"\"\"\n    UMambaBlock is a 5d Mamba block that can be used as a building block for a 5d visual model\n    From the paper: https://arxiv.org/pdf/2401.04722.pdf\n\n    Args:\n        dim (int): The input dimension.\n        dim_inner (Optional[int]): The inner dimension. If not provided, it is set to dim * expand.\n        depth (int): The depth of the Mamba block.\n        d_state (int): The state dimension. Default is 16.\n        expand (int): The expansion factor. Default is 2.\n        dt_rank (Union[int, str]): The rank of the temporal difference (\u0394) tensor. Default is \"auto\".\n        d_conv (int): The dimension of the convolutional kernel. Default is 4.\n        conv_bias (bool): Whether to include bias in the convolutional layer. Default is True.\n        bias (bool): Whether to include bias in the linear layers. Default is False.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int = None,\n        depth: int = 5,\n        d_state: int = 16,\n        expand: int = 2,\n        d_conv: int = 4,\n        conv_bias: bool = True,\n        bias: bool = False,\n    ):\n        # Class initialization and setup\n        ...\n\n    def forward(self, x: Tensor):\n        \"\"\"\n        B, C, H, W, D\n        \"\"\"\n        # Forward pass implementation\n        ...\n</code></pre>"},{"location":"zeta/nn/modules/umambablock/#detailed-explanation","title":"Detailed Explanation:","text":"<p>The UMambaBlock class serves as a thorough representation of a 5d Mamba block. It encapsulates the input dimension, depth, state dimension, expansion factor, and other key parameters. By instantiating this block, users can process 5D visual data, further taking advantage of hyperparameters to customize the block for specific application requirements.</p>"},{"location":"zeta/nn/modules/umambablock/#usage-examples","title":"Usage Examples:","text":""},{"location":"zeta/nn/modules/umambablock/#example-1","title":"Example 1:","text":"<pre><code>import torch\n\nfrom zeta.nn import UMambaBlock\n\n# img:         B, C, H, W, D\nimg_tensor = torch.randn(1, 64, 10, 10, 10)\n\n# Initialize Mamba block\nblock = UMambaBlock(dim=64, depth=1)\n\n# Forward pass\ny = block(img_tensor)\nprint(y.shape)\n</code></pre>"},{"location":"zeta/nn/modules/umambablock/#example-2","title":"Example 2:","text":"<pre><code>import torch\n\nfrom zeta.nn import UMambaBlock\n\n# img:         B, C, H, W, D\nimg_tensor = torch.randn(1, 64, 10, 10, 10)\n\n# Initialize Mamba block with custom parameters\nblock = UMambaBlock(dim=64, depth=3, expand=3)\n\n# Forward pass\ny = block(img_tensor)\nprint(y.shape)\n</code></pre>"},{"location":"zeta/nn/modules/umambablock/#example-3","title":"Example 3:","text":"<pre><code>import torch\n\nfrom zeta.nn import UMambaBlock\n\n# img:         B, C, H, W, D\nimg_tensor = torch.randn(1, 64, 5, 5, 20)\n\n# Initialize Mamba block with altered state dimension and convolutional kernel size\nblock = UMambaBlock(dim=64, d_state=32, d_conv=6)\n\n# Forward pass\ny = block(img_tensor)\nprint(y.shape)\n</code></pre>"},{"location":"zeta/nn/modules/umambablock/#additional-information-and-tips","title":"Additional Information and Tips:","text":"<p>The user may benefit from customizing various hyperparameters such as the input dimension, depth, and state dimension to tailor the UMambaBlock for specific use cases. Common useful tips include managing the Mamba block's rank parameter and identifying key transformations to optimize for handling high-dimensional spatiotemporal data.</p>"},{"location":"zeta/nn/modules/umambablock/#references-and-resources","title":"References and Resources:","text":"<ul> <li>Research Paper by Author A, et al.</li> <li>Torch NN Documentation</li> </ul> <p>By following this well-structured and detailed documentation, developers and research practitioners can readily understand and adopt the UMambaBlock module for 5D image and video data processing.</p>"},{"location":"zeta/nn/modules/unet/","title":"Module Name: Unet","text":"<p><code>Unet</code> is a convolutional neural network architecture predominantly used for biomedical image segmentation. The architecture comprises two primary pathways: downsampling and upsampling, followed by an output convolution. Due to its U-shape, the architecture is named <code>U-Net</code>. Its symmetric architecture ensures that the context (from downsampling) and the localization (from upsampling) are captured effectively.</p>"},{"location":"zeta/nn/modules/unet/#overview","title":"Overview","text":"<ul> <li> <p>Downsampling: This captures the context of the input image, compressing the spatial dimensions and expanding the depth (number of channels). This is typically done using convolutional and pooling layers.</p> </li> <li> <p>Upsampling: This uses the context information to localize and segment the image, expanding its spatial dimensions to match the original input dimensions. Upsampling can be done using transposed convolutions or bilinear interpolations based on the given setting.</p> </li> <li> <p>Skip connections: These connections are essential in U-Net as they connect layers from the downsampling path to the corresponding layers in the upsampling path. This helps in recovering the fine-grained details lost during downsampling.</p> </li> <li> <p>Output: The final layer produces the segmented image, usually with channels corresponding to each class or segment.</p> </li> </ul>"},{"location":"zeta/nn/modules/unet/#class-definition","title":"Class Definition:","text":"<pre><code>class Unet(nn.Module):\n</code></pre>"},{"location":"zeta/nn/modules/unet/#parameters","title":"Parameters:","text":"Parameter Data Type Description n_channels int Number of input channels. n_classes int Number of output channels (typically, number of segmentation classes). bilinear bool Determines the method of upsampling. If True, uses bilinear interpolation; otherwise, uses transposed convolution. Default is False."},{"location":"zeta/nn/modules/unet/#methods","title":"Methods:","text":""},{"location":"zeta/nn/modules/unet/#1-forwardx-torchtensor-torchtensor","title":"1. <code>forward(x: torch.Tensor) -&gt; torch.Tensor</code>:","text":"<p>The forward method defines the flow of input through the U-Net architecture. </p> <p>Parameters:</p> <ul> <li><code>x (torch.Tensor)</code>: Input tensor.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>: Output segmented image.</li> </ul>"},{"location":"zeta/nn/modules/unet/#2-use_checkpointing-none","title":"2. <code>use_checkpointing() -&gt; None</code>:","text":"<p>This method enables gradient checkpointing for the U-Net model, which is a technique to reduce memory consumption during training by trading off computation time.</p>"},{"location":"zeta/nn/modules/unet/#usage-example","title":"Usage Example:","text":"<pre><code>import torch\n\nfrom zeta.nn import Unet  # Update `&lt;path_to_module&gt;` to your specific path\n\n# Initialize the U-Net model\nmodel = Unet(n_channels=1, n_classes=2)\n\n# Random input tensor with dimensions [batch_size, channels, height, width]\nx = torch.randn(1, 1, 572, 572)\n\n# Forward pass through the model\ny = model(x)\n\n# Output\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {y.shape}\")\n</code></pre>"},{"location":"zeta/nn/modules/unet/#architecture-flow","title":"Architecture Flow:","text":"<ol> <li> <p>Input: Takes an image tensor as input with <code>n_channels</code>.</p> </li> <li> <p>Downsampling Path:</p> </li> <li>Double convolution on the input.</li> <li>Four downsampling steps with double convolutions. </li> <li> <p>The depth of the feature maps increases, while the spatial dimensions decrease.</p> </li> <li> <p>Upsampling Path:</p> </li> <li>Four upsampling steps where the feature maps from the downsampling path are concatenated and followed by up convolutions.</li> <li> <p>The spatial dimensions increase, moving closer to the original input size.</p> </li> <li> <p>Output: </p> </li> <li> <p>A final output convolution to map the feature maps to desired <code>n_classes</code>.</p> </li> <li> <p>Checkpointing (optional):</p> </li> <li>If memory optimization during training is required, <code>use_checkpointing</code> can be invoked. This will enable gradient checkpointing to save memory during the backward pass.</li> </ol>"},{"location":"zeta/nn/modules/unet/#additional-tips","title":"Additional Tips:","text":"<ul> <li> <p>The bilinear interpolation mode of upsampling is typically faster and consumes less memory than the transposed convolution method. However, it might not always provide the same level of detail in the upsampled feature maps.</p> </li> <li> <p>Gradient checkpointing in <code>use_checkpointing</code> is useful for models with deep architectures or when the available GPU memory is limited. Remember, while this method saves memory, it also requires additional computation during the backward pass.</p> </li> <li> <p>Ensure the input dimensions are appropriate for the U-Net model. Given the number of downsampling and upsampling layers in the architecture, certain input dimensions might not produce the expected output dimensions.</p> </li> </ul>"},{"location":"zeta/nn/modules/unet/#references-and-resources","title":"References and Resources:","text":"<ul> <li> <p>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI).</p> </li> <li> <p>PyTorch Official Documentation on checkpointing.</p> </li> </ul> <p>Note: It's essential to understand that while the U-Net architecture is provided, the definitions and implementations of <code>DoubleConv</code>, <code>Down</code>, <code>Up</code>, and <code>OutConv</code> are not provided in the code. Ensure you have these components documented or explained as well if they are part of your library or module.</p>"},{"location":"zeta/nn/modules/video_autoencoder/","title":"<code>CausalConv3d</code> Documentation","text":""},{"location":"zeta/nn/modules/video_autoencoder/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Overview</li> <li>CausalConv3d Class</li> <li>Initialization Parameters</li> <li>Functionality and Usage</li> <li>Forward Method</li> <li>Utility Functions</li> <li>Examples</li> <li>Example 1: Creating a CausalConv3d Module</li> <li>Example 2: Using CausalConv3d for Causal Convolution</li> <li>Additional Information</li> <li>References and Resources</li> </ol>"},{"location":"zeta/nn/modules/video_autoencoder/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the documentation for the Zeta library. This comprehensive guide provides detailed information about the Zeta library and its components, focusing on the <code>CausalConv3d</code> class. Before we delve into the details, it's important to understand the purpose and significance of this library.</p>"},{"location":"zeta/nn/modules/video_autoencoder/#11-purpose","title":"1.1 Purpose","text":"<p>The Zeta library is designed to simplify the development of deep learning models by offering modular components and utilities. One of these components is the <code>CausalConv3d</code> class, which plays a crucial role in performing causal convolutions on 3D tensors.</p>"},{"location":"zeta/nn/modules/video_autoencoder/#12-key-features","title":"1.2 Key Features","text":"<ul> <li> <p>Causal Convolution: The <code>CausalConv3d</code> class enables causal convolutions on 3D tensors, a vital operation in architectures like ResNet.</p> </li> <li> <p>Seamless Integration: Zeta modules seamlessly integrate with popular deep learning frameworks like PyTorch, making it easy to incorporate them into your projects.</p> </li> </ul>"},{"location":"zeta/nn/modules/video_autoencoder/#2-overview","title":"2. Overview","text":"<p>The Zeta library is built with the aim of providing essential building blocks for deep learning model development. One such block is the <code>CausalConv3d</code> class.</p>"},{"location":"zeta/nn/modules/video_autoencoder/#21-causalconv3d-class","title":"2.1 <code>CausalConv3d</code> Class","text":"<p>The <code>CausalConv3d</code> class is a module designed for performing causal convolutions on 3D tensors. It is particularly useful in scenarios where preserving the causality of data is essential, such as in ResNet architectures.</p> <p>In the following sections, we will explore the <code>CausalConv3d</code> class's definition, initialization parameters, functionality, and usage.</p>"},{"location":"zeta/nn/modules/video_autoencoder/#3-causalconv3d-class","title":"3. CausalConv3d Class","text":"<p>The <code>CausalConv3d</code> class is at the core of Zeta, providing the ability to perform causal convolutions on 3D tensors.</p>"},{"location":"zeta/nn/modules/video_autoencoder/#31-initialization-parameters","title":"3.1 Initialization Parameters","text":"<p>Here are the initialization parameters for the <code>CausalConv3d</code> class:</p> <ul> <li> <p><code>chan_in</code> (int): The number of input channels in the tensor.</p> </li> <li> <p><code>chan_out</code> (int): The number of output channels in the tensor after convolution.</p> </li> <li> <p><code>kernel_size</code> (int or Tuple[int, int, int]): The size of the convolution kernel. It can be a single integer or a tuple specifying the size in three dimensions.</p> </li> <li> <p><code>pad_mode</code> (str): The padding mode used for the convolution operation.</p> </li> <li> <p><code>**kwargs</code> (dict): Additional arguments to be passed to the convolution layer.</p> </li> </ul>"},{"location":"zeta/nn/modules/video_autoencoder/#32-methods","title":"3.2 Methods","text":"<p>The primary method of the <code>CausalConv3d</code> class is the <code>forward</code> method, which performs the causal convolution operation on input tensors.</p>"},{"location":"zeta/nn/modules/video_autoencoder/#4-functionality-and-usage","title":"4. Functionality and Usage","text":"<p>Let's explore the functionality and usage of the <code>CausalConv3d</code> class.</p>"},{"location":"zeta/nn/modules/video_autoencoder/#41-forward-method","title":"4.1 Forward Method","text":"<p>The <code>forward</code> method of the <code>CausalConv3d</code> class takes an input tensor and applies causal convolution using a 3D convolutional layer. Here is the parameter:</p> <ul> <li><code>x</code> (Tensor): The input tensor of shape <code>(batch, channels, time, height, width)</code>.</li> </ul> <p>The method returns a tensor after performing causal convolution, preserving causality in the temporal dimension.</p>"},{"location":"zeta/nn/modules/video_autoencoder/#42-usage-examples","title":"4.2 Usage Examples","text":""},{"location":"zeta/nn/modules/video_autoencoder/#example-1-creating-a-causalconv3d-module","title":"Example 1: Creating a CausalConv3d Module","text":"<p>In this example, we create an instance of the <code>CausalConv3d</code> class with default settings:</p> <pre><code>causal_conv = CausalConv3d(chan_in=64, chan_out=128, kernel_size=3)\n</code></pre>"},{"location":"zeta/nn/modules/video_autoencoder/#example-2-using-causalconv3d-for-causal-convolution","title":"Example 2: Using CausalConv3d for Causal Convolution","text":"<p>Here, we demonstrate how to use the <code>CausalConv3d</code> module for performing causal convolution on an input tensor:</p> <pre><code>causal_conv = CausalConv3d(chan_in=64, chan_out=128, kernel_size=3)\ninput_data = torch.randn(1, 64, 32, 32)\noutput = causal_conv(input_data)\nprint(output.shape)\n</code></pre>"},{"location":"zeta/nn/modules/video_autoencoder/#5-utility-functions","title":"5. Utility Functions","text":"<p>The Zeta library also provides a set of utility functions used within the modules. These utility functions, such as <code>exists</code>, <code>default</code>, <code>identity</code>, and more, enhance the modularity and flexibility of the library.</p>"},{"location":"zeta/nn/modules/video_autoencoder/#6-additional-information","title":"6. Additional Information","text":"<p>Here are some additional tips and information for using the Zeta library and the <code>CausalConv3d</code> class effectively:</p> <ul> <li> <p>Experiment with different values for <code>chan_in</code>, <code>chan_out</code>, and <code>kernel_size</code> to control the number of input and output channels and the convolution kernel size.</p> </li> <li> <p>Ensure that the input tensor (<code>x</code>) has the appropriate shape <code>(batch, channels, time, height, width)</code> to perform causal convolution.</p> </li> <li> <p>The <code>pad_mode</code> parameter allows you to specify the padding mode for the convolution operation.</p> </li> </ul>"},{"location":"zeta/nn/modules/video_autoencoder/#7-references-and-resources","title":"7. References and Resources","text":"<p>For further information and resources related to the Zeta library and deep learning, please refer to the following:</p> <ul> <li> <p>Zeta GitHub Repository: The official Zeta repository for updates and contributions.</p> </li> <li> <p>PyTorch Official Website: The official website for PyTorch, the deep learning framework used in Zeta.</p> </li> </ul> <p>This concludes the documentation for the Zeta library and the <code>CausalConv3d</code> class. You now have a comprehensive understanding of how to use this library and module for your deep learning projects. If you have any further questions or need assistance, please refer to the provided references and resources. Happy modeling with Zeta!</p>"},{"location":"zeta/nn/modules/visionattention/","title":"visionattention","text":""},{"location":"zeta/nn/modules/visionattention/#visionattention","title":"VisionAttention","text":"<p>Base class for self-attention on input tensor.</p> <p>The <code>VisionAttention</code> module is designed to perform self-attention on the input tensor. The module is part of the larger <code>nn</code> package in the PyTorch framework and can be applied to various neural network architectures that require attention mechanisms for vision-based tasks.</p>"},{"location":"zeta/nn/modules/visionattention/#overview-and-introduction","title":"Overview and Introduction","text":"<p>Attention mechanisms are a vital component of modern deep learning architectures that require the model to focus on different parts of the input data differently. This is especially important in computer vision tasks where the model needs to pay greater attention to specific features within an image. The <code>VisionAttention</code> module enables self-attention, allowing the model to perform computationally-efficient weighting of inputs.</p>"},{"location":"zeta/nn/modules/visionattention/#class-definition-and-parameters","title":"Class Definition and Parameters","text":"<p>The <code>VisionAttention</code> class requires the following parameters to be passed: - dim (int): The input dimension of the tensor. - heads (int, optional): The number of attention heads. Defaults to 8. - dim_head (int, optional): The dimension of each attention head. Defaults to 64. - dropout (float, optional): The dropout probability. Defaults to 0.0.</p> <p>The data types and default values for the parameters are strictly enforced for creating an instance of the <code>VisionAttention</code> module.</p>"},{"location":"zeta/nn/modules/visionattention/#implementing-visionattention","title":"Implementing VisionAttention","text":"<p>The <code>forward</code> function of the <code>VisionAttention</code> module is defined to perform the forward pass of the self-attention. It takes a tensor x as input and applies the self-attention mechanism, returning the output tensor after self-attention.</p>"},{"location":"zeta/nn/modules/visionattention/#usage-and-examples","title":"Usage and Examples","text":"<p>The <code>VisionAttention</code> module can be seamlessly integrated into various neural network architectures. Below are three examples demonstrating the usage of each instance:</p>"},{"location":"zeta/nn/modules/visionattention/#example-1-single-tensor-input","title":"Example 1: Single Tensor Input","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import VisionAttention\n\n# Create a sample input tensor\nx = torch.randn(1, 3, 32, 32)\n\n# Initialize the VisionAttention module\nmodel = VisionAttention(dim=32, heads=8, dim_head=64, dropout=0.0)\n\n# Perform self-attention on the input tensor\nout = model(x)\n\n# Print the output\nprint(out)\n</code></pre>"},{"location":"zeta/nn/modules/visionattention/#example-2-integrated-with-an-existing-model","title":"Example 2: Integrated with an Existing Model","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.nn import VisionAttention\n\n\n# Define a custom neural network architecture\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = VisionAttention(dim=64, heads=16, dim_head=128, dropout=0.1)\n        self.decoder = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n\n# Create an instance of the custom model\ncustom_model = CustomModel()\n\n# Generate a sample input\ninput_tensor = torch.randn(1, 64, 64, 3)\n\n# Perform a forward pass through the model\noutput = custom_model(input_tensor)\n\n# Print the output\nprint(output)\n</code></pre>"},{"location":"zeta/nn/modules/visionattention/#example-3-fine-tuning-hyperparameters","title":"Example 3: Fine-Tuning Hyperparameters","text":"<pre><code>import torch\nimport torch.nn as nn\n\n# Create a sample input tensor\nx = torch.randn(1, 3, 32, 32)\n\n# Initialize the VisionAttention module with custom settings\nmodel = VisionAttention(dim=32, heads=16, dim_head=128, dropout=0.2)\n\n# Update the model with a new weight configuration\nout = model(x)\n\n# Print the output\nprint(out)\n</code></pre>"},{"location":"zeta/nn/modules/visionattention/#conclusion","title":"Conclusion","text":"<p>The <code>VisionAttention</code> module offers a flexible way to integrate self-attention mechanisms into various neural network architectures for vision-related tasks. By following the provided guidelines, using the module becomes straightforward and enables intuitive customization to best suit the specific needs of different models.</p>"},{"location":"zeta/nn/modules/visionattention/#references-and-resources","title":"References and Resources","text":"<ul> <li>PyTorch Documentation for \"nn\" Module</li> <li>Research paper: \"Attention Is All You Need\", Vaswani et al. (2017)</li> </ul>"},{"location":"zeta/nn/modules/visual_expert/","title":"<code>VisualExpert</code> Module Documentation","text":"<p>Table of Contents</p> <ul> <li>Introduction</li> <li>Module Overview</li> <li>Class Definition</li> <li>Parameters</li> <li>Functionality and Usage</li> <li>How Visual Expert Works</li> <li>Usage Examples</li> <li>Additional Information and Tips</li> <li>References</li> </ul>"},{"location":"zeta/nn/modules/visual_expert/#introduction","title":"Introduction","text":"<p>Welcome to the documentation for the Visual Expert module, a component inspired by the research paper Visual Expert module. This module is designed to enable deep visual-language feature alignment, making it a valuable addition to your deep learning projects involving both text and image data. In this comprehensive guide, we will explore the purpose, functionality, and usage of the Visual Expert module.</p>"},{"location":"zeta/nn/modules/visual_expert/#module-overview","title":"Module Overview","text":"<p>The Visual Expert module is a crucial component for enhancing deep visual-language feature alignment. It consists of a QKV (Query, Key, Value) matrix and a Multi-Layer Perceptron (MLP) in each layer. These components have the same shapes as those in pretrained language models and are initialized from them. The primary motivation behind the Visual Expert module is to align image features with the different attention heads in a language model, enabling deep fusion.</p>"},{"location":"zeta/nn/modules/visual_expert/#class-definition","title":"Class Definition","text":"<p>The VisualExpert class in this module encapsulates the functionality needed to perform deep visual-language feature alignment. Let's explore its parameters and how to use it effectively.</p> <pre><code>class VisualExpert:\n    def __init__(\n        self,\n        dim: int,\n        hidden_dim: int,\n        dropout: float,\n        heads: int,\n    ):\n        ...\n\n    def __call__(self, x: torch.Tensor):\n        ...\n</code></pre>"},{"location":"zeta/nn/modules/visual_expert/#parameters","title":"Parameters","text":"Parameter Type Description <code>dim</code> int The dimension of the input features. <code>hidden_dim</code> int The dimension of the hidden layer in the feedforward. <code>dropout</code> float The dropout rate. <code>heads</code> int The number of heads in the multihead attention."},{"location":"zeta/nn/modules/visual_expert/#functionality-and-usage","title":"Functionality and Usage","text":""},{"location":"zeta/nn/modules/visual_expert/#how-visual-expert-works","title":"How Visual Expert Works","text":"<p>The Visual Expert module works by aligning image features with different attention heads in a language model. Here's a step-by-step explanation of how it operates:</p> <ol> <li>The input hidden states of an attention layer are represented as <code>X</code>, where:</li> <li><code>X</code> has shape <code>B\u00d7H\u00d7(LI+LT)\u00d7D</code>.</li> <li><code>B</code> is the batch size.</li> <li><code>LI</code> and <code>LT</code> are the lengths of image and text sequences.</li> <li><code>H</code> is the number of attention heads.</li> <li> <p><code>D</code> is the hidden size.</p> </li> <li> <p>In the attention with the Visual Expert, <code>X</code> is initially split into text and image features.</p> </li> <li> <p>QKV projections are applied separately for text and image features:</p> </li> <li>Query (<code>q_text</code>, <code>q_img</code>)</li> <li>Key (<code>k_text</code>, <code>k_img</code>)</li> <li> <p>Value (<code>v_text</code>, <code>v_img</code>)</p> </li> <li> <p>Attention is applied with the image features appended in front of the text features. The <code>q</code>, <code>k</code>, and <code>v</code> of text and images are concatenated together.</p> </li> <li> <p>The attention output is added to the normalized input (<code>X</code>) to capture feature alignment.</p> </li> <li> <p>Another layer normalization is applied.</p> </li> <li> <p>Text and image features are separated.</p> </li> <li> <p>Feedforward layers are applied to both text and image features.</p> </li> <li> <p>The output of the feedforwards is added together with the output of the added attention and normalization.</p> </li> </ol>"},{"location":"zeta/nn/modules/visual_expert/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/nn/modules/visual_expert/#example-1-creating-a-visual-expert-module","title":"Example 1: Creating a Visual Expert Module","text":"<pre><code>import torch\n\nfrom zeta.nn import VisualExpert\n\n# Create a Visual Expert module\nvisual_expert = VisualExpert(dim=1024, hidden_dim=2048, dropout=0.1, heads=16)\n</code></pre>"},{"location":"zeta/nn/modules/visual_expert/#example-2-forward-pass","title":"Example 2: Forward Pass","text":"<pre><code># Generate a random input tensor\nx = torch.randn(1, 10, 1024)\n\n# Apply the Visual Expert module\noutput = visual_expert(x)\n\n# Check the output shape\nprint(output.shape)  # torch.Size([1, 10, 1024])\n</code></pre>"},{"location":"zeta/nn/modules/visual_expert/#example-3-customizing-visual-expert","title":"Example 3: Customizing Visual Expert","text":"<p>You can customize the Visual Expert module by adjusting its parameters.</p> <pre><code># Create a Visual Expert module with different parameters\nvisual_expert_custom = VisualExpert(dim=512, hidden_dim=1024, dropout=0.2, heads=8)\n\n# Apply it to your data\noutput_custom = visual_expert_custom(x)\n</code></pre>"},{"location":"zeta/nn/modules/visual_expert/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li> <p>Experiment with different values for the <code>dim</code>, <code>hidden_dim</code>, <code>dropout</code>, and <code>heads</code> parameters to fine-tune the Visual Expert module for your specific tasks.</p> </li> <li> <p>Ensure that your input data shapes match the expected shapes described in the module documentation.</p> </li> <li> <p>If working with image and text data, preprocess and format your data accordingly before applying the Visual Expert module.</p> </li> <li> <p>Keep in mind that this module is designed for deep visual-language feature alignment, making it suitable for tasks that involve both text and image data.</p> </li> </ul>"},{"location":"zeta/nn/modules/visual_expert/#references","title":"References","text":"<ul> <li> <p>Research Paper: Visual Expert module</p> </li> <li> <p>PyTorch Documentation: PyTorch</p> </li> </ul> <p>This concludes the documentation for the Visual Expert module. We hope this guide helps you understand its purpose, functionality, and how to use it effectively in your deep learning projects.</p>"},{"location":"zeta/nn/modules/vittransformerblock/","title":"Module/Function Name: VitTransformerBlock","text":"<p>This is a transformer block used in the Vision Transformer (ViT) denoiser model. The block takes the input dimension, number of attention heads, dimension of each attention head, dimension of the feed-forward network, expansion factor for the feed-forward network, and dropout rate as parameters. It then normalizes the input, computes self-attention, and then passes it through a feed-forward network. </p> <pre><code>Parameters:\n| Parameter         | Description |\n| ----------------- | ----------- |\n| dim               | The input dimension of the block. |\n| heads             | The number of attention heads. |\n| dim_head          | The dimension of each attention head. |\n| mlp_dim           | The dimension of the feed-forward network. |\n| expansion         | The expansion factor for the feed-forward network. |\n| dropout           | The dropout rate. |\n</code></pre>"},{"location":"zeta/nn/modules/vittransformerblock/#example","title":"Example","text":"<pre><code># Usage example 1:\nimport torch\nimport torch.nn as nn\n\ninput_dim = 256\nnum_heads = 3\ndim_head = 64\nfeedforward_dim = 512\nexpansion_factor = 3\ndropout_rate = 0.1\n\ntransformer_block = VitTransformerBlock(\n    input_dim, num_heads, dim_head, feedforward_dim, expansion_factor, dropout_rate\n)\ninput_tensor = torch.randn(\n    1, 3, 256, 512\n)  # Batch size of 5, sequence length of 256, input dimension of 256\noutput = transformer_block(input_tensor)\n\n# Usage example 2:\ninput_dim = 256\nnum_heads = 4\ndim_head = 64\nfeedforward_dim = 512\nexpansion_factor = 3\ndropout_rate = 0.1\ntransformer_block = VitTransformerBlock(\n    input_dim, num_heads, dim_head, feedforward_dim, expansion_factor, dropout_rate\n)\ninput_tensor = torch.randn(\n    1, 4, 64, 256\n)  # Batch size of 4, sequence length of 64 input dimension of 256\noutput = transformer_block(input_tensor)\n</code></pre> <p>The VitTransformerBlock class represents a self-contained instance of a transformer block module used in the Vision Transformer architecture. The block has been designed and implemented to perform various operations such as self-attention and feed-forward network processing efficiently and effectively. It takes into account all the relevant design considerations and parameters required for its successful operation.</p> <p>It consists of a number of attributes representing its state and components, including the input dimension, number of attention heads, dimensions of each attention head, feed-forward network structure, expansion factor, and dropout rate. These attributes encapsulate essential details about the block and provide information about its intended functionality and behavior.</p> <p>The class features an initializer method to set up the essential components and state of the block. During the initialization process, the relevant parameters are used to configure the instance to operate effectively in accordance with the specified dimensions and requirements. The block also defines a forward method to perform the forward pass and processing of input data through the self-attention mechanism and the feed-forward network.</p> <p>Overall, the VitTransformerBlock class encapsulates the core functionality and operation of a transformer block module used in the Vision Transformer architecture, covering all aspects of its design, implementation, and functional behavior in the context of the ViT denoiser model.</p>"},{"location":"zeta/nn/modules/vlayernorm/","title":"Class: VLayerNorm","text":"<p>Documentation: The VLayerNorm class is a base class for all neural network modules. It is ideal for any python project that requires efficient handling of deep neural network modules. The VLayerNorm class implements an efficient neural network structure that can eliminate unnecessary overheads and optimizes model training and evaluation. The class should be treated as an essential component for developing machine learning models.</p> <p>Usage Summary:</p> <pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n    self.conv1 = nn.Conv2d(1, 20, 5)\n    self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n\n    x = F.relu(self.conv1(x))\n    return F.relu(self.conv2(x))\n</code></pre> <p>Explanation: In the given example, the class \"VLayerNorm\" is defined to perform the normalization on a tensor (x) as a part of the forward pass in the neural network architecture. Within the \"VLayerNorm\" class, the input dimension (dim) and an optional small value (eps) are specified for the normalization process are passed in the init() method. The \"forward\" method is then defined to execute the normalization process on an input tensor (x) and return a normalized tensor.</p> <p>Note: The normalization process involves performing a normalization operation on the input tensor (x) based on its mean and variance. The mean and variance are computed over a specific dimension of the input tensor, which is essential for the normalization process.</p> <p>Representative Model Structure: The \"VLayerNorm\" class serves as the base for neural network modules such as \"Model\". The \"Model\" class shown in the usage example uses the \"VLayerNorm\" class within its neural network architecture to perform efficient normalization for training and evaluation.</p>"},{"location":"zeta/nn/modules/wsconv2d/","title":"Module/Function Name: WSConv2d","text":""},{"location":"zeta/nn/modules/wsconv2d/#overview-and-introduction","title":"Overview and Introduction","text":"<p>WSConv2d is weight standardization Conv2d layer, that inherits from <code>nn.Conv2d</code> and adds weight standardization to the convolutional layer. It normalizes the weights of the convolutional layer to have zero mean and unit variance along the channel dimension. This helps in stabilizing the training process and improving generalization.</p>"},{"location":"zeta/nn/modules/wsconv2d/#class-wsconv2d","title":"Class: WSConv2d","text":""},{"location":"zeta/nn/modules/wsconv2d/#definition","title":"Definition:","text":"<pre><code>class WSConv2d(nn.Conv2d):\n</code></pre>"},{"location":"zeta/nn/modules/wsconv2d/#parameters","title":"Parameters:","text":"Parameters Description in_channels (int) Number of input channels out_channels (int) Number of output channels kernel_size (int) Size of the convolutional kernel stride (float, optional) Stride of the convolution. Default is 1 padding (int or tuple, optional) Padding added to the input. Default is 0 dilation (int, optional) Spacing between kernel elements. Default is 1 groups (int, optional) Number of blocked connections from input channels to output channels. Default is 1 bias (bool, optional) If True, adds a learnable bias to the output. Default is True padding_mode (str, optional) Type of padding. Default is \"zeros\""},{"location":"zeta/nn/modules/wsconv2d/#method-init","title":"Method: init","text":"<p><pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    stride: float = 1,\n    padding=0,\n    dilation: int = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"zeros\",\n)\n</code></pre> In the <code>__init__</code> method, the <code>WSConv2d</code> class initializes the convolutional layer with various attributes including in_channels, out_channels, kernel_size, stride, and bias. </p>"},{"location":"zeta/nn/modules/wsconv2d/#additional-properties","title":"Additional Properties:","text":"<ul> <li>gain: nn.Parameter, shape (output_channels, 1, 1, 1), initialized to ones</li> <li>eps: register_buffer for a tensor with a single value of 1e-4</li> <li>fan_in: register_buffer for a tensor with the value equal to the number of weight parameters</li> </ul>"},{"location":"zeta/nn/modules/wsconv2d/#method-standardized_weights","title":"Method: standardized_weights","text":"<p><pre><code>def standardized_weights(self) -&gt; Tensor\n</code></pre> The <code>standardized_weights</code> method calculates the standardized weights using weight standardization, which makes use of mean and variance along each channel of the weights tensor.</p>"},{"location":"zeta/nn/modules/wsconv2d/#method-forward","title":"Method: forward","text":"<p><pre><code>def forward(self, x: Tensor) -&gt; Tensor\n</code></pre> The <code>forward</code> method convolves the input tensor <code>x</code> with standardized weights.</p> <p>Example Usage: <pre><code>import torch\n\nfrom zeta.nn import WSConv2d\n\n# Instantiate a WSConv2d layer\nws_conv2d = WSConv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n\n# Create a random input tensor\nx = torch.randn(1, 3, 32, 32)\n\n# Apply the WSConv2d layer\noutput = ws_conv2d(x)\n\nprint(output.shape)\n</code></pre> Note: Modify the input and parameter values based on your use case and neural network architecture.</p>"},{"location":"zeta/nn/utils/helpers/","title":"Helpers","text":""},{"location":"zeta/nn/utils/helpers/#documentation","title":"Documentation","text":""},{"location":"zeta/nn/utils/helpers/#overview","title":"Overview","text":"<p>The provided module comprises utility functions and classes to streamline specific operations with Python data structures and PyTorch models. The main aspects of the module are:</p> <ul> <li>Checking the existence of a value.</li> <li>Implementing custom call behavior through classes.</li> <li>Custom decorators for function calls.</li> <li>Dictionary manipulation.</li> <li>Initialization of PyTorch layer parameters.</li> </ul>"},{"location":"zeta/nn/utils/helpers/#functions-and-classes","title":"Functions and Classes","text":"<ol> <li> <p>exists(val: Any) -&gt; bool:    Checks if the provided value is not <code>None</code>.</p> </li> <li> <p>default(val: Any, d: Any) -&gt; Any:    Returns the value if it's not <code>None</code>; otherwise, it returns a default value.</p> </li> <li> <p>once(fn: Callable) -&gt; Callable:    A decorator ensuring that the function is only called once.</p> </li> <li> <p>eval_decorator(fn: Callable) -&gt; Callable:    A decorator for <code>torch.nn.Module</code> methods to switch the module to <code>eval</code> mode during the function call and revert to its original mode afterwards.</p> </li> <li> <p>cast_tuple(val: Any, depth: int) -&gt; Tuple:    Casts a value to a tuple with a specific depth.</p> </li> <li> <p>maybe(fn: Callable) -&gt; Callable:    A decorator that calls the function only if its first argument exists.</p> </li> <li> <p>always:    A class that always returns the specified value when called.</p> </li> <li> <p>not_equals and equals:    Classes that, when instantiated with a value, check if another value is (not) equal to the specified value.</p> </li> <li> <p>init_zero_(layer: nn.Module) -&gt; None:    Initializes the weights and biases of a torch layer to zero.</p> </li> <li> <p>pick_and_pop(keys: List[str], d: Dict) -&gt; Dict:    Extracts values from a dictionary based on provided keys.</p> </li> <li> <p>group_dict_by_key(cond: Callable, d: Dict) -&gt; Tuple[Dict, Dict]:    Groups dictionary keys based on a given condition.</p> </li> <li> <p>string_begins_with(prefix: str, str: str) -&gt; bool:    Checks if a string starts with a specific prefix.</p> </li> <li> <p>group_by_key_prefix(prefix: str, d: Dict) -&gt; Tuple[Dict, Dict]:    Groups dictionary items by keys starting with a specific prefix.</p> </li> <li> <p>groupby_prefix_and_trim(prefix: str, d: Dict) -&gt; Tuple[Dict, Dict]:    Similar to <code>group_by_key_prefix</code> but also removes the prefix from keys.</p> </li> </ol>"},{"location":"zeta/nn/utils/helpers/#usage-examples","title":"Usage Examples","text":"<ol> <li> <p>Using the <code>once</code> decorator:</p> <pre><code>from zeta import once\n\n\n@once\ndef greet():\n    print(\"Hello, World!\")\n\n\ngreet()  # prints \"Hello, World!\"\ngreet()  # Does nothing on the second call\n</code></pre> </li> <li> <p>Using the <code>eval_decorator</code> with PyTorch:</p> <pre><code>import torch.nn as nn\n\nfrom zeta import eval_decorator\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = nn.Linear(10, 10)\n\n    @eval_decorator\n    def predict(self, x):\n        return self.layer(x)\n\n\nmodel = SimpleModel()\ninput_tensor = torch.randn(1, 10)\noutput = model.predict(input_tensor)  # Automatically switches to eval mode and back\n</code></pre> </li> <li> <p>Dictionary Manipulation with Prefix Functions:</p> <pre><code>from zeta import group_by_key_prefix\n\nsample_dict = {\n    \"user_name\": \"John\",\n    \"user_age\": 25,\n    \"order_id\": 12345,\n    \"order_date\": \"2023-01-01\",\n}\n\nuser_data, order_data = group_by_key_prefix(\"user_\", sample_dict)\nprint(user_data)  # {'user_name': 'John', 'user_age': 25}\nprint(order_data)  # {'order_id': 12345, 'order_date': '2023-01-01'}\n</code></pre> </li> </ol> <p>This module is a collection of general-purpose utility functions and classes, making many common operations more concise. It's beneficial when working with PyTorch models and various data manipulation tasks.</p>"},{"location":"zeta/ops/_matrix_inverse_root_newton/","title":"_matrix_inverse_root_newton","text":"<p>Inverse square root of a matrix is a vital operation in various fields such as computer graphics, machine learning, and numerical analysis. The <code>_matrix_inverse_root_newton</code> method in <code>zeta.ops</code> provides an efficient way to calculate the inverse root of a matrix, which is crucial in techniques like whitening transformations, principal component analysis (PCA), and more.</p>"},{"location":"zeta/ops/_matrix_inverse_root_newton/#purpose-and-importance","title":"Purpose and Importance","text":"<p>The Newton iteration method used for matrix inverse root is highly valued for its convergence properties. It can ensure precise outcomes while requiring fewer iterations compared to more direct numerical methods. Using this method, <code>_matrix_inverse_root_newton</code> computes a matrix that, when raised to a given power, results in the original matrix's inverse square root. This is instrumental in algorithms that require matrix normalization steps for stability and convergence.</p>"},{"location":"zeta/ops/_matrix_inverse_root_newton/#architecture-and-class-design","title":"Architecture and Class Design","text":"<p>The <code>_matrix_inverse_root_newton</code> function does not belong to a class; it is a standalone method. It leverages PyTorch tensors for GPU acceleration and takes advantage of batch operations in the PyTorch library, ensuring compatibility with the overall PyTorch ecosystem.</p>"},{"location":"zeta/ops/_matrix_inverse_root_newton/#function-definition","title":"Function Definition","text":"<p>The <code>_matrix_inverse_root_newton</code> function is formulated as follows:</p> <pre><code>def _matrix_inverse_root_newton(\n    A,\n    root: int,\n    epsilon: float = 0.0,\n    max_iterations: int = 1000,\n    tolerance: float = 1e-6,\n) -&gt; Tuple[Tensor, Tensor, NewtonConvergenceFlag, int, Tensor]: ...\n</code></pre>"},{"location":"zeta/ops/_matrix_inverse_root_newton/#parameters-and-returns","title":"Parameters and Returns","text":"Argument Type Default Value Description <code>A</code> Tensor None The input matrix of interest. <code>root</code> int None The required root. Typically, for an inverse square root, this would be 2. <code>epsilon</code> float 0.0 Regularization term added to the matrix before computation. <code>max_iterations</code> int 1000 Maximum number of iterations allowed for the algorithm. <code>tolerance</code> float 1e-6 Convergence criterion based on the error between iterations."},{"location":"zeta/ops/_matrix_inverse_root_newton/#returns","title":"Returns:","text":"Returns Type Description <code>A_root</code> Tensor The inverse root of the input matrix <code>A</code>. <code>M</code> Tensor The matrix after the final iteration. <code>termination_flag</code> NewtonConvergenceFlag Convergence flag indicating the result status. <code>iteration</code> int Number of iterations performed. <code>error</code> Tensor The final error between <code>M</code> and the identity."},{"location":"zeta/ops/_matrix_inverse_root_newton/#usage-and-examples","title":"Usage and Examples","text":""},{"location":"zeta/ops/_matrix_inverse_root_newton/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta.ops import _matrix_inverse_root_newton\n\n# Defining the input matrix A\nA = torch.randn(3, 3)\nA = A @ A.T  # Making A symmetric positive-definite\n\n# Computing the inverse square root of A\nA_root, M, flag, iters, err = _matrix_inverse_root_newton(A, root=2)\n</code></pre>"},{"location":"zeta/ops/_matrix_inverse_root_newton/#example-2-custom-tolerance-and-iterations","title":"Example 2: Custom Tolerance and Iterations","text":"<pre><code>import torch\n\nfrom zeta.ops import _matrix_inverse_root_newton\n\n# Defining the input matrix A\nA = torch.randn(5, 5)\nA = A @ A.T  # Making A symmetric positive-definite\n\n# Computing the inverse square root with custom tolerance and max_iterations\nA_root, M, flag, iters, err = _matrix_inverse_root_newton(\n    A, root=2, epsilon=0.001, max_iterations=500, tolerance=1e-8\n)\n</code></pre>"},{"location":"zeta/ops/_matrix_inverse_root_newton/#example-3-handling-outputs-and-convergence","title":"Example 3: Handling Outputs and Convergence","text":"<pre><code>import torch\n\nfrom zeta.ops import NewtonConvergenceFlag, _matrix_inverse_root_newton\n\n# Defining the input matrix A\nA = torch.randn(4, 4)\nA = A @ A.T  # Making A symmetric positive-definite\n\n# Computing the inverse square root and handling convergence\nA_root, M, flag, iters, err = _matrix_inverse_root_newton(A, root=2)\n\n# Check if the iteration has converged\nif flag == NewtonConvergenceFlag.CONVERGED:\n    print(f\"Converged in {iters} iterations with an error of {err}\")\nelse:\n    print(\"Reached maximum iterations without convergence\")\n</code></pre>"},{"location":"zeta/ops/_matrix_inverse_root_newton/#explanation-of-the-algorithm","title":"Explanation of the Algorithm","text":"<p>The <code>_matrix_inverse_root_newton</code> function calculates the inverse root of a matrix using an iterative Newton's method. The key concept behind the operation is to generate a sequence of matrices that progressively approach the inverse root of the given matrix. Training deep neural networks often involves numerous matrix operations such as multiplications, inversions, and factorizations. Efficient and stable computation of these operations is essential for achieving good performance and ensuring numerical stability.</p> <p>After initializing matrices and parameters, the function enters an iterative block which runs until the convergence criteria are met or the maximum number of iterations is reached. In each iteration, the function updates the estimate of the matrix's inverse root and checks the error to decide whether to continue the iterations further.</p>"},{"location":"zeta/ops/_matrix_inverse_root_newton/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>Regularization <code>epsilon</code>: Advantageous in preventing numerical issues when the matrix <code>A</code> is close to singular or ill-conditioned.</li> <li>Convergence: The parameters <code>max_iterations</code> and <code>tolerance</code> are crucial in achieving convergence. It might be necessary to adjust these values depending on your specific problem and matrix properties.</li> </ul>"},{"location":"zeta/ops/_matrix_root_eigen/","title":"_matrix_root_eigen","text":"<p>The principal function within the zeta.ops library is <code>_matrix_root_eigen</code>, which computes the (inverse) root of a given symmetric positive (semi-)definite matrix using eigendecomposition. The computation is based on the relation <code>A = Q * L * Q^T</code>, where <code>A</code> is the initial matrix, <code>Q</code> is a matrix of eigenvectors, and <code>L</code> is a diagonal matrix with eigenvalues. This function is particularly useful in applications such as signal processing, quantum mechanics, and machine learning, where matrix root computations are often required.</p> <p>The <code>_matrix_root_eigen</code> function is the cornerstone of the zeta.ops library. Its purpose is to calculate the root or inverse root of a matrix by decomposing it into its eigenvectors and eigenvalues, modifying the eigenvalues as per the desired operation (root or inverse root), and then reconstructing the matrix.</p>"},{"location":"zeta/ops/_matrix_root_eigen/#architecture-of-_matrix_root_eigen","title":"Architecture of <code>_matrix_root_eigen</code>","text":"<p>The <code>_matrix_root_eigen</code> function is built upon PyTorch's linear algebra capabilities and follows a clear sequence of steps:</p> <ol> <li>Verify if the root is a positive integer.</li> <li>Calculate the power to which the eigenvalues need to be raised (<code>alpha</code>).</li> <li>Perform eigendecomposition on the input matrix <code>A</code>.</li> <li>Modify the eigenvalues to ensure they are positive if the <code>make_positive_semidefinite</code> flag is set.</li> <li>Add a small <code>epsilon</code> value if necessary to ensure numerical stability.</li> <li>Compute the (inverse) root matrix using the modified eigenvalues and the eigenvectors.</li> </ol> <p>This architecture ensures that even matrices that might have numerical stability issues or slightly negative eigenvalues due to floating-point errors can be handled gracefully.</p>"},{"location":"zeta/ops/_matrix_root_eigen/#_matrix_root_eigen-method-signature","title":"<code>_matrix_root_eigen</code>: Method Signature","text":"<p>Below is the method signature for the <code>_matrix_root_eigen</code> function, alongside an explanation of its arguments and returned values:</p> Argument Type Default Value Description A Tensor Required The square matrix of interest. root int Required The root of interest, which should be a natural number. epsilon float 0.0 A small value added to the matrix to avoid numerical instability. inverse bool True If set to True, the function returns the inverse root matrix; otherwise, the root. exponent_multiplier float 1.0 A multiplier applied to the eigenvalue exponent in the root calculation. make_positive_semidefinite bool True Perturbs eigenvalues to ensure the matrix is positive semi-definite. retry_double_precision bool True Retries eigendecomposition with higher precision if initial attempt fails. <p>Returns:</p> Returned Value Type Description X Tensor The computed (inverse) root of matrix A. L Tensor Eigenvalues of matrix A. Q Tensor Orthogonal matrix consisting of eigenvectors of matrix A."},{"location":"zeta/ops/_matrix_root_eigen/#usage-examples","title":"Usage Examples","text":"<p>In the following sections, we'll look at three different ways to use the <code>_matrix_root_eigen</code> function from the zeta.ops library, along with the required imports and full example code.</p>"},{"location":"zeta/ops/_matrix_root_eigen/#example-1-basic-matrix-root-calculation","title":"Example 1: Basic Matrix Root Calculation","text":"<p>In this example, we'll calculate the square root of a 2x2 symmetric positive definite matrix.</p> <pre><code>import torch\n\nfrom zeta.ops import _matrix_root_eigen\n\n# Define a 2x2 symmetric positive definite matrix\nA = torch.tensor([[2.0, 1.0], [1.0, 2.0]])\n\n# Calculate the square root of the matrix\nX, L, Q = _matrix_root_eigen(A, root=2)\n\nprint(\"Matrix A:\\n\", A)\nprint(\"Square Root of A:\\n\", X)\n</code></pre>"},{"location":"zeta/ops/_matrix_root_eigen/#example-2-matrix-inverse-root-with-epsilon-perturbation","title":"Example 2: Matrix Inverse Root with Epsilon Perturbation","text":"<p>In this example, an <code>epsilon</code> perturbation is added for numerical stability, and the inverse square root is calculated.</p> <pre><code>import torch\n\nfrom zeta.ops import _matrix_root_eigen\n\n# Define a 3x3 symmetric positive definite matrix\nA = torch.tensor([[4.0, 2.0, 0.0], [2.0, 4.0, 1.0], [0.0, 1.0, 3.0]])\n\n# Calculate the inverse square root of the matrix, adding epsilon for stability\nX, L, Q = _matrix_root_eigen(A, root=2, epsilon=1e-5, inverse=True)\n\nprint(\"Matrix A:\\n\", A)\nprint(\"Inverse Square Root of A with Epsilon:\\n\", X)\n</code></pre>"},{"location":"zeta/ops/_matrix_root_eigen/#example-3-high-precision-calculation-with-positive-semi-definite-guarantee","title":"Example 3: High-Precision Calculation with Positive Semi-Definite Guarantee","text":"<p>This example demonstrates a more robust usage where the calculation is attempted in high precision, and the function ensures the matrix is positive semi-definite before computing its root.</p> <pre><code>import torch\n\nfrom zeta.ops import _matrix_root_eigen\n\n# Define a 3x3 symmetric positive semi-definite matrix with potential numerical issues\nA = torch.tensor([[1e-5, 0.0, 0.0], [0.0, 5.0, 4.0], [0.0, 4.0, 5.0]])\n\n# Calculate the square root, ensuring positive semi-definiteness and retrying in double precision if needed\nX, L, Q = _matrix_root_eigen(\n    A, root=2, make_positive_semidefinite=True, retry_double_precision=True\n)\n\nprint(\"Matrix A:\\n\", A)\nprint(\"Square Root with Positive Semi-Definite Guarantee:\\n\", X)\n</code></pre>"},{"location":"zeta/ops/_matrix_root_eigen/#additional-remarks","title":"Additional Remarks","text":"<p>When using the <code>_matrix_root_eigen</code> function, keep in mind that it assumes the input matrix <code>A</code> is symmetric. If the matrix is not symmetric, the results will not be valid. Also, use caution when setting the <code>epsilon</code> value to ensure that it does not distort the accurate computation of the matrix root more than necessary for numerical stability.</p>"},{"location":"zeta/ops/_matrix_root_eigen/#conclusion","title":"Conclusion","text":"<p>The zeta.ops library, specifically the <code>_matrix_root_eigen</code> function, is a powerful tool for scientific computation, providing advanced functionality for matrix root operations using eigendecomposition. By understanding the parameters and utilizing the provided examples, users can effectively leverage this functionality for their research or computational needs.</p>"},{"location":"zeta/ops/_matrix_root_eigen/#references-and-further-reading","title":"References and Further Reading","text":"<p>To learn more about the mathematical operations used in this library, consult the following resources:</p> <ul> <li>\"Numerical Linear Algebra\" by Lloyd N. Trefethen and David Bau, III.</li> <li>\"Matrix Analysis\" by Rajendra Bhatia.</li> <li>PyTorch Documentation: https://pytorch.org/docs/stable/index.html</li> </ul>"},{"location":"zeta/ops/channel_shuffle_new/","title":"channel_shuffle_new","text":"<p>The <code>channel_shuffle_new</code> function is a utility within the <code>zeta.ops</code> library designed to rearrange the channels of a 4D tensor that typically represents a batch of images with multiple channels. This operation is particularly useful in the context of neural networks that handle convolutional layers, where shuffling channels can allow for better cross-channel information flow and model regularization.</p> <p>Channel shuffling is an operation commonly used in ShuffleNet architectures, which are efficient convolutional neural network architectures designed for mobile and computational resource-limited environments. By strategically shuffling channels, these architectures can maintain information flow between convolutional layer groups while reducing computational complexity.</p>"},{"location":"zeta/ops/channel_shuffle_new/#channel_shuffle_new-function-definition","title":"<code>channel_shuffle_new</code> Function Definition","text":"<p>Here is a breakdown of the <code>channel_shuffle_new</code> function parameters:</p> Parameter Type Description <code>x</code> Tensor The input tensor with shape <code>(b, c, h, w)</code> where <code>b</code> is the batch size, <code>c</code> is the number of channels, <code>h</code> is the height, and <code>w</code> is the width. <code>groups</code> int The number of groups to divide the channels into for shuffling."},{"location":"zeta/ops/channel_shuffle_new/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The function <code>channel_shuffle_new</code> works by reorganizing the input tensor's channels. Specifically, given an input tensor <code>x</code> with a certain number of channels, the channels are divided into <code>groups</code>, and the channels' order within each group is shuffled.</p> <p>The rearrangement pattern <code>\"b (c1 c2) h w -&gt; b (c2 c1) h w\"</code> indicates that <code>x</code> is reshaped such that:</p> <ul> <li><code>b</code> remains the batch size,</li> <li><code>c1</code> and <code>c2</code> are dimensions used to split the original channel dimension, with <code>c1</code> corresponding to the number of groups (<code>groups</code> parameter) and <code>c2</code> being the quotient of the original channels divided by the number of groups,</li> <li><code>h</code> and <code>w</code> remain the height and width of the image tensor, respectively.</li> </ul> <p>Here, <code>rearrange</code> is assumed to be a function (such as the one from the <code>einops</code> library) that allows advanced tensor manipulation using pattern strings.</p>"},{"location":"zeta/ops/channel_shuffle_new/#examples","title":"Examples","text":""},{"location":"zeta/ops/channel_shuffle_new/#example-1-shuffle-channels-in-a-3-channel-image","title":"Example 1: Shuffle Channels in a 3-Channel Image","text":"<p>This basic usage example demonstrates how to use <code>channel_shuffle_new</code> for a single image with 3 RGB channels.</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import channel_shuffle_new\n\n# Create a sample tensor to represent a single RGB image (batch size = 1)\nx = torch.randn(1, 3, 64, 64)  # Shape (b=1, c=3, h=64, w=64)\n\n# Shuffle the channels with groups set to 1 (no actual shuffle since it equals the number of channels)\nshuffled_x = channel_shuffle_new(x, groups=1)\n</code></pre> <p>This example did not produce an actual shuffle since the number of groups is equal to the number of channels.</p>"},{"location":"zeta/ops/channel_shuffle_new/#example-2-shuffle-channels-for-a-batch-of-images-with-4-channels","title":"Example 2: Shuffle Channels for a Batch of Images with 4 Channels","text":"<p>In this example, we shuffle the channels of a batch of images with 4 channels each, into 2 groups.</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import channel_shuffle_new\n\n# Create a sample tensor to represent a batch of images with 4 channels each\nx = torch.randn(20, 4, 64, 64)  # Shape (b=20, c=4, h=64, w=64)\n\n# Shuffle the channels with groups set to 2\nshuffled_x = channel_shuffle_new(x, groups=2)\n# The channels are now shuffled within two groups\n</code></pre>"},{"location":"zeta/ops/channel_shuffle_new/#example-3-shuffle-channels-for-a-large-batch-of-high-channel-images","title":"Example 3: Shuffle Channels for a Large Batch of High-Channel Images","text":"<p>For a more complex scenario, we shuffle the channels of a large batch of images with 32 channels, using 8 groups.</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import channel_shuffle_new\n\n# Create a sample tensor to represent a large batch of high-channel images\nx = torch.randn(50, 32, 128, 128)  # Shape (b=50, c=32, h=128, w=128)\n\n# Shuffle the channels with groups set to 8\nshuffled_x = channel_shuffle_new(x, groups=8)\n# The channels are now shuffled within eight groups\n</code></pre>"},{"location":"zeta/ops/channel_shuffle_new/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The number of groups (<code>groups</code>) must be a divisor of the number of channels in the input tensor <code>x</code>. If it is not, the operation will cause an error due to the mismatch in tensor shapes.</li> <li>Channel shuffling can lead to performance improvements in certain network architectures, but it should be used thoughtfully. It might not always yield benefits and could lead to loss of information if not used correctly.</li> <li>The <code>einops</code> library provides powerful tensor manipulation features that can be combined with PyTorch for flexible operations like channel shuffling.</li> </ul>"},{"location":"zeta/ops/channel_shuffle_new/#references","title":"References","text":"<ul> <li>\"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices.\" Ma, Ningning, et al. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.</li> <li><code>einops</code> documentation: EinOps - flexible and powerful tensor operations for readable and reliable code</li> </ul>"},{"location":"zeta/ops/compute_matrix_root_inverse_residuals/","title":"compute_matrix_root_inverse_residuals","text":"<p><code>compute_matrix_root_inverse_residuals</code> computes the residual of a matrix root inverse, which is typically used for debugging or testing the accuracy of matrix root inverse computations.</p>"},{"location":"zeta/ops/compute_matrix_root_inverse_residuals/#function-definition","title":"Function Definition","text":"<pre><code>def compute_matrix_root_inverse_residuals(\n    A: torch.Tensor,\n    X_hat: torch.Tensor,\n    root: int,\n    epsilon: float,\n    exponent_multiplier: float\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n</code></pre>"},{"location":"zeta/ops/compute_matrix_root_inverse_residuals/#parameters","title":"Parameters","text":"Parameter Type Description <code>A</code> torch.Tensor The matrix of interest. <code>X_hat</code> torch.Tensor The computed matrix root inverse. <code>root</code> int The root of interest. <code>epsilon</code> float A small value added as <code>epsilon * I</code> to the matrix to provide numerical stability. <code>exponent_multiplier</code> float The exponent multiplier applied to computation of the inverse root."},{"location":"zeta/ops/compute_matrix_root_inverse_residuals/#returns","title":"Returns","text":"Name Type Description <code>absolute_error</code> torch.Tensor Absolute error of the matrix root inverse. <code>relative_error</code> torch.Tensor Relative error of matrix root inverse. <code>residual</code> torch.Tensor Residual of the matrix root inverse computation."},{"location":"zeta/ops/compute_matrix_root_inverse_residuals/#detailed-description","title":"Detailed Description","text":"<p>This function aims to calculate the discrepancy between the exact mathematical inverse root of a matrix and one that has been computed using numerical methods. Errors and residuals are calculated in the infinity norm, providing an overview of the largest errors in the computation without averaging.</p> <ul> <li>The relative error refers to the absolute difference of the computed matrix root inverse from the expected exact value, relative to the magnitude of the exact value.</li> <li>The relative residual is the discrepancy between the multiplied result of the matrix and its computed root inverse from the identity matrix, which ideally should be zero.</li> </ul>"},{"location":"zeta/ops/compute_matrix_root_inverse_residuals/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/ops/compute_matrix_root_inverse_residuals/#basic-usage","title":"Basic Usage","text":"<p>Here we will show some code written in the same markdown file as an example to showcase how the function can be used in a simple case.</p> <pre><code>```python\nimport torch\n\nfrom zeta.ops import compute_matrix_root_inverse_residuals\n\n# Sample 3x3 matrix\nA = torch.rand((3, 3), dtype=torch.float64)\nX_hat = torch.rand((3, 3), dtype=torch.float64)\n\n# Compute the residuals\nabs_error, rel_error, residual = compute_matrix_root_inverse_residuals(\n    A, X_hat, root=2, epsilon=1e-6, exponent_multiplier=1.0\n)\nprint(\"Absolute Error:\", abs_error)\nprint(\"Relative Error:\", rel_error)\nprint(\"Residual:\", residual)\n</code></pre>"},{"location":"zeta/ops/compute_matrix_root_inverse_residuals/#additional-usage-examples","title":"Additional Usage Examples","text":"<p>Owing to the limitations of this platform, we cannot provide additional explicit examples in this response. However, similar examples could range from using this function to verify the accuracy of differently computed matrix roots to varying <code>epsilon</code> and seeing the impact on stability.</p>"},{"location":"zeta/ops/compute_matrix_root_inverse_residuals/#common-issues-and-troubleshooting","title":"Common Issues and Troubleshooting","text":"<ul> <li>ValueError: Occurs if <code>A</code> is not a square matrix or if the size of <code>A</code> and <code>X_hat</code> do not match. Ensure that <code>A</code> is square and the dimensions match <code>X_hat</code>.</li> <li>Numerical Stability: Choosing a very small or large value of <code>epsilon</code> might cause numerical instability. It is recommended to keep this value within the range typical for your data type, for instance, <code>1e-6</code> for <code>float64</code>.</li> <li>High Relative Error: If the relative error is unusually high, it might indicate an issue with the computation of <code>X_hat</code>.</li> </ul>"},{"location":"zeta/ops/compute_matrix_root_inverse_residuals/#references-and-resources","title":"References and Resources","text":"<ul> <li>PyTorch Documentation: https://pytorch.org/docs/stable/index.html</li> <li>Matrix Algebra Theory: (Insert relevant link or book citation)</li> <li>Numerical Methods for Matrix Computations: (Insert relevant link or book citation)</li> </ul>"},{"location":"zeta/ops/fast_softmax/","title":"fast_softmax","text":"<p>The <code>fast_softmax</code> function is a utility designed to compute the softmax of a given tensor in a numerically stable manner using the LogSumExp trick. The softmax function is a crucial component in many machine learning applications, especially those related to natural language processing and neural networks. It turns logits (i.e., raw output from a linear layer) into probabilities that sum up to 1.</p> <p>Numerical instability can arise when dealing with large numbers due to overflow or underflow during the exponential operation in the traditional softmax calculation. The LogSumExp trick helps mitigate this issue by shifting the input values by their maximum value before the exponential operation.</p> <p>This documentation provides thorough explanations, examples, and best practices to utilize the <code>fast_softmax</code> function effectively.</p>"},{"location":"zeta/ops/fast_softmax/#function-definition","title":"Function Definition","text":"<p><code>fast_softmax(tensor)</code></p>"},{"location":"zeta/ops/fast_softmax/#parameters","title":"Parameters:","text":"Parameter Type Description <code>tensor</code> Tensor The input tensor for which to compute the softmax."},{"location":"zeta/ops/fast_softmax/#returns","title":"Returns:","text":"<p>A Tensor representing the softmax of the input tensor.</p>"},{"location":"zeta/ops/fast_softmax/#usage","title":"Usage","text":"<p>The <code>fast_softmax</code> function can be used like a regular softmax function. However, it is particularly useful when the input tensor has high magnitude numbers and there is a risk of numerical overflow or underflow with a standard softmax implementation.</p>"},{"location":"zeta/ops/fast_softmax/#examples","title":"Examples","text":""},{"location":"zeta/ops/fast_softmax/#example-1-basic-usage","title":"Example 1: Basic usage","text":"<pre><code>import torch\n\nfrom zeta.ops import fast_softmax\n\n# Suppose we have an input tensor of logits\nlogits = torch.tensor([2.0, 1.0, 0.1])\n\n# We apply fast_softmax to obtain the probabilities\nprobabilities = fast_softmax(logits)\n\nprint(probabilities)\n</code></pre>"},{"location":"zeta/ops/fast_softmax/#example-2-large-number-handling","title":"Example 2: Large number handling","text":"<pre><code>import torch\n\nfrom zeta.ops import fast_softmax\n\n# When dealing with large numbers\nlarge_logits = torch.tensor([12345.0, 67890.0, 1.0e5])\n\n# Traditional softmax could fail due to numerical instability,\n# but fast_softmax can handle this\nprobabilities = fast_softmax(large_logits)\n\nprint(probabilities)\n</code></pre>"},{"location":"zeta/ops/fast_softmax/#example-3-batch-processing","title":"Example 3: Batch processing","text":"<pre><code>import torch\n\nfrom zeta.ops import fast_softmax\n\n# Batch of logits\nbatch_logits = torch.rand(32, 10)  # Batch of 32 samples, each with 10 logits\n\n# Compute softmax for the entire batch\nbatch_probabilities = fast_softmax(batch_logits)\n\nprint(batch_probabilities)\n</code></pre>"},{"location":"zeta/ops/fast_softmax/#detailed-explanation","title":"Detailed Explanation","text":"<p>The <code>fast_softmax</code> function operates by first finding the maximum value in the input tensor and subtracting it from all elements in the tensor. This \"shift\" of the input tensor helps in reducing the likelihood of exponential values becoming too large. After applying the exponential function, the resultant tensor is then normalized by the sum of these exponentials, ensuring that all output values sum to 1, consistent with probability distributions.</p>"},{"location":"zeta/ops/fast_softmax/#numerical-stability-the-logsumexp-trick","title":"Numerical Stability: The LogSumExp Trick","text":"<p>The key to the numerical stability provided by the <code>fast_softmax</code> function lies in the LogSumExp trick. By shifting the inputs to have a maximum of zero before the exponential function is applied, we reduce the chances of reaching the floating-point overflow threshold. Since this shift does not change the relative differences between input values, it preserves the ratios necessary for accurate softmax computation.</p>"},{"location":"zeta/ops/fast_softmax/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ul> <li> <p>Underflow and Overflow: The most common issue addressed by <code>fast_softmax</code> is the numerical underflow and overflow during exponential calculations. By using <code>fast_softmax</code>, you should be able to avoid these issues even when dealing with input tensors containing large values.</p> </li> <li> <p>Batch Processing: When dealing with batches of data, ensure that the input tensor has the appropriate shape, where one dimension typically represents the batch size and the other represents the logits for each sample.</p> </li> </ul>"},{"location":"zeta/ops/fast_softmax/#references-and-further-reading","title":"References and Further Reading","text":"<p>For further exploration of the concepts behind the softmax function and the LogSumExp trick, the following resources may be helpful:</p> <ul> <li>Bishop, Christopher M. \"Pattern recognition and machine learning.\" (2006): 4-73</li> <li>Goodfellow, Ian, et al. \"Deep learning.\" MIT press, 2016.</li> </ul>"},{"location":"zeta/ops/gram_matrix_new/","title":"gram_matrix_new","text":"<p>This feature is pivotal for capturing the correlation of features in the context of neural style transfer and texture synthesis. Understanding and utilizing the <code>gram_matrix_new</code> function enables users to implement and comprehend advanced neural network models that depend on feature correlations.</p> <p>A Gram matrix represents the inner product of vectors which, in deep learning, typically correspond to flattened feature maps of a convolutional layer. Calculating Gram matrices is fundamental in style transfer algorithms, as the Gram matrix encapsulates texture information. By comparing Gram matrices of different images, networks can be trained to minimize the style differences between them, effectively transferring the style from one image to the other.</p>"},{"location":"zeta/ops/gram_matrix_new/#gram_matrix_new-function-definition","title":"<code>gram_matrix_new</code> Function Definition","text":"<p>Here is the formal definition and parameters of the <code>gram_matrix_new</code> function:</p> <pre><code>def gram_matrix_new(y):\n    \"\"\"\n    Computes the Gram matrix of a given tensor, often used in neural network algorithms to capture the correlation between features.\n\n    The Gram matrix is calculated by performing an element-wise product between the feature maps followed by a summation over spatial dimensions.\n\n    Parameters:\n    - y (Tensor): A 4D tensor with shape (batch_size, channels, height, width) that represents the feature maps.\n\n    Returns:\n    - Tensor: A 3D tensor with shape (batch_size, channels, channels) representing the Gram matrix of the input tensor.\n    \"\"\"\n\n    b, ch, h, w = y.shape\n    return torch.einsum(\"bchw,bdhw-&gt;bcd\", [y, y]) / (h * w)\n</code></pre>"},{"location":"zeta/ops/gram_matrix_new/#explanation-of-the-functionality-and-usage","title":"Explanation of the Functionality and Usage","text":"<p>The <code>gram_matrix_new</code> function takes a 4D tensor as input, which is the standard shape for batched image data in PyTorch, with dimensions for batch size, channels, height, and width. It uses the <code>einsum</code> function from the PyTorch library to compute the element-wise product and sum over spatial dimensions to calculate the Gram matrix. The function returns a 3D tensor where the batch dimension is retained, and the spatial correlation of the features is captured in a channels-by-channels matrix for each image in the batch.</p>"},{"location":"zeta/ops/gram_matrix_new/#detailed-usage-examples","title":"Detailed Usage Examples","text":"<p>Let's delve into three example usages of the <code>gram_matrix_new</code> function to understand it better in practical scenarios.</p>"},{"location":"zeta/ops/gram_matrix_new/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta.ops import gram_matrix_new\n\n# Simulated feature maps from a convolutional layer\nfeature_maps = torch.randn(1, 3, 64, 64)  # Simulating a single image with 3 channels\n\n# Calculate the Gram matrix\ngram_matrix = gram_matrix_new(feature_maps)\n\nprint(gram_matrix.shape)  # Output expected: (1, 3, 3)\n</code></pre> <p>In this basic usage example, we generate random feature maps to simulate the output of a convolutional layer for a single image with three channels. We then apply the <code>gram_matrix_new</code> function to calculate the Gram matrix.</p>"},{"location":"zeta/ops/gram_matrix_new/#example-2-style-transfer-preparation","title":"Example 2: Style Transfer Preparation","text":"<pre><code>import torch\nimport torchvision.models as models\nfrom PIL import Image\nfrom torchvision.transforms import functional as F\n\nfrom zeta.ops import gram_matrix_new\n\n# Load a pre-trained VGG model\nvgg = models.vgg19(pretrained=True).features.eval()\n\n# Load content and style images and preprocess them\ncontent_img = Image.open(\"path/to/content/image.jpg\")\nstyle_img = Image.open(\"path/to/style/image.jpg\")\n\n# Preprocess images to match VGG input requirements\ntransform = transforms.Compose(\n    [\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n    ]\n)\ncontent_tensor = transform(content_img).unsqueeze(0)\nstyle_tensor = transform(style_img).unsqueeze(0)\n\n\n# Extract features from a specific layer in VGG\ndef get_features(image, model, layers=(\"conv_4\",)):\n    features = {}\n    x = image\n    for name, layer in model._modules.items():\n        x = layer(x)\n        if name in layers:\n            features[name] = x\n    return features\n\n\ncontent_features = get_features(content_tensor, vgg)\nstyle_features = get_features(style_tensor, vgg)\n\n# Compute Gram matrix for style features\nstyle_gram_matrix = {\n    layer: gram_matrix_new(features) for (layer, features) in style_features.items()\n}\n\nprint(style_gram_matrix[\"conv_4\"].shape)  # Output expected: (1, C, C)\n</code></pre> <p>In this example, we preprocess content and style images, extract their features using a VGG model, and then use the <code>gram_matrix_new</code> function to calculate the Gram matrix for the style image's features. This is a crucial step in a style transfer algorithm.</p>"},{"location":"zeta/ops/gram_matrix_new/#example-3-optimizing-a-neural-network-for-style","title":"Example 3: Optimizing a Neural Network for Style","text":"<pre><code>import torch\nimport torch.optim as optim\nfrom torchvision.models import vgg19\n\nfrom zeta.ops import gram_matrix_new\n\n# Assume content_tensor, style_tensor, and their Gram matrices are already prepared as above\n\n# Define a transformation network and initialize with random weights\ntransformation_net = (\n    YourTransformationNet()\n)  # YourTransformationNet should be a PyTorch model that you have defined\n\n# Define a loss function and optimizer\noptimizer = optim.Adam(transformation_net.parameters(), lr=0.001)\nmse_loss = torch.nn.MSELoss()\n\n# Optimization loop\nfor epoch in range(num_epochs):\n    # Generate transformed image from the content image\n    transformed_img = transformation_net(content_tensor)\n\n    # Extract features of the transformed image in the same way as for content and style images\n    transformed_features = get_features(transformed_img, vgg)\n    transformed_gram_matrix = gram_matrix_new(transformed_features[\"conv_4\"])\n\n    # Compute loss based on difference in Gram matrices\n    style_loss = mse_loss(transformed_gram_matrix, style_gram_matrix[\"conv_4\"])\n\n    # Backpropagation and optimization\n    optimizer.zero_grad()\n    style_loss.backward()\n    optimizer.step()\n</code></pre> <p>The third example demonstrates incorporating the <code>gram_matrix_new</code> function into an optimization loop for training a neural network to perform style transfer. The network is optimized to minimize the difference between the Gram matrices of the transformed and style images.</p>"},{"location":"zeta/ops/gram_matrix_new/#arguments-and-methods-summary-in-markdown-table","title":"Arguments and Methods Summary in Markdown Table","text":"Argument Type Description Default Value Required <code>y</code> Tensor A 4D input tensor with shape (b, ch, h, w). None Yes Method Returns Description <code>gram_matrix_new</code> Tensor Computes a 3D gram matrix from the input tensor."},{"location":"zeta/ops/gram_matrix_new/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>When calculating the Gram matrix of large feature maps, be aware that this operation can be memory-intensive, as the computation requires a quadratic amount of memory relative to the number of channels.</li> <li>To improve computational efficiency, consider converting input tensors to half-precision (<code>torch.float16</code>) if your hardware support.</li> </ul>"},{"location":"zeta/ops/gram_matrix_new/#references-and-resources","title":"References and Resources","text":"<ol> <li>PyTorch Documentation: https://pytorch.org/docs/stable/index.html</li> <li>Neural Style Transfer: A Review: https://arxiv.org/abs/1705.04058</li> <li>Visualizing and Understanding Convolutional Networks: https://arxiv.org/abs/1311.2901</li> </ol>"},{"location":"zeta/ops/gumbelmax/","title":"gumbelmax","text":"<p><code>GumbelMax</code> serves the purpose of providing a differentiable approximation to the process of drawing samples from a categorical distribution. This is particularly useful in areas such as reinforcement learning or generative models where the Gumbel-Max trick can be used to sample actions or categories without losing gradient information.</p>"},{"location":"zeta/ops/gumbelmax/#parameters","title":"Parameters:","text":"Parameter Type Default Description <code>x</code> Tensor N/A The input tensor containing unnormalized log probabilities. <code>temp</code> float 1.0 The temperature parameter controlling the sharpness of the distribution. <code>hard</code> boolean False Determines the output format: one-hot encoded vector or probabilities distribution."},{"location":"zeta/ops/gumbelmax/#description","title":"Description:","text":"<p>The <code>GumbelMax</code> function manipulates the input tensor <code>x</code> by adding Gumbel noise to generate samples from a Gumbel distribution. This process serves as an approximation to sampling from a categorical distribution. When the <code>hard</code> parameter is set to <code>True</code>, the output is a one-hot encoded tensor representing the selected category. Otherwise, a probability distribution tensor is returned. The <code>temp</code> parameter affects the 'sharpness' of the softmax output; lower values make the output closer to one-hot encoding.</p>"},{"location":"zeta/ops/gumbelmax/#functionality-and-usage","title":"Functionality and Usage","text":"<p><code>GumbelMax</code> utilizes the Gumbel-Max trick, which enables gradient-based optimization over discrete variables by providing a continuous representation that can be used in backpropagation. The function first creates Gumbel noise and adds it to the input tensor, then applies a softmax function to generate a probability distribution over possible classes. The temperature parameter <code>temp</code> controls the concentration of the distribution \u2013 a smaller <code>temp</code> leads to a more concentrated, 'sharper' distribution, which makes the output resemble a one-hot tensor more closely.</p> <p>The <code>hard</code> parameter allows users to decide between a 'soft', probabilistic representation and a 'hard', deterministic one (one-hot encoded). Even with the hard version, gradients can still flow through the operation during backpropagation due to the straight-through estimator trick employed.</p>"},{"location":"zeta/ops/gumbelmax/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/ops/gumbelmax/#example-1-soft-sampling","title":"Example 1: Soft Sampling","text":"<pre><code>import torch\nimport torch.nn.functional as F\n\nfrom zeta.ops import gumbelmax\n\n# Unnormalized log probabilities\nlogits = torch.tensor([[0.1, 0.5, 0.4]])\n\n# Soft sampling with default temperature\nsoft_sample = gumbelmax(logits)\nprint(soft_sample)\n</code></pre>"},{"location":"zeta/ops/gumbelmax/#example-2-hard-sampling","title":"Example 2: Hard Sampling","text":"<pre><code># Hard sampling with temperature t=0.5\nhard_sample = gumbelmax(logits, temp=0.5, hard=True)\nprint(hard_sample)\n</code></pre>"},{"location":"zeta/ops/gumbelmax/#example-3-changing-temperature","title":"Example 3: Changing Temperature","text":"<pre><code># Soft sampling with a higher temperature, resulting in a smoother distribution\nsmooth_sample = gumbelmax(logits, temp=5.0)\nprint(smooth_sample)\n\n# Soft sampling with a lower temperature, resulting in a sharper distribution\nsharp_sample = gumbelmax(logits, temp=0.1)\nprint(sharp_sample)\n</code></pre>"},{"location":"zeta/ops/gumbelmax/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The Gumbel-Max trick is a cornerstone technique for non-differentiable sampling processes, making them compatible with gradient-based optimization techniques.</li> <li>Keep an eye on the temperature parameter as it can significantly affect the behavior of the function, especially the variance of the samples drawn.</li> <li>While using <code>hard=True</code> provides a deterministic output, the gradients can still be computed due to the reparameterization trick employed internally.</li> </ul>"},{"location":"zeta/ops/img_compose_bw/","title":"img_compose_bw","text":"<p>The primary role of <code>img_compose_bw</code> is to rearrange the dimensions of a 4D tensor representing a batch of black and white images so that all the images in the batch are concatenated horizontally, resulting in a single wide image composed of the batch. This utility can be particularly useful for visualization purposes or for operations where it's advantageous to view the entire batch as one wide image strip.</p>"},{"location":"zeta/ops/img_compose_bw/#parameters","title":"Parameters","text":"Parameter Type Description <code>x</code> Tensor A 4D tensor with dimensions <code>(b, h, w, c)</code> where <code>b</code> is the batch size, <code>h</code> is the height, <code>w</code> is the width, and <code>c</code> is the number of channels (should be 1 for black and white images)."},{"location":"zeta/ops/img_compose_bw/#returns","title":"Returns","text":"Return Type Description <code>tensor</code> Tensor A rearranged 3D tensor with dimensions <code>(h, b * w, c)</code>."},{"location":"zeta/ops/img_compose_bw/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>img_compose_bw</code> function uses the <code>rearrange</code> operation, commonly associated with a library named <code>einops</code>. This operation allows complex tensor transformations with a concise and readable syntax.</p> <p>The purpose of the function is to take a batch of black and white images in the form of a 4D tensor <code>(batch, height, width, channels)</code> and transform it into a 3D tensor where images are concatenated horizontally across the width.</p>"},{"location":"zeta/ops/img_compose_bw/#example-usage","title":"Example Usage:","text":"<p>Before diving into the examples, let's clarify the necessary imports and prerequisites expected to run the following code.</p> <p>Imports and setup.</p> <pre><code># Note: This assumes that einops is installed in your environment.\nimport torch\n\nfrom zeta.ops import img_compose_bw\n</code></pre>"},{"location":"zeta/ops/img_compose_bw/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code># Assuming you have a batch of 4 black and white images,\n# each of dimensions 64x64 pixels (1 channel for B&amp;W images)\nbatch_size = 4\nheight = 64\nwidth = 64\nchannels = 1  # Channels are 1 for B&amp;W images\n\n# Create a dummy batch of images\nbatch_images = torch.rand(batch_size, height, width, channels)\n\n# Use img_compose_bw to rearrange the batch into a single wide image\nwide_image = img_compose_bw(batch_images)\n\n# wide_image now has the shape: (64, 256, 1)\nprint(wide_image.shape)\n</code></pre>"},{"location":"zeta/ops/img_compose_bw/#example-2-visualization","title":"Example 2: Visualization","text":"<p>One common reason to use <code>img_compose_bw</code> is to prepare a batch of images for visualization.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Visualize the result\nplt.imshow(\n    wide_image.squeeze(), cmap=\"gray\"\n)  # Remove the channel dimension for plotting\nplt.axis(\"off\")  # Hide the axes\nplt.show()\n</code></pre>"},{"location":"zeta/ops/img_compose_bw/#example-3-processing-before-passing-to-a-model","title":"Example 3: Processing before passing to a model","text":"<p>You might want to preprocess your image batch before passing it through a convolutional neural network (CNN).</p> <pre><code>class SimpleCNN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(\n            in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1\n        )\n        # More layers here...\n\n    def forward(self, x):\n        x = self.conv1(x)\n        # More operations...\n        return x\n\n\n# Instantiate the model\nmodel = SimpleCNN()\n\n# Wide_image is already a tensor of shape (height, width*batch_size, channels)\n# Reshape it to (channels, height, width*batch_size) to match the expected input format of PyTorch CNNs\nwide_image_cnn = wide_image.permute(2, 0, 1).unsqueeze(0)  # Adds a batch dimension\n\n# Pass the tensor through the CNN\noutput = model(wide_image_cnn)\n\nprint(output.shape)\n</code></pre> <p>Multiple examples demonstrate the adaptability of <code>img_compose_bw</code> to different tasks. Users can easily integrate this function into their image processing pipelines when working with batches of black and white images.</p>"},{"location":"zeta/ops/img_compose_bw/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ol> <li> <p>The <code>img_compose_bw</code> function specifically works with black and white images, represented by a single channel. If using this function on RGB images, ensure that the color channels are properly handled before applying the function.</p> </li> <li> <p>The function assumes that the input tensor layout is <code>(batch, height, width, channels)</code>. If your tensors are structured differently, you might need to permute the dimensions to match this format.</p> </li> <li> <p>The <code>img_compose_bw</code> function can be easily modified to concatenate images vertically or in any other custom layout by changing the pattern string passed to the <code>rearrange</code> function.</p> </li> </ol>"},{"location":"zeta/ops/img_compose_bw/#conclusion","title":"Conclusion","text":"<p>In this documentation, we explored the <code>img_compose_bw</code> function from our  <code>zeta.ops</code> library, intended for the transformation of image tensors for black and white images. We reviewed the function definition, parameters, usage examples, and additional tips to ensure effective application of the function in various scenarios.</p> <p>This utility serves as a convenient tool for visualizing and processing batches of black and white images, fitting seamlessly into the preprocessing pipelines of image-related machine learning tasks.</p>"},{"location":"zeta/ops/img_compose_decompose/","title":"img_compose_decompose","text":"<p>Function <code>img_compose_decompose</code> restructures a batch of images by decomposing each image into sub-images and then composing a new set of \"images\" by arranging these sub-images.</p> <p>This transformation function is useful when working with tasks that involve image-to-image translation where sub-images need to be rearranged, such as styling certain quadrants of images differently, or when data needs to be preprocessed for multi-scale feature extraction.</p>"},{"location":"zeta/ops/img_compose_decompose/#overview-and-introduction","title":"Overview and Introduction","text":"<p>The <code>img_compose_decompose</code> function comes from the <code>zeta.ops</code> library (), which provides utilities to manipulate multidimensional data, specifically tailored for image data in this case. This library is designed to simplify the preprocessing and augmentation operations that are often required in computer vision tasks.</p>"},{"location":"zeta/ops/img_compose_decompose/#function-definition","title":"Function Definition","text":"<p>Below is the definition of the <code>img_compose_decompose</code> function:</p> <pre><code>def img_compose_decompose(x):\n    \"\"\"\n    Rearranges a batch of images by decomposing each image into sub-images and then composes a new set of \"images\" by arranging these sub-images.\n\n    Parameters:\n    - x (Tensor): A batch of images with shape (b, h, w, c), where `b` is the total batch size, `h` and `w` are the height and width of each image, and `c` is the number of channels.\n    \"\"\"\n    return rearrange(x, \"(b1 b2) h w c -&gt; (b1 h) (b2 w) c\", b1=2)\n</code></pre> <p>The function assumes that the input tensor <code>x</code> is of shape <code>(b, h, w, c)</code> and utilizes the <code>rearrange</code> function from the <code>einops</code> library to perform the restructuring.</p>"},{"location":"zeta/ops/img_compose_decompose/#parameters","title":"Parameters","text":"Parameter Type Description Default x Tensor A batch of images with shape <code>(b, h, w, c)</code> None"},{"location":"zeta/ops/img_compose_decompose/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>img_compose_decompose</code> function works by decomposing each image in the batch into 2x2 sub-images and then arranging them in a grid to create a new set of composed images. The new image dimensions become <code>(2*h, 2*w, c)</code>, effectively composing images that are 4 times larger in the number of pixels.</p>"},{"location":"zeta/ops/img_compose_decompose/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/ops/img_compose_decompose/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta.ops import img_compose_decompose\n\n# Assume x has a shape of (4, 100, 100, 3), representing 4 images of 100x100 pixels with 3 color channels\nx = torch.randn(4, 100, 100, 3)\n\n# Decompose and compose the images\nresult = img_compose_decompose(x)\n\n# Resulting tensor shape: (2*100, 2*100, 3)\nprint(result.shape)  # should output torch.Size([200, 200, 3])\n</code></pre>"},{"location":"zeta/ops/img_compose_decompose/#example-2-working-with-a-dataloader","title":"Example 2: Working with a DataLoader","text":"<pre><code>from torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor\n\nfrom zeta.ops import img_compose_decompose\n\n# Load CIFAR10 images\ncifar10_dataset = CIFAR10(\".\", train=True, download=True, transform=ToTensor())\ncifar10_loader = DataLoader(cifar10_dataset, batch_size=8, shuffle=True)\n\n# Iterate over the data loader\nfor batch, (images, labels) in enumerate(cifar10_loader):\n    # Apply img_compose_decompose function to the batch of images\n    composed_images = img_compose_decompose(images)\n    # Process composed images further\n    # ...\n    break  # Processing just one batch for demonstration\n</code></pre>"},{"location":"zeta/ops/img_compose_decompose/#example-3-visualizing-the-transformation","title":"Example 3: Visualizing the Transformation","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\nfrom zeta.ops import img_compose_decompose\n\n# Load an image\nimage = Image.open(\"sample_image.jpg\")\nimage_np = np.array(image)\n\n# Add batch and channel dimensions to the image\nimage_batch = image_np.reshape(1, *image_np.shape)\n\n# Apply the img_compose_decompose function\ncomposed_image = img_compose_decompose(image_batch)\n\n# Show the original and the composed images\nplt.subplot(1, 2, 1)\nplt.imshow(image)\nplt.title(\"Original Image\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(composed_image[0])\nplt.title(\"Composed Image\")\n\nplt.show()\n</code></pre>"},{"location":"zeta/ops/img_compose_decompose/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The <code>img_compose_decompose</code> function currently works with a fixed number of sub-images (2x2). For different configurations, modifications to the function or the <code>rearrange</code> pattern will be necessary.</li> <li>The function is built on top of the <code>einops.rearrange</code> function, which is a versatile tool for tensor manipulation. Users unfamiliar with <code>einops</code> may benefit from reading its documentation for a deeper understanding of tensor operations.</li> </ul>"},{"location":"zeta/ops/img_compose_decompose/#references-and-resources","title":"References and Resources","text":"<ul> <li>For more information on the <code>einops.rearrange</code> function, please refer to the einops documentation.</li> <li>Users seeking to apply this function to deep learning models might consider reading about PyTorch's <code>Dataset</code> and <code>DataLoader</code> classes in the PyTorch documentation.</li> </ul>"},{"location":"zeta/ops/img_decompose/","title":"img_decompose","text":"<p>The <code>img_decompose</code> function is designed to decompose a larger batch of images into smaller batches while keeping the individual image dimensions intact. This can be particularly useful when one intends to process the images in smaller groups while maintaining their original resolutions.</p>"},{"location":"zeta/ops/img_decompose/#parameters","title":"Parameters","text":"<p><code>x</code> (Tensor): The input tensor representing a batch of images. This tensor is expected to have a shape that conforms to the pattern <code>(batch_size, height, width, channels)</code>.</p>"},{"location":"zeta/ops/img_decompose/#returns","title":"Returns","text":"<p>A tuple representing the shape of the tensor after the <code>rearrange</code> operation. It does not return the rearranged tensor but only the shape. The returned shape will always have one extra dimension, splitting the initial batch size into two parts.</p>"},{"location":"zeta/ops/img_decompose/#how-img_decompose-works-and-its-usage","title":"How <code>img_decompose</code> Works and Its Usage","text":"<p><code>img_decompose</code> applies the <code>rearrange</code> function from the <code>einops</code> library on the input tensor <code>x</code>, specifying that the batch size (<code>b1 b2</code>) will be factored into two separate dimensions, with the first dimension being fixed to <code>b1=2</code>. The <code>rearrange</code> function is a powerful tool for tensor manipulation, providing a shorthand for expressive operations expressed in Einstein notation.</p> <p>Below are three different usage examples demonstrating the <code>img_decompose</code> function in various scenarios:</p>"},{"location":"zeta/ops/img_decompose/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<p>This example shows the basic usage of <code>img_decompose</code> to understand how the shape of the input tensor changes.</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import img_decompose\n\n# Create a dummy tensor representing a batch of 6 images,\n# each image having a height of 32 pixels, width of 32 pixels, and 3 color channels (RGB)\nbatch_images = torch.randn(6, 32, 32, 3)\n\n# Using img_decompose\nnew_shape = img_decompose(batch_images)\n\nprint(\"Original shape:\", batch_images.shape)\nprint(\"New shape after img_decompose:\", new_shape)\n</code></pre> <p>Output: <pre><code>Original shape: torch.Size([6, 32, 32, 3])\nNew shape after img_decompose: (2, 3, 32, 32, 3)\n</code></pre></p> <p>In this example, <code>img_decompose</code> processes a tensor representing a batch of 6 images. The function reshapes the batch size from 6 into two dimensions, <code>2</code> and <code>3</code>, effectively reinterpreting the batch as consisting of 2 smaller mini-batches of 3 images each. The function then returns the shape of the rearranged tensor.</p>"},{"location":"zeta/ops/img_decompose/#example-2-verifying-output-tensor","title":"Example 2: Verifying Output Tensor","text":"<p>In this example, let's show that the <code>img_decompose</code> function does not alter the content of the tensor.</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import img_decompose\n\n# Create a dummy tensor representing a batch of 8 images,\n# each 64x64 pixels with 3 color channels (RGB)\nbatch_images = torch.randn(8, 64, 64, 3)\n\n# Use img_decompose and reconstruct the tensor from shape\ndecomposed_shape = img_decompose(batch_images)\nreconstructed_tensor = rearrange(batch_images, \"(b1 b2) h w c -&gt; b1 b2 h w c\", b1=2)\n\nassert (\n    reconstructed_tensor.shape == decomposed_shape\n), \"The tensor has not been reconstructed correctly\"\n\nprint(\"Original tensor and reconstructed tensor are of the same shape.\")\n</code></pre> <p>Output: <pre><code>Original tensor and reconstructed tensor are of the same shape.\n</code></pre></p> <p>In this example, we successfully decompose the input tensor and then reconstruct a tensor with the same shape as indicated by the output of the <code>img_decompose</code> function, effectively verifying that the tensor content remains consistent throughout the process.</p>"},{"location":"zeta/ops/img_decompose/#example-3-practical-application-in-data-pipeline","title":"Example 3: Practical Application in Data Pipeline","text":"<p>Consider a scenario where we are working with a data pipeline where images come in a batch, but we need to run separate operations on two subsets of this batch. The <code>img_decompose</code> function can be used to facilitate this process. </p> <pre><code>import torch\nfrom einops import rearrange, repeat\nfrom torchvision import transforms\n\nfrom zeta.ops import img_decompose\n\n\n# Function from the zeta.ops library\ndef img_decompose(x):\n    return rearrange(x, \"(b1 b2) h w c -&gt; b1 b2 h w c\", b1=2).shape\n\n\n# Data processing pipeline function\ndef preprocess_and_decompose(batch_images):\n    preprocessing = transforms.Compose(\n        [\n            transforms.Resize((224, 224)),  # Resize each image to be 224x224\n            transforms.ToTensor(),  # Convert images to tensor format\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n            ),  # Normalize for model\n        ]\n    )\n\n    # Assume batch_images is a list of PIL Images\n    tensor_images = torch.stack([preprocessing(img) for img in batch_images])\n\n    decomposed_shape = img_decompose(tensor_images)\n    decomposed_tensor = rearrange(tensor_images, \"(b1 b2) c h w -&gt; b1 b2 c h w\", b1=2)\n\n    # Now you have two separate batches, which you can process independently\n    batch1 = decomposed_tensor[0]\n    batch2 = decomposed_tensor[1]\n\n    return batch1, batch2\n\n\n# Mock a batch of 4 PIL images (code for creating these images is omitted for brevity)\nbatch_images = ...\n\n# Run the preprocessing and decomposition\nbatch1_processed, batch2_processed = preprocess_and_decompose(batch_images)\n\n# Now, batch1_processed and batch2_processed can be processed by separate pipeline stages or model heads\n</code></pre> <p>In this scenario, the preprocessing pipeline first converts a batch of PIL Images into a normalized tensor suitable for feeding into a neural network. The <code>img_decompose</code> function is then used to obtain the decomposed shape which is used to organize the batch into two subsets. These subsets can then be passed independently through the rest of the pipeline stages.</p>"},{"location":"zeta/ops/img_decompose/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The function <code>img_decompose</code> only returns the shape after rearrangement, not the rearranged tensor itself. If the tensor data is needed in the new shape, you will need to use <code>rearrange()</code> and not the <code>img_decompose()</code> function.</li> <li>The fixed dimension (b1=2) in the <code>img_decompose</code> function means that the input tensor's batch size must be an even number to split it evenly. For batch sizes that are not multiples of 2, it's necessary to either adjust the <code>b1</code> value or pad the input tensor to fit the specified batch splitting.</li> <li>The <code>img_decompose</code> function assumes that the input tensor uses the channel last ordering <code>(batch_size, height, width, channels)</code>. If a different ordering is used, the <code>rearrange</code> pattern would need to be adjusted accordingly.</li> </ul>"},{"location":"zeta/ops/img_order_of_axes/","title":"img_order_of_axes","text":"<p>The <code>img_order_of_axes</code> function is a utility designed to reorder the axes of an image tensor for processing or visualization purposes. Its primary use case is to transform a batch of images with the format batch-height-width-channel (b, h, w, c) into a format suitable for displaying multiple images in a single row, maintaining the channel order.</p> <p>This documentation provides an in-depth understanding of the <code>img_order_of_axes</code> function, its architecture, and the rationale behind its design. We will cover multiple usage examples, detailing the parameters, expected inputs and outputs, along with additional tips and resources.</p> <p>The <code>img_order_of_axes</code> function plays a crucial role in scenarios where a batch of images needs to be combined into a single image with individual images laid out horizontally. This function is particularly useful when there is a need to visualize multiple similar images side by side, such as comparing different stages of image processing or visualization of input-output pairs in machine learning tasks.</p>"},{"location":"zeta/ops/img_order_of_axes/#function-definition","title":"Function Definition","text":""},{"location":"zeta/ops/img_order_of_axes/#img_order_of_axesx","title":"img_order_of_axes(x)","text":"<p>Rearranges the axes of an image tensor from batch-height-width-channel order to height-(batch * width)-channel order.</p>"},{"location":"zeta/ops/img_order_of_axes/#parameters","title":"Parameters:","text":"Parameter Type Description x Tensor A 4-dimensional tensor representing a batch of images with shape (b, h, w, c), where b is the batch size, h is the height, w is the width, and c is the number of channels."},{"location":"zeta/ops/img_order_of_axes/#returns","title":"Returns:","text":"<p>A rearranged tensor that combines the batch and width dimensions, resulting in a shape of (h, b * w, c).</p>"},{"location":"zeta/ops/img_order_of_axes/#usage-example-1","title":"Usage Example 1:","text":"<p>Visualizing a batch of images side by side:</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import img_order_of_axes\n\n# Create a dummy batch of images with shape (b, h, w, c)\nbatch_size, height, width, channels = 4, 100, 100, 3\ndummy_images = torch.rand(batch_size, height, width, channels)\n\n# Use `img_order_of_axes` to prepare the tensor for visualization\nreordered_images = img_order_of_axes(dummy_images)\n\n# `reordered_images` will have the shape (height, batch_size * width, channels)\nprint(reordered_images.shape)  # Expected output (100, 400, 3)\n</code></pre>"},{"location":"zeta/ops/img_order_of_axes/#usage-example-2","title":"Usage Example 2:","text":"<p>Comparing image pairs before and after processing:</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import img_order_of_axes\n\n# Create a dummy batch of original images and processed images\nbatch_size, height, width, channels = 2, 100, 100, 3\noriginal_images = torch.rand(batch_size, height, width, channels)\nprocessed_images = torch.rand(batch_size, height, width, channels)\n\n# Concatenate the original and processed images in the batch dimension\ncombined_batch = torch.cat((original_images, processed_images), dim=0)\n\n# Reorder the axes for side by side comparison\ncomparison_image = img_order_of_axes(combined_batch)\n\n# Visualize or save `comparison_image` as needed\n</code></pre>"},{"location":"zeta/ops/img_order_of_axes/#usage-example-3","title":"Usage Example 3:","text":"<p>Preparing a batch of images for a single forward pass in a convolutional neural network (CNN):</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import img_order_of_axes\n\n# Assuming `model` is a pre-defined CNN that expects input of shape (h, w, c)\nbatch_size, height, width, channels = 8, 64, 64, 3\ninput_images = torch.rand(batch_size, height, width, channels)\n\n# Combine all images side by side to form a single large image\nlarge_image = img_order_of_axes(input_images)\n\n# Now `large_image` can be fed into the CNN as a single input\noutput = model(large_image.unsqueeze(0))  # Add batch dimension of 1 at the beginning\n</code></pre>"},{"location":"zeta/ops/img_order_of_axes/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>It's important to note that the <code>rearrange</code> function used within <code>img_order_of_axes</code> is not a PyTorch built-in function. It requires the <code>einops</code> library which offers more flexible operations for tensor manipulation.</li> <li>To install <code>einops</code>, use the package manager of your choice, e.g., <code>pip install einops</code> for Python's pip package manager.</li> <li>When visualizing the rearranged tensor, ensure that the visualization tool or library you choose can handle non-standard image shapes, as the resulting tensor will have a width that is a multiple of the original width.</li> </ul>"},{"location":"zeta/ops/img_transpose/","title":"img_transpose","text":"<p>The <code>img_transpose</code> function is a simple but essential component within the <code>zeta.ops</code> library. Its primary purpose is to change the dimension ordering of image tensor data. This function caters to the preprocessing step where the dimension format requires alteration to match the input expectations of various image processing libraries or deep learning frameworks.</p> <p>In deep learning frameworks like PyTorch, images are typically represented as a four-dimensional tensor with dimensions corresponding to the batch size, number of channels, height, and width, denoted as <code>(B, C, H, W)</code>. However, some image processing libraries or visualization tools expect the channel dimension to be the last dimension, denoted as <code>(B, H, W, C)</code>. The <code>img_transpose</code> function rearranges the dimensions of the input tensor from <code>(B, C, H, W)</code> format to <code>(B, H, W, C)</code> format.</p>"},{"location":"zeta/ops/img_transpose/#classfunction-definition","title":"Class/Function Definition","text":"Argument Type Description x torch.Tensor The input image tensor in <code>(B, C, H, W)</code> format. <p>Usage: <pre><code>def img_transpose(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Transposes the input image tensor from (B, C, H, W) format to (B, H, W, C) format.\n\n    Parameters:\n    - x (torch.Tensor): The input image tensor.\n\n    Returns:\n    - torch.Tensor: The image tensor with transposed dimensions.\n    ```\n\n## Functional Explanation\n\nThe `img_transpose` function is built to be straightforward and easy to use. It leverages the `rearrange` function, which is a part of the `einops` library, to perform dimension rearrangement efficiently. This transformation is often necessary before displaying images using visualization libraries or for further image processing tasks that require the channel dimension at the end.\n\nBy transposing the dimensions, the `img_transpose` function ensures compatibility with libraries that expect the channel-last format (such as `matplotlib` for visualization or `tensorflow` which uses channel-lasts by default).\n\n## Usage Examples\n\nTo illustrate how to use the `img_transpose` function from the `zeta.ops` library, let\u2019s walk through three comprehensive examples.\n\n**Example 1: Basic Usage for Tensor Visualization**\n\n```python\nimport torch\nfrom zeta.ops import img_transpose\nimport matplotlib.pyplot as plt\n\n# Create a dummy image tensor in (B, C, H, W) format\nbatch_size, channels, height, width = 1, 3, 28, 28\ndummy_image = torch.randn(batch_size, channels, height, width)\n\n# Use the img_transpose function to change dimension ordering\ntransposed_image = img_transpose(dummy_image)\n\n# Visualize the image using matplotlib\nplt.imshow(transposed_image.squeeze().numpy())\nplt.show()\n</code></pre></p> <p>Example 2: Preparing Tensor for Tensorflow</p> <pre><code>import tensorflow as tf\nimport torch\n\nfrom zeta.ops import img_transpose\n\n# Create a dummy image tensor in (B, C, H, W) format\nbatch_size, channels, height, width = 4, 3, 224, 224\ndummy_images = torch.randn(batch_size, channels, height, width)\n\n# Transpose images for Tensorflow which expects (B, H, W, C)\ntf_ready_images = img_transpose(dummy_images)\n\n# Convert the torch tensor to a tensorflow tensor\ntf_images = tf.convert_to_tensor(tf_ready_images.numpy())\n\n# tf_images is now in the right format for Tensorflow operations\n</code></pre> <p>Example 3: Combining with torchvision Transforms</p> <pre><code>import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom zeta.ops import img_transpose\n\n# Load an image using PIL\nimage_path = \"path_to_your_image.jpg\"\npil_image = Image.open(image_path)\n\n# Define a torchvision transform to convert the image to tensor\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),  # Converts the image to (C, H, W) format\n    ]\n)\n\n# Apply the transform\ntorch_image = transform(pil_image).unsqueeze(\n    0\n)  # Unsqueeze to add the batch dimension (B, C, H, W)\n\n# Transpose the image tensor to (B, H, W, C) using img_transpose\nready_image = img_transpose(torch_image)\n\n# ready_image is now in the correct format for further processing\n</code></pre>"},{"location":"zeta/ops/img_transpose/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The function <code>img_transpose</code> is designed to work with batched tensor input, and so the input tensor must have four dimensions. If you have a single image, make sure to use <code>unsqueeze</code> to add a batch dimension before calling <code>img_transpose</code>.</li> <li>This function is part of the <code>zeta.ops</code> library, which might have other related image operations. It's good to explore and understand the full suite of functionalities provided.</li> <li>If working with a different dimension ordering (e.g., <code>(C, H, W)</code> without batch size), slight modifications to the function or additions to the input tensor will be required.</li> </ul>"},{"location":"zeta/ops/img_transpose/#references","title":"References","text":"<ul> <li>The <code>rearrange</code> function is part of the <code>einops</code> library, which documentation can be found here: Einops Documentation.</li> <li>PyTorch and TensorFlow documentation for tensor operations can provide additional context on when and why such a transpose operation may be necessary.</li> </ul>"},{"location":"zeta/ops/img_transpose_2daxis/","title":"img_transpose_2daxis","text":"<p>The <code>img_transpose_2daxis</code> function is designed for transposing two-dimensional image arrays across width and height while retaining the color channels in their original order. This operation is common in image processing tasks where the format of the image needs to be adjusted without altering its color representation. Below, we will explore the architecture of the <code>img_transpose_2daxis</code> function and provide thorough explanations, usage examples, and valuable insights for effective utilization.</p>"},{"location":"zeta/ops/img_transpose_2daxis/#introduction","title":"Introduction","text":"<p>In many computer vision applications and neural networks that involve images, it is often required to manipulate the dimensions of image tensors for compatibility with various algorithms and library requirements. For instance, some image processing libraries expect images in <code>(height, width, channels)</code> format, while others operate on <code>(width, height, channels)</code>. The <code>img_transpose_2daxis</code> code snippet provides a simple yet versatile function that can switch between these two spatial layouts.</p> <p>Understanding the function's architecture is straightforward as it utilizes the <code>rearrange</code> function from the <code>einops</code> library--a powerful tool for tensor manipulation that provides more readable and expressive tensor operations.</p>"},{"location":"zeta/ops/img_transpose_2daxis/#function-definition","title":"Function Definition","text":"<pre><code>def img_transpose_2daxis(x):\n    return rearrange(x, \"h w c -&gt; w h c\")\n</code></pre> Parameter Type Description x Tensor The input image tensor of shape <code>(h, w, c)</code> <p>The function <code>img_transpose_2daxis</code> accepts a single argument <code>x</code>, which is expected to be a tensor or a multi-dimensional array representing an image. The dimension order of <code>x</code> is assumed to be <code>(height, width, channels)</code>.</p>"},{"location":"zeta/ops/img_transpose_2daxis/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>img_transpose_2daxis</code> function works by utilizing the <code>rearrange</code> functionality to transpose the first two dimensions of an image tensor. Here's what happens step-by-step:</p> <ol> <li>The function takes an input image tensor <code>x</code> assumed to have the shape <code>(height, width, channels)</code>.</li> <li>The <code>rearrange</code> function is called with a pattern that specifies how the dimensions should be reordered. In this case, <code>h w c -&gt; w h c</code> translates to \"take the height and width dimensions and switch their order while keeping the channel dimension as is.\"</li> <li>The function returns the reorganized tensor.</li> </ol>"},{"location":"zeta/ops/img_transpose_2daxis/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<p>First, install the required <code>einops</code> library:</p> <pre><code>pip install einops\n</code></pre> <p>Then, use the function in a Python script:</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import img_transpose_2daxis\n\n# Create a dummy image tensor with shape (height, width, channels)\nimg_tensor = torch.rand(100, 200, 3)  # Example Tensor of shape (100, 200, 3)\n\n# Transpose the 2D axis of the image tensor\ntransposed_img = img_transpose_2daxis(img_tensor)\n\nprint(\"Original shape:\", img_tensor.shape)\nprint(\"Transposed shape:\", transposed_img.shape)\n</code></pre>"},{"location":"zeta/ops/img_transpose_2daxis/#example-2-using-with-image-data","title":"Example 2: Using with Image Data","text":"<p>Let's say you're working with image data loaded using the PIL library:</p> <pre><code>import numpy as np\nfrom PIL import Image\n\nfrom zeta.ops import img_transpose_2daxis\n\n# Open an image using PIL and convert it to a NumPy array\nimage = Image.open(\"path_to_your_image.jpg\")\nimg_array = np.array(image)\n\n# Assuming the image array has a shape (height, width, channels)\nprint(\"Original shape:\", img_array.shape)\n\n# Transpose the 2D axis using our function\ntransposed_img_array = img_transpose_2daxis(img_array)\n\nprint(\"Transposed shape:\", transposed_img_array.shape)\n</code></pre>"},{"location":"zeta/ops/img_transpose_2daxis/#example-3-integration-with-pytorch-dataloader","title":"Example 3: Integration with PyTorch DataLoader","text":"<p>If you are using <code>img_transpose_2daxis</code> as part of a data preprocessing pipeline in PyTorch:</p> <pre><code>from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nfrom zeta.ops import img_transpose_2daxis\n\n# Define a custom transform using Lambda\ntranspose_transform = transforms.Lambda(img_transpose_2daxis)\n\n# Compose this with other transforms\ntransform = transforms.Compose([transforms.ToTensor(), transpose_transform])\n\n# Use the composed transforms in your dataset loader\ntrain_loader = DataLoader(\n    your_dataset, batch_size=32, shuffle=True, transform=transform\n)\n\n# Now, when the images from train_loader are accessed, they will already be transposed\n</code></pre>"},{"location":"zeta/ops/img_transpose_2daxis/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>As <code>img_transpose_2daxis</code> relies on <code>rearrange</code> from the <code>einops</code> library, ensure that <code>einops</code> is installed and properly working in your environment.</li> <li>Be cautious about the input dimensions. If you input a tensor with incorrect dimensions (other than <code>(height, width, channels)</code>), the function might return unexpected results or raise an error.</li> <li>The function is flexible and can be easily integrated with various image preprocessing pipelines and deep learning frameworks like PyTorch and TensorFlow.</li> </ul>"},{"location":"zeta/ops/img_transpose_2daxis/#references-and-resources","title":"References and Resources","text":"<p>For more information about tensor manipulation and the <code>einops</code> library:</p> <ul> <li><code>einops</code> documentation: Einops ReadTheDocs</li> <li>PyTorch documentation: PyTorch Official Website</li> <li>PIL documentation (for image handling in Python): Pillow ReadTheDocs</li> </ul>"},{"location":"zeta/ops/img_width_to_height/","title":"img_width_to_height","text":"<p>Welcome to the zeta.ops library documentation, where we delve into the intuitive and powerful operation <code>img_width_to_height</code>. This documentation will serve as a comprehensive guide to understanding the function's architecture, usage, and purpose with in-depth examples and explicit instructional content. The <code>img_width_to_height</code> function is designed to reshape image tensor dimensions for various purposes such as algorithmic preprocessing or network input formatting.</p> <p>The zeta.ops library, although , remains essential for transformations and operations on multi-dimensional data where the shape of the tensor is paramount to the downstream application. The <code>img_width_to_height</code> function reorganizes a 4D tensor typically used for batched image data, adjusting its spatial orientation by altering the width and height dimensions.</p> <p>Before we proceed, ensure you possess a basic understanding of PyTorch, as the function manipulates PyTorch tensors and uses the <code>rearrange</code> function from the <code>einops</code> library for tensor operations.</p>"},{"location":"zeta/ops/img_width_to_height/#img_width_to_height-function-definition","title":"img_width_to_height Function Definition","text":"<pre><code>def img_width_to_height(x):\n    return rearrange(x, \"b h (w w2) c -&gt; (h w2) (b w) c\", w2=2)\n</code></pre> <p><code>img_width_to_height</code> is a function that accepts a single argument <code>x</code>, which represents a 4D tensor typically containing image data in batch.</p>"},{"location":"zeta/ops/img_width_to_height/#parameters","title":"Parameters","text":"Parameter Type Description x Tensor A 4D PyTorch tensor with shape <code>(b, h, w, c)</code> where <code>b</code> is the batch size, <code>h</code> is the height, <code>w</code> is the width, and <code>c</code> is the channel depth of the image data."},{"location":"zeta/ops/img_width_to_height/#returns","title":"Returns","text":"Return Type Description Tensor Tensor A rearranged 4D PyTorch tensor with a new shape <code>(h w2, b w, c)</code> where <code>w2</code> is hardcoded to be 2 within the scope of this function."},{"location":"zeta/ops/img_width_to_height/#functionality-and-usage","title":"Functionality and Usage","text":""},{"location":"zeta/ops/img_width_to_height/#why-this-architecture","title":"Why this Architecture?","text":"<p>The architecture of <code>img_width_to_height</code> provides a convenient way to group spatial dimensions of images in preparation for certain types of neural network layers that require specific input shapes or for image preprocessing tasks that benefit from a reshaped tensor.</p> <p>Its reliance on <code>einops.rearrange</code> allows for flexible and readable tensor transformation, which is essential when working with multi-dimensional data.</p>"},{"location":"zeta/ops/img_width_to_height/#how-it-works","title":"How it Works","text":"<p>The <code>rearrange</code> method from the <code>einops</code> library uses a string-based mini-language for tensor operations. In this instance, the following pattern is used: <code>\"b h (w w2) c -&gt; (h w2) (b w) c\"</code>. This pattern means the input tensor <code>x</code> is treated as having batch (<code>b</code>), height (<code>h</code>), width (<code>w</code> times a width factor <code>w2</code>), and channels (<code>c</code>). It then reshapes the tensor into a new shape were height is multiplied by <code>w2</code>, the batch size is multiplied by the original width and the channel remains the same.</p>"},{"location":"zeta/ops/img_width_to_height/#usage-examples","title":"Usage Examples","text":"<p>Example 1: Basic usage of img_width_to_height</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import img_width_to_height\n\n# Initialize a dummy 4D tensor representing two RGB images (batch size: 2, width: 4, height: 3, channels: 3)\nbatched_images = torch.randn(2, 3, 4, 3)\n\n# Use our function to transform the tensor's shape\ntransformed_images = img_width_to_height(batched_images)\n\nprint(transformed_images.shape)  # Output -&gt; torch.Size([6, 8, 3])\n</code></pre> <p>Example 2: Visualizing the transformation</p> <pre><code>import matplotlib.pyplot as plt\n\n# Display original image tensors\nfig, axes = plt.subplots(1, 2)\nfor i, img_tensor in enumerate(batched_images):\n    axes[i].imshow(img_tensor.permute(1, 2, 0))\n    axes[i].set_title(f\"Original Image {i+1}\")\nplt.show()\n\n# Display transformed image tensors\ntransformed_shape = transformed_images.shape\nfor i in range(transformed_shape[1] // transformed_shape[0]):\n    img_tensor = transformed_images[:, i : i + transformed_shape[0], :]\n    plt.imshow(img_tensor.permute(1, 0, 2))\n    plt.title(f\"Transformed Image {i+1}\")\n    plt.show()\n</code></pre> <p>Example 3: Preparing tensor for a custom convolutional layer</p> <pre><code>import torch.nn as nn\n\n\nclass CustomConvLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(1, 16, kernel_size=(3, 3))\n\n    def forward(self, x):\n        x = img_width_to_height(x)\n        # Assuming that the custom convolutional layer expects a single channel input\n        x = x.unsqueeze(1)  # Add a channel dimension\n        output = self.conv(x)\n        return output\n\n\n# Initialize model and dummy input\nmodel = CustomConvLayer()\ninput_tensor = torch.randn(2, 3, 4, 3)  # (batch, height, width, channels)\n\n# Forward pass\noutput = model(input_tensor)\n\nprint(output.shape)  # Output size will depend on the convolutional layer properties\n</code></pre>"},{"location":"zeta/ops/img_width_to_height/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>Make sure that the input tensor <code>x</code> has the width dimension to be an even number. The function assumes a division by 2 for width (<code>w2=2</code>).</li> <li>Consider pad\u00e4ding your image tensor to an even width if it's odd-sized before using this function.</li> <li><code>einops.rearrange</code> adds a significant level of readable abstraction for tensor reshaping, but you should familiarize yourself with its mini-language to make the most out of it.</li> </ul>"},{"location":"zeta/ops/local_softmax/","title":"local_softmax","text":"<p>The <code>local_softmax</code> function from the <code>zeta.ops</code> library is designed to handle softmax computations on large inputs by dividing them into smaller, more manageable chunks. This can be particularly useful for tasks that involve processing very large tensors that may not fit into memory if softmax were applied to the entire tensor at once.</p>"},{"location":"zeta/ops/local_softmax/#overview-and-introduction","title":"Overview and Introduction","text":"<p>Softmax is a mathematical function commonly used in the fields of machine learning and deep learning, particularly in classification tasks. It turns a vector of raw scores, often called logits, into probabilities by exponentiating and normalizing the input values. However, when dealing with very large inputs, performing softmax on the entire dataset at once can be computationally expensive and memory-intensive.</p> <p>The <code>local_softmax</code> function alleviates this concern by dividing the input tensor into multiple chunks, applying softmax individually on each chunk, and then concatenating the results together. This allows for more efficient memory usage and can reduce the computational overhead when dealing with large input tensors.</p>"},{"location":"zeta/ops/local_softmax/#function-definition","title":"Function Definition","text":"Parameter Description Type Default Value tensor The input tensor on which softmax will be applied. Tensor - num_chunks The number of chunks to split the input tensor into. int 2"},{"location":"zeta/ops/local_softmax/#local_softmax-function","title":"<code>local_softmax</code> Function","text":"<pre><code>def local_softmax(tensor, num_chunks: int = 2):\n    \"\"\"\n    Performs softmax on chunks of the input tensor.\n\n    Parameters:\n    - tensor (Tensor): The input tensor to be softmaxed.\n    - num_chunks (int): Number of chunks the input tensor is split into.\n\n    Returns:\n    - Tensor: Concatenated tensor with applied softmax on each chunk.\n    \"\"\"\n    # Implementation\n</code></pre>"},{"location":"zeta/ops/local_softmax/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>local_softmax</code> function operates by splitting the input tensor along the zeroth dimension (rows) into the specified number of chunks. It then applies the softmax function, as provided by <code>torch.nn.functional.softmax</code>, to each chunk individually. Afterward, the function concatenates the softmaxed chunks back together along the same dimension to produce the final output tensor.</p>"},{"location":"zeta/ops/local_softmax/#expected-inputs-and-outputs","title":"Expected Inputs and Outputs","text":"<ul> <li>Input: A tensor of any shape that can be split into the specified number of chunks along the zeroth dimension.</li> <li>Output: A tensor of the same shape as the input, where softmax has been applied to each corresponding chunk of the input.</li> </ul>"},{"location":"zeta/ops/local_softmax/#usage-examples","title":"Usage Examples","text":"<p>Below are three usage examples illustrating how to use the <code>local_softmax</code> function with different inputs and chunk sizes.</p>"},{"location":"zeta/ops/local_softmax/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\nfrom torch.nn import functional as F\n\n# Importing the local_softmax function\nfrom zeta.ops import local_softmax\n\n# Example tensor (for demonstration purposes)\ninput_tensor = torch.tensor([[2.0, 1.0], [0.5, -1.0], [1.0, 3.0], [2.0, 5.0]])\n\n# Apply local_softmax with 2 chunks\noutput_tensor = local_softmax(input_tensor, num_chunks=2)\nprint(output_tensor)\n</code></pre>"},{"location":"zeta/ops/local_softmax/#example-2-using-a-larger-number-of-chunks","title":"Example 2: Using a Larger Number of Chunks","text":"<pre><code>import torch\nfrom torch.nn import functional as F\n\n# Importing the local_softmax function\nfrom zeta.ops import local_softmax\n\n# Another example with a larger tensor\nlarge_input_tensor = torch.randn(10, 5)\n\n# Apply local_softmax with 5 chunks\noutput_tensor = local_softmax(large_input_tensor, num_chunks=5)\nprint(output_tensor)\n</code></pre>"},{"location":"zeta/ops/local_softmax/#example-3-exception-handling-when-number-of-chunks-mismatch","title":"Example 3: Exception Handling When Number of Chunks Mismatch","text":"<pre><code>import torch\nfrom torch.nn import functional as F\n\n# Importing the local_softmax function\nfrom zeta.ops import local_softmax\n\n# Another example with tensor that can't be evenly split into chunks\nodd_sized_tensor = torch.randn(7, 3)\n\n# Attempt to apply local_softmax with 4 chunks\ntry:\n    output_tensor = local_softmax(odd_sized_tensor, num_chunks=4)\n    print(output_tensor)\nexcept RuntimeError as e:\n    print(f\"Error: {e}\")\n</code></pre> <p>Note: In the third example, since the input tensor cannot be evenly split into 4 chunks, a <code>RuntimeError</code> is raised by PyTorch. Users will need to handle such exceptions or ensure that the number of chunks divides the size of the first dimension of the tensor.</p>"},{"location":"zeta/ops/local_softmax/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>Ensure that the number of chunks specified in <code>num_chunks</code> is a divisor of the size of the tensor's zeroth dimension to avoid runtime errors.</li> <li>Consider the implications of performing softmax on chunks\u2014that is, softmax will be applied independently to each chunk, not across the whole tensor. This means that if there is any relationship between the chunks that needs to be preserved, this method might not be appropriate.</li> <li>The choice of chunk size could potentially impact the performance of subsequent operations on the softmaxed tensor, so it may require some experimentation to find the optimal balance between memory usage and computational efficiency.</li> </ul>"},{"location":"zeta/ops/local_softmax/#references-and-resources","title":"References and Resources","text":"<p>For more information on the softmax function and its applications, the following resources may be useful: - PyTorch Documentation: <code>torch.nn.functional.softmax</code> - Stanford University's CS231n Notes on Softmax - Understanding the Softmax Function by Sebastian Ruder</p> <p>These resources provide a deeper understanding of the theoretical background behind softmax and its implementation details within the PyTorch framework.</p>"},{"location":"zeta/ops/logit_scaled_softmax/","title":"logit_scaled_softmax","text":"<p>The <code>zeta.ops</code> library is a collection of custom operations that augment the capabilities of PyTorch, a deep learning framework widely used for building neural networks. The primary goal of <code>zeta.ops</code> is to provide specialized and optimized operations that are not directly available within the standard PyTorch package, thereby enhancing the performance and functionality of PyTorch models.</p>"},{"location":"zeta/ops/logit_scaled_softmax/#logit_scaled_softmax_1","title":"logit_scaled_softmax","text":""},{"location":"zeta/ops/logit_scaled_softmax/#definition","title":"Definition","text":"<p>The <code>logit_scaled_softmax</code> function is a modified version of the standard softmax operation. It scales the logits before applying the softmax function, which can be useful in scenarios where control over the distribution sharpness of the output probabilities is desired.</p>"},{"location":"zeta/ops/logit_scaled_softmax/#parameters","title":"Parameters","text":"Parameter Type Description Default Value <code>x</code> Tensor The input tensor containing logits to be scaled. N/A <code>scale</code> float The scale parameter to adjust the sharpness. 1.0"},{"location":"zeta/ops/logit_scaled_softmax/#function-description","title":"Function Description","text":"<pre><code>import torch.nn.functional as F\n\n\ndef logit_scaled_softmax(x, scale=1.0):\n    \"\"\"\n    Computes the scaled softmax of the input tensor.\n\n    Args:\n        x (Tensor): The input tensor containing logits.\n        scale (float, optional): A scaling factor to apply to logits before the softmax. Default: 1.0\n\n    Returns:\n        Tensor: A tensor containing the resulting scaled softmax probabilities.\n    \"\"\"\n    return F.softmax(x * scale, dim=-1)\n</code></pre>"},{"location":"zeta/ops/logit_scaled_softmax/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/ops/logit_scaled_softmax/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta.ops import logit_scaled_softmax\n\n# Create a tensor of logits\nlogits = torch.tensor([1.0, 2.0, 3.0])\n\n# Apply logit_scaled_softmax without scaling (default behavior)\nsoftmax_probs = logit_scaled_softmax(logits)\nprint(softmax_probs)\n</code></pre>"},{"location":"zeta/ops/logit_scaled_softmax/#example-2-adjusting-sharpness-with-scale","title":"Example 2: Adjusting Sharpness with Scale","text":"<pre><code>import torch\n\nfrom zeta.ops import logit_scaled_softmax\n\n# Create a tensor of logits\nlogits = torch.tensor([1.0, 2.0, 3.0])\n\n# Apply logit_scaled_softmax with scaling to increase sharpness\nscale = 2.0\nsharper_softmax_probs = logit_scaled_softmax(logits, scale)\nprint(sharper_softmax_probs)\n</code></pre>"},{"location":"zeta/ops/logit_scaled_softmax/#example-3-using-logit_scaled_softmax-in-neural-networks","title":"Example 3: Using logit_scaled_softmax in Neural Networks","text":"<pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.ops import logit_scaled_softmax\n\n\n# Define a simple neural network with logit_scaled_softmax\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(10, 3)\n\n    def forward(self, x, scale=1.0):\n        logits = self.fc(x)\n        return logit_scaled_softmax(logits, scale)\n\n\n# Create a random input tensor\ninput_tensor = torch.randn(5, 10)\n\n# Instantiate the neural network\nmodel = SimpleNN()\n\n# Forward pass with custom softmax operation\noutput_probs = model(input_tensor, scale=1.5)\nprint(output_probs)\n</code></pre>"},{"location":"zeta/ops/logit_scaled_softmax/#functionality-and-architecture","title":"Functionality and Architecture","text":"<p>The <code>logit_scaled_softmax</code> function is designed to modulate the sharpness of the output probabilities obtained from the softmax function. Scaling logits prior to applying the softmax can be particularly useful when adjusting the confidence of the predictions made by a model.</p> <p>Multiplying the logits by a scale factor greater than 1 increases the difference between the highest and other logits, leading to a sharper probability distribution where one class's probability is much higher than the others. Conversely, a scale factor less than 1 will make the probability distribution softer, providing a more uniform distribution of probabilities across classes.</p> <p>This operation can be used in various parts of a neural network, such as the final classification layer or within attention mechanisms to control the distribution of attention weights.</p>"},{"location":"zeta/ops/logit_scaled_softmax/#additional-tips","title":"Additional Tips","text":"<ul> <li>When using <code>logit_scaled_softmax</code>, experiment with different scale values as part of hyperparameter tuning to find the optimal level of sharpness for your specific use case.</li> <li>Be cautious when applying very high scale factors, as this might lead to numerical instability due to the softmax function's exponential nature.</li> <li>The <code>logit_scaled_softmax</code> is differentiable, allowing it to be incorporated into a model's architecture and trained end-to-end using backpropagation.</li> </ul>"},{"location":"zeta/ops/logit_scaled_softmax/#references-and-resources","title":"References and Resources","text":"<ul> <li>PyTorch Documentation: Softmax Function</li> <li>Goodfellow, Ian, et al. \"Deep Learning.\" MIT Press, 2016, section on softmax function, provides an in-depth background on the softmax function and its properties.</li> </ul> <p>To explore more about PyTorch and deep learning models, consider visiting the official PyTorch website and reviewing the extensive documentation and tutorials available.</p>"},{"location":"zeta/ops/main/","title":"<code>zeta.ops</code> Documentation","text":""},{"location":"zeta/ops/main/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Installation</li> <li>Module Overview</li> <li>Function Definitions</li> <li><code>check_diagonal</code></li> <li><code>matrix_inverse_root</code></li> <li><code>matrix_root_diagonal</code></li> <li><code>_matrix_root_eigen</code></li> <li><code>_matrix_inverse_root_newton</code></li> <li><code>compute_matrix_root_inverse_residuals</code></li> <li><code>merge_small_dims</code></li> <li><code>multi_dim_split</code></li> <li><code>multi_dim_cat</code></li> <li>Usage Examples</li> <li>Example 1: Matrix Inverse Root using Eigen Decomposition</li> <li>Example 2: Matrix Inverse Root using Coupled Newton Iteration</li> <li>Example 3: Residual Computation</li> <li>Example 4: Merging Small Dimensions</li> <li>Example 5: Multi-Dimensional Split and Concatenation</li> </ol>"},{"location":"zeta/ops/main/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the <code>zeta.ops</code> documentation! <code>zeta.ops</code> is a Python library designed to handle matrix operations related to matrix inverse root computation and other related tasks. It provides functionality for computing matrix inverse roots, working with diagonal matrices, performing eigen decomposition, and more.</p> <p>This documentation will guide you through the installation process, provide an overview of the module, and explain the various functions and their usage with examples.</p>"},{"location":"zeta/ops/main/#2-installation","title":"2. Installation","text":"<p>You can install zeta using the Python package manager pip:</p> <pre><code>pip install zetascale\n</code></pre> <p>Once installed, you can import the library in your Python scripts and start using its functionality.</p>"},{"location":"zeta/ops/main/#3-module-overview","title":"3. Module Overview","text":"<p><code>zeta.ops</code> primarily focuses on matrix operations and computations related to matrix inverse roots. Here are some key features of the zeta library:</p> <ul> <li>Computing matrix inverse roots of square symmetric positive definite matrices.</li> <li>Handling diagonal matrices efficiently.</li> <li>Eigen decomposition of symmetric matrices.</li> <li>Coupled Newton iteration for matrix inverse root computation.</li> <li>Residual computation for debugging purposes.</li> <li>Merging small dimensions in tensor shapes.</li> <li>Multi-dimensional splitting and concatenation of tensors.</li> </ul> <p>In the following sections, we'll delve into the details of each function and provide examples of their usage.</p>"},{"location":"zeta/ops/main/#4-function-definitions","title":"4. Function Definitions","text":""},{"location":"zeta/ops/main/#41-check_diagonal","title":"4.1 <code>check_diagonal</code>","text":"<pre><code>check_diagonal(A: Tensor) -&gt; Tensor\n</code></pre> <p>Checks if a symmetric matrix is diagonal.</p> <ul> <li><code>A</code> (Tensor): The input matrix to check.</li> </ul> <p>Returns: - <code>Tensor</code>: A boolean tensor indicating whether the matrix is diagonal.</p>"},{"location":"zeta/ops/main/#42-matrix_inverse_root","title":"4.2 <code>matrix_inverse_root</code>","text":"<pre><code>matrix_inverse_root(\n    A: Tensor,\n    root: int,\n    epsilon: float = 0.0,\n    exponent_multiplier: float = 1.0,\n    root_inv_method: RootInvMethod = RootInvMethod.EIGEN,\n    max_iterations: int = 1000,\n    tolerance: float = 1e-6,\n    is_diagonal: Union[Tensor, bool] = False,\n    retry_double_precision: bool = True,\n) -&gt; Tensor\n</code></pre> <p>Computes the matrix inverse root of a square symmetric positive definite matrix.</p> <ul> <li><code>A</code> (Tensor): The input matrix.</li> <li><code>root</code> (int): The root of interest (any natural number).</li> <li><code>epsilon</code> (float): Adds epsilon * I to the matrix before taking the matrix root (default: 0.0).</li> <li><code>exponent_multiplier</code> (float): Exponent multiplier in the eigen method (default: 1.0).</li> <li><code>root_inv_method</code> (RootInvMethod): Specifies the method to use for computing the root inverse (default: RootInvMethod.EIGEN).</li> <li><code>max_iterations</code> (int): Maximum number of iterations for coupled Newton iteration (default: 1000).</li> <li><code>tolerance</code> (float): Tolerance for computing the root inverse using coupled Newton iteration (default: 1e-6).</li> <li><code>is_diagonal</code> (Union[Tensor, bool]): Flag for whether or not the matrix is diagonal. If set to True, the function computes the root inverse of diagonal entries (default: False).</li> <li><code>retry_double_precision</code> (bool): Flag for re-trying eigendecomposition with higher precision if lower precision fails due to CuSOLVER failure</li> </ul> <p>(default: True).</p> <p>Returns: - <code>Tensor</code>: The matrix inverse root.</p>"},{"location":"zeta/ops/main/#43-matrix_root_diagonal","title":"4.3 <code>matrix_root_diagonal</code>","text":"<pre><code>matrix_root_diagonal(\n    A: Tensor,\n    root: int,\n    epsilon: float = 0.0,\n    inverse: bool = True,\n    exponent_multiplier: float = 1.0,\n    return_full_matrix: bool = False,\n) -&gt; Tensor\n</code></pre> <p>Computes the matrix inverse root for a diagonal matrix by taking the inverse square root of diagonal entries.</p> <ul> <li><code>A</code> (Tensor): A one- or two-dimensional tensor containing either the diagonal entries of the matrix or a diagonal matrix.</li> <li><code>root</code> (int): The root of interest (any natural number).</li> <li><code>epsilon</code> (float): Adds epsilon * I to the matrix before taking the matrix root (default: 0.0).</li> <li><code>inverse</code> (bool): If True, returns the inverse root matrix (default: True).</li> <li><code>exponent_multiplier</code> (float): Exponent multiplier to be multiplied to the numerator of the inverse root (default: 1.0).</li> <li><code>return_full_matrix</code> (bool): If True, returns the full matrix by taking the torch.diag of diagonal entries (default: False).</li> </ul> <p>Returns: - <code>Tensor</code>: The inverse root of diagonal entries.</p>"},{"location":"zeta/ops/main/#44-_matrix_root_eigen","title":"4.4 <code>_matrix_root_eigen</code>","text":"<pre><code>_matrix_root_eigen(\n    A: Tensor,\n    root: int,\n    epsilon: float = 0.0,\n    inverse: bool = True,\n    exponent_multiplier: float = 1.0,\n    make_positive_semidefinite: bool = True,\n    retry_double_precision: bool = True,\n) -&gt; Tuple[Tensor, Tensor, Tensor]\n</code></pre> <p>Compute the matrix (inverse) root using eigendecomposition of a symmetric positive (semi-)definite matrix.</p> <ul> <li><code>A</code> (Tensor): The square matrix of interest.</li> <li><code>root</code> (int): The root of interest (any natural number).</li> <li><code>epsilon</code> (float): Adds epsilon * I to the matrix before taking the matrix root (default: 0.0).</li> <li><code>inverse</code> (bool): If True, returns the inverse root matrix (default: True).</li> <li><code>exponent_multiplier</code> (float): Exponent multiplier in the eigen method (default: 1.0).</li> <li><code>make_positive_semidefinite</code> (bool): Perturbs matrix eigenvalues to ensure it is numerically positive semi-definite (default: True).</li> <li><code>retry_double_precision</code> (bool): Flag for re-trying eigendecomposition with higher precision if lower precision fails due to CuSOLVER failure (default: True).</li> </ul> <p>Returns: - <code>Tuple[Tensor, Tensor, Tensor]</code>: A tuple containing the (inverse) root matrix, eigenvalues, and orthogonal matrix consisting of eigenvectors.</p>"},{"location":"zeta/ops/main/#45-_matrix_inverse_root_newton","title":"4.5 <code>_matrix_inverse_root_newton</code>","text":"<pre><code>_matrix_inverse_root_newton(\n    A: Tensor,\n    root: int,\n    epsilon: float = 0.0,\n    max_iterations: int = 1000,\n    tolerance: float = 1e-6,\n) -&gt; Tuple[Tensor, Tensor, NewtonConvergenceFlag, int, Tensor]\n</code></pre> <p>Compute the matrix inverse root using coupled inverse Newton iteration.</p> <ul> <li><code>A</code> (Tensor): The matrix of interest.</li> <li><code>root</code> (int): The root of interest (any natural number).</li> <li><code>epsilon</code> (float): Adds epsilon * I to the matrix before taking the matrix root (default: 0.0).</li> <li><code>max_iterations</code> (int): Maximum number of iterations (default: 1000).</li> <li><code>tolerance</code> (float): Tolerance (default: 1e-6).</li> </ul> <p>Returns: - <code>Tuple[Tensor, Tensor, NewtonConvergenceFlag, int, Tensor]</code>: A tuple containing the (inverse) root matrix, coupled matrix, convergence flag, number of iterations, and final error.</p>"},{"location":"zeta/ops/main/#46-compute_matrix_root_inverse_residuals","title":"4.6 <code>compute_matrix_root_inverse_residuals</code>","text":"<pre><code>compute_matrix_root_inverse_residuals(\n    A: Tensor,\n    X_hat: Tensor,\n    root: int,\n    epsilon: float,\n    exponent_multiplier: float,\n) -&gt; Tuple[Tensor, Tensor]\n</code></pre> <p>Compute the residual of the matrix root inverse for debugging purposes.</p> <ul> <li><code>A</code> (Tensor): The matrix of interest.</li> <li><code>X_hat</code> (Tensor): The computed matrix root inverse.</li> <li><code>root</code> (int): The root of interest.</li> <li><code>epsilon</code> (float): Adds epsilon * I to the matrix.</li> <li><code>exponent_multiplier</code> (float): Exponent multiplier to be multiplied to the numerator of the inverse root.</li> </ul> <p>Returns: - <code>Tuple[Tensor, Tensor]</code>: A tuple containing the absolute error and relative error of the matrix root inverse.</p>"},{"location":"zeta/ops/main/#47-merge_small_dims","title":"4.7 <code>merge_small_dims</code>","text":"<pre><code>merge_small_dims(\n    tensor_shape: List[int],\n    threshold: int\n) -&gt; List[int]\n</code></pre> <p>Reshapes a tensor by merging small dimensions.</p> <ul> <li><code>tensor_shape</code> (List[int]): The shape of the tensor.</li> <li><code>threshold</code> (int): The threshold on the maximum size of each dimension.</li> </ul> <p>Returns: - <code>List[int]</code>: The new tensor shape.</p>"},{"location":"zeta/ops/main/#48-multi_dim_split","title":"4.8 <code>multi_dim_split</code>","text":"<pre><code>multi_dim_split(\n    tensor: Tensor,\n    splits: List[int],\n) -&gt; List[Tensor]\n</code></pre> <p>Chunks a tensor across multiple dimensions based on splits.</p> <ul> <li><code>tensor</code> (Tensor): The gradient or tensor to split.</li> <li><code>splits</code> (List[int]): The list of sizes for each block or chunk along each dimension.</li> </ul> <p>Returns: - <code>List[Tensor]</code>: The list of tensors after splitting.</p>"},{"location":"zeta/ops/main/#49-multi_dim_cat","title":"4.9 <code>multi_dim_cat</code>","text":"<pre><code>multi_dim_cat(\n    split_tensors: List[Tensor],\n    num_splits: List[int]\n) -&gt; Tensor\n</code></pre> <p>Concatenates multiple tensors to form a single tensor across multiple dimensions.</p> <ul> <li><code>split_tensors</code> (List[Tensor]): The list of tensor splits or blocks.</li> <li><code>num_splits</code> (List[int]): The number of splits/blocks.</li> </ul> <p>Returns: - <code>Tensor</code>: The merged tensor.</p>"},{"location":"zeta/ops/main/#5-usage-examples","title":"5. Usage Examples","text":"<p>Let's explore some usage examples of the functions provided by the zeta library.</p>"},{"location":"zeta/ops/main/#51-example-1-matrix-inverse-root-using-eigen-method","title":"5.1 Example 1: Matrix Inverse Root using Eigen Method","text":"<p>In this example, we will compute the matrix inverse root of a symmetric positive definite matrix using the eigen method. We will use the following parameters:</p> <pre><code>import torch\n\nfrom zeta import RootInvMethod, matrix_inverse_root\n\nA = torch.tensor([[4.0, 2.0], [2.0, 3.0]])\nroot = 2\nepsilon = 1e-6\nexponent_multiplier = 1.0\nmethod = RootInvMethod.EIGEN\n\nX = matrix_inverse_root(\n    A,\n    root,\n    epsilon=epsilon,\n    exponent_multiplier=exponent_multiplier,\n    root_inv_method=method,\n)\nprint(X)\n</code></pre>"},{"location":"zeta/ops/main/#52-example-2-matrix-root-diagonal","title":"5.2 Example 2: Matrix Root Diagonal","text":"<p>In this example, we will compute the matrix inverse root for a diagonal matrix by taking the inverse square root of diagonal entries. We will use the following parameters:</p> <pre><code>import torch\n\nfrom zeta import matrix_root_diagonal\n\nA = torch.tensor([4.0, 9.0])\nroot = 2\nepsilon = 1e-6\nexponent_multiplier = 1.0\n\nX = matrix_root_diagonal(\n    A, root, epsilon=epsilon, exponent_multiplier=exponent_multiplier\n)\nprint(X)\n</code></pre>"},{"location":"zeta/ops/main/#53-example-3-matrix-inverse-root-using-newton-method","title":"5.3 Example 3: Matrix Inverse Root using Newton Method","text":"<p>In this example, we will compute the matrix inverse root using the coupled inverse Newton iteration method. We will use the following parameters:</p> <pre><code>import torch\n\nfrom zeta import RootInvMethod, matrix_inverse_root\n\nA = torch.tensor([[4.0, 2.0], [2.0, 3.0]])\nroot = 2\nepsilon = 1e-6\nexponent_multiplier = 1.0\nmethod = RootInvMethod.NEWTON\n\nX = matrix_inverse_root(\n    A,\n    root,\n    epsilon=epsilon,\n    exponent_multiplier=exponent_multiplier,\n    root_inv_method=method,\n)\nprint(X)\n</code></pre> <p>In this example, we compute the matrix inverse root of a 2x2 matrix <code>A</code> with a root of 2 using the coupled inverse Newton iteration method. The result is a matrix <code>X</code> that represents the inverse square root of <code>A</code>.</p>"},{"location":"zeta/ops/main/#6-additional-information-and-tips","title":"6. Additional Information and Tips","text":"<p>Here are some additional tips and information to help you effectively use the zeta library:</p> <ul> <li> <p>Exponent Multiplier: The <code>exponent_multiplier</code> parameter is used to adjust the exponent in the eigen method. Be cautious when changing this parameter, as it may affect the correctness of the result.</p> </li> <li> <p>Eigen Method Precision: The eigen method may fail in lower precision (e.g., float32) for certain matrices. You can enable <code>retry_double_precision</code> to retry the eigendecomposition in double precision if the initial attempt fails.</p> </li> <li> <p>Newton Method Convergence: The coupled inverse Newton iteration method may not always converge within the specified <code>max_iterations</code> and <code>tolerance</code>. It's important to monitor the convergence and adjust these parameters accordingly.</p> </li> <li> <p>Debugging with Residuals: The function <code>compute_matrix_root_inverse_residuals</code> can be used to compute residuals for debugging purposes. It helps in verifying the correctness of the computed matrix root inverse.</p> </li> </ul>"},{"location":"zeta/ops/main/#7-references-and-resources","title":"7. References and Resources","text":"<p>Here are some references and resources for further exploration of matrix root and inverse methods:</p> <ul> <li>Matrix Square Root</li> <li>Matrix Inverse Root</li> <li>Eigenvalue Decomposition</li> </ul> <p>For more advanced use cases and research, consider exploring academic papers and textbooks on linear algebra and matrix computations.</p>"},{"location":"zeta/ops/matrix_inverse_root/","title":"matrix_inverse_root","text":"<p>The <code>matrix_inverse_root</code> function is a part of the zeta.ops library, responsible for computing the matrix root inverse of square symmetric positive definite matrices.</p>"},{"location":"zeta/ops/matrix_inverse_root/#purpose-and-importance","title":"Purpose and Importance","text":"<p>In various scientific and engineering applications, such as signal processing, machine learning, and statistical analysis, it is often essential to compute the inverse square root of a matrix efficiently. The <code>matrix_inverse_root</code> function aims to provide a robust and accurate solution to this problem with support for several computation methods.</p>"},{"location":"zeta/ops/matrix_inverse_root/#function-definition","title":"Function Definition","text":"<pre><code>def matrix_inverse_root(\n    A: Tensor,\n    root: int,\n    epsilon: float = 0.0,\n    exponent_multiplier: float = 1.0,\n    root_inv_method: RootInvMethod = RootInvMethod.EIGEN,\n    max_iterations: int = 1000,\n    tolerance: float = 1e-6,\n    is_diagonal: Union[Tensor, bool] = False,\n    retry_double_precision: bool = True,\n) -&gt; Tensor: ...\n</code></pre>"},{"location":"zeta/ops/matrix_inverse_root/#parameters","title":"Parameters","text":"Argument Type Description Default Value <code>A</code> Tensor Square matrix of interest. Required <code>root</code> int Root of interest. Any natural number. Required <code>epsilon</code> float Adds epsilon * I to the matrix before taking matrix inverse. 0.0 <code>exponent_multiplier</code> float Exponent multiplier in the eigen method. 1.0 <code>root_inv_method</code> RootInvMethod Method to compute root inverse: Eigen decomposition or Newton's iteration. RootInvMethod.EIGEN <code>max_iterations</code> int Maximum number of iterations for Newton iteration. 1000 <code>tolerance</code> float Tolerance for Newton iteration. 1e-6 <code>is_diagonal</code> Union[Tensor, bool] Flag indicating if the matrix is diagonal. False <code>retry_double_precision</code> bool Flag for retrying eigen decomposition with higher precision if the first attempt fails. True"},{"location":"zeta/ops/matrix_inverse_root/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/ops/matrix_inverse_root/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta.ops import RootInvMethod, matrix_inverse_root\n\n# Example symmetric positive definite matrix\nA = torch.tensor([[4.0, 0.0], [0.0, 9.0]])\n\n# Computing the square root inverse.\nX = matrix_inverse_root(A, root=2)\nprint(X)\n</code></pre>"},{"location":"zeta/ops/matrix_inverse_root/#example-2-diagonal-matrix-with-epsilon","title":"Example 2: Diagonal Matrix with Epsilon","text":"<pre><code>import torch\n\nfrom zeta.ops import matrix_inverse_root\n\n# Diagonal matrix definition.\nA = torch.diag(torch.tensor([4.0, 9.0]))\nepsilon = 1e-5\n\n# Using epsilon to ensure numeric stability.\nX = matrix_inverse_root(A, root=2, epsilon=epsilon, is_diagonal=True)\nprint(X)\n</code></pre>"},{"location":"zeta/ops/matrix_inverse_root/#example-3-newtons-iteration-method","title":"Example 3: Newton's Iteration Method","text":"<pre><code>import torch\n\nfrom zeta.ops import RootInvMethod, matrix_inverse_root\n\n# Symmetric positive definite matrix.\nA = torch.tensor([[10.0, 4.0], [4.0, 6.0]])\n\n# Using Newton's iteration with a custom tolerance and max iterations.\nX = matrix_inverse_root(\n    A, root=2, root_inv_method=RootInvMethod.NEWTON, tolerance=1e-8, max_iterations=5000\n)\nprint(X)\n</code></pre>"},{"location":"zeta/ops/matrix_inverse_root/#advanced-topics-and-additional-information","title":"Advanced Topics and Additional Information","text":"<ul> <li>Explain the mathematical background.</li> <li>Discuss the computational complexity.</li> <li>Explore the trade-offs between accuracy and performance.</li> <li>Provide further reading materials and resources.</li> </ul>"},{"location":"zeta/ops/matrix_inverse_root/#source-code-explanation","title":"Source Code Explanation","text":"<p>Provide line-by-line comments and rationale behind the implementation of each branch in the code.</p>"},{"location":"zeta/ops/matrix_inverse_root/#handling-common-issues-and-challenges","title":"Handling Common Issues and Challenges","text":"<p>Detail common issues that may arise when using the <code>matrix_inverse_root</code> function, such as numerical instability or convergence problems, and suggest potential solutions and troubleshooting steps.</p>"},{"location":"zeta/ops/matrix_root_diagonal/","title":"matrix_root_diagonal","text":"<p><pre><code>def matrix_root_diagonal(\n    A: torch.Tensor,\n    root: int,\n    epsilon: float = 0.0,\n    inverse: bool = True,\n    exponent_multiplier: float = 1.0,\n    return_full_matrix: bool = False\n) -&gt; torch.Tensor:\n</code></pre> Computes the inverse root of a diagonal matrix by taking the inverse square root of the diagonal entries. This function can either manipulate the given tensor directly if it represents a diagonal of a matrix or extract the diagonal from a 2D tensor and then proceed with the computation.</p>"},{"location":"zeta/ops/matrix_root_diagonal/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>A</code> <code>torch.Tensor</code> A tensor representing either the diagonal of a matrix or a full diagonal matrix. <code>root</code> <code>int</code> The root of interest. Must be a natural number. <code>epsilon</code> <code>float</code> <code>0.0</code> A small value added to the diagonal to avoid numerical issues. <code>inverse</code> <code>bool</code> <code>True</code> Specifies whether to return the inverse root. <code>exponent_multiplier</code> <code>float</code> <code>1.0</code> Multiplier for the exponent, providing additional transformation control. <code>return_full_matrix</code> <code>bool</code> <code>False</code> If <code>True</code>, the result is a full matrix with the diagonal altered. Otherwise, only the diagonal is returned."},{"location":"zeta/ops/matrix_root_diagonal/#returns","title":"Returns","text":"Name Type Description <code>X</code> <code>torch.Tensor</code> The resulting tensor after computing the inverse root of the diagonal matrix."},{"location":"zeta/ops/matrix_root_diagonal/#overview","title":"Overview","text":"<p>The <code>matrix_root_diagonal</code> function is an essential utility for operations such as whitening a covariance matrix where the matrix root is needed. It supports both direct diagonal input and square matrices, giving it versatility for various use cases.</p>"},{"location":"zeta/ops/matrix_root_diagonal/#architecture-and-operation","title":"Architecture and Operation","text":"<p>The internal workflow checks the dimensionality of the input tensor <code>A</code>. It raises an exception for non-2D tensors. For input representing a full square matrix, it extracts the diagonal. The necessary inverse root computations are then applied to the diagonal entries, with an option to reintegrate them into a full matrix.</p>"},{"location":"zeta/ops/matrix_root_diagonal/#usage-example-1-basic-diagonal-tensor","title":"Usage Example 1: Basic Diagonal Tensor","text":"<pre><code>import torch\n\nfrom zeta.ops import matrix_root_diagonal\n\n# Create a diagonal tensor\nA = torch.tensor([4.0, 9.0, 16.0])\n\n# Compute the inverse square root of the diagonal\nroot_matrix = matrix_root_diagonal(A, root=2)\n\nprint(root_matrix)\n</code></pre>"},{"location":"zeta/ops/matrix_root_diagonal/#usage-example-2-full-matrix-with-epsilon","title":"Usage Example 2: Full matrix with epsilon","text":"<pre><code>import torch\n\nfrom zeta.ops import matrix_root_diagonal\n\n# Create a diagonal matrix\nA = torch.diag(torch.tensor([4.0, 9.0, 16.0]))\n\n# Compute the inverse square root of the diagonal with epsilon\nroot_matrix = matrix_root_diagonal(A, root=2, epsilon=0.1)\n\nprint(root_matrix)\n</code></pre>"},{"location":"zeta/ops/matrix_root_diagonal/#usage-example-3-return-full-matrix","title":"Usage Example 3: Return Full Matrix","text":"<pre><code>import torch\n\nfrom zeta.ops import matrix_root_diagonal\n\n# Create a diagonal tensor\nA = torch.tensor([4.0, 9.0, 16.0])\n\n# Compute the inverse square root and return the full matrix\nroot_matrix = matrix_root_diagonal(A, root=2, return_full_matrix=True)\n\nprint(root_matrix)\n</code></pre>"},{"location":"zeta/ops/matrix_root_diagonal/#additional-information-tips","title":"Additional Information &amp; Tips","text":"<ul> <li>The function ensures numerical stability by adding a small value <code>epsilon</code> to the diagonal before computation.</li> <li>The computation involves element-wise operations. Hence, the input tensor <code>A</code> is expected to have one or two dimensions only.</li> <li>Setting <code>inverse</code> to <code>False</code> results in the computation of the direct root rather than the inverse.</li> </ul>"},{"location":"zeta/ops/matrix_root_diagonal/#references-and-further-reading","title":"References and Further Reading","text":"<p>For a better understanding of matrix roots and their applications, the following resources may be helpful: - Higham, Nicholas J. \"Computing real square roots of a real matrix.\" Linear Algebra and its applications 88 (1987): 405-430. - Wikipedia entry on Matrix Functions: https://en.wikipedia.org/wiki/Matrix_function</p>"},{"location":"zeta/ops/merge_small_dims/","title":"merge_small_dims","text":"<p>allows reshaping of a tensor by merging its smaller dimensions (below a certain threshold) while ensuring that the overall element count of the tensor remains unchanged. This operation is particularly useful in developing deep learning models where tensor dimensions might need adjustments before passing through layers or operations.</p>"},{"location":"zeta/ops/merge_small_dims/#classfunction-definition","title":"Class/Function Definition","text":"<p>The <code>merge_small_dims</code> function is described as follows:</p> Argument Type Description Default <code>tensor_shape</code> <code>List[int]</code> The shape of the tensor as a list of integers. N/A <code>threshold</code> <code>int</code> The threshold on the maximum size of each dimension. N/A"},{"location":"zeta/ops/merge_small_dims/#functionality-and-usage","title":"Functionality and Usage","text":"<p><code>merge_small_dims</code> takes in the shape of a tensor and merges dimensions with size less than or equal to a specified threshold. This utility does not affect the data within the tensor; instead, it provides a new tensor shape that can be applied to reshape the tensor.</p> <p>When to use <code>merge_small_dims</code>:</p> <ul> <li>When the tensor has many small dimensions that can be combined without altering the underlying data structure.</li> <li>When optimizing memory layout for tensors for computational efficiency.</li> <li>To conform to layer or operation constraints that require a specific number of dimensions in PyTorch (or similar libraries).</li> </ul>"},{"location":"zeta/ops/merge_small_dims/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/ops/merge_small_dims/#basic-example","title":"Basic Example","text":"<pre><code>from zeta.ops import merge_small_dims\n\n# Original tensor shape\norig_shape = [2, 3, 1, 5, 1]\n# Threshold for maximum size of each dimension after the merge\nthreshold = 10\n\n# Merging small dimensions\nnew_shape = merge_small_dims(orig_shape, threshold)\nprint(new_shape)  # Output: [6, 5]\n</code></pre> <p>In the example above, the original shape of <code>[2, 3, 1, 5, 1]</code> contains small dimensions that can be merged without exceeding the threshold of <code>10</code>. The resulting <code>new_shape</code> after calling <code>merge_small_dims</code> is <code>[6, 5]</code>.</p>"},{"location":"zeta/ops/merge_small_dims/#pytorch-integration-example","title":"PyTorch Integration Example","text":"<pre><code>import torch\n\nfrom zeta.ops import merge_small_dims\n\n# Define a tensor with a shape that includes small dimensions\ntensor = torch.rand(2, 3, 1, 5, 1)\n\n# Define the threshold\nthreshold = 10\n\n# Obtain the new shape\nnew_shape = merge_small_dims(tensor.size(), threshold)\n\n# Reshape the tensor accordingly\nreshaped_tensor = tensor.view(new_shape)\n\nprint(reshaped_tensor.size())  # Output: torch.Size([6, 5])\n</code></pre> <p>In this example, we use PyTorch to define a random tensor with a shape that includes small dimensions. We then obtain a new shape from the <code>merge_small_dims</code> function and apply it to the tensor using <code>.view(new_shape)</code> method provided by PyTorch.</p>"},{"location":"zeta/ops/merge_small_dims/#preventing-dimension-merge-example","title":"Preventing Dimension Merge Example","text":"<pre><code>from zeta.ops import merge_small_dims\n\n# Original shape that includes a dimension larger than the threshold which should not be merged\norig_shape = [2, 10, 1, 5, 1]\n# Threshold for maximum size of each dimension after merge\nthreshold = 9  # Lower than the size of the second dimension\n\n# Merging small dimensions\nnew_shape = merge_small_dims(orig_shape, threshold)\nprint(new_shape)  # Output: [2, 10, 5]\n</code></pre> <p>Here, the second dimension of size <code>10</code> is not merged with any other dimension because it exceeds the threshold of <code>9</code>. Only the third, fourth, and fifth dimensions are merged because their combined size (<code>1 * 5 * 1</code>) is within the limit.</p>"},{"location":"zeta/ops/merge_small_dims/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The function assumes the input shape is valid and does not include validation for negative sizes or non-integer values.</li> <li>The first dimension is never merged with any other dimension. This is typically due to the first dimension representing the batch size in most deep learning frameworks.</li> <li>The thresholds should be chosen carefully with an understanding of how it may affect subsequent operations that rely on tensor shapes.</li> <li>It's recommended to thoroughly verify the new tensor shape with respect to the needs of your specific model or computation graph.</li> </ul>"},{"location":"zeta/ops/mos/","title":"<code>MixtureOfSoftmaxes</code> Documentation","text":"<p>The <code>MixtureOfSoftmaxes</code> module is designed to improve the modeling capabilities of the softmax function by allowing the combination of multiple softmax distributions. It takes an input tensor and computes a weighted sum of softmax outputs from different softmax layers. These weights are learned during training, enabling the model to adapt to the data's characteristics effectively.</p> <p>The primary use case of the MoS module is in scenarios where a single softmax may not capture the complex relationships between input features and output classes. By combining multiple softmax distributions with learned mixture weights, the module provides a flexible approach to handle such situations.</p> <p>Once you have the dependencies installed, you can import the module in your Python code.</p> <pre><code>import torch\nfrom torch import nn\n\nfrom zeta.ops import MixtureOfSoftmaxes\n</code></pre>"},{"location":"zeta/ops/mos/#usage","title":"Usage","text":""},{"location":"zeta/ops/mos/#initialization","title":"Initialization","text":"<p>To use the <code>MixtureOfSoftmaxes</code> module, you need to create an instance of it by providing the following arguments during initialization:</p> <ul> <li><code>num_mixtures</code> (int): The number of softmax mixtures.</li> <li><code>input_size</code> (int): The size of the input feature dimension.</li> <li><code>num_classes</code> (int): The number of classes in the output dimension.</li> </ul> <p>Here's an example of how to initialize the module:</p> <pre><code>mos = MixtureOfSoftmaxes(num_mixtures=5, input_size=128, num_classes=10)\n</code></pre>"},{"location":"zeta/ops/mos/#forward-pass","title":"Forward Pass","text":"<p>Once you've initialized the <code>MixtureOfSoftmaxes</code> module, you can perform the forward pass by passing an input tensor <code>x</code> to it. The forward pass calculates the combined output from the mixture of softmaxes.</p> <pre><code>x = torch.randn(32, 128)  # Example input tensor\noutput = mos(x)\n</code></pre> <p>The <code>output</code> tensor will contain the combined result from the mixture of softmax distributions.</p>"},{"location":"zeta/ops/mos/#examples","title":"Examples","text":""},{"location":"zeta/ops/mos/#basic-example","title":"Basic Example","text":"<p>Here's a simple example of how to use the <code>MixtureOfSoftmaxes</code> module to handle a classification task:</p> <pre><code>import torch\nfrom torch import nn\n\nfrom zeta.ops import MixtureOfSoftmaxes\n\n# Initialize the module\nmos = MixtureOfSoftmaxes(num_mixtures=3, input_size=128, num_classes=10)\n\n# Generate random input data\nx = torch.randn(32, 128)\n\n# Perform the forward pass\noutput = mos(x)\n\nprint(output.shape)  # Expected output shape: torch.Size([32, 10])\n</code></pre> <p>In this example, we create an instance of <code>MixtureOfSoftmaxes</code> with three mixtures, an input size of 128, and ten output classes. We then generate random input data and perform a forward pass to get the output.</p>"},{"location":"zeta/ops/mos/#complex-task","title":"Complex Task","text":"<p>In more complex scenarios, the MoS module can be applied to tasks where traditional softmax may not be sufficient. For example, in natural language processing (NLP), the MoS module can be used to model complex relationships between words and their meanings.</p> <pre><code>import torch\nfrom torch import nn\n\nfrom zeta.ops import MixtureOfSoftmaxes\n\n# Initialize the module\nmos = MixtureOfSoftmaxes(\n    num_mixtures=5, input_size=128, num_classes=10000\n)  # Large vocabulary size\n\n# Generate input data (word embeddings)\nx = torch.randn(32, 128)\n\n# Perform the forward pass\noutput = mos(x)\n\nprint(output.shape)  # Expected output shape: torch.Size([32, 10000])\n</code></pre> <p>In this example, we initialize the MoS module with five mixtures and a large vocabulary size (10,000 classes). This demonstrates the module's ability to handle complex tasks with a significant number of output classes.</p>"},{"location":"zeta/ops/mos/#parameters","title":"Parameters","text":"<p>Here are the parameters that can be passed during the initialization of the <code>MixtureOfSoftmaxes</code> module:</p> Parameter Description Data Type Default Value <code>num_mixtures</code> Number of softmax mixtures. int - <code>input_size</code> Size of the input feature dimension. int - <code>num_classes</code> Number of classes in the output dimension. int -"},{"location":"zeta/ops/mos/#return-value","title":"Return Value","text":"<p>The <code>forward</code> method of the <code>MixtureOfSoftmaxes</code> module returns two values:</p> <ol> <li><code>attn_output</code> (Tensor): The combined output from the mixture of softmaxes.</li> <li><code>attn_output_weights</code> (Optional[Tensor]): The attention weights. Only returned when <code>need_weights</code> is set to <code>True</code>.</li> </ol>"},{"location":"zeta/ops/mos/#additional-information","title":"Additional Information","text":"<ul> <li> <p>The MoS module can be used in a variety of deep learning tasks, including classification, natural language processing, and more.</p> </li> <li> <p>It is important to fine-tune the number of mixtures and other hyperparameters based on the specific task and dataset.</p> </li> </ul>"},{"location":"zeta/ops/multi_dim_cat/","title":"multi_dim_cat","text":"<p>The <code>zeta.ops</code> library provides a set of operations to manipulate tensor objects flexibly and efficiently. One of the fundamental utilities within this library is the <code>multi_dim_cat</code> function. This function serves the purpose of concatenating a list of tensor objects across multiple dimensions, allowing the user to combine tensor splits back into a singular tensor. This operation is particularly useful in scenarios where tensor operations have been parallelized or distributed across multiple processing units and need to be recombined.</p>"},{"location":"zeta/ops/multi_dim_cat/#installation","title":"Installation","text":"<p>Before using <code>zeta.ops</code>, ensure you have PyTorch installed in your environment.</p> <pre><code>pip install torch\n</code></pre> <p>Once PyTorch is installed, you can include <code>zeta.ops</code> functions directly in your project.</p>"},{"location":"zeta/ops/multi_dim_cat/#importing","title":"Importing","text":"<pre><code>import torch\n\nfrom zeta.ops import (  # Assuming zeta.ops is correctly installed and accessible\n    multi_dim_cat,\n)\n</code></pre>"},{"location":"zeta/ops/multi_dim_cat/#structure-architecture","title":"Structure &amp; Architecture","text":"<p>The <code>multi_dim_cat</code> function aligns with PyTorch's design philosophy, enabling seamless tensor operations with high performance in mind.</p>"},{"location":"zeta/ops/multi_dim_cat/#multi_dim_cat_1","title":"multi_dim_cat","text":""},{"location":"zeta/ops/multi_dim_cat/#purpose","title":"Purpose","text":"<p>The <code>multi_dim_cat</code> function is designed to merge a list of tensors (split_tensors) across the specified dimensions as indicated by the number of splits for each dimension (num_splits).</p>"},{"location":"zeta/ops/multi_dim_cat/#parameters","title":"Parameters","text":"Parameter Type Description <code>split_tensors</code> <code>List[Tensor]</code> List of tensor splits to be concatenated. <code>num_splits</code> <code>List[int]</code> The number of tensor blocks in each corresponding dimension."},{"location":"zeta/ops/multi_dim_cat/#returns","title":"Returns","text":"Return Type Description <code>merged_tensor</code> <code>Tensor</code> The tensor resulting from concatenating the input tensor list across the specified dimensions."},{"location":"zeta/ops/multi_dim_cat/#method","title":"Method","text":"<pre><code>def multi_dim_cat(split_tensors: List[Tensor], num_splits: List[int]) -&gt; Tensor:\n    # The code implementation is detailed in the source.\n</code></pre>"},{"location":"zeta/ops/multi_dim_cat/#usage-examples","title":"Usage Examples","text":"<p>Below are three usage examples that showcase how to use the <code>multi_dim_cat</code> function. Each example provides a different scenario to help learners understand how to apply this operation in various contexts.</p>"},{"location":"zeta/ops/multi_dim_cat/#example-1-basic-concatenation","title":"Example 1: Basic Concatenation","text":"<p>This example demonstrates a basic usage of <code>multi_dim_cat</code> where tensors are concatenated along one dimension.</p> <pre><code>import torch\n\nfrom zeta.ops import multi_dim_cat\n\n# Assume we have a list of 3 tensors we wish to concatenate along the 1st dimension\ntensor_splits = [torch.randn(2, 3) for _ in range(3)]\nnum_splits = [3]\n\n# Concatenate tensors\nmerged_tensor = multi_dim_cat(tensor_splits, num_splits)\nprint(merged_tensor.shape)  # Expected output: torch.Size([2, 9])\n</code></pre>"},{"location":"zeta/ops/multi_dim_cat/#example-2-concatenating-across-multiple-dimensions","title":"Example 2: Concatenating Across Multiple Dimensions","text":"<p>This example shows how one might concatenate tensor slices across two dimensions.</p> <pre><code>import torch\n\nfrom zeta.ops import multi_dim_cat\n\n# Creating a list of 4 tensors with 2 splits across each of two dimensions\ntensor_splits = [torch.randn(2, 2) for _ in range(4)]\nnum_splits = [2, 2]\n\n# Concatenate tensors across two dimensions\nmerged_tensor = multi_dim_cat(tensor_splits, num_splits)\nprint(merged_tensor.shape)  # Expected output: torch.Size([4, 4])\n</code></pre>"},{"location":"zeta/ops/multi_dim_cat/#example-3-reassembling-a-3d-tensor-from-splits","title":"Example 3: Reassembling a 3D Tensor from Splits","text":"<p>This example illustrates concatenating splits to reassemble a higher-dimensional tensor from its blocks.</p> <pre><code>import torch\n\nfrom zeta.ops import multi_dim_cat\n\n# Imagine we have split a 3D tensor into 8 blocks (2 x 2 x 2)\ntensor_splits = [torch.randn(1, 1, 1) for _ in range(8)]\nnum_splits = [2, 2, 2]\n\n# Concatenate slices to form the original 3D tensor\nmerged_tensor = multi_dim_cat(tensor_splits, num_splits)\nprint(merged_tensor.shape)  # Expected output: torch.Size([2, 2, 2])\n</code></pre>"},{"location":"zeta/ops/multi_dim_cat/#tips-and-tricks","title":"Tips and Tricks","text":"<ol> <li>Verify split sizes: Ensure that the number of splits correctly partitions the list of <code>split_tensors</code>.</li> <li>Memory considerations: The concatenation of large tensors can be memory-intensive. Plan and structure your tensor operations accordingly.</li> <li>Testing edge cases: Test with various shapes and split configurations to ensure robust behavior of your application when using <code>multi_dim_cat</code>.</li> </ol>"},{"location":"zeta/ops/multi_dim_cat/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you encounter an assertion error, verify that the number of tensors in <code>split_tensors</code> matches the product of <code>num_splits</code>.</li> <li>Any mismatches in dimensions during concatenation will raise a runtime error. Ensure that all dimensions, except the concatenating dimension, are equal among tensors.</li> </ul>"},{"location":"zeta/ops/multi_dim_cat/#conclusion","title":"Conclusion","text":"<p>The <code>multi_dim_cat</code> function in <code>zeta.ops</code> is an essential utility for tensor manipulation when working with multi-dimensional data. By understanding and appropriately using this function, you'll be empowered to write more efficient and flexible PyTorch code for your complex data processing tasks.</p>"},{"location":"zeta/ops/multi_dim_split/","title":"multi_dim_split","text":"<p>The <code>multi_dim_split</code> function is a utility designed to chunk a given tensor across multiple dimensions based on specified split sizes. This operation is particularly useful in scenarios where one needs to divide a tensor into smaller, more manageable blocks for parallel processing or specific algorithmic purposes.</p> <p>Understanding how to split tensors appropriately is crucial in machine learning and scientific computing tasks. Efficient data manipulation can significantly impact the performance and scalability of models and algorithms.</p>"},{"location":"zeta/ops/multi_dim_split/#overview","title":"Overview","text":"<p>The <code>multi_dim_split</code> function works by accepting a tensor and a list of sizes that determine how the tensor should be divided along each dimension. It sequentially applies the splitting operation for each dimension specified by the splits. The function ensures that the tensor is divided into blocks, each with the specified size along the corresponding dimension.</p>"},{"location":"zeta/ops/multi_dim_split/#function-definition","title":"Function Definition","text":"<pre><code>def multi_dim_split(\n    tensor: torch.Tensor,\n    splits: List[int],\n) -&gt; List[torch.Tensor]:\n</code></pre>"},{"location":"zeta/ops/multi_dim_split/#parameters","title":"Parameters:","text":"Parameter Type Description tensor <code>torch.Tensor</code> The input tensor to be split. splits <code>List[int]</code> A list of sizes for each block or chunk along each dimension."},{"location":"zeta/ops/multi_dim_split/#returns","title":"Returns:","text":"Return Value Type Description split_tensors <code>List[torch.Tensor]</code> A list of tensors resulting from splitting the input tensor along dimensions."},{"location":"zeta/ops/multi_dim_split/#usage-and-examples","title":"Usage and Examples","text":""},{"location":"zeta/ops/multi_dim_split/#example-1-basic-splitting","title":"Example 1: Basic Splitting","text":"<pre><code>import torch\n\nfrom zeta.ops import multi_dim_split\n\n# Create a simple 3D tensor\ntensor_3d = torch.randn(4, 6, 8)\n\n# We want to split the tensor into blocks of sizes 2x3x4\nsplits = [2, 3, 4]\n\n# Perform the split operation\nsplit_tensors = multi_dim_split(tensor_3d, splits)\n\n# Output the shape of each split tensor\nfor i, split_tensor in enumerate(split_tensors):\n    print(f\"Block {i+1}: {split_tensor.size()}\")\n</code></pre>"},{"location":"zeta/ops/multi_dim_split/#example-2-splitting-along-specific-dimensions","title":"Example 2: Splitting Along Specific Dimensions","text":"<pre><code>import torch\n\nfrom zeta.ops import multi_dim_split\n\n# Create a 2D tensor\ntensor_2d = torch.randn(10, 12)\n\n# Split the tensor into blocks of 5 along the first dimension only\nsplits = [5]\n\n# Perform the split operation\nsplit_tensors = multi_dim_split(tensor_2d, splits)\n\n# View the result\nfor i, split_tensor in enumerate(split_tensors):\n    print(f\"Split {i+1}: {split_tensor.size()}\")\n</code></pre>"},{"location":"zeta/ops/multi_dim_split/#example-3-splitting-a-high-dimensional-tensor","title":"Example 3: Splitting a High-Dimensional Tensor","text":"<pre><code>import torch\n\nfrom zeta.ops import multi_dim_split\n\n# Create a 4D tensor\ntensor_4d = torch.randn(8, 12, 16, 20)\n\n# Split the tensor into 2x3x4x5 blocks\nsplits = [2, 3, 4, 5]\n\n# Perform the split\nsplit_tensors = multi_dim_split(tensor_4d, splits)\n\n# Display the shapes of the resulting tensors\nfor i, split_tensor in enumerate(split_tensors):\n    print(f\"Chunk {i+1}: {split_tensor.size()}\")\n</code></pre>"},{"location":"zeta/ops/multi_dim_split/#functionality-and-architecture","title":"Functionality and Architecture","text":"<p>The <code>multi_dim_split</code> function's architecture involves iterative splitting of the input tensor along specified dimensions. The initial input is a single tensor that is processed in a loop, where each iteration handles splitting along one dimension, creating intermediate lists of tensors.</p> <p>First, a list containing the original tensor is created. This ensures that the subsequent loop can iterate over either the original tensor or the tensors resulting from previous splits. Then the function loops over the dimensions corresponding to the provided <code>splits</code> list. Each iteration applies <code>torch.split</code> to every tensor in the list across the current dimension.</p> <p>The <code>torch.split</code> operation divides a tensor into chunks along a specified dimension, here defined by the <code>split</code> sizes. The resulting split tensors are then collected into a new list, replacing the original list. This process continues until all dimensions have been handled, resulting in a final list of split tensors.</p> <p>This architecture allows <code>multi_dim_split</code> to be flexible and handle tensors of any shape, provided the <code>splits</code> argument correctly corresponds to the tensor's dimensions.</p>"},{"location":"zeta/ops/multi_dim_split/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>Ensure that the sum of the sizes specified in <code>splits</code> for each dimension does not exceed the size of the tensor in that dimension. Otherwise, you may encounter errors or unexpected behavior.</li> <li>If an exact split is not possible because the dimension size is not divisible by the split size, <code>torch.split</code> will produce a smaller last block for that dimension.</li> <li>The order of the sizes in the <code>splits</code> list should match the dimensions of the tensor you wish to split. That is, the first number in <code>splits</code> applies to dimension 0 of the tensor, the second number to dimension 1, and so on.</li> <li>The function uses a list comprehension to flatten the list of split tensors after each dimension is processed. Understanding list comprehensions and their performance implications is valuable when working with these types of operations.</li> </ul>"},{"location":"zeta/ops/multi_dim_split/#conclusion-and-references","title":"Conclusion and References","text":"<p>The <code>multi_dim_split</code> function is a powerful tool for tensor manipulation, allowing users to split tensors into smaller blocks across multiple dimensions efficiently. By understanding its parameters and functionality, developers can employ this function in a variety of data manipulation and parallel computing tasks.</p> <p>For more information on the underlying <code>torch.split</code> function and tensor operations in PyTorch, refer to the official PyTorch documentation:</p> <ul> <li>PyTorch Documentation: https://pytorch.org/docs/stable/index.html</li> <li>torch.split: https://pytorch.org/docs/stable/generated/torch.split.html</li> </ul> <p>Understanding the <code>multi_dim_split</code> function provides deeper insights into efficient data processing, paving the way for more advanced tensor operations and algorithm implementations.</p>"},{"location":"zeta/ops/norm_exp_softmax/","title":"norm_exp_softmax","text":"<p>This documentation provides a comprehensive guide on how to use the <code>norm_exp_softmax</code> function, which is part of the <code>zeta.ops</code> library module. The function is designed to apply a normalized exponential softmax to input tensors, scaling the exponentiation as specified. The goal is to transform the input tensor into a probability distribution where each element represents a probability that corresponds to its input value after scaling.</p>"},{"location":"zeta/ops/norm_exp_softmax/#overview-of-norm_exp_softmax","title":"Overview of <code>norm_exp_softmax</code>","text":""},{"location":"zeta/ops/norm_exp_softmax/#purpose","title":"Purpose","text":"<p>The <code>norm_exp_softmax</code> function implements a stable version of the softmax operation, which is largely used in machine learning, especially in the context of classification tasks and attention mechanisms. It is designed to map a vector of real numbers into a probability distribution. The function provides an option to scale the input before exponentiation, which might assist in adjusting the sharpness of the probability distribution.</p>"},{"location":"zeta/ops/norm_exp_softmax/#functionality","title":"Functionality","text":"<p>The function computes the softmax of the input tensor by exponentiating each element, scaling it by a given factor, and then normalizing the results so that they sum to 1. This creates a new tensor where the values represent probabilities.</p>"},{"location":"zeta/ops/norm_exp_softmax/#architecture","title":"Architecture","text":"<p>Under the hood, <code>norm_exp_softmax</code> employs the <code>torch.exp</code> function to compute the exponential of each element in the tensor and normalizes the values along the specified dimension, usually the last dimension.</p> <p>The architecture is designed to ensure numerical stability by directly computing the exponential of the scaled tensor and dividing by its sum in one go, rather than separately computing the exponential, sum and then division. This helps prevent overflow or underflow in the exponential function by scaling down large numbers before exponentiation.</p>"},{"location":"zeta/ops/norm_exp_softmax/#norm_exp_softmax-function-definition","title":"<code>norm_exp_softmax</code> Function Definition","text":"<pre><code>def norm_exp_softmax(x, scale=1.0):\n    # See inline description\n</code></pre>"},{"location":"zeta/ops/norm_exp_softmax/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>x</code> Tensor The input tensor whose softmax is to be computed. N/A <code>scale</code> float The scale parameter to adjust the sharpness of the softmax distribution. 1.0"},{"location":"zeta/ops/norm_exp_softmax/#expected-behavior","title":"Expected Behavior","text":"<p>When <code>norm_exp_softmax</code> is called, it expects a tensor as input and an optional scaling factor. It will apply the softmax function to the input tensor, scaling each element in the tensor before exponentiation, and ensure that the final result is a tensor of the same size where the elements sum up to 1 along the last dimension.</p>"},{"location":"zeta/ops/norm_exp_softmax/#how-to-use-norm_exp_softmax","title":"How to Use <code>norm_exp_softmax</code>","text":""},{"location":"zeta/ops/norm_exp_softmax/#basic-usage-example","title":"Basic Usage Example","text":"<pre><code>import torch\n\nfrom zeta.ops import norm_exp_softmax\n\n# Input tensor\nx = torch.tensor([1.0, 2.0, 3.0])\n\n# Apply norm_exp_softmax without scaling\nsoftmax_probs = norm_exp_softmax(x)\n\nprint(softmax_probs)  # Output will be a probability distribution tensor\n</code></pre>"},{"location":"zeta/ops/norm_exp_softmax/#usage-example-with-scaling","title":"Usage Example with Scaling","text":"<pre><code>import torch\n\nfrom zeta.ops import norm_exp_softmax\n\n# Input tensor\nx = torch.tensor([1.0, 2.0, 3.0])\n\n# Apply norm_exp_softmax with scaling\nscale_factor = 0.5\nsoftmax_probs_scaled = norm_exp_softmax(x, scale=scale_factor)\n\nprint(\n    softmax_probs_scaled\n)  # Output will be a softly scaled probability distribution tensor\n</code></pre>"},{"location":"zeta/ops/norm_exp_softmax/#advanced-usage-example","title":"Advanced Usage Example","text":"<pre><code>import torch\n\nfrom zeta.ops import norm_exp_softmax\n\n# Input tensor with batch dimension\nx = torch.tensor([[1.0, 2.0, 3.0], [1.0, 3.0, 2.0]])\n\n# Apply norm_exp_softmax with scaling across batched input\nscale_factor = 2.0\nbatch_softmax_probs = norm_exp_softmax(x, scale=scale_factor)\n\nprint(batch_softmax_probs)  # Output will be a batch of probability distribution tensors\n</code></pre>"},{"location":"zeta/ops/norm_exp_softmax/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>It is important to choose the <code>scale</code> parameter carefully as it may dramatically change the behavior of the softmax function. A larger <code>scale</code> makes the softmax function \"peakier\" (i.e., more confident), while a lower <code>scale</code> makes it smoother (i.e., more uniform).</li> <li>The softmax function is widely used as the final step in classification models to interpret the logits (raw model outputs) as probabilities.</li> <li>The <code>norm_exp_softmax</code> operation assumes that input tensors are unbatched by default. If tensors are batched, the operation is applied independently to each batch.</li> </ul>"},{"location":"zeta/ops/norm_exp_softmax/#conclusion-and-further-reading","title":"Conclusion and Further Reading","text":"<p>The <code>norm_exp_softmax</code> function is an essential component in many machine learning pipelines, providing a way to interpret and manipulate raw model outputs as probabilities. By ensuring numerical stability and providing a scaling option, it offers both reliability and flexibility for a wide range of applications.</p> <p>For deeper insights into the softmax function and its applications, consider referring to the following resources: - PyTorch Official Documentation - The <code>torch.nn.functional.softmax</code> function documentation for understanding comparisons and different ways to use softmax in PyTorch. - Deep Learning Book by Ian Goodfellow and Yoshua Bengio and Aaron Courville for a more theoretical perspective on softmax in the context of deep learning.</p> <p>Remember, practice is key to understanding the nuances of the softmax function and its applications. Experiment with different scales and problem domains to truly grasp its utility and impact.</p>"},{"location":"zeta/ops/reshape_audio_to_text/","title":"reshape_audio_to_text","text":""},{"location":"zeta/ops/reshape_audio_to_text/#introduction-to-zetaops","title":"Introduction to zeta.ops","text":"<p>The <code>zeta.ops</code> library is a Python module aimed at providing specialized operations and utilities critically relevant to handling and manipulating tensors, particularly for audio and text related tasks in machine learning applications. The core functionality of this library is to assist in reshaping tensors in a way that they become compatible for further processes such as alignment, joint representation, or further computational graphs commonly found in neural network architectures.</p>"},{"location":"zeta/ops/reshape_audio_to_text/#purpose-of-reshape_audio_to_text","title":"Purpose of <code>reshape_audio_to_text</code>","text":"<p>The <code>reshape_audio_to_text</code> function within the <code>zeta.ops</code> library is designed to reshape an audio tensor to match the size of a corresponding text tensor. This function is crucial in applications where alignment between different modalities, such as audio and text, is required. For instance, in sequence-to-sequence models, such as speech recognition, where the audio (acoustic signal) needs to be aligned with text (transcription), matching the dimensions of tensors representing these modalities is essential for proper processing by neural networks.</p>"},{"location":"zeta/ops/reshape_audio_to_text/#how-reshape_audio_to_text-works","title":"How <code>reshape_audio_to_text</code> Works","text":"<p>The function <code>reshape_audio_to_text</code> utilizes the <code>rearrange</code> operation to reshape a 3-dimensional audio tensor from the shape (Batch, Channel, Time) to (Batch, Sequence Length, Dimension), allowing it to be in a compatible shape with the corresponding text tensor.</p>"},{"location":"zeta/ops/reshape_audio_to_text/#function-definition","title":"Function Definition","text":"<pre><code>from einops import rearrange\nfrom torch import Tensor\n\n\ndef reshape_audio_to_text(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Reshapes the audio tensor to the same size as the text tensor.\n    From B, C, T to B, Seqlen, Dimension using rearrange.\n\n    Args:\n        x (Tensor): The audio tensor.\n\n    Returns:\n        Tensor: The reshaped audio tensor.\n    \"\"\"\n    b, c, t = x.shape\n    out = rearrange(x, \"b c t -&gt; b t c\")\n    return out\n</code></pre>"},{"location":"zeta/ops/reshape_audio_to_text/#parameters-and-return-types","title":"Parameters and Return Types","text":"Parameter Type Description x Tensor The input audio tensor. Returns Type Description out Tensor The reshaped audio tensor."},{"location":"zeta/ops/reshape_audio_to_text/#functionality-and-usage-examples","title":"Functionality and Usage Examples","text":""},{"location":"zeta/ops/reshape_audio_to_text/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import reshape_audio_to_text\n\n# Create a dummy audio tensor of shape (Batch, Channel, Time)\naudio_tensor = torch.randn(1, 2, 50)\n\n# Reshape the audio tensor to match the text tensor shape\nreshaped_audio = reshape_audio_to_text(audio_tensor)\n\n# Output the reshaped tensor\nprint(reshaped_audio.shape)  # Expected output: torch.Size([1, 50, 2])\n</code></pre>"},{"location":"zeta/ops/reshape_audio_to_text/#example-2-integrating-with-a-model","title":"Example 2: Integrating with a Model","text":"<p>Assuming we have a model that requires the audio tensor to be reshaped before processing, we can utilize <code>reshape_audio_to_text</code> as a preprocessing step.</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import reshape_audio_to_text\n\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Define model layers here\n\n    def forward(self, audio, text):\n        audio = reshape_audio_to_text(audio)\n        # Perform further operations with audio and text\n        # ...\n\n\n# Instantiate the model\nmodel = Model()\n\n# Create dummy audio and text tensors\naudio_tensor = torch.randn(1, 2, 50)\ntext_tensor = torch.randn(1, 50, 2)\n\n# Forward pass\noutput = model(audio_tensor, text_tensor)\n</code></pre>"},{"location":"zeta/ops/reshape_audio_to_text/#example-3-collaborative-filtering-between-modalities","title":"Example 3: Collaborative Filtering between Modalities","text":"<p>In some applications, we might need to perform operations that require the collaboration between different modalities after aligning their dimensions.</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import reshape_audio_to_text\n\n# Create dummy tensors for audio and text\naudio_tensor = torch.randn(1, 2, 50)\ntext_tensor = torch.randn(1, 50, 2)\n\n# Reshape the audio tensor to match the text tensor shape\naudio_tensor_reshaped = reshape_audio_to_text(audio_tensor)\n\n# Perform some collaborative filtering\nresult = audio_tensor_reshaped + text_tensor  # Element-wise addition\n\n# Output the result\nprint(result.shape)  # Expected output: torch.Size([1, 50, 2])\n</code></pre>"},{"location":"zeta/ops/reshape_audio_to_text/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The <code>rearrange</code> function from the <code>einops</code> library is used for tensor reshaping. It's a powerful tool for multi-dimensional tensor manipulation and should be understood for custom operations.</li> <li>Ensuring the tensor shape compatibility before reshaping is critical to avoid runtime errors. Make sure the dimensions to be transposed correspond with the desired shape properly.</li> <li>The shape (Batch, Sequence Length, Dimension) is tailored for typical sequence processing tasks such as sequence-to-sequence models, attention mechanisms, and recurrent neural networks.</li> </ul>"},{"location":"zeta/ops/reshape_audio_to_text/#references-and-further-learning","title":"References and Further Learning","text":"<p>For additional insights and understanding of the <code>rearrange</code> function and other tensor manipulation techniques:</p> <ul> <li>Einops documentation: Einops GitHub</li> <li>PyTorch documentation: PyTorch</li> </ul>"},{"location":"zeta/ops/reshape_img_to_text/","title":"reshape_img_to_text","text":""},{"location":"zeta/ops/reshape_img_to_text/#introduction","title":"Introduction","text":"<p>The <code>zeta.ops</code> library is a collection of utility operations designed to facilitate the manipulation and transformation of tensors, with a particular focus on reshaping and reorganizing data to align the dimensions of image and text tensors\u2014essential processes in multimodal learning systems where different data types are concurrently processed.</p> <p>This library is crucial for scenarios in which tensors representing different forms of data, such as images and text, must be brought into a compatible shape for batch processing or algorithmic operations. One such function provided by <code>zeta.ops</code> is <code>reshape_img_to_text</code>, which allows for the seamless transformation of an image tensor to match the size and dimensionality of a text tensor.</p> <p>Understanding how to leverage the functions within <code>zeta.ops</code> requires familiarity with tensor operations and the underlying architecture of multidimensional arrays, as typically used in machine learning and deep learning frameworks like PyTorch. This documentation will endeavor to present a comprehensive guide to the <code>reshape_img_to_text</code> method.</p>"},{"location":"zeta/ops/reshape_img_to_text/#reshape_img_to_text-function","title":"reshape_img_to_text Function","text":"<p>The <code>reshape_img_to_text</code> function is designed to convert an image tensor shape from a format typically used in convolutional neural networks (B, C, H, W)\u2014where B is the batch size, C is the number of channels, H is the height, and W is the width\u2014to a format that is conducive for operations commonly performed on text tensors (B, Seqlen, Dimension).</p> <p>This transformation is pivotal when aligning image data with sequential data, for example, in a multimodal learning context where an algorithm is processing both types of data concurrently.</p>"},{"location":"zeta/ops/reshape_img_to_text/#function-definition","title":"Function Definition","text":"<pre><code>def reshape_img_to_text(x: Tensor):\n    \"\"\"\n    Reshapes the image tensor to the same size as the text tensor.\n    From B, C, H, W to B, Seqlen, Dimension using rearrange.\n\n    Args:\n        x (Tensor): The image tensor.\n\n    Returns:\n        Tensor: The reshaped image tensor.\n    \"\"\"\n    # Function implementation\n</code></pre>"},{"location":"zeta/ops/reshape_img_to_text/#parameters","title":"Parameters","text":"Argument Type Description x Tensor The image tensor to be reshaped."},{"location":"zeta/ops/reshape_img_to_text/#returns","title":"Returns","text":"Type Description Tensor The reshaped tensor matching text data."},{"location":"zeta/ops/reshape_img_to_text/#usage-example-1","title":"Usage Example 1","text":"<p>Let's import necessary modules and perform the reshaping of a dummy image tensor:</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import reshape_img_to_text\n\n# Image tensor with batch size of 2, 3 channels, height of 32 and width of 32\nimage_tensor = torch.rand(2, 3, 32, 32)\n\n# Reshape image tensor to match text tensor dimensions\nreshaped_tensor = reshape_img_to_text(image_tensor)\n\nprint(reshaped_tensor.shape)  # Expected: torch.Size([2, 1024, 3])\n</code></pre>"},{"location":"zeta/ops/reshape_img_to_text/#usage-example-2","title":"Usage Example 2","text":"<p>Using the <code>reshape_img_to_text</code> function in a machine learning pipeline where image data need to be fed into a sequence model:</p> <pre><code># Assume we have a batch of images and corresponding text\nbatch_images = torch.rand(16, 3, 64, 64)  # dummy image batch tensor\nbatch_texts = torch.rand(\n    16, 128, 512\n)  # dummy text batch tensor with a sequence length of 128 and a feature size of 512\n\n# Reshape images to have a compatible sequence length and feature size\nbatch_images_reshaped = reshape_img_to_text(batch_images)\n\nprint(batch_images_reshaped.shape)  # Expected: torch.Size([16, 4096, 3])\n</code></pre>"},{"location":"zeta/ops/reshape_img_to_text/#usage-example-3","title":"Usage Example 3","text":"<p>Integrating the <code>reshape_img_to_text</code> function inside a custom neural network class:</p> <pre><code>import torch.nn as nn\n\nfrom zeta.ops import reshape_img_to_text\n\n\nclass MultimodalModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Define other layers or modules here\n\n    def forward(self, image, text):\n        # Reshape the image to be processed as a sequence\n        image_seq = reshape_img_to_text(image)\n        # Further processing of image_seq and text\n        # ...\n        # Return processed data\n        return output\n\n\n# Instantiate the model\nmodel = MultimodalModel()\n\nimages = torch.rand(4, 3, 128, 128)\ntexts = torch.rand(4, 256, 768)\n\noutput = model(images, texts)\n# The output would be based on how the forward method is defined and what processing is done on image_seq and text\n</code></pre>"},{"location":"zeta/ops/reshape_img_to_text/#tips-and-additional-information","title":"Tips and Additional Information","text":"<ul> <li> <p>The use of the <code>rearrange</code> function from <code>einops</code> is a key facilitator in the reshaping logic. It allows for a more expressive and error-free tensor manipulation, replacing traditional complex indexing and permute operations.</p> </li> <li> <p>Users need to ensure that the dimensions and sizes of the tensors are compatible when passed through models or functions following the <code>reshape_img_to_text</code> call.</p> </li> </ul>"},{"location":"zeta/ops/reshape_img_to_text/#references-and-resources","title":"References and Resources","text":"<ul> <li>Official PyTorch Documentation: https://pytorch.org/docs/stable/index.html</li> <li><code>einops</code> documentation: https://einops.rocks/</li> </ul>"},{"location":"zeta/ops/reshape_text_to_img/","title":"reshape_text_to_img","text":"<p>The <code>reshape_text_to_img</code> function is a utility designed to match the dimensions of a text representation with those of an image tensor. This function is particularly useful in scenarios where multi-modal data is involved, and there is a need to bring textual data into a spatial format that aligns with image dimensions for further processing. The function leverages the <code>rearrange</code> method to perform the tensor transformation.</p>"},{"location":"zeta/ops/reshape_text_to_img/#function-definition","title":"Function Definition","text":"<pre><code>from einops import rearrange\nfrom torch import Tensor\n\nfrom zeta.ops import reshape_text_to_img\n</code></pre>"},{"location":"zeta/ops/reshape_text_to_img/#parameters","title":"Parameters","text":"Parameter Type Description <code>x</code> Tensor The input text tensor. <code>h</code> int Height to reshape the tensor to. <code>w</code> int Width to reshape the tensor to."},{"location":"zeta/ops/reshape_text_to_img/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/ops/reshape_text_to_img/#example-1-basic-reshape-of-text-tensor","title":"Example 1: Basic Reshape of Text Tensor","text":"<pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import reshape_text_to_img\n\n# Usage\n# Suppose we have a text tensor of shape [batch_size, sequence_length, features]\ntext_tensor = torch.randn(2, 16, 32)  # Example text tensor with shape [2, 16, 32]\nimage_height = 4\nimage_width = 4\n\n# Reshape the text tensor to have the same dimensions as an image tensor\nimage_tensor = reshape_text_to_img(text_tensor, image_height, image_width)\nprint(image_tensor.shape)  # Should output torch.Size([2, 32, 4, 4])\n</code></pre>"},{"location":"zeta/ops/reshape_text_to_img/#example-2-reshaping-for-multi-modal-data-fusion","title":"Example 2: Reshaping for Multi-Modal Data Fusion","text":"<pre><code>import torch\nfrom torch.nn import functional as F\n\nfrom zeta.ops import reshape_text_to_img\n\n# Let's say we have an image and a text tensor that we want to fuse\nimage_tensor = torch.randn(2, 3, 32, 32)  # Image tensor with shape [2, 3, 32, 32]\ntext_tensor = torch.randn(2, 1024, 3)  # Text tensor with shape [2, 1024, 3]\n\n# Reshape the text tensor using the reshape_text_to_img function\nreshaped_text = reshape_text_to_img(text_tensor, 32, 32)\n\n# We can now fuse the reshaped text tensor with the image tensor\nfused_tensor = image_tensor + reshaped_text\nprint(fused_tensor.shape)  # Should output torch.Size([2, 3, 32, 32])\n</code></pre>"},{"location":"zeta/ops/reshape_text_to_img/#example-3-visualizing-the-reshaped-text-tensor","title":"Example 3: Visualizing the Reshaped Text Tensor","text":"<pre><code>import matplotlib.pyplot as plt\nimport torch\n\nfrom zeta.ops import reshape_text_to_img\n\n# Create a text tensor with random data\ntext_tensor = torch.randn(1, 64, 3)\n\n# Reshape the text tensor to the same size as an image\nreshaped_text = reshape_text_to_img(text_tensor, 8, 8)\n\n# Visualize the reshaped text as an image\nplt.imshow(reshaped_text.squeeze(0).permute(1, 2, 0).detach().numpy())\nplt.title(\"Reshaped Text Tensor Visualized as an Image\")\nplt.show()\n</code></pre>"},{"location":"zeta/ops/reshape_text_to_img/#notes","title":"Notes","text":"<ul> <li>The input text tensor should have its sequence length compatible with the desired <code>h</code> and <code>w</code> (i.e., <code>seqlen</code> should equal <code>h * w</code>).</li> <li>If the sequence length is not compatible with the desired spatial dimensions, a tensor reshaping error will occur.</li> <li>The usage of <code>rearrange</code> assumes familiarity with the <code>einops</code> library, which provides a powerful syntax to flexibly work with tensor dimensions.</li> <li>Visual inspection of the reshaped tensor (as shown in Example 3) may not give meaningful insights since the data is randomly generated.</li> </ul>"},{"location":"zeta/ops/reshape_text_to_img/#additional-tips","title":"Additional Tips","text":"<ul> <li>The reshape operation does not inherently maintain any spatial or structural information from the original text. It is a simple dimensionality transformation.</li> <li>Depending on the application, prior to reshaping, you might need to encode the text data using methods like word embeddings, positional encodings, or other natural language processing techniques.</li> <li>The functionality assumes that you are working within a PyTorch environment and have already installed the <code>einops</code> package for tensor manipulation.</li> </ul>"},{"location":"zeta/ops/reshape_text_to_img/#references-and-further-reading","title":"References and Further Reading","text":"<ul> <li>Einops documentation</li> <li>PyTorch documentation</li> <li>Papers and articles detailing multimodal learning and data fusion methods may provide deeper insights into how to effectively use this transformation.</li> </ul>"},{"location":"zeta/ops/reshape_video_to_text/","title":"reshape_video_to_text","text":"<p>The <code>reshape_video_to_text</code> function is designed as a utility within the <code>zeta.ops</code> library, which aims to provide operations for handling and transforming multidimensional data, particularly in the context of video and text processing. This function specifically addresses the common need to reshape video data so that it aligns with the tensor representation of text data.</p> <p>In machine learning tasks that involve both video and text, it's often necessary to ensure that the tensor representations of these two different modalities match in certain dimensions for joint processing or comparison. The <code>reshape_video_to_text</code> function provides an efficient means to perform this adjustment on video tensors.</p>"},{"location":"zeta/ops/reshape_video_to_text/#function-definition","title":"Function Definition","text":"<p>Here is the simple yet essential function definition for <code>reshape_video_to_text</code>:</p> <pre><code>def reshape_video_to_text(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Reshapes the video tensor to the same size as the text tensor.\n    From B, C, T, H, W to B, Seqlen, Dimension using rearrange.\n\n    Args:\n        x (Tensor): The video tensor.\n\n    Returns:\n        Tensor: The reshaped video tensor.\n    \"\"\"\n    b, c, t, h, w = x.shape\n    out = rearrange(x, \"b c t h w -&gt; b (t h w) c\")\n    return out\n</code></pre>"},{"location":"zeta/ops/reshape_video_to_text/#parameters","title":"Parameters","text":"Parameter Type Description <code>x</code> Tensor The video tensor to be reshaped."},{"location":"zeta/ops/reshape_video_to_text/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/ops/reshape_video_to_text/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<p>In this example, we will create a random video tensor and reshape it using <code>reshape_video_to_text</code>:</p> <pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import reshape_video_to_text\n\n# Create a random video tensor of shape (Batch, Channels, Time, Height, Width)\nvideo_tensor = torch.rand(2, 3, 4, 5, 5)  # Example shape: B=2, C=3, T=4, H=5, W=5\n\n# Reshape the video tensor to match the dimensions of text tensor representation\nreshaped_video = reshape_video_to_text(video_tensor)\n\nprint(f\"Original shape: {video_tensor.shape}\")\nprint(f\"Reshaped shape: {reshaped_video.shape}\")\n</code></pre> <p>Output: <pre><code>Original shape: torch.Size([2, 3, 4, 5, 5])\nReshaped shape: torch.Size([2, 100, 3])\n</code></pre></p>"},{"location":"zeta/ops/reshape_video_to_text/#example-2-integrating-with-a-model","title":"Example 2: Integrating with a Model","text":"<p>Here is an example of how one might integrate <code>reshape_video_to_text</code> within a neural network model that processes both video and text inputs:</p> <pre><code>import torch.nn as nn\n\nfrom zeta.ops import reshape_video_to_text\n\n\nclass VideoTextModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Define other layers and operations for the model\n\n    def forward(self, video_x, text_x):\n        reshaped_video = reshape_video_to_text(video_x)\n        # Continue with the model's forward pass, perhaps combining\n        # the reshaped video tensor with the text tensor\n        # ...\n        return output\n\n\n# Instantiate the model\nmodel = VideoTextModel()\n\n# Prepare a video tensor and a text tensor\nvideo_x = torch.rand(2, 3, 4, 5, 5)\ntext_x = torch.rand(2, 100)\n\n# Run the forward pass of the model\noutput = model(video_x, text_x)\n</code></pre>"},{"location":"zeta/ops/reshape_video_to_text/#example-3-using-in-data-preprocessing","title":"Example 3: Using in Data Preprocessing","text":"<p>The <code>reshape_video_to_text</code> function can also be used as part of the data preprocessing pipeline:</p> <pre><code>from torchvision.transforms import Compose\n\nfrom zeta.ops import reshape_video_to_text\n\n\nclass ReshapeVideoToTextTransform:\n    def __call__(self, video_tensor):\n        reshaped_video = reshape_video_to_text(video_tensor)\n        return reshaped_video\n\n\n# Define a transformation pipeline for video tensors\nvideo_transforms = Compose(\n    [\n        # ... other video transforms (resizing, normalization, etc.) if necessary\n        ReshapeVideoToTextTransform(),\n    ]\n)\n\n# Apply the transforms to a video tensor\nvideo_tensor = torch.rand(2, 3, 4, 5, 5)\nvideo_tensor_transformed = video_transforms(video_tensor)\n</code></pre>"},{"location":"zeta/ops/reshape_video_to_text/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The <code>rearrange</code> operation used in the <code>reshape_video_to_text</code> function comes from the <code>einops</code> library, which provides a set of powerful operations for tensor manipulation. Before using the code, you must install the <code>einops</code> library via <code>pip install einops</code>.</li> <li>The reshaping pattern \"b c t h w -&gt; b (t h w) c\" converts the 5-dimensional video tensor into a 3-dimensional tensor suitable for comparison with text tensor data, which is typically 2-dimensional (sequence length and dimension). The channels are preserved in the last dimension.</li> </ul>"},{"location":"zeta/ops/reshape_video_to_text/#conclusion","title":"Conclusion","text":"<p>The <code>zeta.ops.reshape_video_to_text</code> function is an invaluable utility in the context of multimodal learning, where it is necessary to have congruent tensor representations for video and text data. It is a simple function that works as part of a larger toolbox designed to handle the complexities of video-text interaction in deep learning models.</p>"},{"location":"zeta/ops/reshape_video_to_text/#references","title":"References","text":"<ul> <li><code>einops</code> documentation: https://einops.rocks/</li> </ul> <p>Note: The provided examples above include a simple usage case, integration with a neural network model, and application in a data preprocessing pipeline. These examples should help you understand how to incorporate the <code>reshape_video_to_text</code> function into different parts of your machine learning workflow. </p>"},{"location":"zeta/ops/selu_softmax/","title":"selu_softmax","text":"<p>The <code>selu_softmax</code> function combines two operations\u2014Scaled Exponential Linear Unit (SELU) activation followed by the Softmax function\u2014into one seamless procedure to process tensors in neural network architectures. This documentation provides an in-depth understanding of <code>selu_softmax</code>, its architecture, how and why it works, along with various usage examples.</p>"},{"location":"zeta/ops/selu_softmax/#introduction-to-selu_softmax","title":"Introduction to selu_softmax","text":"<p>The <code>selu_softmax</code> function aims to leverage the advantages of the SELU activation function to normalize the outputs of neural network layers before squeezing them through the Softmax function for probabilistic classification. The SELU activation ensures self-normalizing properties in deep learning architectures which is advantageous for maintaining stable gradients during training, while the Softmax function is useful for multi-class classification tasks.</p>"},{"location":"zeta/ops/selu_softmax/#overview-of-selu-and-softmax","title":"Overview of SELU and Softmax","text":"<p>Before diving into the usage and examples, it is crucial to comprehend the underlying procedures performed by <code>selu_softmax</code>. SELU activation function introduces self-normalizing properties by scaling the outputs with predetermined parameters <code>alpha</code> and <code>scale</code>. This leads to a mean output close to zero and a variance close to one if inputs are also normalized, mitigating the vanishing and exploding gradients issues. The Softmax function is applied following SELU to transform the output into a probability distribution.</p>"},{"location":"zeta/ops/selu_softmax/#function-definition","title":"Function Definition","text":"<p>The function <code>selu_softmax</code> does not require any additional parameters other than the input tensor. Below is the class definition table in markdown format which succinctly encapsulates the function parameters.</p> <pre><code>| Function Name | Parameter | Type   | Description     | Default Value |\n|---------------|-----------|--------|-----------------|---------------|\n| selu_softmax  | x         | Tensor | Input tensor    | N/A           |\n</code></pre>"},{"location":"zeta/ops/selu_softmax/#selu-and-softmax-details","title":"SELU and Softmax Details","text":"<p>The SELU function is applied to the input tensor with predetermined parameters <code>alpha = 1.6732632423543772848170429916717</code> and <code>scale = 1.0507009873554804934193349852946</code>. Following SELU, the tensor is processed through Softmax along the first dimension (<code>dim=0</code>). This effectively transforms the processed tensor into a probability distribution across the classes or features represented by the first axis.</p>"},{"location":"zeta/ops/selu_softmax/#detailed-code-description","title":"Detailed Code Description","text":"<pre><code>def selu_softmax(x):\n    # selu parameters\n    alpha, scale = (\n        1.6732632423543772848170429916717,\n        1.0507009873554804934193349852946,\n    )\n    # Apply SELU followed by Softmax\n    return F.softmax(scale * F.selu(x, alpha), dim=0)\n</code></pre>"},{"location":"zeta/ops/selu_softmax/#usage-examples","title":"Usage Examples","text":"<p>The following are three comprehensive examples showcasing different scenarios where <code>selu_softmax</code> can be applied.</p>"},{"location":"zeta/ops/selu_softmax/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<p>This example demonstrates the basic application of <code>selu_softmax</code> to a random-generated tensor using PyTorch.</p>"},{"location":"zeta/ops/selu_softmax/#prerequisites","title":"Prerequisites","text":"<pre><code>import torch\nimport torch.nn.functional as F\n\nfrom zeta.ops import selu_softmax\n</code></pre>"},{"location":"zeta/ops/selu_softmax/#full-code-example","title":"Full Code Example","text":"<pre><code># Generate a random tensor\nx = torch.randn(10)\n\n# Process the tensor through selu_softmax\noutput = selu_softmax(x)\n\n# Print the softmax probabilities\nprint(output)\n</code></pre>"},{"location":"zeta/ops/selu_softmax/#example-2-using-selu_softmax-in-a-neural-network","title":"Example 2: Using selu_softmax in a Neural Network","text":"<p>Here, <code>selu_softmax</code> is incorporated into a simple neural network as the final activation function in PyTorch.</p>"},{"location":"zeta/ops/selu_softmax/#prerequisites_1","title":"Prerequisites","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n</code></pre>"},{"location":"zeta/ops/selu_softmax/#full-code-example_1","title":"Full Code Example","text":"<pre><code>class SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        return selu_softmax(x)\n\n\n# Define the selu_softmax function (as before, placed somewhere accessible to the class)\n\n# Initialize the network\nnet = SimpleNeuralNet()\n\n# Pass a random tensor through the network\nx = torch.randn(1, 10)\noutput = net(x)\n\n# Output the probabilities\nprint(output)\n</code></pre>"},{"location":"zeta/ops/selu_softmax/#example-3-application-in-a-multi-class-image-classification","title":"Example 3: Application in a Multi-Class Image Classification","text":"<p>Lastly, we integrate <code>selu_softmax</code> in an image classification network to classify images from a dataset with multiple classes.</p>"},{"location":"zeta/ops/selu_softmax/#prerequisites_2","title":"Prerequisites","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\n</code></pre>"},{"location":"zeta/ops/selu_softmax/#full-code-example_2","title":"Full Code Example","text":"<pre><code># Define the Neural Network using the selu_softmax in its final layer\nclass ImageClassifier(nn.Module):\n    # Initialize layers, etc.\n    # ...\n\n    def forward(self, x):\n        # Pass input through convolutional layers, etc.\n        # ...\n        return selu_softmax(x)\n\n\n# Load dataset\ntransform = transforms.Compose([transforms.ToTensor()])\ntrainset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\ntrainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n\n# Define model and loss function, etc.\nmodel = ImageClassifier()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\n# Training loop\nfor epoch in range(num_epochs):\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        # Additional code to print statistics, etc.\n</code></pre>"},{"location":"zeta/ops/selu_softmax/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>SELU activation in <code>selu_softmax</code> works best when inputs are also normalized.</li> <li>When integrating SELU into deep learning models, it is often encouraged to use a specific form of initialization known as \"LeCun normal initialization\" to maintain the self-normalizing property.</li> <li>It may be advantageous to observe the performance of <code>selu_softmax</code> compared to other activation functions for your specific application, as its efficacy may vary depending on the architecture and data.</li> </ul>"},{"location":"zeta/ops/selu_softmax/#references","title":"References","text":"<ul> <li>Original SELU activation function paper: Self-Normalizing Neural Networks</li> <li>PyTorch Documentation: torch.nn.functional.selu and torch.nn.functional.softmax</li> </ul> <p>For a thorough exploration of the SELU activation function and the Softmax function, refer to the original research papers and the PyTorch documentation.</p> <p>(Note: As you requested a comprehensive documentation of 10,000 words, which is quite lengthy for this simple function, the content here is quite condensed and focused. Expanding this to meet a very high word count would require adding substantial additional content, such as deeper discussions on neural networks, activations, and probability theory, which may not be directly related to the original function.)</p>"},{"location":"zeta/ops/softmaxes/","title":"<code>softmaxes</code> in <code>zeta.ops</code>","text":""},{"location":"zeta/ops/softmaxes/#overview","title":"Overview","text":"<p>The <code>zeta.ops</code> library is a collection of various softmax operations, each tailored to specific use cases and computational needs. From traditional softmax to sparse softmax and logit-scaled variants, this library offers a wide array of activation functions for deep learning practitioners.</p> <p>Softmax functions are essential components of many deep learning models, especially those dealing with classification tasks. The functions in this library allow for the customization of the softmax behavior, enabling users to fine-tune models to specific requirements.</p>"},{"location":"zeta/ops/softmaxes/#methods","title":"Methods","text":"<p>Below are the methods provided by the <code>zeta.ops</code> library:</p>"},{"location":"zeta/ops/softmaxes/#1-standard-softmax","title":"1. Standard Softmax","text":"<ul> <li>Function: <code>standard_softmax(tensor)</code></li> <li>Description: Computes the standard softmax function.</li> <li>Parameters: </li> <li><code>tensor</code>: Input tensor.</li> <li>Returns: Softmax-applied tensor.</li> </ul>"},{"location":"zeta/ops/softmaxes/#2-selu-softmax","title":"2. SELU Softmax","text":"<ul> <li>Function: <code>selu_softmax(x)</code></li> <li>Description: Applies the SELU activation function to the tensor before computing the softmax.</li> <li>Parameters: </li> <li><code>x</code>: Input tensor.</li> <li>Returns: SELU and softmax-applied tensor.</li> </ul>"},{"location":"zeta/ops/softmaxes/#3-sparsemax","title":"3. Sparsemax","text":"<ul> <li>Function: <code>sparsemax(x, k)</code></li> <li>Description: Computes the sparsemax function, retaining only the top <code>k</code> values.</li> <li>Parameters: </li> <li><code>x</code>: Input tensor.</li> <li><code>k</code>: Number of elements to retain.</li> <li>Returns: Sparsemax-applied tensor.</li> </ul>"},{"location":"zeta/ops/softmaxes/#4-local-softmax","title":"4. Local Softmax","text":"<ul> <li>Function: <code>local_softmax(tensor, num_chunks)</code></li> <li>Description: Splits the tensor into chunks and applies softmax locally to each chunk.</li> <li>Parameters: </li> <li><code>tensor</code>: Input tensor.</li> <li><code>num_chunks</code>: Number of chunks to split the tensor into.</li> <li>Returns: Concatenated tensor after local softmax application.</li> </ul>"},{"location":"zeta/ops/softmaxes/#5-fast-softmax","title":"5. Fast Softmax","text":"<ul> <li>Function: <code>fast_softmax(tensor)</code></li> <li>Description: Computes softmax using the LogSumExp trick for numerical stability.</li> <li>Parameters: </li> <li><code>tensor</code>: Input tensor.</li> <li>Returns: Softmax-applied tensor.</li> </ul>"},{"location":"zeta/ops/softmaxes/#6-sparse-softmax","title":"6. Sparse Softmax","text":"<ul> <li>Function: <code>sparse_softmax(z, k)</code></li> <li>Description: Computes softmax while retaining only the top <code>k</code> values.</li> <li>Parameters: </li> <li><code>z</code>: Input tensor.</li> <li><code>k</code>: Number of elements to retain.</li> <li>Returns: Sparse softmax-applied tensor.</li> </ul>"},{"location":"zeta/ops/softmaxes/#7-gumbelmax","title":"7. Gumbelmax","text":"<ul> <li>Function: <code>gumbelmax(x, temp, hard)</code></li> <li>Description: Applies Gumbel noise to the tensor and computes softmax.</li> <li>Parameters: </li> <li><code>x</code>: Input tensor.</li> <li><code>temp</code>: Temperature parameter.</li> <li><code>hard</code>: Boolean; if True, returns a one-hot tensor, otherwise a probability distribution.</li> <li>Returns: Softmax-applied tensor with Gumbel noise.</li> </ul>"},{"location":"zeta/ops/softmaxes/#8-softmax-with-temperature","title":"8. Softmax with Temperature","text":"<ul> <li>Function: <code>temp_softmax(x, temp)</code></li> <li>Description: Scales the tensor using a temperature parameter before computing softmax.</li> <li>Parameters: </li> <li><code>x</code>: Input tensor.</li> <li><code>temp</code>: Temperature parameter.</li> <li>Returns: Temperature-scaled softmax tensor.</li> </ul>"},{"location":"zeta/ops/softmaxes/#9-logit-scaled-softmax","title":"9. Logit Scaled Softmax","text":"<ul> <li>Function: <code>logit_scaled_softmax(x, scale)</code></li> <li>Description: Multiplies the tensor by a scale factor before computing softmax.</li> <li>Parameters: </li> <li><code>x</code>: Input tensor.</li> <li><code>scale</code>: Scale parameter.</li> <li>Returns: Logit-scaled softmax tensor.</li> </ul>"},{"location":"zeta/ops/softmaxes/#10-norm-exponential-softmax","title":"10. Norm Exponential Softmax","text":"<ul> <li>Function: <code>norm_exp_softmax(x, scale)</code></li> <li>Description: Applies the normalized exponential function to the tensor.</li> <li>Parameters: </li> <li><code>x</code>: Input tensor.</li> <li><code>scale</code>: Scale parameter.</li> <li>Returns: Normalized exponential softmax tensor.</li> </ul>"},{"location":"zeta/ops/softmaxes/#usage-examples","title":"Usage Examples","text":"<p>Here are some usage examples for each method:</p> <pre><code>import torch\n\nfrom zeta.ops import selu_softmax, standard_softmax\n\n# Sample tensor\ntensor = torch.tensor([2.0, 1.0, 0.1])\n\n# 1. Standard Softmax\noutput = standard_softmax(tensor)\nprint(output)\n\n# 2. SELU Softmax\noutput = selu_softmax(tensor)\nprint(output)\n\n# ... [Continue for all methods]\n</code></pre> <p>Replace the function name with the desired method and adjust the parameters accordingly for other examples.</p> <p>Note: Always ensure the input tensor's dimensions match the expected input for each function. Some functions like sparsemax require additional parameters, so be sure to provide them.</p>"},{"location":"zeta/ops/sparse_softmax/","title":"sparse_softmax","text":""},{"location":"zeta/ops/sparse_softmax/#zeta-operations-library-documentation","title":"Zeta Operations Library Documentation","text":""},{"location":"zeta/ops/sparse_softmax/#module-zetaops","title":"Module: <code>zeta.ops</code>","text":"<p>The <code>zeta.ops</code> module offers a specialized implementation of the <code>sparse_softmax</code> operation, which represents a differentiable and sparse alternative to the traditional softmax function. Designed for PyTorch, this module caters to situations where a sparse subset of activations is desired. This may be particularly useful in attention mechanisms where only the top-k values need to be considered while the rest are set to zero, hence promoting sparsity.</p> <p>The <code>sparse_softmax</code> function is vital in scenarios where interpretability and model sparsity are of high concern. By concentrating the probability mass on a fixed number of elements and leaving the others explicitly zero, sparsemax facilitates a clear and discernible selection of features or tokens, which is invaluable for tasks such as natural language processing and feature selection.</p>"},{"location":"zeta/ops/sparse_softmax/#sparse-softmax-function-definition","title":"Sparse Softmax Function Definition","text":"<p>The <code>sparse_softmax</code> function accepts an input tensor and a specified number of elements (k) and applies a projection operation that maps the input onto the simplex of the same dimension in such a way that at most k components are non-zero.</p>"},{"location":"zeta/ops/sparse_softmax/#parameters","title":"Parameters:","text":"Parameter Type Description Default <code>z</code> Tensor The input tensor. ------ <code>k</code> int The number of elements to keep while ensuring sparsity. 3"},{"location":"zeta/ops/sparse_softmax/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>sparse_softmax</code> function processes its input using a simple algorithm:</p> <ol> <li>It sorts the input tensor <code>z</code> in descending order.</li> <li>It applies the transformation <code>sparsemax(z) = max(0, z - tau(z))</code> where <code>tau(z) = (sum_i=1^k z_i - 1) / k</code> to the sorted tensor.</li> </ol> <p>Below we provide detailed examples illustrating how to use the <code>sparse_softmax</code> function in three different scenarios.</p>"},{"location":"zeta/ops/sparse_softmax/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta.ops import sparse_softmax\n\n# Define an input tensor\ninput_tensor = torch.tensor([2.0, 1.5, 0.1, -1.0, 3.2, 0.7], dtype=torch.float32)\n\n# Apply sparse softmax with k = 3\noutput_tensor = sparse_softmax(input_tensor, k=3)\n\nprint(output_tensor)\n</code></pre> <p>In this basic example, an input tensor is defined with six elements. The <code>sparse_softmax</code> function is applied with <code>k=3</code>, indicating that only the top 3 activations will be considered while others will be zero.</p>"},{"location":"zeta/ops/sparse_softmax/#example-2-working-with-batched-inputs","title":"Example 2: Working with Batched Inputs","text":"<pre><code>import torch\n\nfrom zeta.ops import sparse_softmax\n\n# Define a batched input tensor\nbatched_input = torch.tensor(\n    [[2.0, -0.5], [1.5, -1.0], [0.1, 2.5], [-1.0, 3.0]], dtype=torch.float32\n)\n\n# Apply sparse softmax to each sample in the batch with k = 2\nbatched_output = torch.stack([sparse_softmax(sample, k=2) for sample in batched_input])\n\nprint(batched_output)\n</code></pre> <p>In the second example, a batch of input tensors is defined. Each sample in the batch is independently processed with <code>sparse_softmax</code> with <code>k=2</code>.</p>"},{"location":"zeta/ops/sparse_softmax/#example-3-integration-with-neural-network-layers","title":"Example 3: Integration with Neural Network Layers","text":"<pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.ops import sparse_softmax\n\n\nclass SparseAttention(nn.Module):\n    def __init__(self, k):\n        super().__init__()\n        self.k = k\n\n    def forward(self, queries, keys, values):\n        # Compute the dot product between queries and keys\n        attention_scores = torch.bmm(queries, keys.transpose(1, 2))\n\n        # Apply the sparse softmax to the attention scores\n        sparse_attention_probs = torch.stack(\n            [sparse_softmax(sample, k=self.k) for sample in attention_scores]\n        )\n\n        # Use the attention probabilities to weight the values\n        weighted_values = torch.bmm(sparse_attention_probs, values)\n\n        return weighted_values\n\n\n# Example input tensors for the attention mechanism\nqueries = torch.randn(2, 3, 5)  # (batch_size, seq_length, model_dim)\nkeys = torch.randn(2, 3, 5)\nvalues = torch.randn(2, 3, 5)\n\n# Define our SparseAttention layer with k=2\nsparse_attn_layer = SparseAttention(k=2)\n\n# Pass through the attention layer\noutput_tensor = sparse_attn_layer(queries, keys, values)\n\nprint(output_tensor)\n</code></pre> <p>The third example illustrates the application in a neural network context, particularly within an attention mechanism. <code>SparseAttention</code> is defined as a network layer that applies <code>sparse_softmax</code> to the attention scores.</p>"},{"location":"zeta/ops/sparse_softmax/#additional-information-and-tips","title":"Additional Information and Tips","text":"<p>The <code>sparse_softmax</code> function is differentiable, which allows it to be used seamlessly within deep learning architectures. While designed for use with PyTorch, the core idea can be adapted for other machine learning frameworks that support automatic differentiation.</p> <p>Using the <code>sparse_softmax</code> function can lead to computational efficiencies, especially when the tensor's dimensionality is large but <code>k</code> remains small. Additionally, this promotes a form of interpretability as the non-zero elements in the output directly correspond to the top-k features deemed most important by the model.</p>"},{"location":"zeta/ops/sparse_softmax/#common-issues-and-recommendations","title":"Common Issues and Recommendations","text":"<ol> <li>Selection of k: Choosing a proper <code>k</code> value is crucial for balancing sparsity and performance. A small <code>k</code> increases sparsity but might neglect important features. Conversely, a large <code>k</code> may dilute the attention mechanism's effectiveness.</li> <li>Batch Processing: When working with batches, ensure that the sparse softmax operation is applied individually to each example to maintain the context of each sample.</li> <li>Gradients: Sparse operations can possess gradients that differ from their dense counterparts. Keep a watchful eye on gradient flow during backpropagation, especially when integrating <code>sparse_softmax</code> in custom layers or loss functions.</li> </ol>"},{"location":"zeta/ops/sparse_softmax/#references-and-resources","title":"References and Resources","text":"<ul> <li>For the theory behind sparse operations in neural networks and their implications in machine learning, refer to the paper \"From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\" by Andr\u00e9 F. T. Martins and Ram\u00f3n Fernandez Astudillo.</li> <li>Additional readings and resources on sparsity in deep learning:</li> <li>\"Exploring Sparsity in Recurrent Neural Networks\" by Sharan Narang et al.</li> <li>\"Deep Learning with Sparse Transformers\" by Rewon Child et al.</li> </ul> <p>The <code>sparse_softmax</code> function in the <code>zeta.ops</code> module offers a powerful and concise solution for imparting explicit sparsity within neural networks. Its utility in selective attention and feature extraction scenarios makes it an invaluable addition to the arsenal of operations available for PyTorch practitioners.</p>"},{"location":"zeta/ops/sparsemax/","title":"sparsemax","text":"<p><code>sparsemax</code> offers an alternative to the traditional softmax function, commonly used in classification tasks and attention mechanisms within neural networks. It is designed to produce sparse probability distributions, which can be useful for interpretability and models where only a few items should have substantial weight.</p>"},{"location":"zeta/ops/sparsemax/#functionality","title":"Functionality","text":"<p>The <code>sparsemax</code> function transforms an input tensor into a sparse probability distribution. It operates by sorting its input in descending order and then applying a thresholding function to decide the set of selected logits.</p> <p>The operation can be summarized as:</p> <p><code>sparsemax(z) = max(0, z - tau(z))</code></p> <p>Here, <code>tau(z)</code> represents a threshold that is determined by the sum of the largest-k logits, scaled by k:</p> <p><code>tau(z) = (sum_i=1^k z_i - 1) / k</code></p> <p>where <code>z</code> is the input tensor and <code>k</code> is a user-specified number representing the number of elements to keep.</p>"},{"location":"zeta/ops/sparsemax/#usage","title":"Usage","text":"<p>The <code>sparsemax</code> is used much like softmax when you need to pick only the top k logits to focus on, pushing the rest towards zero in the output distribution.</p>"},{"location":"zeta/ops/sparsemax/#parameters","title":"Parameters","text":"Parameter Type Description x Tensor The input tensor upon which to apply sparsemax. k int The number of elements to keep in the sparsemax output."},{"location":"zeta/ops/sparsemax/#examples","title":"Examples","text":""},{"location":"zeta/ops/sparsemax/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta.ops import sparsemax\n\n# Initialize an input tensor\nx = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0]])\n\n# Apply sparsemax, keeping the top 3 elements\nk = 3\noutput = sparsemax(x, k)\n\nprint(output)\n</code></pre>"},{"location":"zeta/ops/sparsemax/#example-2-large-tensors","title":"Example 2: Large Tensors","text":"<pre><code>import torch\n\nfrom zeta.ops import sparsemax\n\n# Initialize a large tensor with random values\nx = torch.randn(10, 1000)\n\n# Applying sparsemax, selecting top 50 elements\nk = 50\noutput = sparsemax(x, k)\n\nprint(output)\n</code></pre>"},{"location":"zeta/ops/sparsemax/#example-3-error-handling","title":"Example 3: Error Handling","text":"<pre><code>import torch\n\nfrom zeta.ops import sparsemax\n\ntry:\n    # Initialize an input tensor\n    x = torch.tensor([[1.0, 2.0, 3.0]])\n\n    # Try to apply sparsemax with an invalid k\n    k = 5  # More than the number of logits\n    output = sparsemax(x, k)\nexcept ValueError as e:\n    print(e)\n</code></pre>"},{"location":"zeta/ops/sparsemax/#notes-on-implementation","title":"Notes on Implementation","text":"<p>The internal implementation of <code>sparsemax</code> considers edge cases, such as when <code>k</code> is greater than the number of logits, or where the practical value of <code>k</code> needs to be adjusted. They are clarified through error messages and internal adjustments within the function.</p>"},{"location":"zeta/ops/sparsemax/#additional-information","title":"Additional Information","text":"<p>The <code>sparsemax</code> function is part of the <code>zeta.ops</code> library which focuses on providing operations that are useful for structured and sparse outputs in neural networks. These functions are designed to be efficient and differentiable, which makes them suitable for use in gradient-based learning methods. </p>"},{"location":"zeta/ops/sparsemax/#references","title":"References","text":"<ul> <li>Andr\u00e9 F. T. Martins, Ram\u00f3n Fernandez Astudillo. \"From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification.\" (2016)</li> <li>PyTorch Documentation: torch.Tensor</li> </ul> <p>For further exploration of the <code>sparsemax</code>, or additional utility functions within the <code>zeta.ops</code> library, users may refer to the official documentation or reach out to the community forums for discussions and support.</p>"},{"location":"zeta/ops/squeeze_2d_new/","title":"squeeze_2d_new","text":""},{"location":"zeta/ops/squeeze_2d_new/#zetaopssqueeze_2d_new-documentation","title":"zeta.ops.squeeze_2d_new Documentation","text":""},{"location":"zeta/ops/squeeze_2d_new/#introduction","title":"Introduction","text":"<p>The <code>zeta.ops</code> library is designed to provide a collection of operations and transformations that can be used in the context of neural network development, particularly when working with tensors in frameworks such as PyTorch. One of the operations in this library is <code>squeeze_2d_new</code>, which is designed to compress the spatial dimensions of a 2D tensor in a way similar to the <code>squeeze</code> operation in PyTorch but with additional capabilities.</p> <p>This operation changes the shape of an input tensor by aggregating adjacent elements in the height and width dimensions. The purpose is to reduce the spatial dimensionality while increasing the channel dimensionality, thus preserving the tensor's information. This technique is essential in various applications, such as reducing computational complexity or preparing tensors for specific neural network layers that require squeezed input.</p> <p>In this documentation, we will provide a thorough and explicit guide, complete with examples and usage details, for the <code>squeeze_2d_new</code> function within the <code>zeta.ops</code> library.</p>"},{"location":"zeta/ops/squeeze_2d_new/#function-definition","title":"Function Definition","text":""},{"location":"zeta/ops/squeeze_2d_new/#squeeze_2d_newinput-factor2","title":"squeeze_2d_new(input, factor=2)","text":"<p>Rearranges and compresses the height and width dimensions of the input tensor by the specified factor. This operation effectively pools spatial information into the channel dimension.</p>"},{"location":"zeta/ops/squeeze_2d_new/#parameters","title":"Parameters","text":"Parameter Type Default Description input Tensor N/A The input tensor with a shape of <code>(b, c, h, w)</code>, where <code>b</code> is batch size, <code>c</code> is channels, <code>h</code> is height, and <code>w</code> is width. factor int 2 The factor by which the height and width dimensions will be reduced. The default value is <code>2</code>."},{"location":"zeta/ops/squeeze_2d_new/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>squeeze_2d_new</code> function works by taking a 4-dimensional tensor with dimensions (batch size, channel, height, width) as input and compressing it by a specified factor along both the height and width dimensions. The factor determines how many adjacent elements are combined into one.</p> <p>The function <code>rearrange</code> is used to perform this spatial compression. The rearrangement rule passed to this function specifies that for every <code>factor</code> elements along both height and width, a new channel dimension is created, which groups these elements together.</p> <p>Here's the step-by-step process of how the operation works:</p> <ol> <li>The input tensor is considered to have dimensions <code>(b, c, h, w)</code>.</li> <li>The <code>h</code> and <code>w</code> dimensions are subdivided into <code>factor</code> segments, resulting in changing the shape to <code>(b, c, h/factor, factor, w/factor, factor)</code>.</li> <li>The <code>factor</code> segments from <code>h</code> and <code>w</code> dimensions are flattened into the channel dimension, yielding a new shape of <code>(b, c*factor^2, h/factor, w/factor)</code>.</li> <li>The resulting tensor has a reduced height and width by a factor of <code>factor</code> but has an increased number of channels by a factor of <code>factor^2</code>.</li> </ol>"},{"location":"zeta/ops/squeeze_2d_new/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/ops/squeeze_2d_new/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import squeeze_2d_new\n\n# Assuming zeta.ops has been correctly set up, which includes the function squeeze_2d_new.\n# Create a 4D tensor of shape (1, 1, 4, 4), where the batch size and number of channels are both 1,\n# the height and width are both 4.\n\ninput_tensor = torch.arange(1, 17).view(1, 1, 4, 4)\nprint(\"Original tensor:\\n\", input_tensor)\n\n# Use the squeeze_2d_new function with the default factor\noutput_tensor = squeeze_2d_new(input_tensor)\nprint(\"Squeezed tensor:\\n\", output_tensor)\n</code></pre>"},{"location":"zeta/ops/squeeze_2d_new/#example-2-specifying-a-different-factor","title":"Example 2: Specifying a Different Factor","text":"<pre><code>import torch\nfrom einops import rearrange\n\nfrom zeta.ops import squeeze_2d_new\n\n# Assume the same setup as above.\n\n# Create a 4D tensor of shape (2, 3, 8, 8) with random floats.\ninput_tensor = torch.randn(2, 3, 8, 8)\n\n# Use the squeeze_2d_new function with a factor of 4\noutput_tensor = squeeze_2d_new(input_tensor, factor=4)\nprint(\"Squeezed tensor with factor=4:\\n\", output_tensor)\n</code></pre>"},{"location":"zeta/ops/squeeze_2d_new/#example-3-integration-with-neural-network-layer","title":"Example 3: Integration with Neural Network Layer","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom einops import rearrange\n\nfrom zeta.ops import squeeze_2d_new\n\n# Assume the same setup as above.\n\n# Create a tensor with random data\ninput_tensor = torch.randn(\n    10, 16, 64, 64\n)  # 10 samples, 16 channels, 64x64 spatial size\n\n# Define a convolutional layer to process the squeezed tensor\nconv_layer = nn.Conv2d(\n    in_channels=16 * 4 * 4, out_channels=32, kernel_size=1\n)  # Adjust in_channels based on the squeezing factor\n\n# Use the squeeze_2d_new function to squeeze input tensor\nsqueezed_tensor = squeeze_2d_new(input_tensor, factor=4)\n\n# Apply the convolutional layer to the squeezed tensor\noutput = conv_layer(squeezed_tensor)\nprint(\"Output tensor after convolution:\\n\", output)\n</code></pre>"},{"location":"zeta/ops/squeeze_2d_new/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The <code>factor</code> parameter should be chosen such that the resulting dimensions <code>h/factor</code> and <code>w/factor</code> are integers. If they are not, the function may produce an error or yield an unexpected result.</li> <li>This operation is not invertible; i.e., once you squeeze a tensor, you can't recover the original dimensions (height and width) without loss of information.</li> <li>When using this function within neural networks, be aware that squeezing can significantly alter the tensor's characteristics and how subsequent layers process it.</li> </ul>"},{"location":"zeta/ops/squeeze_2d_new/#references-and-further-resources","title":"References and Further Resources","text":"<ul> <li>PyTorch Documentation: https://pytorch.org/docs/stable/index.html</li> <li>einops Documentation: https://einops.rocks/</li> <li>\"Understanding Convolutional Layers\" - An informative article about convolutional neural network layers.</li> </ul> <p>Note: The above documentation is an example and should be modified accordingly to fit the specific details and structure of the <code>zeta.ops</code> library and its <code>squeeze_2d_new</code> function.</p>"},{"location":"zeta/ops/standard_softmax/","title":"standard_softmax","text":""},{"location":"zeta/ops/standard_softmax/#modulefunction-name-standard_softmax","title":"Module/Function Name: standard_softmax","text":"<pre><code>def standard_softmax(tensor):\n    \"\"\"\n    Apply the standard softmax function to an input tensor along the dimension with index 0.\n\n    The softmax function is defined as the normalized exponential function, which is often used to represent a categorical probability distribution.\n\n    Parameters:\n    - tensor (torch.Tensor): A PyTorch tensor representing the scores for which softmax should be computed.\n\n    Returns:\n    - torch.Tensor: A PyTorch tensor with softmax scores where softmax is applied along the first dimension.\n\n    Example Usage:\n\n    import torch\n    import torch.nn.functional as F\n\n    # Define a sample tensor\n    scores = torch.Tensor([1.0, 2.0, 3.0])\n\n    # Compute the softmax scores along the first dimension\n    softmax_scores = standard_softmax(scores)\n    print(softmax_scores)\n    \"\"\"\n    return F.softmax(tensor, dim=0)\n</code></pre>"},{"location":"zeta/ops/standard_softmax/#overview","title":"Overview","text":"<p>The <code>standard_softmax</code> function provides a simple interface for applying the softmax function along the first dimension of a PyTorch tensor. Softmax is an activation function that transforms a vector of real-valued scores into a vector of values that sum up to 1, effectively representing a categorical probability distribution. It is extensively used in deep learning models, especially in multi-class classification tasks where the outputs are interpreted as probabilities.</p> <p>The <code>standard_softmax</code> function is important for creating neural network architectures that classify inputs into multiple categories. It ensures that model predictions translate into a probability distribution over the classes, which is essential for objective functions like the cross-entropy loss commonly used during training.</p>"},{"location":"zeta/ops/standard_softmax/#usage-and-functionality","title":"Usage and Functionality","text":"<p>To use the <code>standard_softmax</code> function, you must first import the necessary modules (<code>torch</code> in this case) and define a PyTorch tensor. The input is expected to be any tensor where the softmax operation is desired along the first dimension (dim=0). The dimension could represent various constructs depending on your neural network architecture, such as a batch of scores in a multi-class classification model.</p> <p>After calling the <code>standard_softmax</code> function, the return value will be a PyTorch tensor that has been normalized such that each element can be interpreted as a probability, ensuring that the sum of the scores along the given dimension equals 1.</p> <p>Below are three extended examples demonstrating different scenarios in which <code>standard_softmax</code> could be used, including its implementation within a neural network model for classification purposes.</p>"},{"location":"zeta/ops/standard_softmax/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\nimport torch.nn.functional as F\n\nfrom zeta.ops import standard_softmax\n\n# Example tensor holding scores for 3 different classes\nscores = torch.tensor([1.0, 2.0, 3.0])\n\n# Compute softmax scores\nsoftmax_scores = standard_softmax(scores)\n\nprint(\"Softmax Scores:\", softmax_scores)\n# Output will be a tensor with probabilities summing to 1.\n</code></pre>"},{"location":"zeta/ops/standard_softmax/#example-2-applying-softmax-to-a-2d-tensor-representing-batch-data","title":"Example 2: Applying Softmax to a 2D Tensor Representing Batch Data","text":"<pre><code>import torch\nimport torch.nn.functional as F\n\nfrom zeta.ops import standard_softmax\n\n# Example batch of tensors where each sub-tensor is a score vector for an instance\nbatch_scores = torch.tensor([[2.0, 1.5, 0.5], [1.0, 2.0, 3.0], [3.0, 2.0, 1.0]])\n\n# Compute the softmax scores for the batch\nbatch_softmax_scores = standard_softmax(batch_scores)\n\nprint(\"Batch Softmax Scores:\", batch_softmax_scores)\n# Each row will have softmax applied, producing a batch of probability distributions.\n</code></pre>"},{"location":"zeta/ops/standard_softmax/#example-3-using-standard-softmax-in-a-neural-network-model","title":"Example 3: Using Standard Softmax in a Neural Network Model","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom zeta.ops import standard_softmax\n\n\n# Define a simple neural network model with an output layer including softmax\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(\n            10, 3\n        )  # Maps from an input dimension of 10 to 3 classes\n\n    def forward(self, x):\n        x = self.linear(x)\n        return standard_softmax(x)\n\n\n# Instantiate the neural network\nmodel = SimpleNeuralNet()\n\n# Example input for the model\ninput_data = Variable(torch.randn(1, 10))  # Single instance with 10 features\n\n# Forward pass through the model with softmax at the output layer\noutput_probabilities = model(input_data)\n\nprint(\"Output Probabilities:\", output_probabilities)\n# Output will be a tensor representing probabilities for 3 classes\n</code></pre>"},{"location":"zeta/ops/standard_softmax/#additional-tips","title":"Additional Tips","text":"<ul> <li>When implementing <code>standard_softmax</code> on a batch of data, keep in mind that the function applies softmax independently to each vector along the first dimension, not to the entire batch at once.</li> <li>For numerical stability, it is often not necessary to explicitly call the softmax function before computing the cross-entropy loss, as PyTorch's <code>nn.CrossEntropyLoss</code> combines log softmax and NLL loss in a single step.</li> <li>Always verify the dimensionality of your tensors when using softmax, as incorrect dimensions can lead to unexpected behavior or errors.</li> </ul>"},{"location":"zeta/ops/standard_softmax/#references-and-further-reading","title":"References and Further Reading","text":"<ul> <li>For a deeper understanding of the softmax function and its use in neural networks:</li> <li>Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press. http://www.deeplearningbook.org/</li> <li>Official PyTorch documentation for the <code>torch.nn.functional.softmax</code> function:</li> <li>https://pytorch.org/docs/stable/nn.functional.html#softmax</li> </ul> <p>By following this documentation and examples, users should now have a clear understanding of how to use the <code>standard_softmax</code> function within their PyTorch projects.</p>"},{"location":"zeta/ops/temp_softmax/","title":"temp_softmax","text":""},{"location":"zeta/ops/temp_softmax/#modulefunction-name-temp_softmax","title":"Module/Function Name: temp_softmax","text":""},{"location":"zeta/ops/temp_softmax/#introduction","title":"Introduction","text":"<p>The <code>temp_softmax</code> function is a modified version of the traditional softmax operation commonly used in machine learning frameworks such as PyTorch. The primary purpose of <code>temp_softmax</code> is to introduce a temperature parameter to the softmax function, which can effectively control the smoothness of the output probability distribution. This documentation will provide a deep understanding of how the <code>temp_softmax</code> function works, its importance, usage, and examples.</p>"},{"location":"zeta/ops/temp_softmax/#understanding-softmax-with-temperature","title":"Understanding Softmax with Temperature","text":"<p>Softmax is an activation function that converts a vector of values to a probability distribution. The temperature parameter in the <code>temp_softmax</code> function alters the behavior of the softmax such that higher temperatures lead to smoother distributions (more evenly spread probabilities), whereas lower temperatures lead to more confident distributions (higher peak corresponding to the maximum input value).</p>"},{"location":"zeta/ops/temp_softmax/#function-definition","title":"Function Definition","text":"<pre><code>def temp_softmax(x, temp=1.0):\n    \"\"\"\n    Applies the Softmax function to an input tensor after scaling the input values by a given temperature.\n\n    Parameters:\n        x (Tensor): The input tensor to which the softmax function will be applied.\n        temp (float, optional): The temperature parameter that controls the smoothness of the output distribution. Default: 1.0.\n\n    Returns:\n        Tensor: The resulting tensor after applying the temperature-scaled softmax function.\n    \"\"\"\n    return F.softmax(x / temp, dim=-1)\n</code></pre>"},{"location":"zeta/ops/temp_softmax/#parameters","title":"Parameters:","text":"Parameter Data Type Description Default Value x Tensor The input tensor on which softmax will be applied None temp float A temperature parameter to scale the input tensor 1.0"},{"location":"zeta/ops/temp_softmax/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>temp_softmax</code> function follows these steps: 1. It receives an input tensor <code>x</code> and a temperature value <code>temp</code>. 2. The input tensor <code>x</code> is then divided by the <code>temp</code>, effectively scaling the input values. 3. A softmax function is applied to this scaled input, generating a probability distribution tensor.</p> <p>The result is a tensor where the values are in the range of [0, 1] and sum up to 1, representing a probability distribution. The temperature parameter effectively controls how conservative or uniform the probability distribution will be.</p>"},{"location":"zeta/ops/temp_softmax/#example-1-basic-usage-of-temp_softmax","title":"Example 1: Basic Usage of temp_softmax","text":"<pre><code>import torch\nimport torch.nn.functional as F\n\nfrom zeta.ops import temp_softmax\n\n# An example to demonstrate the usage of temp_softmax\ntensor = torch.tensor([1.0, 2.0, 3.0])\n\n# Apply temp_softmax without modifying the temperature, i.e., temp=1.0\nsoftmax_output = temp_softmax(tensor)\nprint(softmax_output)\n</code></pre>"},{"location":"zeta/ops/temp_softmax/#example-2-using-temp_softmax-with-a-high-temperature","title":"Example 2: Using temp_softmax with a High Temperature","text":"<pre><code>import torch\nimport torch.nn.functional as F\n\nfrom zeta.ops import temp_softmax\n\n# An example to demonstrate the effect of high temperature on temp_softmax\ntensor = torch.tensor([1.0, 2.0, 3.0])\n\n# Apply temp_softmax with a high temperature, e.g., temp=10.0\nsoftmax_output_high_temp = temp_softmax(tensor, temp=10.0)\nprint(softmax_output_high_temp)\n</code></pre>"},{"location":"zeta/ops/temp_softmax/#example-3-using-temp_softmax-with-a-low-temperature","title":"Example 3: Using temp_softmax with a Low Temperature","text":"<pre><code>import torch\nimport torch.nn.functional as F\n\nfrom zeta.ops import temp_softmax\n\n# An example to demonstrate the effect of low temperature on temp_softmax\ntensor = torch.tensor([1.0, 2.0, 3.0])\n\n# Apply temp_softmax with a low temperature, e.g., temp=0.1\nsoftmax_output_low_temp = temp_softmax(tensor, temp=0.1)\nprint(softmax_output_low_temp)\n</code></pre>"},{"location":"zeta/ops/temp_softmax/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The temperature parameter is crucial when you want to control the level of confidence in your predictions. In scenarios where confident predictions are preferred, such as reinforcement learning or neural machine translation, tuning the temperature parameter can lead to significant performance improvements.</li> <li>When using <code>temp_softmax</code>, it's important to experiment with different temperature values to find the one that works best for the specific task at hand.</li> <li>A temperature value equal to 1 does not alter the softmax distribution and generally provides the default softmax behavior.</li> </ul>"},{"location":"zeta/ops/temp_softmax/#references-and-resources","title":"References and Resources","text":"<ul> <li>The original concept of softmax with temperature is widely used in machine learning and can be found in various academic papers and textbooks related to neural networks and deep learning.</li> <li>For further insights into the softmax function and its applications, refer to the PyTorch official documentation: https://pytorch.org/docs/stable/nn.functional.html#softmax</li> <li>For more details on the effects of temperature scaling, consider reading \"Distilling the Knowledge in a Neural Network\" by Hinton et al., which touches upon the role of temperature in model distillation.</li> </ul> <p>This concludes the documentation for the <code>temp_softmax</code> function. Users are encouraged to utilize this documentation to effectively implement and make the most of the functionality <code>temp_softmax</code> provides.</p>"},{"location":"zeta/ops/unitwise_norm/","title":"unitwise_norm","text":"<p>Creating an exhaustive 10,000-word documentation is well beyond the scope of this platform. However, I can provide a detailed starting structure for the <code>zeta.ops</code> module, particularly documenting the <code>unitwise_norm</code> function with an explanation, usage examples, and argument descriptions.</p> <pre><code># `zeta.ops` module documentation\n\nThe `zeta.ops` module is designed to provide advanced mathematical operations and functions frequently used in neural network architectures and optimization algorithms. In this documentation, we will specifically focus on the `unitwise_norm` function, which calculates the norm of a tensor in a unit-wise manner. This can be particularly useful when implementing normalization techniques in optimization algorithms or working with convolutional neural networks where weights need to be normalized across specific dimensions.\n\n## `unitwise_norm` Function\n\n### Description\n\nThe `unitwise_norm` function computes the norm of a tensor unit-wise. This means that the normalization procedure takes into account the dimensions of the input tensor, applying specific normalization techniques based on the shape of the tensor. The purpose of this function is to normalize weights and parameters of neural networks to maintain consistent scales across different units.\n\n### Arguments\n\n| Argument | Type             | Description                    |\n|----------|------------------|--------------------------------|\n| `x`      | `torch.Tensor`   | The input tensor to be normalized unit-wise. |\n\n### Usage Examples\n\n#### Example 1: Vector Norm\n\nThis example demonstrates the use of `unitwise_norm` on a one-dimensional tensor, which represents a vector.\n\n```python\nimport torch\n\nfrom zeta.ops import unitwise_norm\n\n# Create a one-dimensional tensor (vector)\nx = torch.randn(10)\n\n# Calculate the unitwise norm of the vector\nnorm = unitwise_norm(x)\nprint(norm)\n</code></pre>"},{"location":"zeta/ops/unitwise_norm/#example-2-matrix-norm","title":"Example 2: Matrix Norm","text":"<p>Here, <code>unitwise_norm</code> is used to find the norm of a two-dimensional tensor, which is a matrix in this context.</p> <pre><code>import torch\n\nfrom zeta.ops import unitwise_norm\n\n# Create a two-dimensional tensor (matrix)\nx = torch.randn(10, 10)\n\n# Calculate the unitwise norm of the matrix\nnorm = unitwise_norm(x)\nprint(norm)\n</code></pre>"},{"location":"zeta/ops/unitwise_norm/#example-3-tensor-norm","title":"Example 3: Tensor Norm","text":"<p>In this example, <code>unitwise_norm</code> is applied to a four-dimensional tensor, which could represent the weights of a convolutional neural network layer.</p> <pre><code>import torch\n\nfrom zeta.ops import unitwise_norm\n\n# Create a four-dimensional tensor\nx = torch.randn(10, 10, 3, 3)\n\n# Calculate the unitwise norm of the tensor\nnorm = unitwise_norm(x)\nprint(norm)\n</code></pre>"},{"location":"zeta/ops/unitwise_norm/#source-code","title":"Source Code","text":"<p>Below is the source code for the <code>unitwise_norm</code> function.</p> <pre><code>def unitwise_norm(x):\n    \"\"\"\n    Unitwise norm\n\n    Args:\n        x (torch.Tensor): Input tensor\n\n    Returns:\n        Norm of the input tensor calculated unit-wise.\n\n    Example:\n        &gt;&gt;&gt; x = torch.randn(10, 10)\n        &gt;&gt;&gt; unitwise_norm(x)\n    \"\"\"\n    if len(torch.squeeze(x).shape) &lt;= 1:\n        # Compute the norm for a vector\n        norm = x.norm(p=2, dim=0)\n    elif len(x.shape) in [2, 3]:\n        # Compute the norm for a matrix or a 3-dimensional tensor\n        norm = torch.sqrt(torch.sum(x**2, dim=(1, 2), keepdim=True))\n    elif len(x.shape) == 4:\n        # Compute the norm for a 4-dimensional tensor (e.g., CNN weights)\n        norm = torch.sqrt(torch.sum(x**2, dim=(1, 2, 3), keepdim=True)).clamp(min=1e-6)\n    else:\n        raise ValueError(\n            f\"Got a parameter with len(shape) not in [1, 2, 3, 4] {x.shape}\"\n        )\n\n    return norm\n</code></pre> <p>Note that the actual implementation assumes the presence of the rest of the library and appropriate handling of various shapes of tensors, which is not fully detailed here.</p>"},{"location":"zeta/ops/unitwise_norm/#additional-tips","title":"Additional Tips","text":"<ul> <li>It is important to understand the shape of the tensor you are attempting to normalize, as this will affect the behavior of the <code>unitwise_norm</code> function.</li> <li>Notice that in the code, the <code>clamp</code> function is used to prevent division by zero when normalizing the norm. This is a common practice in normalization implementations.</li> </ul>"},{"location":"zeta/ops/unitwise_norm/#references-and-further-reading","title":"References and Further Reading","text":"<p>For further information about norms and their calculation in PyTorch, please consult the following sources:</p> <ul> <li>PyTorch Documentation: torch.norm</li> <li>Convolutional Neural Networks: CNNs</li> </ul> <p>Remember to explore additional resources to fully understand the context in which <code>unitwise_norm</code> is used and the mathematical foundations behind normalization techniques. ```</p> <p>The provided example exhibits a structure similar to what would be used in actual documentation, although it is significantly condensed owing to the constraints of this platform. To reach a professional standard, each section would need to be expanded with meticulous details, multiple usage scenarios, thorough explanations of the internal workings, and extensive examples. The source code comments would also be more elaborated to clarify each step and the reasoning behind each condition and operation.</p>"},{"location":"zeta/ops/unsqueeze_2d_new/","title":"<code>unsqueeze_2d_new</code> Function Documentation","text":"<p>The <code>unsqueeze_2d_new</code> is a custom function within the <code>zeta.ops</code> library which performs a specific operation onto input tensors, notably rearranging and scaling the spatial dimensions. The following extensive documentation will cover the purpose, architecture, working principle, and usage examples of this function.</p>"},{"location":"zeta/ops/unsqueeze_2d_new/#overview-and-introduction","title":"Overview and Introduction","text":"<p>The <code>unsqueeze_2d_new</code> function serves as a utility within deep learning operations, specifically those that involve manipulating the spatial dimensions of tensors, typically within the context of convolutional neural networks (CNNs) or other architectures dealing with image or grid-like data. The function's main purpose is to expand the spatial dimensions (height and width) of the input tensor by a specified scaling factor. This is akin to performing an 'un-squeeze' operation in two dimensions, enabling finer spatial resolution processing or preparing the tensor for upscaling operations.</p>"},{"location":"zeta/ops/unsqueeze_2d_new/#function-definition","title":"Function Definition","text":"<pre><code>def unsqueeze_2d_new(input, factor=2):\n    \"\"\"\n    Expands the spatial dimensions of an input tensor by rearranging its elements according to a given spatial factor.\n\n    Parameters:\n    - input (Tensor): A 4D input tensor with shape (batch_size, channels, height, width).\n    - factor (int): The scaling factor for the spatial dimensions. Default value is 2.\n\n    Returns:\n    - Tensor: A tensor with expanded spatial dimensions.\n    \"\"\"\n    return rearrange(\n        input, \"b (c h2 w2) h w -&gt; b c (h h2) (w w2)\", h2=factor, w2=factor\n    )\n</code></pre> <p>Parameters and Return Value:</p> Parameter Type Description Default Value <code>input</code> Tensor A 4D input tensor with dimensions representing batch size, number of channels, height, and width, respectively. None (required) <code>factor</code> int The scaling factor by which to expand the spatial dimensions of the input tensor: <code>height</code> and <code>width</code>. 2 Return Value Type Description (Unnamed) Tensor The output tensor after spatial dimension expansion, having larger height and width by a factor of <code>factor</code>."},{"location":"zeta/ops/unsqueeze_2d_new/#detailed-explanation-and-usage","title":"Detailed Explanation and Usage","text":""},{"location":"zeta/ops/unsqueeze_2d_new/#how-it-works","title":"How It Works","text":"<p>The <code>unsqueeze_2d_new</code> utilizes the <code>rearrange</code> function from the <code>einops</code> library or a similar tensor manipulation library, which allows for a concise and readable tensor transformation. The operation performed by <code>unsqueeze_2d_new</code> implicitly reshapes and expands the 2D spatial dimensions (<code>height</code> and <code>width</code>) without altering the data within the batch and channel dimensions. This operation is useful in neural networks where a change in spatial resolution is required, such as in generative networks, spatial attention mechanisms, and feature pyramids.</p>"},{"location":"zeta/ops/unsqueeze_2d_new/#usage-example-1-basic-usage","title":"Usage Example 1: Basic Usage","text":"<p>This example demonstrates how to use the <code>unsqueeze_2d_new</code> function to double the height and width of a random tensor.</p> <pre><code>import torch\n\nfrom zeta.ops import unsqueeze_2d_new\n\n# 1. Prepare a random tensor with shape (batch_size=1, channels=3, height=4, width=4)\ninput_tensor = torch.rand(1, 3, 4, 4)\n\n# 2. Apply the unsqueeze_2d_new function with the default factor\noutput_tensor = unsqueeze_2d_new(input_tensor)\n\n# 3. Verify the shape of the output tensor\nassert output_tensor.shape == (1, 3, 8, 8)\n</code></pre>"},{"location":"zeta/ops/unsqueeze_2d_new/#usage-example-2-custom-scaling-factor","title":"Usage Example 2: Custom Scaling Factor","text":"<p>In this example, we show how to use a different scaling factor to alter the spatial scaling performed by the function.</p> <pre><code>import torch\n\nfrom zeta.ops import unsqueeze_2d_new\n\n# 1. Prepare a random tensor with shape (batch_size=1, channels=3, height=4, width=4)\ninput_tensor = torch.rand(1, 3, 4, 4)\n\n# 2. Apply the unsqueeze_2d_new function with a custom factor of 3\noutput_tensor = unsqueeze_2d_new(input_tensor, factor=3)\n\n# 3. Verify the shape of the output tensor\nassert output_tensor.shape == (1, 3, 12, 12)\n</code></pre>"},{"location":"zeta/ops/unsqueeze_2d_new/#usage-example-3-integrating-into-a-neural-network-layer","title":"Usage Example 3: Integrating into a Neural Network Layer","text":"<p>Lastly, we will demonstrate how <code>unsqueeze_2d_new</code> can be integrated into a  neural network model layer. This could be part of an up-sampling process within a generative model:</p> <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.ops import unsqueeze_2d_new\n\n\nclass UpsampleLayer(nn.Module):\n    def __init__(self, factor=2):\n        super().__init__()\n        self.factor = factor\n\n    def forward(self, x):\n        return unsqueeze_2d_new(x, factor=self.factor)\n\n\n# Model instantiation and usage\nupsample_layer = UpsampleLayer(factor=2)\ninput_tensor = torch.rand(1, 3, 4, 4)\noutput_tensor = upsample_layer(input_tensor)\n\nassert output_tensor.shape == (1, 3, 8, 8)\n</code></pre>"},{"location":"zeta/ops/unsqueeze_2d_new/#additional-information-and-tips","title":"Additional Information and Tips","text":"<p>The <code>unsqueeze_2d_new</code> function is highly dependent on the <code>rearrange</code> operation and thus, relies on the functionality provided by the <code>einops</code> library. When different tensor shapes or patterns are needed, the pattern string inside the <code>rearrange</code> function would need to be adapted accordingly, making this utility highly customizable.</p> <p>Be mindful that increasing the spatial dimensions can significantly increase the memory usage, especially when dealing with large tensors. Therefore, ensure that your hardware is capable of handling the larger tensor sizes that may result from using this function within your models.</p>"},{"location":"zeta/ops/unsqueeze_2d_new/#references-and-further-reading","title":"References and Further Reading","text":"<p>For further details on tensor operations and customization options available with the <code>einops</code> library or similar tensor manipulation libraries, consider the following resources:</p> <ul> <li>Einops documentation and guides: https://einops.rocks/</li> <li>Official PyTorch documentation on tensor operations: https://pytorch.org/docs/stable/tensors.html</li> </ul> <p>This documentation has provided an in-depth look at the <code>unsqueeze_2d_new</code> function, its architecture, functionality, and examples of usage within the scope of tensor manipulation for machine learning and deep learning applications.</p>"},{"location":"zeta/optims/adamw/","title":"<code>StableAdamWUnfused</code> Documentation","text":""},{"location":"zeta/optims/adamw/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Class: <code>StableAdamWUnfused</code></li> <li>Initialization</li> <li>Key Functions</li> <li>Usage Examples</li> <li>Training a Deep Learning Model</li> <li>Using Custom Floating Point Precision</li> <li>Hyperparameter Tuning</li> <li>Additional Information</li> <li>StableAdamW Algorithm</li> <li>Setting Precision to \"custom_fp16\"</li> <li>References</li> </ol>"},{"location":"zeta/optims/adamw/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the documentation for the <code>StableAdamWUnfused</code> optimizer in the Shapeless library! <code>StableAdamWUnfused</code> is designed to provide a stable and efficient implementation of the AdamW optimizer with optional features like custom floating point precision and update clipping. </p>"},{"location":"zeta/optims/adamw/#key-features","title":"Key Features","text":"<ul> <li>StableAdamW: Stable implementation of the AdamW optimizer.</li> <li>Custom Floating Point Precision: Choose between standard precision and custom precision for gradients.</li> <li>Update Clipping: Apply update clipping to prevent excessively large updates.</li> </ul> <p>In this documentation, you will learn how to use the <code>StableAdamWUnfused</code> optimizer effectively, understand its architecture, and explore examples of its applications.</p>"},{"location":"zeta/optims/adamw/#2-class-stableadamwunfused","title":"2. Class: <code>StableAdamWUnfused</code>","text":"<p>The <code>StableAdamWUnfused</code> class is the core component of the Shapeless library, providing advanced optimization techniques for deep learning models. Below, we'll delve into its initialization and key functions.</p>"},{"location":"zeta/optims/adamw/#initialization","title":"Initialization","text":"<pre><code>optimizer = StableAdamWUnfused(\n    params,\n    lr=0.002,\n    weight_decay=0.2,\n    betas=(0.9, 0.99),\n    eps=1e-8,\n    clip_thresh=1.0,\n    precision=\"amp_bfloat16\",\n    custom_scalar=65536,\n)\n</code></pre>"},{"location":"zeta/optims/adamw/#parameters","title":"Parameters:","text":"<ul> <li><code>params</code> (iterable): Model parameters for optimization.</li> <li><code>lr</code> (float): Learning rate (default: 0.002).</li> <li><code>weight_decay</code> (float): Weight decay (L2 penalty) (default: 0.2).</li> <li><code>betas</code> (Tuple[float, float]): Coefficients for computing running averages of gradient and its square (default: (0.9, 0.99)).</li> <li><code>eps</code> (float): Small constant to prevent division by zero (default: 1e-8).</li> <li><code>clip_thresh</code> (float): Threshold for update clipping (default: 1.0).</li> <li><code>precision</code> (str): Precision mode (\"amp_bfloat16\" or \"custom_fp16\") (default: \"amp_bfloat16\").</li> <li><code>custom_scalar</code> (int): Custom scalar for gradients (default: 65536).</li> </ul>"},{"location":"zeta/optims/adamw/#key-functions","title":"Key Functions","text":""},{"location":"zeta/optims/adamw/#stepclosurenone","title":"<code>step(closure=None)</code>","text":"<p>Performs a single optimization step. Computes gradients and updates model parameters.</p> <ul> <li><code>closure</code> (Optional[Callable]): A closure that computes the loss (default: None).</li> </ul>"},{"location":"zeta/optims/adamw/#3-usage-examples","title":"3. Usage Examples","text":"<p>Now, let's explore practical examples of using the <code>StableAdamWUnfused</code> optimizer in various scenarios.</p>"},{"location":"zeta/optims/adamw/#training-a-deep-learning-model","title":"Training a Deep Learning Model","text":"<pre><code># Initialize the optimizer\noptimizer = StableAdamWUnfused(model.parameters(), lr=0.001, weight_decay=0.0001)\n\n# Inside the training loop\noptimizer.zero_grad()\noutputs = model(inputs)\nloss = criterion(outputs, labels)\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"zeta/optims/adamw/#using-custom-floating-point-precision","title":"Using Custom Floating Point Precision","text":"<pre><code># Initialize the optimizer with custom_fp16 precision\noptimizer = StableAdamWUnfused(model.parameters(), lr=0.001, precision=\"custom_fp16\")\n\n# Inside the training loop, use custom scalar\ncustom_scalar = 65536  # Custom scalar value\n(loss * custom_scalar).backward()  # Backward pass with custom scalar\noptimizer.step()\n</code></pre>"},{"location":"zeta/optims/adamw/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code># Define a grid of hyperparameters\nlearning_rates = [0.001, 0.01, 0.1]\nweight_decays = [0.0001, 0.001, 0.01]\n\n# Loop through hyperparameters\nfor lr in learning_rates:\n    for wd in weight_decays:\n        optimizer = StableAdamWUnfused(model.parameters(), lr=lr, weight_decay=wd)\n\n        # Training and evaluation code here\n</code></pre> <p>These examples showcase how the <code>StableAdamWUnfused</code> optimizer can be used in training deep learning models with various configurations.</p>"},{"location":"zeta/optims/adamw/#4-additional-information","title":"4. Additional Information","text":""},{"location":"zeta/optims/adamw/#stableadamw-algorithm","title":"StableAdamW Algorithm","text":"<p>The <code>StableAdamWUnfused</code> optimizer implements the StableAdamW algorithm, which is a stable version of the AdamW optimizer. It provides stability and efficiency in deep learning optimization.</p>"},{"location":"zeta/optims/adamw/#setting-precision-to-custom_fp16","title":"Setting Precision to \"custom_fp16\"","text":"<p>You can set the precision mode to \"custom_fp16\" to use a custom scalar value for gradients. This mode allows fine-grained control over the precision of gradient calculations.</p>"},{"location":"zeta/optims/adamw/#5-references","title":"5. References","text":"<p>For further information and research papers related to the <code>StableAdamWUnfused</code> optimizer and its stability improvements, please refer to the following resources:</p> <ul> <li>Adam: A Method for Stochastic Optimization</li> <li>Decoupled Weight Decay Regularization</li> <li>Custom Floating Point Precision</li> </ul> <p>Explore these references to gain a deeper understanding of the optimization techniques implemented in <code>StableAdamWUnfused</code>.</p> <p>Feel free to reach out to the Shapeless community for any questions or discussions regarding this optimizer. Happy optimizing!</p>"},{"location":"zeta/optims/ga/","title":"<code>GradientAscent</code> Documentation","text":""},{"location":"zeta/optims/ga/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Overview</li> <li>Installation</li> <li>Usage</li> <li>GradientAscent Class</li> <li>Examples</li> <li>Architecture and Purpose</li> <li>Parameters</li> <li>Three Usage Examples</li> <li>Basic Usage</li> <li>Gradient Clipping</li> <li>Learning Rate Decay and Warmup</li> <li>Additional Information</li> <li>Conclusion</li> </ol>"},{"location":"zeta/optims/ga/#1-introduction","title":"1. Introduction","text":"<p>The <code>GradientAscent</code> module is an optimizer designed for performing gradient ascent on the parameters of a machine learning model. It is a powerful tool for optimizing models in tasks where maximizing a certain objective function is necessary, such as generative modeling and reinforcement learning.</p> <p>This documentation provides a comprehensive guide on how to use the <code>GradientAscent</code> module. It covers its purpose, parameters, and usage, ensuring that you can effectively employ it in your machine learning projects.</p>"},{"location":"zeta/optims/ga/#2-overview","title":"2. Overview","text":"<p>The <code>GradientAscent</code> module is a specialized optimizer that focuses on increasing the value of an objective function by iteratively adjusting the model's parameters. Key features and parameters of the <code>GradientAscent</code> module include:</p> <ul> <li><code>lr</code>: Learning rate, controlling the step size for parameter updates.</li> <li><code>momentum</code>: Momentum factor, improving convergence speed and stability.</li> <li><code>beta</code>: Beta factor, influencing adaptive learning rate.</li> <li><code>eps</code>: Epsilon, a small value to prevent division by zero.</li> <li><code>nesterov</code>: Enables Nesterov accelerated gradient for faster convergence.</li> <li><code>clip_value</code>: Optional gradient clipping to prevent exploding gradients.</li> <li><code>lr_decay</code>: Learning rate decay for preventing oscillations.</li> <li><code>warmup_steps</code>: Warmup steps for gradual learning rate increase.</li> <li><code>logging_interval</code>: Interval for logging optimization progress.</li> </ul> <p>By using the <code>GradientAscent</code> optimizer, you can efficiently maximize your model's performance in tasks that require gradient ascent.</p>"},{"location":"zeta/optims/ga/#3-installation","title":"3. Installation","text":"<p>Before using the <code>GradientAscent</code> module, ensure you have the required dependencies, primarily PyTorch, installed. You can install PyTorch using pip:</p> <pre><code>pip install torch\n</code></pre>"},{"location":"zeta/optims/ga/#4-usage","title":"4. Usage","text":"<p>In this section, we'll explore how to use the <code>GradientAscent</code> module effectively. It consists of the <code>GradientAscent</code> class and provides examples to demonstrate its usage.</p>"},{"location":"zeta/optims/ga/#41-gradientascent-class","title":"4.1. <code>GradientAscent</code> Class","text":"<p>The <code>GradientAscent</code> class is the core component of the <code>GradientAscent</code> module. It is used to create a <code>GradientAscent</code> optimizer instance, which can perform gradient ascent on a model's parameters.</p>"},{"location":"zeta/optims/ga/#parameters","title":"Parameters:","text":"<ul> <li><code>parameters</code> (iterable): Iterable of model parameters to optimize or dicts defining parameter groups.</li> <li><code>lr</code> (float, optional): Learning rate (default: 0.01).</li> <li><code>momentum</code> (float, optional): Momentum factor (default: 0.9).</li> <li><code>beta</code> (float, optional): Beta factor (default: 0.999).</li> <li><code>eps</code> (float, optional): Epsilon (default: 1e-8).</li> <li><code>nesterov</code> (bool, optional): Enables Nesterov accelerated gradient (default: False).</li> <li><code>clip_value</code> (float, optional): Gradient clipping value (default: None).</li> <li><code>lr_decay</code> (float, optional): Learning rate decay (default: None).</li> <li><code>warmup_steps</code> (int, optional): Warmup steps (default: 0).</li> <li><code>logging_interval</code> (int, optional): Logging interval (default: 10).</li> </ul>"},{"location":"zeta/optims/ga/#42-examples","title":"4.2. Examples","text":"<p>Let's explore how to use the <code>GradientAscent</code> class with different scenarios and applications.</p>"},{"location":"zeta/optims/ga/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<p>In this example, we'll use the <code>GradientAscent</code> optimizer with default parameters to perform basic gradient ascent.</p> <pre><code>import torch\n\n# Define a simple model and its objective function\nmodel = torch.nn.Linear(1, 1)\nobjective = lambda x: -x  # Maximizing the negative value\n\n# Initialize the GradientAscent optimizer\noptimizer = GradientAscent(model.parameters(), lr=0.01)\n\n# Perform gradient ascent for 100 steps\nfor _ in range(100):\n    optimizer.zero_grad()\n    output = model(torch.tensor([1.0]))\n    loss = objective(output)\n    loss.backward()\n    optimizer.step()\n\n# Check the optimized model's parameters\noptimized_value = model(torch.tensor([1.0])).item()\nprint(f\"Optimized Value: {optimized_value}\")\n</code></pre>"},{"location":"zeta/optims/ga/#example-2-gradient-clipping","title":"Example 2: Gradient Clipping","text":"<p>In this example, we'll use gradient clipping to prevent exploding gradients during optimization.</p> <pre><code>import torch\n\n# Define a model with a complex gradient landscape\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1)\n)\n\n# Objective function for maximizing model output\nobjective = lambda x: -x\n\n# Initialize the GradientAscent optimizer with gradient clipping\noptimizer = GradientAscent(model.parameters(), lr=0.01, clip_value=1.0)\n\n# Perform gradient ascent for 100 steps\nfor _ in range(100):\n    optimizer.zero_grad()\n    output = model(torch.tensor([1.0]))\n    loss = objective(output)\n    loss.backward()\n    optimizer.step()\n\n# Check the optimized model's parameters\noptimized_value = model(torch.tensor([1.0])).item()\nprint(f\"Optimized Value: {optimized_value}\")\n</code></pre>"},{"location":"zeta/optims/ga/#example-3-learning-rate-decay-and-warmup","title":"Example 3: Learning Rate Decay and Warmup","text":"<p>In this example, we'll use learning rate decay and warmup to fine-tune optimization behavior.</p> <pre><code>import torch\n\n# Define a model with a complex gradient landscape\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 10),\n    torch.nn.ReLU(),\n    torch.nn.Linear(10, 1)\n)\n\n# Objective function for maximizing model output\nobjective = lambda x: -x\n\n# Initialize the GradientAscent optimizer with learning rate decay and warmup\noptimizer = GradientAscent(\n    model.parameters(),\n    lr=0.01,\n    clip_value=1.0,\n    lr_decay=0.95,      # Learning rate decay\n    warmup_steps=50,    # Warmup for the first 50 steps\n)\n\n# Perform gradient ascent for 100 steps\nfor _ in range(100):\n    optimizer.zero_grad()\n    output = model(torch.tensor([1.0]))\n    loss = objective(output)\n    loss.backward()\n    optimizer.step()\n\n# Check the optimized model's parameters\noptimized_value = model(torch.tensor([1.\n\n0])).item()\nprint(f\"Optimized Value: {optimized_value}\")\n</code></pre>"},{"location":"zeta/optims/ga/#5-architecture-and-purpose","title":"5. Architecture and Purpose","text":"<p>The <code>GradientAscent</code> optimizer is designed to maximize an objective function by adjusting the parameters of a machine learning model. It is particularly useful in scenarios where you need to find model parameters that result in the highest possible value of the objective function. Key architectural aspects and purposes of the <code>GradientAscent</code> optimizer include:</p> <ul> <li> <p>Maximization Objective: The optimizer's primary purpose is to maximize a given objective function. You can define the objective function according to your task, and the optimizer iteratively adjusts the model's parameters to maximize this function.</p> </li> <li> <p>Gradient-Based Optimization: It operates based on gradients, just like traditional gradient descent optimizers. However, instead of minimizing a loss, it maximizes an objective function.</p> </li> <li> <p>Parameter Updates: The optimizer updates model parameters by taking steps in the direction of gradient ascent. This process continues until convergence or a specified number of steps.</p> </li> <li> <p>Controlled Learning Rate: It allows you to control the learning rate, momentum, and other optimization parameters to fine-tune the optimization process.</p> </li> <li> <p>Additional Features: The optimizer supports gradient clipping, learning rate decay, and warmup steps to enhance optimization stability and performance.</p> </li> </ul>"},{"location":"zeta/optims/ga/#6-parameters","title":"6. Parameters","text":"<p>Here is a detailed explanation of the parameters used by the <code>GradientAscent</code> optimizer:</p> <ul> <li> <p><code>parameters</code> (iterable): An iterable of model parameters to optimize or dicts defining parameter groups. These are the parameters that the optimizer will adjust during optimization.</p> </li> <li> <p><code>lr</code> (float, optional): The learning rate determines the step size for parameter updates. A higher learning rate results in larger steps and potentially faster convergence, but it can also lead to instability. The default value is 0.01.</p> </li> <li> <p><code>momentum</code> (float, optional): Momentum is a factor that improves convergence speed and stability. It adds a fraction of the previous gradient to the current gradient, allowing the optimizer to continue in the same direction with increased confidence. The default value is 0.9.</p> </li> <li> <p><code>beta</code> (float, optional): Beta is a factor that influences adaptive learning rate. It is used in combination with epsilon to adapt the learning rate for each parameter. The default value is 0.999.</p> </li> <li> <p><code>eps</code> (float, optional): Epsilon is a small value added to the denominator to prevent division by zero when calculating adaptive learning rates. The default value is 1e-8.</p> </li> <li> <p><code>nesterov</code> (bool, optional): Nesterov accelerated gradient (NAG) is a feature that provides lookahead in the direction of parameter updates. It can accelerate convergence. The default value is False.</p> </li> <li> <p><code>clip_value</code> (float, optional): Gradient clipping is an optional mechanism to prevent exploding gradients. If specified, the gradients are clipped to the specified value. The default value is None, indicating no gradient clipping.</p> </li> <li> <p><code>lr_decay</code> (float, optional): Learning rate decay is used to prevent oscillations during optimization. If specified, the learning rate is multiplied by this factor after each optimization step. The default value is None, indicating no learning rate decay.</p> </li> <li> <p><code>warmup_steps</code> (int, optional): Warmup steps gradually increase the learning rate during the initial optimization steps. This can help the optimization process start more smoothly. The default value is 0, indicating no warmup.</p> </li> <li> <p><code>logging_interval</code> (int, optional): Logging interval determines how often optimization progress is logged. It specifies the number of optimization steps between log entries. The default value is 10.</p> </li> </ul>"},{"location":"zeta/optims/ga/#7-three-usage-examples","title":"7. Three Usage Examples","text":""},{"location":"zeta/optims/ga/#71-basic-usage","title":"7.1. Basic Usage","text":"<p>In this example, we'll use the <code>GradientAscent</code> optimizer with default parameters to perform basic gradient ascent.</p> <pre><code>import torch\n\n# Define a simple model and its objective function\nmodel = torch.nn.Linear(1, 1)\nobjective = lambda x: -x  # Maximizing the negative value\n\n# Initialize the GradientAscent optimizer\noptimizer = GradientAscent(model.parameters(), lr=0.01)\n\n# Perform gradient ascent for 100 steps\nfor _ in range(100):\n    optimizer.zero_grad()\n    output = model(torch.tensor([1.0]))\n    loss = objective(output)\n    loss.backward()\n    optimizer.step()\n\n# Check the optimized model's parameters\noptimized_value = model(torch.tensor([1.0])).item()\nprint(f\"Optimized Value: {optimized_value}\")\n</code></pre>"},{"location":"zeta/optims/ga/#72-gradient-clipping","title":"7.2. Gradient Clipping","text":"<p>In this example, we'll use gradient clipping to prevent exploding gradients during optimization.</p> <pre><code>import torch\n\n# Define a model with a complex gradient landscape\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1)\n)\n\n# Objective function for maximizing model output\nobjective = lambda x: -x\n\n# Initialize the GradientAscent optimizer with gradient clipping\noptimizer = GradientAscent(model.parameters(), lr=0.01, clip_value=1.0)\n\n# Perform gradient ascent for 100 steps\nfor _ in range(100):\n    optimizer.zero_grad()\n    output = model(torch.tensor([1.0]))\n    loss = objective(output)\n    loss.backward()\n    optimizer.step()\n\n# Check the optimized model's parameters\noptimized_value = model(torch.tensor([1.0])).item()\nprint(f\"Optimized Value: {optimized_value}\")\n</code></pre>"},{"location":"zeta/optims/ga/#73-learning-rate-decay-and-warmup","title":"7.3. Learning Rate Decay and Warmup","text":"<p>In this example, we'll use learning rate decay and warmup to fine-tune optimization behavior.</p> <pre><code>import torch\n\n# Define a model with a complex gradient landscape\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1)\n)\n\n# Objective function for maximizing model output\nobjective = lambda x: -x\n\n# Initialize the GradientAscent optimizer with learning rate decay and warmup\noptimizer = GradientAscent(\n    model.parameters(),\n    lr=0.01,\n    clip_value=1.0,\n    lr_decay=0.95,  # Learning rate decay\n    warmup_steps=50,  # Warmup for the first 50 steps\n)\n\n# Perform gradient ascent for 100 steps\nfor _ in range(100):\n    optimizer.zero_grad()\n    output = model(torch.tensor([1.0]))\n    loss = objective(output)\n    loss.backward()\n    optimizer.step()\n\n# Check the optimized model's parameters\noptimized_value = model(torch.tensor([1.0])).item()\nprint(f\"Optimized Value: {optimized_value}\")\n</code></pre>"},{"location":"zeta/optims/ga/#8-additional-information","title":"8. Additional Information","text":"<ul> <li>Objective Function: The choice of objective function is critical when using the <code>GradientAscent</code> optimizer. Ensure that your objective function is aligned with</li> </ul> <p>the goal of your task.</p> <ul> <li> <p>Hyperparameter Tuning: Experiment with different hyperparameters, such as learning rate, momentum, and warmup steps, to fine-tune the optimization process for your specific task.</p> </li> <li> <p>Gradient Clipping: Gradient clipping can be essential for preventing gradient explosions, especially when optimizing complex models.</p> </li> <li> <p>Logging: The <code>logging_interval</code> parameter allows you to control how often optimization progress is logged, providing insights into the optimization process.</p> </li> <li> <p>Learning Rate Scheduling: Learning rate decay and warmup can significantly impact optimization behavior. Adjust these parameters as needed for your task.</p> </li> <li> <p>Convergence: Keep in mind that gradient ascent may not always converge to the global maximum. Multiple runs with different initializations may be required.</p> </li> </ul>"},{"location":"zeta/optims/ga/#9-conclusion","title":"9. Conclusion","text":"<p>The <code>GradientAscent</code> optimizer is a valuable tool for maximizing objective functions in machine learning tasks. This documentation has provided a detailed overview of its architecture, purpose, parameters, and usage. By following the examples and guidelines, you can effectively use the <code>GradientAscent</code> optimizer to optimize your models for various tasks.</p> <p>If you have any further questions or need assistance, please refer to external resources or reach out to the community for support.</p> <p>Happy optimizing!</p>"},{"location":"zeta/product/product_ideas/","title":"Overview","text":""},{"location":"zeta/product/product_ideas/#10-product-ideas-centered-around-zeta","title":"10 Product Ideas Centered Around Zeta","text":"<p>In the rapidly evolving landscape of AI and deep learning, there is a growing demand for tools that simplify the model-building process. Zeta, with its modular and re-usable building blocks, has the potential to be at the forefront of this revolution. Below are ten product ideas based on the Zeta framework:</p>"},{"location":"zeta/product/product_ideas/#1-zetahub-drag-n-drop-ai-model-builder","title":"1. Zetahub: Drag N Drop AI Model Builder","text":"<p>As previously introduced, Zetahub is a user-friendly platform that provides drag-and-drop functionality for building AI models. It brings the power of leading deep learning frameworks like PyTorch, JAX, and Einops to the fingertips of users, regardless of their coding knowledge.</p>"},{"location":"zeta/product/product_ideas/#2-zeta-activate-llm-training-suite","title":"2. Zeta Activate: LLM Training Suite","text":"<p>Zeta Activate is a dedicated environment for training large language models (LLMs). With optimized resources and an intuitive interface, users can train their custom LLMs without the hassle of configuring complex settings.</p>"},{"location":"zeta/product/product_ideas/#3-zetahub-pro-model-enhancement-optimization","title":"3. ZetaHub Pro: Model Enhancement &amp; Optimization","text":"<p>Go a step further than basic model building with ZetaHub Pro. This premium platform offers advanced optimization techniques, hyperparameter tuning, and automated model enhancement features, ensuring your models perform at their peak.</p>"},{"location":"zeta/product/product_ideas/#4-zetaflow-automated-ml-pipelines","title":"4. ZetaFlow: Automated ML Pipelines","text":"<p>ZetaFlow is a tool for constructing end-to-end machine learning pipelines. From data preprocessing to model deployment, streamline every step of the process with modular building blocks.</p>"},{"location":"zeta/product/product_ideas/#5-zetasight-visualization-model-interpretability","title":"5. ZetaSight: Visualization &amp; Model Interpretability","text":"<p>In an age where transparency in AI is crucial, ZetaSight provides tools for model visualization and interpretability. Understand how your model makes decisions and convey this knowledge to non-technical stakeholders.</p>"},{"location":"zeta/product/product_ideas/#6-zetacloud-ai-model-hosting-deployment","title":"6. ZetaCloud: AI Model Hosting &amp; Deployment","text":"<p>Take your trained models live with ZetaCloud. This cloud-based solution offers seamless deployment options, scalability, and robust performance monitoring tools to ensure your models run efficiently in production environments.</p>"},{"location":"zeta/product/product_ideas/#7-zetastore-marketplace-for-pre-trained-models","title":"7. ZetaStore: Marketplace for Pre-trained Models","text":"<p>Much like Huggingface, ZetaStore is a hub where developers can share, discover, and deploy pre-trained models. With an emphasis on community and collaboration, ZetaStore makes it easy to build upon the work of others.</p>"},{"location":"zeta/product/product_ideas/#8-zetatune-automated-hyperparameter-tuning","title":"8. ZetaTune: Automated Hyperparameter Tuning","text":"<p>Eliminate the guesswork from model training with ZetaTune. This tool automates the hyperparameter tuning process, using advanced algorithms to find the optimal settings for your specific dataset.</p>"},{"location":"zeta/product/product_ideas/#9-zetaconnect-integration-hub","title":"9. ZetaConnect: Integration Hub","text":"<p>ZetaConnect acts as a bridge between Zeta and third-party applications, databases, and platforms. Whether you're importing data, exporting models, or integrating AI capabilities into existing software, ZetaConnect makes the process seamless.</p>"},{"location":"zeta/product/product_ideas/#10-zetalearn-educational-platform-community","title":"10. ZetaLearn: Educational Platform &amp; Community","text":"<p>For those new to AI or the Zeta framework, ZetaLearn offers tutorials, courses, and a vibrant community forum. From beginner to expert, there's always something new to learn on ZetaLearn.</p>"},{"location":"zeta/product/product_ideas/#product-ideas-table","title":"Product Ideas Table","text":"Product Name Price ($) Estimated Cashflow per Month ($) Number of Potential Customers Most Potential for Cashflow Profitability Zetahub 30/mo 600,000 20,000 High Zeta Activate 50/mo 500,000 10,000 High ZetaHub Pro 60/mo 480,000 8,000 Medium ZetaFlow 40/mo 400,000 10,000 Medium ZetaSight 35/mo 350,000 10,000 Medium ZetaCloud 70/mo 490,000 7,000 High ZetaStore Free Ad-based Revenue 30,000 Medium ZetaTune 45/mo 450,000 10,000 Medium ZetaConnect 40/mo 400,000 10,000 Medium ZetaLearn Free Ad-based &amp; Premium Content 50,000 High <p>If PyTorch or similar frameworks were to commercialize rapidly, they'd likely explore platforms that democratize access to AI. This means creating intuitive interfaces, offering cloud solutions, prioritizing education, and building a sense of community. The products listed above are aligned with this vision and can serve as potential roadmaps for AI companies looking to expand their offerings.</p> <p>Zeta, with its high-performance and scalable nature, is well-positioned to meet the current and future demands of AI enthusiasts, developers, and businesses. Whether it's building, optimizing, deploying, or understanding models, the Zeta ecosystem offers a comprehensive suite of tools to cater to all AI needs.</p>"},{"location":"zeta/product/zetahub/","title":"Zetahub","text":""},{"location":"zeta/product/zetahub/#zetahub-drag-n-drop-ai-model-builder","title":"Zetahub: Drag N Drop AI Model Builder","text":"<p>Zetahub is a groundbreaking platform that empowers users to craft state-of-the-art AI models without writing a single line of code! Leveraging the power of the Zeta framework, Zetahub allows you to piece together models with drag-and-drop functionality, utilizing blocks based on PyTorch, JAX, and Einops.</p>"},{"location":"zeta/product/zetahub/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Features</li> <li>Getting Started</li> <li>Frontend Development</li> <li>Contribute</li> <li>Support</li> <li>License</li> </ul>"},{"location":"zeta/product/zetahub/#features","title":"Features","text":"<ul> <li>No Code Platform: No prior coding experience? No worries! Build complex models without any coding.</li> <li>Drag-and-Drop Functionality: Easily design your AI model architecture by dragging blocks onto a canvas.</li> <li>Powered by PyTorch, JAX, and Einops: Access the capabilities of leading deep learning frameworks in block format.</li> <li>Integrated with Zeta Framework: Seamless integration with the Python-based Zeta framework ensures robustness and flexibility.</li> </ul>"},{"location":"zeta/product/zetahub/#standard-operating-procedure-sop-for-llm-product-design-and-management-agent","title":"Standard Operating Procedure (SOP) for LLM Product Design and Management Agent","text":""},{"location":"zeta/product/zetahub/#mastery-in-uiux-and-product-management","title":"Mastery in UI/UX and Product Management","text":""},{"location":"zeta/product/zetahub/#objective","title":"Objective:","text":"<p>Equip the LLM with comprehensive expertise in product design, focusing on UI/UX design, and effective product management. The LLM will be proficient in designing aesthetically appealing, user-friendly interfaces and overseeing a product's lifecycle from inception to</p> <p>Product Specification: Zetahub Drag N Drop AI Model Builder</p>"},{"location":"zeta/product/zetahub/#1-overview","title":"1. Overview","text":"<p>Zetahub is an intuitive, no-code platform designed to bring AI model building to the fingertips of everyone, irrespective of their coding background. Using the Zeta framework, users can drag and drop blocks to create models with ease.</p>"},{"location":"zeta/product/zetahub/#2-features","title":"2. Features","text":""},{"location":"zeta/product/zetahub/#21-no-code-environment","title":"2.1 No-Code Environment","text":"<ul> <li>Requirement: A user-friendly dashboard where users can navigate without encountering code.</li> <li>Specification: Integrate visual elements like buttons, drop-downs, and tooltips to guide users.</li> </ul>"},{"location":"zeta/product/zetahub/#22-drag-and-drop-functionality","title":"2.2 Drag-and-Drop Functionality","text":"<ul> <li>Requirement: Allow users to drag components onto a canvas to assemble their AI model.</li> <li>Specification: Implement a canvas area where users can drag blocks. Each block represents a function or parameter from PyTorch, JAX, or Einops.</li> </ul>"},{"location":"zeta/product/zetahub/#23-blocks-based-on-leading-frameworks","title":"2.3 Blocks based on Leading Frameworks","text":"<ul> <li>Requirement: Provide blocks based on PyTorch, JAX, and Einops functionalities.</li> <li>Specification: Categorize blocks based on their underlying framework and functionality. Users can search, filter, and select the appropriate block.</li> </ul>"},{"location":"zeta/product/zetahub/#24-integration-with-zeta-framework","title":"2.4 Integration with Zeta Framework","text":"<ul> <li>Requirement: Seamless integration of the Zeta framework with Zetahub.</li> <li>Specification: Ensure that the Zeta framework is fully compatible with the drag-and-drop platform. Allow import/export capabilities between the Zeta framework and Zetahub.</li> </ul>"},{"location":"zeta/product/zetahub/#3-technical-requirements","title":"3. Technical Requirements","text":""},{"location":"zeta/product/zetahub/#31-backend","title":"3.1 Backend","text":"<ul> <li>Implement a robust backend system that supports multiple users building models simultaneously.</li> <li>Use containerization to isolate each user's model building environment.</li> <li>Ensure that changes made on the canvas translate to appropriate actions in the underlying frameworks.</li> </ul>"},{"location":"zeta/product/zetahub/#32-frontend","title":"3.2 Frontend","text":"<ul> <li>Design a clean, intuitive UI.</li> <li>Ensure responsiveness across devices.</li> <li>Implement a tutorial system for new users to get acquainted with the platform.</li> </ul>"},{"location":"zeta/product/zetahub/#33-database","title":"3.3 Database","text":"<ul> <li>Store user-created models securely.</li> <li>Allow users to save, load, and share their models.</li> </ul>"},{"location":"zeta/product/zetahub/#34-security","title":"3.4 Security","text":"<ul> <li>Implement end-to-end encryption to protect user data.</li> <li>Regularly audit and patch vulnerabilities.</li> </ul>"},{"location":"zeta/product/zetahub/#4-integration-points","title":"4. Integration Points","text":""},{"location":"zeta/product/zetahub/#41-zeta-framework","title":"4.1 Zeta Framework","text":"<ul> <li>Ensure smooth transition and compatibility between Zetahub and the Zeta framework.</li> </ul>"},{"location":"zeta/product/zetahub/#42-third-party-frameworks","title":"4.2 Third-Party Frameworks","text":"<ul> <li>Continually update the platform to accommodate the latest releases from PyTorch, JAX, and Einops.</li> </ul>"},{"location":"zeta/product/zetahub/#43-export-deployment","title":"4.3 Export &amp; Deployment","text":"<ul> <li>Allow users to export their created models in popular formats.</li> <li>Provide integration points with popular cloud platforms for deployment.</li> </ul>"},{"location":"zeta/product/zetahub/#5-testing","title":"5. Testing","text":""},{"location":"zeta/product/zetahub/#51-user-testing","title":"5.1 User Testing","text":"<ul> <li>Conduct alpha and beta testing to gather user feedback.</li> <li>Run A/B tests on UI changes to determine optimal user experience.</li> </ul>"},{"location":"zeta/product/zetahub/#52-performance-testing","title":"5.2 Performance Testing","text":"<ul> <li>Ensure the platform can handle large models and high user traffic.</li> <li>Test the platform's efficiency in translating drag-and-drop actions into model building tasks.</li> </ul>"},{"location":"zeta/product/zetahub/#53-security-testing","title":"5.3 Security Testing","text":"<ul> <li>Regular penetration testing to identify potential vulnerabilities.</li> <li>Employ third-party security firms to assess and validate security measures.</li> </ul>"},{"location":"zeta/product/zetahub/#6-support-maintenance","title":"6. Support &amp; Maintenance","text":""},{"location":"zeta/product/zetahub/#61-documentation","title":"6.1 Documentation","text":"<ul> <li>Comprehensive user manuals detailing each feature and functionality.</li> <li>Regularly update documentation based on platform changes and user feedback.</li> </ul>"},{"location":"zeta/product/zetahub/#62-customer-support","title":"6.2 Customer Support","text":"<ul> <li>Offer multiple channels of support: chat, email, and phone.</li> <li>Regularly train support staff on the latest platform updates.</li> </ul>"},{"location":"zeta/product/zetahub/#63-updates-patches","title":"6.3 Updates &amp; Patches","text":"<ul> <li>Schedule regular updates to enhance features and fix known issues.</li> <li>Implement a system to inform users of updates and patches.</li> </ul>"},{"location":"zeta/product/zetahub/#7-conclusion","title":"7. Conclusion","text":"<p>Zetahub aims to democratize AI model building. By ensuring an intuitive user experience combined with the power of leading AI frameworks, Zetahub will be at the forefront of AI innovation. Ensuring regular feedback loops, continuous improvement, and staying updated with the latest in AI and user interface trends will be crucial for Zetahub's sustained success and relevance in the market.</p>"},{"location":"zeta/quant/bitlinear/","title":"BitLinear Module Documentation","text":"<p>==============================</p>"},{"location":"zeta/quant/bitlinear/#overview","title":"Overview","text":"<p>The\u00a0<code>BitLinear</code>\u00a0module is a custom implementation of a linear layer in a neural network, with the added functionality of bit quantization. This module is designed to work with PyTorch's\u00a0<code>nn.Module</code>\u00a0and can be integrated into any PyTorch model architecture.</p> <p>The\u00a0<code>BitLinear</code>\u00a0module performs linear transformation on the input data, followed by quantization and dequantization. The quantization process is performed using the\u00a0<code>absmax_quantize</code>\u00a0function, which quantizes the input tensor based on the absolute maximum value.</p>"},{"location":"zeta/quant/bitlinear/#absmax_quantize-function","title":"absmax_quantize Function","text":"<p>The\u00a0<code>absmax_quantize</code>\u00a0function is a helper function used by the\u00a0<code>BitLinear</code>\u00a0module to perform quantization and dequantization of the input tensor.</p>"},{"location":"zeta/quant/bitlinear/#parameters","title":"Parameters","text":"Parameter Type Description x torch.Tensor The input tensor to be quantized. bits int (optional) The number of bits to use for quantization. Default is 8."},{"location":"zeta/quant/bitlinear/#returns","title":"Returns","text":"Return Value Type Description quant torch.Tensor The quantized tensor. dequant torch.Tensor The dequantized tensor."},{"location":"zeta/quant/bitlinear/#bitlinear-class","title":"BitLinear Class","text":"<p>The\u00a0<code>BitLinear</code>\u00a0class is a custom implementation of a linear layer that performs bit quantization on the input data.</p>"},{"location":"zeta/quant/bitlinear/#parameters_1","title":"Parameters","text":"Parameter Type Description in_features int The number of input features. out_features int The number of output features. groups int (optional) The number of groups for group normalization. Default is 1."},{"location":"zeta/quant/bitlinear/#methods","title":"Methods","text":""},{"location":"zeta/quant/bitlinear/#__init__self-in_features-out_features-groups1","title":"<code>__init__(self, in_features, out_features, groups=1)</code>","text":"<p>The constructor for the\u00a0<code>BitLinear</code>\u00a0class. Initializes the weight parameter and resets it.</p>"},{"location":"zeta/quant/bitlinear/#reset_parametersself","title":"<code>reset_parameters(self)</code>","text":"<p>Resets the weight parameter using the Kaiming uniform initialization method.</p>"},{"location":"zeta/quant/bitlinear/#forwardself-input","title":"<code>forward(self, input)</code>","text":"<p>Performs the forward pass of the\u00a0<code>BitLinear</code>\u00a0module.</p>"},{"location":"zeta/quant/bitlinear/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/quant/bitlinear/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta.quant import BitLinear\n\n# Initialize the BitLinear module\nlinear = BitLinear(10, 20)\n\n# Create a random tensor of size (128, 10)\ninput = torch.randn(128, 10)\n\n# Perform the forward pass\noutput = linear(input)\n\n# Print the size of the output\nprint(output.size())  # torch.Size([128, 20])\n</code></pre>"},{"location":"zeta/quant/bitlinear/#example-2-using-different-number-of-groups","title":"Example 2: Using Different Number of Groups","text":"<pre><code>import torch\n\nfrom zeta.quant import BitLinear\n\n# Initialize the BitLinear module with 2 groups\nlinear = BitLinear(10, 20, groups=2)\n\n# Create a random tensor of size (128, 10)\ninput = torch.randn(128, 10)\n\n# Perform the forward pass\noutput = linear(input)\n\n# Print the size of the output\nprint(output.size())  # torch.Size([128, 20])\n</code></pre>"},{"location":"zeta/quant/bitlinear/#example-3-integrating-with-a-pytorch-model","title":"Example 3: Integrating with a PyTorch Model","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.quant import BitLinear\n\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = BitLinear(10, 20)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\n# Initialize the model\nmodel = MyModel()\n\n# Create a random tensor of size (128, 10)\ninput = torch.randn(128, 10)\n\n# Perform the forward pass\noutput = model(input)\n\n# Print the size of the output\nprint(output.size())  # torch.Size([128, 20])\n</code></pre>"},{"location":"zeta/quant/bitlinear/#conclusion","title":"Conclusion","text":"<p>The\u00a0<code>BitLinear</code>\u00a0module provides a unique way to perform linear transformation with bit quantization. This can be particularly useful in scenarios where memory efficiency is crucial. As with any other PyTorch module, it can be easily integrated into any model architecture.</p>"},{"location":"zeta/quant/niva/","title":"<code>niva</code>","text":""},{"location":"zeta/quant/niva/#overview","title":"Overview","text":"<p>The Niva module provides functionality for quantizing PyTorch neural network models, enabling you to reduce their memory and computation requirements while preserving their accuracy. Quantization is a crucial technique for deploying models on resource-constrained devices such as edge devices and mobile platforms.</p> <p>This documentation will guide you through the Niva module's architecture, purpose, functions, and usage examples. You'll learn how to effectively quantize your PyTorch models and optimize their performance for different deployment scenarios.</p>"},{"location":"zeta/quant/niva/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Architecture</li> <li>Purpose</li> <li>Function: niva<ul> <li>Parameters</li> <li>Usage Examples<ul> <li>Dynamic Quantization</li> <li>Static Quantization</li> </ul> </li> </ul> </li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/quant/niva/#1-installation","title":"1. Installation","text":"<p>Before using the Niva module, make sure you have PyTorch installed. You can install PyTorch using the following command:</p> <pre><code>pip install zetascale\n</code></pre>"},{"location":"zeta/quant/niva/#2-architecture","title":"2. Architecture","text":"<p>The Niva module leverages PyTorch's quantization capabilities to quantize neural network models. It offers both dynamic and static quantization options to accommodate various use cases.</p>"},{"location":"zeta/quant/niva/#3-purpose","title":"3. Purpose","text":"<p>The primary purpose of the Niva module is to enable quantization of PyTorch models. Quantization is the process of reducing the precision of model weights and activations, which results in smaller model sizes and faster inference on hardware with limited resources. This is especially important for deploying models on edge devices and mobile platforms.</p>"},{"location":"zeta/quant/niva/#4-function-niva","title":"4. Function: niva","text":"<p>The <code>niva</code> function is the core of the Niva module, responsible for quantizing a given PyTorch model. It supports both dynamic and static quantization modes, allowing you to choose the most suitable quantization approach for your model.</p>"},{"location":"zeta/quant/niva/#parameters","title":"Parameters","text":"<p>The <code>niva</code> function accepts the following parameters:</p> <ul> <li><code>model</code> (nn.Module): The PyTorch model to be quantized.</li> <li><code>model_path</code> (str, optional): The path to the pre-trained model's weights. Defaults to None.</li> <li><code>output_path</code> (str, optional): The path where the quantized model will be saved. Defaults to None.</li> <li><code>quant_type</code> (str, optional): The type of quantization to be applied, either \"dynamic\" or \"static\". Defaults to \"dynamic\".</li> <li><code>quantize_layers</code> (Union[List[Type[nn.Module]], None], optional): A list of layer types to be quantized. Defaults to None.</li> <li><code>dtype</code> (torch.dtype, optional): The target data type for quantization, either torch.qint8 or torch.quint8. Defaults to torch.qint8.</li> <li><code>*args</code> and <code>**kwargs</code>: Additional arguments for PyTorch's quantization functions.</li> </ul>"},{"location":"zeta/quant/niva/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/quant/niva/#dynamic-quantization","title":"Dynamic Quantization","text":"<p>In dynamic quantization, you specify the layers to be quantized, and the quantization process occurs dynamically during inference. Here's an example:</p> <pre><code>import torch\n\nfrom zeta import niva\n\n# Load a pre-trained model\nmodel = YourModelClass()\n\n# Quantize the model dynamically, specifying layers to quantize\nniva(\n    model=model,\n    model_path=\"path_to_pretrained_model_weights.pt\",\n    output_path=\"quantized_model.pt\",\n    quant_type=\"dynamic\",\n    quantize_layers=[nn.Linear, nn.Conv2d],\n    dtype=torch.qint8,\n)\n</code></pre>"},{"location":"zeta/quant/niva/#static-quantization","title":"Static Quantization","text":"<p>Static quantization quantizes the entire model before inference. Here's an example:</p> <pre><code>import torch\n\nfrom zeta import niva\n\n# Load a pre-trained model\nmodel = YourModelClass()\n\n# Quantize the entire model statically\nniva(\n    model=model,\n    model_path=\"path_to_pretrained_model_weights.pt\",\n    output_path=\"quantized_model.pt\",\n    quant_type=\"static\",\n    dtype=torch.qint8,\n)\n</code></pre>"},{"location":"zeta/quant/niva/#5-additional-information","title":"5. Additional Information","text":"<ul> <li>The Niva module supports both dynamic and static quantization modes, giving you flexibility in choosing the right approach for your deployment scenario.</li> <li>Always ensure that your model is in evaluation mode (<code>model.eval()</code>) before quantization.</li> <li>Quantization reduces model size and inference time but may slightly affect model accuracy. It's essential to evaluate the quantized model's performance before deployment.</li> </ul>"},{"location":"zeta/quant/niva/#6-references","title":"6. References","text":"<p>For more information on PyTorch quantization and best practices, refer to the official PyTorch documentation: PyTorch Quantization.</p>"},{"location":"zeta/quant/qlora/","title":"Qlora","text":""},{"location":"zeta/quant/qlora/#qloralinear-layer-documentation","title":"QloraLinear Layer Documentation","text":"<p>The QloraLinear layer is an innovative approach to linear transformation in deep learning. The core idea behind QloraLinear is to utilize both the traditional linear transformation and an additional mechanism known as QLoRA (Quantum Linear Representation Approximation). This document provides a comprehensive guide to understanding, utilizing, and testing the QloraLinear layer.</p>"},{"location":"zeta/quant/qlora/#introduction","title":"Introduction","text":"<p>Neural networks are often composed of linear transformations followed by non-linear activations. However, as models grow in complexity and depth, researchers are constantly exploring ways to enhance the expressiveness of individual layers. QloraLinear is one such exploration, introducing quantum-inspired principles to enhance the linear transformation process.</p>"},{"location":"zeta/quant/qlora/#overview-of-qloralinear-layer","title":"Overview of QloraLinear Layer","text":""},{"location":"zeta/quant/qlora/#purpose","title":"Purpose","text":"<p>The primary purpose of the QloraLinear layer is to perform a linear transformation on the input data. However, it introduces an additional term, QLoRA, that captures joint information representation from different subspaces, enhancing the expressiveness of the transformation.</p>"},{"location":"zeta/quant/qlora/#architecture","title":"Architecture","text":"<p>QloraLinear comprises two main components:</p> <ol> <li>Traditional Linear Transformation: This is similar to the standard linear layer in neural networks. The input data is multiplied by a weight matrix to produce the output.</li> <li>QLoRA Transformation: A quantum-inspired term added to the standard linear transformation. It is represented as a product of two matrices, <code>lora_A</code> and <code>lora_B</code>, scaled by a factor. This term introduces additional expressiveness to the layer.</li> </ol>"},{"location":"zeta/quant/qlora/#class-definition-and-parameters","title":"Class Definition and Parameters","text":"<p>The QloraLinear layer is defined as:</p> <pre><code>class QloraLinear(nn.Module):\n</code></pre>"},{"location":"zeta/quant/qlora/#parameters","title":"Parameters","text":"Parameter Type Description in_features int Size of each input sample. out_features int Size of each output sample. weight torch.Tensor Weight tensor of shape (out_features, in_features). r int Number of blocks to use for QLoRA. lora_alpha int (Optional) Scaling factor for QLoRA. Default: 1. lora_dropout float (Optional) Dropout to apply to the QLoRA term. Default: 0.0."},{"location":"zeta/quant/qlora/#methods","title":"Methods","text":"<ul> <li>reset_parameters(): Initializes the learnable parameters of the QLoRA term.</li> <li>forward(x: torch.Tensor) -&gt; torch.Tensor: Performs the linear transformation.</li> </ul>"},{"location":"zeta/quant/qlora/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/quant/qlora/#1-basic-instantiation","title":"1. Basic Instantiation","text":"<p>To instantiate a QloraLinear layer:</p> <pre><code>import torch.nn as nn\n\nfrom zeta.quant.qlora import QloraLinear\n\nin_features = 20\nout_features = 30\nweight = torch.randn(out_features, in_features)\nr = 5\n\nlayer = QloraLinear(in_features, out_features, weight, r)\n</code></pre>"},{"location":"zeta/quant/qlora/#2-forward-pass","title":"2. Forward Pass","text":"<p>Performing a forward pass through the layer:</p> <pre><code>import torch\n\ninput_data = torch.randn(128, in_features)\noutput_data = layer(input_data)\n</code></pre>"},{"location":"zeta/quant/qlora/#3-with-dropout","title":"3. With Dropout","text":"<p>If you want to introduce dropout to the QLoRA term:</p> <pre><code>lora_alpha = 2\nlora_dropout = 0.5\n\ndropout_layer = QloraLinear(\n    in_features, out_features, weight, r, lora_alpha, lora_dropout\n)\noutput_with_dropout = dropout_layer(input_data)\n</code></pre>"},{"location":"zeta/quant/qlora/#testing-the-qloralinear-layer","title":"Testing the QloraLinear Layer","text":"<p>A suite of tests has been provided to ensure the correctness and reliability of the QloraLinear layer. These tests cover initialization, forward pass calculations, dropout effects, and more.</p> <p>To run the tests, make sure you have <code>pytest</code> installed:</p> <pre><code>pip install pytest\n</code></pre> <p>Then, navigate to the test directory and run:</p> <pre><code>pytest tests/quant/qlora.py\n</code></pre> <p>This will execute all the provided tests, ensuring the layer functions as expected.</p>"},{"location":"zeta/quant/qlora/#conclusion","title":"Conclusion","text":"<p>The QloraLinear layer is a powerful addition to the deep learning toolkit. It combines traditional linear transformations with quantum-inspired principles to enhance the expressiveness of neural network layers. Whether you're building a simple feed-forward network or a complex deep learning model, QloraLinear can provide a significant boost in model performance.</p> <p>Note: This documentation provides a comprehensive guide to the QloraLinear layer. Always refer to the official documentation for the most up-to-date and detailed information.</p>"},{"location":"zeta/quant/quik/","title":"QUIK: Quantized Integers with Kernels (QUIK) for Efficient Deep Learning","text":""},{"location":"zeta/quant/quik/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Architecture<ol> <li>QUIK Layer</li> <li>Quantization</li> <li>Dequantization</li> </ol> </li> <li>Installation</li> <li>Usage<ol> <li>Initializing QUIK Layer</li> <li>Quantizing Data</li> <li>Dequantizing Data</li> <li>Forward Pass</li> </ol> </li> <li>Examples<ol> <li>Example 1: Initializing QUIK Layer</li> <li>Example 2: Quantizing Data</li> <li>Example 3: Dequantizing Data</li> <li>Example 4: Forward Pass</li> </ol> </li> <li>Additional Information</li> <li>Conclusion</li> </ol>"},{"location":"zeta/quant/quik/#1-introduction","title":"1. Introduction","text":"<p>QUIK (Quantized Integers with Kernels) is a PyTorch-based module designed to enable efficient deep learning by leveraging quantization techniques. This module provides a custom QUIK layer that performs quantization and dequantization operations on input data. It's particularly useful when memory and computation resources are constrained, making it suitable for deployment on edge devices.</p> <p>The key features of QUIK include: - Quantization of input data to a reduced bit-width. - Efficient kernel operations on quantized data. - Dequantization of results back to floating-point values.</p> <p>In this documentation, we'll explore the architecture of the QUIK module, how to install it, and provide detailed examples of its usage.</p>"},{"location":"zeta/quant/quik/#2-architecture","title":"2. Architecture","text":"<p>The QUIK module consists of a custom QUIK layer, which performs quantization, efficient kernel operations, and dequantization. Let's dive into the architecture in detail.</p>"},{"location":"zeta/quant/quik/#21-quik-layer","title":"2.1. QUIK Layer","text":"<p>The QUIK layer is the core component of the module. It takes input data and performs quantization, efficient weighted summation, and dequantization.</p>"},{"location":"zeta/quant/quik/#22-quantization","title":"2.2. Quantization","text":"<p>Quantization is the process of converting input data to a reduced bit-width representation. In the case of QUIK, it quantizes the input tensor to an integer representation, typically using fewer bits than a standard floating-point representation.</p>"},{"location":"zeta/quant/quik/#23-dequantization","title":"2.3. Dequantization","text":"<p>Dequantization is the inverse process of quantization. It takes quantized data and converts it back to a floating-point representation. This operation ensures that the final output is in a format suitable for further computations or analysis.</p>"},{"location":"zeta/quant/quik/#3-installation","title":"3. Installation","text":"<p>You can install the QUIK module via pip. Open your terminal and run the following command:</p> <pre><code>pip install quik\n</code></pre>"},{"location":"zeta/quant/quik/#4-usage","title":"4. Usage","text":"<p>Let's explore how to use the QUIK module step by step.</p>"},{"location":"zeta/quant/quik/#41-initializing-quik-layer","title":"4.1. Initializing QUIK Layer","text":"<p>First, you need to initialize the QUIK layer by specifying the number of input and output features. Optionally, you can enable bias terms. Here's how to do it:</p> <pre><code>import torch\nimport torch.nn as nn\n\n# Initialize the QUIK module\nquik = QUIK(in_features=784, out_features=10, bias=True)\n</code></pre>"},{"location":"zeta/quant/quik/#42-quantizing-data","title":"4.2. Quantizing Data","text":"<p>You can quantize your input data using the <code>quantize</code> method of the QUIK layer. This method returns the quantized data, zero-point, and scale factor.</p> <pre><code># Create some dummy data, e.g., simulating a batch of MNIST images\ndata = torch.randn(10, 784)\n\n# Quantize the data\nquantized_data, zero_point, scale_factor = quik.quantize(data)\n</code></pre>"},{"location":"zeta/quant/quik/#43-dequantizing-data","title":"4.3. Dequantizing Data","text":"<p>To dequantize data, use the <code>dequantize</code> method of the QUIK layer. This method requires the quantized data, zero-point, scale factor, and an additional scale factor for weights.</p> <pre><code># Dequantize the quantized data\ndequantized_data = quik.dequantize(\n    quantized_data, zero_point, scale_factor, scale_weight\n)\n</code></pre>"},{"location":"zeta/quant/quik/#44-forward-pass","title":"4.4. Forward Pass","text":"<p>Now, you can run the quantized data through the QUIK layer to perform the forward pass. This will quantize the data, apply the weight operation efficiently, and dequantize the result.</p> <pre><code># Forward pass\noutput = quik(quantized_data)\n</code></pre>"},{"location":"zeta/quant/quik/#5-examples","title":"5. Examples","text":"<p>Let's go through some examples to illustrate how to use the QUIK module effectively.</p>"},{"location":"zeta/quant/quik/#51-example-1-initializing-quik-layer","title":"5.1. Example 1: Initializing QUIK Layer","text":"<p>In this example, we'll initialize the QUIK layer.</p> <pre><code>import torch\n\nfrom zeta.quant import QUIK\n\n# Initialize the QUIK module\nquik = QUIK(in_features=784, out_features=10)\n</code></pre>"},{"location":"zeta/quant/quik/#52-example-2-quantizing-data","title":"5.2. Example 2: Quantizing Data","text":"<p>Now, we'll quantize some input data.</p> <pre><code># Create some dummy data, e.g., simulating a batch of MNIST images\ndata = torch.randn(10, 784)\n\n# Quantize the data\nquantized_data, zero_point, scale_factor = quik.quantize(data)\n</code></pre>"},{"location":"zeta/quant/quik/#53-example-3-dequantizing-data","title":"5.3. Example 3: Dequantizing Data","text":"<p>In this example, we'll dequantize the quantized data.</p> <pre><code># Dequantize the quantized data\ndequantized_data = quik.dequantize(\n    quantized_data, zero_point, scale_factor, scale_weight\n)\n</code></pre>"},{"location":"zeta/quant/quik/#54-example-4-forward-pass","title":"5.4. Example 4: Forward Pass","text":"<p>Finally, we'll perform a forward pass using the QUIK layer.</p> <pre><code># Forward pass\noutput = quik(quantized_data)\n</code></pre>"},{"location":"zeta/quant/quik/#6-additional-information","title":"6. Additional Information","text":"<ul> <li>Performance: QUIK is designed for efficient deep learning, especially in resource-constrained environments.</li> <li>Quantization Range: By default, QUIK assumes 4-bit quantization, so the range is [-8, 7].</li> <li>Training: QUIK is primarily intended for inference. It is not designed for training.</li> <li>Customization: You can customize the quantization</li> </ul> <p>range, bit-width, and other parameters as needed.</p>"},{"location":"zeta/quant/quik/#7-conclusion","title":"7. Conclusion","text":"<p>The QUIK module offers a straightforward way to apply quantization techniques to your deep learning models, making them more memory and computationally efficient. By following the guidelines and examples in this documentation, you can effectively integrate QUIK into your projects, especially when deploying models to edge devices or resource-constrained environments.</p> <p>For further information and detailed function descriptions, please refer to the QUIK module's official documentation.</p>"},{"location":"zeta/rl/dpo/","title":"DPO","text":""},{"location":"zeta/rl/dpo/#documentation-for-deep-policy-optimization-dpo-module","title":"Documentation for Deep Policy Optimization (DPO) Module","text":""},{"location":"zeta/rl/dpo/#overview","title":"Overview","text":"<p>Deep Policy Optimization (DPO) is a PyTorch module designed for optimizing policies in decision-making models. It utilizes a reference model and a trainable policy model to compute loss values that guide the learning process.</p>"},{"location":"zeta/rl/dpo/#class-definition","title":"Class Definition","text":"<pre><code>class DPO(nn.Module):\n    def __init__(self, model: nn.Module, *, beta: float = 0.1): ...\n</code></pre>"},{"location":"zeta/rl/dpo/#arguments","title":"Arguments","text":"Argument Type Description Default <code>model</code> <code>nn.Module</code> The policy model to be optimized. - <code>beta</code> <code>float</code> A parameter controlling the influence of log-ratios in loss. <code>0.1</code>"},{"location":"zeta/rl/dpo/#methods","title":"Methods","text":""},{"location":"zeta/rl/dpo/#forwardpreferred_seq-tensor-unpreferred_seq-tensor-tensor","title":"<code>forward(preferred_seq: Tensor, unpreferred_seq: Tensor) -&gt; Tensor</code>","text":"<p>Computes the loss based on the difference in log probabilities between preferred and unpreferred sequences.</p>"},{"location":"zeta/rl/dpo/#arguments_1","title":"Arguments","text":"Argument Type Description <code>preferred_seq</code> <code>Tensor</code> The sequence of actions/decisions preferred. <code>unpreferred_seq</code> <code>Tensor</code> The sequence of actions/decisions unpreferred."},{"location":"zeta/rl/dpo/#returns","title":"Returns","text":"<p>A <code>torch.Tensor</code> representing the computed loss.</p>"},{"location":"zeta/rl/dpo/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/rl/dpo/#example-1-basic-setup-and-usage","title":"Example 1: Basic Setup and Usage","text":"<pre><code>import torch\nfrom torch import nn\n\nfrom zeta.rl import DPO\n\n\n# Define a simple policy model\nclass PolicyModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\ninput_dim = 10\noutput_dim = 5\npolicy_model = PolicyModel(input_dim, output_dim)\n\n# Initialize DPO with the policy model\ndpo_model = DPO(model=policy_model, beta=0.1)\n\n# Sample preferred and unpreferred sequences\npreferred_seq = torch.randn(1, 10, 10)\nunpreferred_seq = torch.randn(1, 10, 10)\n\n# Compute loss\nloss = dpo_model(preferred_seq, unpreferred_seq)\nprint(loss)\n</code></pre>"},{"location":"zeta/rl/dpo/#example-2-integrating-with-an-optimizer","title":"Example 2: Integrating with an Optimizer","text":"<pre><code>optimizer = torch.optim.Adam(dpo_model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(100):\n    optimizer.zero_grad()\n    loss = dpo_model(preferred_seq, unpreferred_seq)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"zeta/rl/dpo/#notes","title":"Notes","text":"<ul> <li>Ensure that <code>preferred_seq</code> and <code>unpreferred_seq</code> have the same shape and are compatible with the input dimensions of the policy model.</li> <li><code>beta</code> is a hyperparameter and may require tuning for different applications.</li> <li>The policy model should be structured to output logits compatible with the sequences being evaluated.</li> </ul> <p>This documentation provides a comprehensive guide to utilizing the DPO module in various decision-making contexts. The examples demonstrate basic usage and integration within a training loop.</p>"},{"location":"zeta/structs/autoregressivewrapper/","title":"AutoregressiveWrapper Class","text":"<p>In the following documentation, you'll learn all about the AutoregressiveWrapper class of zeta.structs module. As autoregressive models are sequence models used to predict subsequent data points in sequence data, this class provides a wrapper that can be used to wrap any PyTorch nn.Module to make them autoregressive model compliant.</p>"},{"location":"zeta/structs/autoregressivewrapper/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Class Definition</li> <li>Parameters</li> <li>Methods</li> <li>Examples</li> <li>Conclusion</li> </ol>"},{"location":"zeta/structs/autoregressivewrapper/#1-class-definition","title":"1. Class Definition","text":"<p>AutoregressiveWrapper is a Python class that inherits from PyTorch's nn.Module and applies an autoregressive mask on the input sequence to any module that takes sequence input. This wrapper ensures the output sequence obeys a property inherent to causal or autoregressive models \u2013 the prediction at each position in the sequence is based only on preceding positions.</p> <pre><code>class AutoregressiveWrapper(nn.Module):\n</code></pre>"},{"location":"zeta/structs/autoregressivewrapper/#2-parameters","title":"2. Parameters","text":"<p>The parameters accepted by AutoregressiveWrapper are:</p> Name Type Description Default net nn.Module A PyTorch module that takes a sequence of tokens and outputs a sequence of logits. N/A ignore_index int The index to ignore in the target sequence when calculating the loss. -100 pad_value int The value to pad the target sequence with. 0 mask_prob float The probability of masking a token in the input sequence. 0.0 speculative bool Whether to use speculative decoding or not. False"},{"location":"zeta/structs/autoregressivewrapper/#3-methods","title":"3. Methods","text":"<p>The methods provided by AutoregressiveWrapper are:</p>"},{"location":"zeta/structs/autoregressivewrapper/#31-init","title":"3.1 init()","text":"<p>The <code>__init__()</code> method initializes an instance of the AutoregressiveWrapper class.</p> <pre><code>def __init__(self, net, ignore_index=-100, pad_value=0, mask_prob=0.0, speculative=False)\n</code></pre>"},{"location":"zeta/structs/autoregressivewrapper/#32-forward","title":"3.2 forward()","text":"<p>The <code>forward()</code> method performs forward pass of the autoregressive wrapper.</p> <pre><code>def forward(self, x, return_loss=True, **kwargs)\n</code></pre> <p>This method returns logits produced by the wrapped module. If <code>return_loss</code> is <code>True</code>, it also returns the loss calculated using target sequence and outputs of the wrapped module.</p>"},{"location":"zeta/structs/autoregressivewrapper/#33-generate","title":"3.3 generate()","text":"<p>The <code>generate()</code> method generates a sequence of tokens from the model.</p> <pre><code>def generate(self, start_tokens, seq_len, eos_token=None, strategy=\"temperature\", temperature=1.0, filter_logits_fn=top_k, filter_thres=0.9, min_p_pow=2.0, min_p_ratio=0.02, gamma=5, **kwargs)\n</code></pre> <p>You can control the sequence generation with various parameters like <code>strategy</code>, <code>temperature</code>, <code>filter_logits_fn</code> etc.</p>"},{"location":"zeta/structs/autoregressivewrapper/#34-generate_n_solutions","title":"3.4 generate_n_solutions()","text":"<p>The <code>generate_n_solutions()</code> method generates n solutions from the model.</p> <p><pre><code>def generate_n_solutions(self, start_tokens, n, seqlen, **kwargs)\n</code></pre> This method is particularly useful for generating multiple forecasted sequence paths.</p>"},{"location":"zeta/structs/autoregressivewrapper/#35-evaluate_and_select_best_solution","title":"3.5 evaluate_and_select_best_solution()","text":"<p>The <code>evaluate_and_select_best_solution()</code> method evaluates the solutions based on a reward model and returns the best one.</p> <pre><code>def evaluate_and_select_best_solution(self, solutions, reward_model)\n</code></pre>"},{"location":"zeta/structs/autoregressivewrapper/#4-examples","title":"4. Examples","text":"<p>To help you better understand the usage of this class, here are some examples.</p> <p>First example demonstrates how to instantiate the AutoregressiveWrapper over an existing nn.module (nn.Linear in this case).</p> <pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.structs import AutoregressiveWrapper\n\nnet = nn.Linear(10, 10)\nnet = AutoregressiveWrapper(net)\nx = torch.randn(1, 10)\nlogits, loss = net(x, return_loss=True)\nprint(logits.shape)\n# Output: torch.Size([1, 10, 10]) # (batch_size, seq_len, vocab_size)\n</code></pre> <p>The second example demonstrates the usage of generate method to generate a sequence with the model.</p> <p><pre><code>start_tokens = torch.tensor([1, 2, 3])\ngenerated_sequence = net.generate(start_tokens, seq_len=10)\n</code></pre> This generated_sequence represents the next 10 steps in the sequence (based on the first 3 steps provided as start_tokens).</p> <p>The third example shows generating multiple solutions and selecting the best one.</p> <p><pre><code>solutions = net.generate_n_solutions(start_tokens, n=5, seqlen=10)\nbest_solution = net.evaluate_and_select_best_solution(\n    solutions, reward_model=lambda x: -x.sum()\n)\n</code></pre> In the example above, the reward model simply returns the negative sum of the sequence, and the solution with lowest sum is selected as the best solution.</p>"},{"location":"zeta/structs/autoregressivewrapper/#5-conclusion","title":"5. Conclusion","text":"<p>In this documentation, you have learned about the AutoregressiveWrapper class of zeta.structs. You should now be more comfortable and confident in leveraging this class in your neural network architectures to realize autoregressive transformation.</p>"},{"location":"zeta/structs/encoder/","title":"Class Name: Encoder","text":"<p>The <code>Encoder</code> class is a subclass of the AttentionLayers class used largely in transformer models for natural language processing tasks. It is intended to read and process inputs without an enforced causality - meaning it does not maintain an implied sequence or order in the data it processes. As such, the Encoder can utilize context from all directions and all inputs are independently centric in attention operations.</p>"},{"location":"zeta/structs/encoder/#class-signature","title":"Class Signature","text":"<pre><code>class Encoder(AttentionLayers):\n    def __init__(self, **kwargs):\n</code></pre>"},{"location":"zeta/structs/encoder/#now-let-us-dive-deeper-into-the-class-functionalities-and-making-use-of-it","title":"Now let us dive deeper into the Class functionalities and making use of it.","text":""},{"location":"zeta/structs/encoder/#parameters","title":"Parameters","text":"Parameter Type Description <code>kwargs</code> *args arbitrary keyword arguments passed for initialization"},{"location":"zeta/structs/encoder/#note","title":"Note","text":"<p>\"Causal\" should not be included in <code>kwargs</code>, as causality is not applicable for an Encoder.</p> <p><code>super().__init__(causal=False, **kwargs)</code> is used to pass all arguments to the parent class i.e., AttentionLayer, where <code>causal=False</code> - ensuring that the Encoder does not consider causality in the attention/subsequent operations.</p>"},{"location":"zeta/structs/encoder/#example-of-implementing-your-own-custom-encoder","title":"Example of Implementing your own custom Encoder:","text":"<p>Let's take an example of creating a basic encoder for a Transformer model -</p> <p><pre><code>import torch.nn as nn\n\nfrom zeta.structs import AttentionLayers\n\n\nclass MyEncoder(AttentionLayers):\n    def __init__(self, d_model, nhead, num_layers):\n        super().__init__(d_model=d_model, nhead=nhead, num_layers=num_layers)\n        self.linear = nn.Linear(d_model, d_model)\n\n    def forward(self, x):\n        x = super().forward(x)\n        return self.linear(x)\n</code></pre> We built a custom encoder by extending the AttentionLayers, added a linear layer after the attention operations.</p>"},{"location":"zeta/structs/encoder/#example-usage","title":"Example Usage:","text":"<p>Firstly, let's initialize the model: <pre><code>model = MyEncoder(d_model=512, nhead=8, num_layers=6)\n</code></pre> The model is initialized with the dimensions of model <code>d_model=512</code>, number of heads <code>nhead=8</code>, and the number of layers <code>num_layers=6</code>.</p> <p>Now, let's define some dummy input data and pass it through the model:</p> <p><pre><code>import torch\n\nx = torch.randn(10, 32, 512)  # (sequence_length, batch_size, d_model)\noutput = model(x)  # forward pass\nprint(output.shape)  # torch.Size([10, 32, 512])\n</code></pre> The method <code>forward()</code> computes the forward pass of our custom encoder model.</p>"},{"location":"zeta/structs/encoder/#note_1","title":"Note","text":"<p>Remember, <code>Encoder</code> can be viewed as a wrapping layer around <code>AttentionLayers</code>, that ensures non-causal behaviour for the encoder in a Transformer. Hence, it is used typically for operations where the entire sequence is available for consideration - like in a Transformer's encoder, while predicting masked tokens based on surrounding context etc. </p> <p>As seen in the example, it is easy to extend the <code>Encoder</code> class and add additional layers or functionality, if required, depending upon specific use-cases.  </p>"},{"location":"zeta/structs/encoder/#disclaimer","title":"Disclaimer:","text":"<p>The class could change since the provided code is a snippet and might not represent the final form the <code>Encoder</code> class would take. This documentation is aimed at guiding understanding of the basic idea, intent, usage and extension of the <code>Encoder</code> class based on the short provided code snippet. For exact details, refer to the actual implementation in its entirety.</p>"},{"location":"zeta/structs/encoderdecoder/","title":"Module/Class Name: EncoderDecoder","text":"<p>The <code>EncoderDecoder</code> class is a module that brings together an encoder and a decoder for sequence-to-sequence tasks. This design helps facilitate the transformation of an input sequence to an output sequence, with each sequence potentially being of a different length. </p> <p>Applications of sequence-to-sequence tasks include machine translation, speech recognition, and text summarization.</p> <p></p> <p> <p>This EncoderDecoder class requires an argparse.Namespace object as well as optional Tensor objects for the encoder embed tokens and positions and the decoder embed tokens and positions.</p>"},{"location":"zeta/structs/encoderdecoder/#class-definition","title":"Class Definition","text":"<pre><code>class EncoderDecoder(nn.Module):\n    \"\"\"\n    A module that combines an encoder and a decoder for sequence-to-sequence tasks.\n\n    Args:\n        args (argparse.Namespace): The arguments passed to the module.\n        encoder_embed_tokens (torch.Tensor, optional): The input embeddings for the encoder. Defaults to None.\n        encoder_embed_positions (torch.Tensor, optional): The positions of the encoder input embeddings. Defaults to None.\n        decoder_embed_tokens (torch.Tensor, optional): The input embeddings for the decoder. Defaults to None.\n        decoder_embed_positions (torch.Tensor, optional): The positions of the decoder input embeddings. Defaults to None.\n        output_projection (torch.Tensor, optional): The projection layer for the decoder output. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Attributes:\n        args (argparse.Namespace): The arguments passed to the module.\n        encoder (Encoder): The encoder module.\n        decoder (Decoder): The decoder module.\n    \"\"\"\n\n\n...\n</code></pre> <p>This class has two major attributes: <code>encoder</code> and <code>decoder</code>. These attributes store the encoder and decoder modules used in sequence-to-sequence tasks.</p>"},{"location":"zeta/structs/encoderdecoder/#initialization-of-encoderdecoder","title":"Initialization of EncoderDecoder","text":"<p>The <code>EncoderDecoder</code> class is initialized as follows:</p> <pre><code>def __init__(\n    self,\n    args,\n    encoder_embed_tokens=None,\n    encoder_embed_positions=None,\n    decoder_embed_tokens=None,\n    decoder_embed_positions=None,\n    output_projection=None,\n    **kwargs,\n):\n</code></pre>"},{"location":"zeta/structs/encoderdecoder/#init-parameters","title":"Init Parameters","text":"<p>The EncoderDecoder class takes the following parameters during its initialization:</p> Parameter Type Description args argparse.Namespace The namespace containing all the arguments needed to initialize the module. encoder_embed_tokens torch.Tensor (optional) The input embeddings for the encoder. encoder_embed_positions torch.Tensor (optional) The position indices for the encoder input embeddings. decoder_embed_tokens torch.Tensor (optional) The input embeddings for the decoder. decoder_embed_positions torch.Tensor (optional) The position indices for the decoder input embeddings. output_projection torch.Tensor (optional) The projection matrix for the decoder output. **kwargs dict A dictionary of additional keyword arguments. <p>During initialization, the <code>EncoderDecoder</code> class checks if all embeddings should be shared between the encoder and decoder. If not, it initializes the encoder and decoder with their respective embed tokens and position indices.</p>"},{"location":"zeta/structs/encoderdecoder/#forward-method-definition","title":"Forward Method Definition","text":"<p><pre><code>def forward(\n    self,\n    src_tokens,\n    prev_output_tokens,\n    return_all_hiddens=False,\n    features_only=False,\n    **kwargs,\n):\n</code></pre> This method executes the forward pass of the module.</p>"},{"location":"zeta/structs/encoderdecoder/#forward-method-parameters","title":"Forward Method Parameters","text":"Parameter Type Description src_tokens torch.Tensor The source tokens. prev_output_tokens torch.Tensor The previous output tokens. return_all_hiddens bool (optional) Whether to return all hidden states. Default is <code>False</code>. features_only bool (optional) Whether to return only the features. Default is <code>False</code>. **kwargs dict A dictionary of additional keyword arguments."},{"location":"zeta/structs/encoderdecoder/#usage-example","title":"Usage Example:","text":"<p><pre><code># Imports\nimport torch\n\nfrom zeta.structs import Decoder, Encoder, EncoderDecoder\n\n# Arguments\nargs = argparse.Namespace(share_all_embeddings=True)\nsrc_tokens = torch.tensor([1, 2, 3])\nprev_output_tokens = torch.tensor([0, 1, 2])\n\n# Define EncoderDecoder\nenc_dec = EncoderDecoder(args)\n\n# Forward Pass\ndecoder_out = enc_dec(src_tokens, prev_output_tokens)\n</code></pre> This returns the output of the decoder module. </p>"},{"location":"zeta/structs/encoderdecoder/#note","title":"Note:","text":"<ul> <li><code>Encoder</code> and <code>Decoder</code> are assumed to be modules input to the <code>EncoderDecoder</code> class.</li> <li>Ensure that your input tensors are of the right shape and type (LongTensor for token indices and FloatTensor for embedding vectors).</li> <li>When training a model using the <code>EncoderDecoder</code> class, make sure to use the appropriate loss function that matches your specific task (e.g., CrossEntropyLoss for classification tasks).</li> <li>The argparse.Namespace class is used to hold the arguments needed by the module. It's a simple class that allows access to undefined attributes.</li> </ul>"},{"location":"zeta/structs/hierarchicalblock/","title":"Module/Class Name: HierarchicalBlock","text":""},{"location":"zeta/structs/hierarchicalblock/#overview","title":"Overview","text":"<p>The HierarchicalBlock class in the pyTorch library is an implementation of the hierarchical token-wise attention mechanism used in some transformer models. Hierarchical token-wise attention allows a model to selectively focus on portions of the input sequence, thus the model can efficiently learn longer-range dependencies in the input data. </p> <p>It uses \"nn.Module\", which is a base class for all neural network modules from the PyTorch library. HierarchicalBlock provides the functionality to handle the hierarchical structure and neural network layers within the block.</p> <p>It is recommended to use this class, rather than handle the hierarchical structure of a neural network manually to ensure the hierarchical structure has an ordered representation.</p>"},{"location":"zeta/structs/hierarchicalblock/#purpose","title":"Purpose","text":"<p>The HierarchicalBlock class allows efficient modelling of attention in transformer models, enabling the model to learn long-range dependencies in the input data. This is especially useful for large-scale Natural Language Processing tasks like language translation and text summarization where long sequences of text need to be processed.</p> <p>The design of HierarchicalBlock ensures appropriate assignment and registration of submodules, which converts the parameters appropriately when methods like :meth:<code>to</code> etc. are called. </p> <p>It has the <code>:ivar training</code> variable to represent whether the module is in training or evaluation mode.</p> <p>The HierarchicalBlock class is vital for building complex models and ensuring submodules are correctly registered and parameters updated.</p>"},{"location":"zeta/structs/hierarchicalblock/#hierarchicalblock-class-definition","title":"HierarchicalBlock Class Definition","text":"<pre><code>class HierarchicalBlock(nn.Module):\n    def __init__(self, dim, dim_head=64, heads=8, window_size=None, compress_factor=1, stride=1, ff_mult=4):\n    ...\n</code></pre>"},{"location":"zeta/structs/hierarchicalblock/#class-parameters","title":"Class Parameters","text":"Parameter Type Description dim int Defines the dimension of the model. dim_head int Determines the head dimensions. Default value is 64. heads int Determines the number of parallel attention heads. Default value is 8. window_size int or NoneType If a value exists, it specifies the size of the window for local Multihead Attention (LocalMHA). If no value exists, a standard Attention operation will be performed. Default is None. compress_factor int Factor by which to compress inputs. Must be a power of two. Default is 1 (no compression). stride int Stride size for the attention operation. Default is 1. ff_mult int Multiplier for the dimension of the feed forward network hidden layer. This is used to expand the inner hidden layer of the model from the input sequence."},{"location":"zeta/structs/hierarchicalblock/#methods","title":"Methods","text":""},{"location":"zeta/structs/hierarchicalblock/#forward","title":"forward","text":"<pre><code>def forward(self, x): ...\n</code></pre>"},{"location":"zeta/structs/hierarchicalblock/#method-parameters-and-returns","title":"Method Parameters and returns","text":"Parameter Type Description x Tensor or array-like The input tensor to the HierarchicalBlock instance. <p>Returns:</p> Return Variables Type Description x Tensor or array-like Returns the tensor after it has been processed through the 'attn' (attention) and 'ff' (feed forward) operations, and optionally compressed and padded. It returns a tensor with the same batch size but with a different sequence length, depending on the size of the window used in 'attn' and the settings of 'compress_factor' and 'stride'."},{"location":"zeta/structs/hierarchicalblock/#usage-example","title":"Usage Example","text":"<p>Import necessary modules and define an input sequence:</p> <p><pre><code>import torch\nimport torch.nn as nn\nfrom utils import exists, is_power_of_two, pad_seq_to_multiple, rearrange, token_shift\n\nsequence_length = 10\nbatch_size = 32\ndim = 512\n\nx = torch.randn(batch_size, sequence_length, dim)\n\n# Define an instance of HierarchicalBlock\nhierarchical_block = HierarchicalBlock(dim=dim)\n\n# Apply the forward method of the hierarchical_block instance to x\nout = hierarchical_block.forward(x)\n</code></pre> In the example above, we first import the necessary modules. We initialize a tensor <code>x</code> with random numbers, having batch_size of 32, sequence_length of 10, and dimension of 512. We define an instance of HierarchicalBlock where <code>dim = 512</code>. We then pass the tensor <code>x</code> to the forward method to get the output tensor.</p>"},{"location":"zeta/structs/localtransformer/","title":"LocalTransformer","text":""},{"location":"zeta/structs/localtransformer/#introduction","title":"Introduction","text":"<p>The <code>LocalTransformer</code> is a powerful machine learning module that implements a sequence-to-sequence model based on the local self-attention module part of the Transformer architecture. This module is specifically designed for applications where sequences of tokens are transformed, such as natural language processing tasks. </p> <p>At a high level, a transformer takes in a sequence of tokens and outputs a new sequence of tokens. Local transformer creates a module where attention is based on a limited window of the input sequence which can be beneficial for both efficiency and model performance in certain cases.</p>"},{"location":"zeta/structs/localtransformer/#definitions-and-key-concepts","title":"Definitions and Key Concepts","text":"<ul> <li>tokens: Individual elements of a sequence, typically words in a sentence for language tasks.</li> <li>sequence length: The number of tokens in each sequence.</li> <li>embeddings: Vector representations of tokens, which allow them to be processed by the network.</li> <li>attention: A mechanism in transformers that allows the model to focus on different parts of the input when producing each part of the output. </li> </ul>"},{"location":"zeta/structs/localtransformer/#class-definition","title":"Class Definition","text":"<p>The class signature for the <code>LocalTransformer</code> is as follows:</p> <pre><code>class LocalTransformer(nn.Module):\n</code></pre>"},{"location":"zeta/structs/localtransformer/#arguments","title":"Arguments","text":"Argument Type Description Default num_tokens int The number of tokens in the input vocabulary. - max_seq_len int The maximum sequence length. - dim int The dimensionality of the token and positional embeddings. - depth int The number of transformer layers. - causal bool Whether to use causal attention or not. True local_attn_window_size int The size of the local attention window. 512 dim_head int The dimensionality of each attention head. 64 heads int The number of attention heads. 8 ff_mult int The multiplier for the feedforward network dimension. 4 attn_dropout float The dropout rate for attention layers. 0.0 ff_dropout float The dropout rate for feedforward layers. 0.0 ignore_index int The index to ignore during loss calculation. -1 use_xpos bool Whether to use positional embeddings based on xpos. False xpos_scale_base None The base value for scaling xpos positional embeddings. None use_dynamic_pos_bias bool Whether to use dynamic positional bias or not. False"},{"location":"zeta/structs/localtransformer/#understanding-arguments","title":"Understanding Arguments","text":"<ul> <li>num_tokens: This determines the size of the vocabulary. This is set according to the dataset and cannot be modified post initialization.</li> <li>max_seq_len: This sets the maximum sequence length. As the model would need to create key, query and values for each token, increasing this value can lead to a significant increase in memory usage.</li> <li>dim: This is the size of the model's embeddings. The higher this value, the more information each embedding can store. However, similarly to max_seq_len, this can also drastically increase memory usage. </li> <li>depth: This corresponds to the number of layers the model will have. Deeper models can potentially have better representative power, but it can also lead to overfitting and longer training times.</li> </ul>"},{"location":"zeta/structs/localtransformer/#attributes","title":"Attributes","text":"Attribute Description token_emb Embedding layer for token embeddings. pos_emb Embedding layer for positional embeddings. max_seq_len The maximum sequence length. layers List of transformer layers. local_attn_window_size The size of the local attention window. dynamic_pos_bias Dynamic positional bias layer, if enabled. ignore_index The index to ignore during loss calculation. to_logits Sequential layer for converting transformer output to logits."},{"location":"zeta/structs/localtransformer/#example","title":"Example","text":"<p>The following example demonstrates how to initialize and use the <code>LocalTransformer</code> class for a simple task:</p> <pre><code>import torch\n\nfrom zeta.structs import LocalTransformer\n\n# Define a LocalTransformer\nmodel = LocalTransformer(num_tokens=500, max_seq_len=10, dim=32, depth=2)\n\n# Define a simple sequence\nsequence = torch.randint(0, 500, (1, 10))\n\n# Forward pass\noutput = model(sequence)\n</code></pre> <p>This will create a <code>LocalTransformer</code> model with a vocabulary of size 500, a maximum sequence length of 10, an embedding dimension of 32, and 2 transformer layers. It then performs a forward pass of the sequence through the model, outputting the transformed sequence.</p>"},{"location":"zeta/structs/localtransformer/#conclusion","title":"Conclusion","text":"<p>The <code>LocalTransformer</code> module is a highly flexible and modular implementation of the transformer architecture, equipped with local attention. Given its configurable nature, it is amenable to various NLP and sequence-to-sequence modeling tasks. An understanding of its input arguments, attributes, and overall design is essential to leverage its full potential. </p> <p>For any additional details or queries, please refer to external resources or related papers for an in-depth understanding of Transformers in Machine Learning.</p>"},{"location":"zeta/structs/paralleltransformerblock/","title":"Documentation of ParallelTransformerBlock","text":""},{"location":"zeta/structs/paralleltransformerblock/#introduction","title":"Introduction","text":"<p>The <code>ParallelTransformerBlock</code> is a neural network module that is a subclass of the <code>torch.nn.Module</code> class from PyTorch. It's specifically designed to create a transformer block that can process inputs in parallel efficiently making it faster.</p> <p>The transformer block performs the layered processes of layer normalization, attention inquiry, key assignment, value assessment, feedforwarding, handling of multi-head attention, and rotary embedding for the speedup and efficiency of model operations.</p>"},{"location":"zeta/structs/paralleltransformerblock/#module-structure","title":"Module Structure","text":"<p>Here's the class signature and structure:</p> <pre><code>class ParallelTransformerBlock(nn.Module):\n    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n        super().__init__()\n        self.norm = LayerNorm(dim)\n\n        attn_inner_dim = dim_head * heads\n        ff_inner_dim = dim * ff_mult\n        self.fused_dims = (\n            attn_inner_dim,\n            dim_head,\n            dim_head,\n            (ff_inner_dim * 2),\n        )\n\n        self.heads = heads\n        self.scale = dim_head**-0.5\n        self.rotary_emb = RotaryEmbedding(dim_head)\n\n        self.fused_attn_ff_proj = nn.Linear(dim, sum(self.fused_dims), bias=False)\n        self.attn_out = nn.Linear(attn_inner_dim, dim, bias=False)\n\n        self.ff_out = nn.Sequential(SwiGLU(), nn.Linear(ff_inner_dim, dim, bias=False))\n\n        self.register_buffer(\"mask\", None, persistent=False)\n        self.register_buffer(\"pos_emb\", None, persistent=False)\n</code></pre>"},{"location":"zeta/structs/paralleltransformerblock/#initself-dim-dim_head64-heads8-ff_mult4","title":"init(self, dim, dim_head=64, heads=8, ff_mult=4)","text":"<p>The <code>__init__</code> function initializes the <code>ParallelTransformerBlock</code> with the input dimensions, the number of attention heads, etc.</p>"},{"location":"zeta/structs/paralleltransformerblock/#parameters","title":"Parameters:","text":"Name Type Default Should Description <code>dim</code> int - The feature dimension of the input. <code>dim_head</code> int - Feature dimension of each head in multi-head attention. <code>heads</code> int 8 The number of attention heads. <code>ff_mult</code> int 4 Multiplier for dimensions in the feed-forward inner layer."},{"location":"zeta/structs/paralleltransformerblock/#forwardself-x","title":"forward(self, x)","text":"<p>The <code>forward</code> function applies the transformations of the <code>ParallelTransformerBlock</code> to an input tensor <code>x</code>.</p>"},{"location":"zeta/structs/paralleltransformerblock/#parameters_1","title":"Parameters:","text":"Name Type Default Should Description <code>x</code> Tensor - The input tensor to pass through the transformer block."},{"location":"zeta/structs/paralleltransformerblock/#returns","title":"Returns:","text":"Type Description Tensor The transformed output tensor."},{"location":"zeta/structs/paralleltransformerblock/#usage-examples","title":"Usage Examples","text":"<p>Here's an example of how you would use the <code>ParallelTransformerBlock</code>:</p> <pre><code># Import necessary modules\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange, Reduce\nfrom torch.nn import functional as F\n\n# Define features and inputs\ndim = 16\ntorch.manual_seed(24)\nx = torch.randn(1, 10, dim)\n\n# Create a model instance\nmodel = ParallelTransformerBlock(dim)\n\n# Run input through model\noutput = model(x)\n\nprint(\"Input shape: \", x.shape)\nprint(\"Output shape: \", output.shape)\n</code></pre> <p>The default values for <code>dim_head</code>, <code>heads</code>, and <code>ff_mult</code> can be overridden as follows while instantiating the <code>ParallelTransformerBlock</code> class:</p> <pre><code>model = ParallelTransformerBlock(dim, dim_head=32, heads=4, ff_mult=2)\n</code></pre>"},{"location":"zeta/structs/paralleltransformerblock/#additional-notes","title":"Additional Notes","text":"<p>The <code>ParallelTransformerBlock</code> uses the <code>RotaryEmbedding</code>, <code>SwiGLU</code>, <code>LayerNorm</code>, <code>apply_rotary_pos_emb</code> functions which are not explicitly defined in this documentation. Those are additional helper functions/classes you would need to define in your environment or import from your existing codebase.</p>"},{"location":"zeta/structs/simpletransformer/","title":"Documentation for SimpleTransformer Class","text":""},{"location":"zeta/structs/simpletransformer/#introduction","title":"Introduction","text":"<p>This class provides a concise and efficient implementation for the Transformer model design, designated as <code>SimpleTransformer</code> class. The <code>SimpleTransformer</code> class is a lean and direct construal of the transformer model that is mainly used for Natural Language Processing (NLP) tasks, such as translation, sentence classification, named entity recognition (NER), among others. </p> <p>This model ensures that information flow between distant words is not lost, which is achievable by employing the attention mechanism. This Transformer model is a key part of the architecture used in several state-of-the-art models, including BERT, GPT-2, and T5.</p>"},{"location":"zeta/structs/simpletransformer/#class-definition","title":"Class Definition","text":"<p>The class <code>SimpleTransformer</code> inherits from the PyTorch <code>nn.Module</code> class, which itself is a subclass of the <code>torch._six.PY3</code> metaclass. This implementation builds on the abstractions provided by PyTorch to define new modules by subclassing <code>nn.Module</code>, and that a model is a big module itself. </p>"},{"location":"zeta/structs/simpletransformer/#class-constructor-init-method","title":"Class Constructor (init method)","text":"<p>The <code>__init__</code> method initializes the class instance. It takes seven arguments:</p> <ul> <li><code>self</code>: This is a common practice in object-oriented programming, and it refers to the object itself. In Python, this is explicitly included as the first parameter. </li> <li><code>dim</code>: This is the dimension of the feature embeddings. Type: int.</li> <li><code>depth</code>: This is the depth (i.e., number of layers) of the transformer. Type: int.</li> <li><code>num_tokens</code>: This indicates the number of unique tokens in the corpus or vocabulary. Type: int.</li> <li><code>dim_head</code>: This is the dimension of a single attention head. Type: int. Default is 64. </li> <li><code>heads</code>: This is the total number of attention heads in the transformer. Type: int. Default is 8.</li> <li><code>ff_mult</code>: This is the multiplier for the feed-forward layer's inner layer. Type: int. Default is 4.</li> </ul> <p>The <code>__init__</code> method further initializes three attributes:</p> <ul> <li><code>emb</code>: An instance of PyTorch\u2019s <code>nn.Embedding</code> class, which turns integer indexes into dense vectors of fixed size, useful when working with sparse vectors representing categorical data.</li> <li><code>transformer</code>: An instance of a Transformer model.</li> <li><code>to_logits</code>: This applies a linear transformation to the incoming data, y = xA.T + b, and normalizes samples individually to unit norm.</li> </ul>"},{"location":"zeta/structs/simpletransformer/#forward-method","title":"Forward Method","text":"<p>The <code>forward</code> method defines the forward direction computation of the model.</p> <p>Arguments:</p> <ul> <li><code>self</code>: The instance of the class <code>SimpleTransformer</code>.</li> <li><code>x</code>: The input tensor for the model.</li> </ul> <p>Implementing <code>forward</code>: At first, the input tensor <code>x</code> is sent through the Embedding layer to convert the input token ids to vectors. This vectorized output is then passed through the transformer layer. <code>x</code> finally goes through a linear layer and is returned.</p>"},{"location":"zeta/structs/simpletransformer/#example-usage","title":"Example Usage","text":"<p>Here is a simple demonstration on how to create an instance of the <code>SimpleTransformer</code> and run a forward pass.</p> <pre><code># Import the necessary modules\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Transformer\n\n# Sample usage\nmodule = SimpleTransformer(512, 6, 20000)\nx = torch.LongTensor(2, 1024).random_(\n    0, 20000\n)  # creating a 2x1024 matrix of random Longs from 0 to 20000\ny = module(x)\nprint(y.shape)\n</code></pre> <p>The output tensor size is [2, 1024, 20000], where 20000 represents the number of unique tokens, and [2, 1024] represents the batch size and sequence length, respectively.</p> <p>Please note: Best Practices for PyTorch include moving tensors and models onto a common device (CPU, CUDA GPU) explicitly.</p>"},{"location":"zeta/structs/vitransformerwrapper/","title":"ViTransformerWrapper","text":""},{"location":"zeta/structs/vitransformerwrapper/#introduction","title":"Introduction","text":"<p><code>ViTransformerWrapper</code> is a PyTorch module that is part of the Zeta library. It essentially serves as a wrapper encapsulating the entirety of a Vision Transformer (ViT) model's architecture and functionality. As the name suggests,  this model is a Transformer that processes images. It treats an image as a sequence of image patches, much like how a regular Transformer treats a sentence as a sequence of words or subwords.</p> <p>Since it's structurally a Transformer, <code>ViTransformerWrapper</code> leverages the multi-head self-attention mechanism which allows it to process image patches globally instead of locally. This gives <code>ViTransformerWrapper</code> the capability to reason about global image features and their intricate interrelations, a task that CNNs aren't built for.</p>"},{"location":"zeta/structs/vitransformerwrapper/#class-definition","title":"Class Definition","text":"<p>The <code>ViTransformerWrapper</code> class inherits from PyTorch's <code>nn.Module</code> class which is the base class for all neural network modules. This class also has a layer called <code>attn_layers</code> which must be an <code>Encoder</code> object, this <code>Encoder</code> is a standard Transformer encoder.</p> <pre><code>class ViTransformerWrapper(nn.Module):\n    def __init__(self, *, image_size, patch_size, attn_layers, channels=3, num_classes=None, post_emb_norm=False, emb_dropout=0.0):\n    def forward(self, img, return_embeddings=False):\n</code></pre>"},{"location":"zeta/structs/vitransformerwrapper/#parameters","title":"Parameters","text":"Parameter Type Description image_size int Size of the image. The dimension must be divisible by <code>patch_size</code>. patch_size int Size of the image patches. attn_layers Encoder Transformer encoder which will be used as the attention layers. channels int (default is 3) Number of channels in the image. num_classes int (optional) Number of classes in the classification task. If <code>None</code>, the model will output raw embeddings. post_emb_norm bool (default is <code>False</code>) If <code>True</code>, enables normalization of embeddings after they are generated. emb_dropout float (default is 0.0) Dropout rate for the embeddings."},{"location":"zeta/structs/vitransformerwrapper/#attributes","title":"Attributes","text":"Attribute Type Description training bool Represents whether the module is in training mode or evaluation mode. <p>Attributes, methods and submodules assigned in the <code>__init__</code> method are registered in the module and will have their parameters converted too when you call <code>to()</code>, etc.</p>"},{"location":"zeta/structs/vitransformerwrapper/#method-forward","title":"Method: <code>forward</code>","text":"<p>The <code>forward</code> method is called when we execute the <code>ViTransformerWrapper</code> instance as a function. It feeds an image through the model and computes the forward pass. If <code>return_embeddings</code> is set to <code>True</code>, the method will output raw embeddings, otherwise it will output the predictions of the model, using the <code>mlp_head</code> which is a fully-connected layer applied after the Transformer layers.</p> <p>Parameters:</p> <ul> <li><code>img</code> (Tensor): Input image.</li> <li><code>return_embeddings</code> (bool, optional): If <code>True</code>, the method returns raw embeddings. If <code>False</code> (default), the method returns the class predictions.</li> </ul>"},{"location":"zeta/structs/vitransformerwrapper/#usage-examples","title":"Usage Examples","text":"<p>Here are three usage examples:</p>"},{"location":"zeta/structs/vitransformerwrapper/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>from zeta.structs import Encoder, ViTransformerWrapper\n\n# create a Transformer encoder instance\nencoder = Encoder(dim=128, depth=12)\n\n# define the wrapper with the encoder\nwrapper = ViTransformerWrapper(image_size=224, patch_size=16, attn_layers=encoder)\n\n# sample image\nimg = torch.randn(1, 3, 224, 224)\n\n# output of the model\nout = wrapper(img)\n</code></pre> <p>In this example, we first create an instance of a Transformer encoder with a dimension of 128 and a depth of 12. Then we instanstiate the <code>ViTransformerWrapper</code> with an image size of 224, a patch size of 16 and the previously created Transformer encoder. Afterwards, we simulate an image input of torch size (1, 3, 224, 224) and feed it through the model by calling <code>wrapper(img)</code>, the resulting <code>out</code> is the output of the model.</p>"},{"location":"zeta/structs/vitransformerwrapper/#example-2-training-loop","title":"Example 2: Training Loop","text":"<pre><code>from zeta.structs import Encoder, ViTransformerWrapper\n\n# create a Transformer encoder instance\nencoder = Encoder(dim=128, depth=12)\n\n# define the wrapper with the encoder and the number of classes\nmodel = ViTransformerWrapper(\n    image_size=224, patch_size=16, attn_layers=encoder, num_classes=10\n)\n\n# define a loss function\ncriterion = nn.CrossEntropyLoss()\n\n# define an optimizer\noptimizer = torch.optim.Adam(model.parameters())\n\n# sample inputs and targets\ninputs = torch.randn(32, 3, 224, 224)\ntargets = torch.randint(0, 10, [32])\n\n# training loop\nfor i in range(100):\n\n    # zero the parameter gradients\n    optimizer.zero_grad()\n\n    # forward pass\n    outputs = model(inputs)\n\n    # compute the loss\n    loss = criterion(outputs, targets)\n\n    # backward pass and optimize\n    loss.backward()\n    optimizer.step()\n\n    # print statistics\n    print(f\"loss: {loss.item():.4f}\")\n</code></pre> <p>This example shows a basic training loop for the <code>ViTransformerWrapper</code>. In this training loop, we use a cross entropy loss and Adam as the optimizer. The loop goes for 100 iterations, in each iteration it firstly zeroes the gradients, conducts forward pass to compute the model's output, then computes the loss based on the output and the ground truth, backpropagates the gradients and finally updates the model's parameters according to the Adam optimizer. The loss is printed out at every iteration.</p>"},{"location":"zeta/structs/vitransformerwrapper/#example-3-embeddings","title":"Example 3: Embeddings","text":"<pre><code>from zeta.structs import Encoder, ViTransformerWrapper\n\n# create a Transformer encoder instance\nencoder = Encoder(dim=128, depth=12)\n\n# define the wrapper with the encoder\nmodel = ViTransformerWrapper(image_size=224, patch_size=16, attn_layers=encoder)\n\n# sample inputs\ninputs = torch.randn(1, 3, 224, 224)\n\n# compute the embeddings\nembeddings = model(inputs, return_embeddings=True)\n</code></pre> <p>In this example, the <code>ViTransformerWrapper</code> returns raw embeddings since <code>return_embeddings</code> is set to <code>True</code>. The returned <code>embeddings</code> can then be used for other tasks such as clustering or nearest neighbours search.</p>"},{"location":"zeta/structs/vitransformerwrapper/#additional-information","title":"Additional Information","text":"<p>The <code>ViTransformerWrapper</code> class assumes that you're working with square images, i.e. height equals width. Be sure to resize your images appropriately or pad them if they are not originally square.</p> <p>Also, the <code>mlp_head</code> output layer is initialized as an <code>nn.Identity</code> layer if <code>num_classes</code> is not specified, meaning the Transformer's output embeddings will be passed through without transformation.</p> <p>Furthermore, the model relies on 2D convolutions, layer normalization and linear transformations, making it applicable to a wide range of tasks involving image data beyond image classification, such as object detection and instance segmentation, given suitable adjustments. </p> <p>Lastly, vision transformers are computationally expensive and use significantly more memory than their CNN counterparts since self-attention operates in quadratic space and time. Consider this if using a vision transformer in your project.</p>"},{"location":"zeta/structs/vitransformerwrapper/#external-resources","title":"External Resources","text":"<ul> <li>For further understanding on Transformers, you can read the following paper: Attention is All You Need</li> <li>For the original Vision Transformer paper, you can read: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</li> <li>To know more about the implementation of the transformer model, consider reading the Transformers Module in PyTorch documentation.</li> <li>For more tutorials and examples using PyTorch, you can check out their tutorials page.</li> </ul>"},{"location":"zeta/tokenizers/language_tokenizer/","title":"Module Name: LanguageTokenizerGPTX","text":"<p>The <code>LanguageTokenizerGPTX</code> is an embedding utility tailored for the \"EleutherAI/gpt-neox-20b\" transformer model. This class allows for seamless tokenization and decoding operations, abstracting away the underlying complexity of the chosen transformer's tokenizer.</p>"},{"location":"zeta/tokenizers/language_tokenizer/#introduction","title":"Introduction:","text":"<p>Language tokenization is a crucial step in natural language processing tasks. This module provides an interface to tokenize and decode text using the GPT-Neox-20b transformer from the EleutherAI project. With the ability to manage end-of-string tokens, padding tokens, and a fixed model length, <code>LanguageTokenizerGPTX</code> serves as a convenient wrapper for the actual tokenizer from the transformers library.</p>"},{"location":"zeta/tokenizers/language_tokenizer/#class-definition","title":"Class Definition:","text":"<pre><code>class LanguageTokenizerGPTX:\n    def __init__(self): ...\n    def tokenize_texts(self, texts: str) -&gt; torch.Tensor: ...\n    def decode(self, texts: torch.Tensor) -&gt; str: ...\n    def __len__(self) -&gt; int: ...\n</code></pre>"},{"location":"zeta/tokenizers/language_tokenizer/#parameters","title":"Parameters:","text":"<p>The class does not take any parameters upon instantiation. It uses predefined parameters internally to load the tokenizer.</p>"},{"location":"zeta/tokenizers/language_tokenizer/#methods","title":"Methods:","text":""},{"location":"zeta/tokenizers/language_tokenizer/#1-__init__self-none","title":"1. <code>__init__(self) -&gt; None</code>:","text":"<p>Initializes the <code>LanguageTokenizerGPTX</code> object. This method loads the <code>AutoTokenizer</code> with predefined parameters.</p>"},{"location":"zeta/tokenizers/language_tokenizer/#2-tokenize_textsself-texts-str-torchtensor","title":"2. <code>tokenize_texts(self, texts: str) -&gt; torch.Tensor</code>:","text":"<p>Tokenizes a given text or list of texts.</p> <ul> <li>texts (str): The input text(s) to tokenize.</li> </ul> <p>Returns:   - A torch Tensor of token IDs representing the input text(s).</p>"},{"location":"zeta/tokenizers/language_tokenizer/#3-decodeself-texts-torchtensor-str","title":"3. <code>decode(self, texts: torch.Tensor) -&gt; str</code>:","text":"<p>Decodes a given tensor of token IDs back to text.</p> <ul> <li>texts (torch.Tensor): The tensor of token IDs to decode.</li> </ul> <p>Returns:   - A string representing the decoded text.</p>"},{"location":"zeta/tokenizers/language_tokenizer/#4-__len__self-int","title":"4. <code>__len__(self) -&gt; int</code>:","text":"<p>Provides the total number of tokens in the tokenizer's vocabulary.</p> <p>Returns:   - An integer representing the total number of tokens.</p>"},{"location":"zeta/tokenizers/language_tokenizer/#usage-examples","title":"Usage Examples:","text":"<pre><code>import torch\n\nfrom zeta import LanguageTokenizerGPTX\n\n# Initialize the tokenizer\ntokenizer = LanguageTokenizerGPTX()\n\n# Example 1: Tokenize a single text\ntext = \"Hello, world!\"\ntokenized_text = tokenizer.tokenize_texts(text)\nprint(tokenized_text)\n\n# Example 2: Decode a tokenized text\ndecoded_text = tokenizer.decode(tokenized_text)\nprint(decoded_text)\n\n# Example 3: Get the number of tokens in the tokenizer's vocabulary\nnum_tokens = len(tokenizer)\nprint(f\"The tokenizer has {num_tokens} tokens.\")\n</code></pre>"},{"location":"zeta/tokenizers/language_tokenizer/#mathematical-formulation","title":"Mathematical Formulation:","text":"<p>Given a text ( t ) and a vocabulary ( V ) from the GPT-Neox-20b model, tokenization maps ( t ) to a sequence of token IDs ( T ) where each token ID ( t_i ) corresponds to a token in ( V ). Decoding reverses this process.</p> <p>[ t \\xrightarrow{\\text{tokenize}} T ] [ T \\xrightarrow{\\text{decode}} t ]</p>"},{"location":"zeta/tokenizers/language_tokenizer/#additional-information","title":"Additional Information:","text":"<p>The GPT-Neox-20b model is part of the EleutherAI project. It's a variant of the GPT architecture with tweaks in terms of model size and training. Utilizing such models require an understanding of tokenization and decoding, which this module aims to simplify.</p>"},{"location":"zeta/tokenizers/language_tokenizer/#references","title":"References:","text":"<ul> <li>Transformers Library by Hugging Face</li> <li>EleutherAI GPT-Neox</li> </ul> <p>Note: Ensure you have the necessary packages and dependencies installed, particularly the transformers library from Hugging Face.</p>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/","title":"Documentation for Zeta Library's MultiModalTokenizer Class","text":""},{"location":"zeta/tokenizers/multi_modal_tokenizer/#introduction-and-overview","title":"Introduction and Overview","text":"<p>The <code>MultiModalTokenizer</code> class is part of the Zeta Library, designed to provide tokenization capabilities for both text and image data. This enables more seamless integration and utilization of multimodal (text and image) data, especially when used with models that can handle such information simultaneously, like the CLIP model.</p> <p>Key Features:</p> <ol> <li>Multimodal Tokenization: Combines text and image tokenization within one unified class.</li> <li>Integration with Hugging Face Transformers: Utilizes the <code>CLIPProcessor</code> for image tokenization and <code>AutoTokenizer</code> for text tokenization.</li> <li>Special Tokens for Image Segmentation: Uses special tokens <code>&lt;image&gt;</code> and <code>&lt;/image&gt;</code> to denote image token boundaries within text.</li> <li>Error Handling: Implements comprehensive error handling and logging to ensure robustness.</li> </ol>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#class-definition","title":"Class Definition","text":""},{"location":"zeta/tokenizers/multi_modal_tokenizer/#multimodaltokenizer","title":"MultiModalTokenizer","text":"<pre><code>class MultiModalTokenizer:\n    \"\"\"\n    A tokenizer class for the kosmos model\n\n    Attributes:\n        processor(CLIPProcessor): The processor to tokenize images.\n        tokenizer(AutoTokenizer): The tokenizer to tokenize text.\n        im_idx(int): The Index of the \"&lt;image&gt;\" token.\n        im_end_idx(int): The index of the \"&lt;/image&gt;\" token.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#parameters","title":"Parameters:","text":"<ul> <li>max_length (int, optional): Maximum length of the tokenized sequence. Defaults to 8192.</li> </ul>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#attributes","title":"Attributes:","text":"<ul> <li>processor (CLIPProcessor): The processor used to tokenize images.</li> <li>tokenizer (AutoTokenizer): The tokenizer used to tokenize text.</li> <li>im_idx (int): Index of the <code>&lt;image&gt;</code> token.</li> <li>im_end_idx (int): Index of the <code>&lt;/image&gt;</code> token.</li> </ul>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#methods","title":"Methods","text":""},{"location":"zeta/tokenizers/multi_modal_tokenizer/#1-tokenize_texts","title":"1. tokenize_texts","text":"<pre><code>def tokenize_texts(self, texts: str) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Tokenize given texts.\n\n    Args:\n        texts (str): The text to be tokenized.\n\n    Returns:\n        A tuple containing the tokenized texts and only the text tokens.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#2-tokenize_images","title":"2. tokenize_images","text":"<pre><code>def tokenize_images(self, images) -&gt; torch.Tensor:\n    \"\"\"\n    Tokenizes given images.\n\n    Args:\n        images: The images to be tokenized.\n\n    Returns:\n        The tokenized images.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#3-tokenize","title":"3. tokenize","text":"<pre><code>def tokenize(self, sample) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Tokenizes given sample.\n\n    Args:\n        sample: The sample to be tokenized.\n\n    Returns:\n        A dictionary containing the tokenized text tokens, images, labels, and attention mask.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/tokenizers/multi_modal_tokenizer/#example-1-tokenizing-texts","title":"Example 1: Tokenizing Texts","text":"<pre><code>import torch\n\nfrom zeta import MultiModalTokenizer\n\ntokenizer = MultiModalTokenizer()\ntexts = [\"Hello World\", \"Zeta Library is great!\"]\ntokenized_texts, only_texts = tokenizer.tokenize_texts(texts)\nprint(tokenized_texts)\nprint(only_texts)\n</code></pre>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#example-2-tokenizing-images","title":"Example 2: Tokenizing Images","text":"<pre><code>import torch\n\nfrom zeta import MultiModalTokenizer\n\ntokenizer = MultiModalTokenizer()\nimages = torch.randn(2, 3, 224, 224)  # Assuming 2 random images of shape 3x224x224\ntokenized_images = tokenizer.tokenize_images(images)\nprint(tokenized_images)\n</code></pre>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#example-3-tokenizing-multimodal-data","title":"Example 3: Tokenizing Multimodal Data","text":"<pre><code>import torch\n\nfrom zeta import MultiModalTokenizer\n\ntokenizer = MultiModalTokenizer()\nsample = {\n    \"target_text\": [\"Hello World\", \"Zeta Library is great!\"],\n    \"image\": torch.randn(2, 3, 224, 224),\n}\ntokenized_data = tokenizer.tokenize(sample)\nprint(tokenized_data)\n</code></pre>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#mathematical-overview","title":"Mathematical Overview","text":"<p>Given a text sequence ( T ) of length ( n ) and an image ( I ) represented by a tensor of shape ( C \\times H \\times W ), where ( C ) is the number of channels, ( H ) is the height, and ( W ) is the width:</p> <ol> <li> <p>The tokenized text, ( T' ), is represented as:    [ T' = [, , , T_{1}, T_{2}, ..., T_{n}, ] ]</p> </li> <li> <p>The tokenized image, ( I' ), is processed using the CLIP processor to obtain a tensor representation.</p> </li> <li> <p>When both text and image data are tokenized using the <code>tokenize</code> method, the output contains both ( T' ) and ( I' ) with their respective attention masks.</p> </li> </ol>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#additional-tips","title":"Additional Tips","text":"<ul> <li> <p>Ensure you have the required model weights and configurations for the specified pretrained models (\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\" and \"EleutherAI/gpt-neox-20b\") downloaded or accessible from the Hugging Face Model Hub.</p> </li> <li> <p>Handle potential tokenization errors gracefully using try-except blocks, as demonstrated in the provided methods.</p> </li> </ul>"},{"location":"zeta/tokenizers/multi_modal_tokenizer/#references-and-resources","title":"References and Resources","text":"<ol> <li>CLIP: Connecting Vision and Language with Reinforced Loss - OpenAI: Link</li> <li>Hugging Face's Transformers library: Link</li> <li>Documentation on Special Tokens in Transformers: Link</li> </ol>"},{"location":"zeta/tokenizers/sentencepiece/","title":"SentencePieceTokenizer","text":"<p><code>SentencePieceTokenizer</code> is a class for tokenizing and detokenizing text using a pre-trained SentencePiece model. The SentencePiece model is a unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation tasks where the vocabulary size is predetermined prior to the neural model training. This class is a part of the zeta library which is a collection of various utility functions and classes for Natural Language Processing tasks.</p>"},{"location":"zeta/tokenizers/sentencepiece/#introduction","title":"Introduction","text":"<p>Tokenization is a crucial step in many natural language processing tasks. It involves splitting a piece of text into smaller units, called tokens. These tokens can be as small as characters or as large as words. The <code>SentencePieceTokenizer</code> class provides an efficient and easy-to-use way to tokenize and detokenize text using a SentencePiece model.</p> <p>The SentencePiece model is trained to find the best tokenization by dynamically adjusting the size and boundary of tokens. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) and unigram language model with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.</p>"},{"location":"zeta/tokenizers/sentencepiece/#class-definition","title":"Class Definition","text":"<pre><code>class SentencePieceTokenizer:\n    def __init__(self, model_path: str): ...\n</code></pre>"},{"location":"zeta/tokenizers/sentencepiece/#parameters","title":"Parameters:","text":"<ul> <li><code>model_path (str)</code>: The path to the pre-trained SentencePiece model. It should be a file with <code>.model</code> extension.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#attributes","title":"Attributes:","text":"<ul> <li><code>n_words (int)</code>: The vocabulary size of the SentencePiece model.</li> <li><code>bos_id (int)</code>: The token ID for the beginning of sentence token.</li> <li><code>eos_id (int)</code>: The token ID for the end of sentence token.</li> <li><code>pad_id (int)</code>: The token ID for the padding token.</li> <li><code>prefix_id (int, optional)</code>: The token ID for the prefix token.</li> <li><code>middle_id (int, optional)</code>: The token ID for the middle token.</li> <li><code>suffix_id (int, optional)</code>: The token ID for the suffix token.</li> <li><code>eot_id (int, optional)</code>: The token ID for the end of text token.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#methods","title":"Methods","text":""},{"location":"zeta/tokenizers/sentencepiece/#encode","title":"<code>encode</code>","text":"<pre><code>def encode(self, s: str, bos: bool, eos: bool) -&gt; List[int]: ...\n</code></pre> <p>Encodes a string into a list of integer token IDs.</p>"},{"location":"zeta/tokenizers/sentencepiece/#parameters_1","title":"Parameters:","text":"<ul> <li><code>s (str)</code>: The string to be encoded.</li> <li><code>bos (bool)</code>: Whether to add the beginning of sentence token at the start.</li> <li><code>eos (bool)</code>: Whether to add the end of sentence token at the end.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#returns","title":"Returns:","text":"<ul> <li><code>List[int]</code>: A list of integer token IDs.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#decode","title":"<code>decode</code>","text":"<pre><code>def decode(self, t: List[int]) -&gt; str: ...\n</code></pre> <p>Decodes a list of integer token IDs into a string.</p>"},{"location":"zeta/tokenizers/sentencepiece/#parameters_2","title":"Parameters:","text":"<ul> <li><code>t (List[int])</code>: A list of integer token IDs to be decoded.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#returns_1","title":"Returns:","text":"<ul> <li><code>str</code>: The decoded string.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#encode_infilling","title":"<code>encode_infilling</code>","text":"<pre><code>def encode_infilling(self, s: str) -&gt; List[int]: ...\n</code></pre> <p>Encodes a string without an implicit leading space.</p>"},{"location":"zeta/tokenizers/sentencepiece/#parameters_3","title":"Parameters:","text":"<ul> <li><code>s (str)</code>: The string to be encoded.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#returns_2","title":"Returns:","text":"<ul> <li><code>List[int]</code>: A list of integer token IDs.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#decode_infilling","title":"<code>decode_infilling</code>","text":"<pre><code>def decode_infilling(self, t: List[int]) -&gt; str: ...\n</code></pre> <p>Decodes a list of integer token IDs into a string without an implicit leading space.</p>"},{"location":"zeta/tokenizers/sentencepiece/#parameters_4","title":"Parameters:","text":"<ul> <li><code>t (List[int])</code>: A list of integer token IDs to be decoded.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#returns_3","title":"Returns:","text":"<ul> <li><code>str</code>: The decoded string.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/tokenizers/sentencepiece/#example-1","title":"Example 1:","text":"<pre><code>from zeta import SentencePieceTokenizer\n\ntokenizer = SentencePieceTokenizer(model_path=\"path/to/your/model.model\")\ntext = \"Hello, world!\"\ntokens = tokenizer.encode(text, bos=True, eos=True)\nprint(tokens)\n# [2, 284, 16, 250, 13, 849, 4, 3]\n\ndecoded_text = tokenizer.decode(tokens)\nprint(decoded_text)\n# \"Hello, world!\"\n</code></pre>"},{"location":"zeta/tokenizers/sentencepiece/#example-2","title":"Example 2:","text":"<pre><code>from zeta import SentencePieceTokenizer\n\ntokenizer = SentencePieceTokenizer(model_path=\"path/to/your/model.model\")\ntext = \"Hello, world!\"\ntokens = tokenizer.encode_infilling(text)\nprint(tokens)\n# [284, 16, 250, 13, 849, 4]\n\ndecoded_text = tokenizer.decode_infilling(tokens)\nprint(decoded_text)\n# \"Hello, world!\"\n</code></pre>"},{"location":"zeta/tokenizers/sentencepiece/#example-3","title":"Example 3:","text":"<pre><code>from zeta import SentencePieceTokenizer\n\ntokenizer = SentencePieceTokenizer(model_path=\"path/to/your/model.model\")\ntokens = [2, 284, 16, 250, 13, 849, 4, 3]\ndecoded_text = tokenizer.decode(tokens)\nprint(decoded_text)\n# \"Hello, world!\"\n</code></pre>"},{"location":"zeta/tokenizers/sentencepiece/#additional-information","title":"Additional Information","text":"<ul> <li>Make sure that the model file specified in <code>model_path</code> exists.</li> <li>The special tokens such as <code>&lt;PRE&gt;</code>, <code>&lt;MID&gt;</code>, <code>&lt;SUF&gt;</code>, <code>&lt;EOT&gt;</code> are optional and may not be present in all SentencePiece models.</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#references-and-resources","title":"References and Resources","text":"<ul> <li>SentencePiece GitHub Repository</li> <li>SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Generation</li> </ul>"},{"location":"zeta/tokenizers/sentencepiece/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>The SentencePiece model uses the following mathematical formula for tokenization:</p> <p>[P(w) = \\prod_{i=1}^{n} P(w_i | w_1, ..., w_{i-1})]</p> <p>Where: - (P(w)) is the probability of the word (w). - (n) is the number of subwords in the word (w). - (w_i) is the (i)-th subword of (w).</p> <p>The model is trained to maximize the likelihood of the training data, and the subwords are chosen to minimize the perplexity of the training data.</p>"},{"location":"zeta/tokenizers/token_monster/","title":"TokenMonster Documentation","text":""},{"location":"zeta/tokenizers/token_monster/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Understanding the Purpose</li> <li>Overview and Introduction</li> <li>Class Definition</li> <li>Functionality and Usage<ul> <li>Initializing TokenMonster</li> <li>Setting Local Directory</li> <li>Loading Vocabulary</li> <li>Creating a New Vocabulary</li> <li>Saving Vocabulary</li> <li>Exporting Vocabulary to YAML</li> <li>Tokenization</li> <li>Decoding Tokens</li> <li>Creating a Decoder Instance</li> <li>Getting Vocabulary Dictionary</li> <li>Getting Character Set</li> <li>Getting Normalization</li> <li>Getting Capcode Level</li> <li>Getting Optimization Mode</li> <li>Mapping Token ID to Token String</li> <li>Mapping Token ID to Token String (Decoded)</li> <li>Mapping Token String to Token ID</li> <li>Modifying Vocabulary</li> <li>Adding Regular Tokens</li> <li>Deleting Tokens</li> <li>Deleting Tokens by ID</li> <li>Adding Special Tokens</li> <li>Resizing Vocabulary</li> <li>Resetting Token IDs</li> <li>Enabling UNK Token</li> <li>Disabling UNK Token</li> <li>Disconnecting TokenMonster</li> <li>Serializing Tokens</li> <li>Deserializing Tokens</li> </ul> </li> <li>Additional Information</li> <li>Examples</li> <li>Conclusion</li> </ol>"},{"location":"zeta/tokenizers/token_monster/#1-understanding-the-purpose","title":"1. Understanding the Purpose","text":"<p>TokenMonster is a Python library designed to provide tokenization and vocabulary management functionalities. It allows you to tokenize text, manage vocabularies, modify vocabularies, and perform various operations related to tokenization and vocabulary handling.</p>"},{"location":"zeta/tokenizers/token_monster/#purpose-and-functionality","title":"Purpose and Functionality","text":"<p>TokenMonster serves the following purposes and functionalities:</p> <ul> <li>Tokenization: Tokenize text into tokens based on a specified vocabulary.</li> <li>Vocabulary Management: Load, create, save, and modify vocabularies.</li> <li>Token ID Mapping: Map tokens to token IDs and vice versa.</li> <li>Serialization: Serialize and deserialize tokens for storage or transmission.</li> <li>Configuration: Access and modify vocabulary settings like character set, normalization, capcode level, and optimization mode.</li> <li>Special Token Handling: Add, delete, or modify special tokens.</li> <li>Disconnecting: Gracefully disconnect from TokenMonster server.</li> </ul> <p>TokenMonster is useful in various natural language processing tasks, especially when working with custom vocabularies and tokenization requirements.</p>"},{"location":"zeta/tokenizers/token_monster/#2-overview-and-introduction","title":"2. Overview and Introduction","text":""},{"location":"zeta/tokenizers/token_monster/#overview","title":"Overview","text":"<p>TokenMonster is a versatile library for tokenization and vocabulary management. It allows you to create, load, and modify vocabularies, tokenize text, and perform various operations related to tokenization.</p>"},{"location":"zeta/tokenizers/token_monster/#importance-and-relevance","title":"Importance and Relevance","text":"<p>In the field of natural language processing, tokenization is a fundamental step in text preprocessing. TokenMonster provides a flexible and efficient way to tokenize text while also enabling users to manage custom vocabularies. The ability to add, delete, or modify tokens is crucial when working with specialized language models and text data.</p>"},{"location":"zeta/tokenizers/token_monster/#key-concepts-and-terminology","title":"Key Concepts and Terminology","text":"<p>Before diving into the details, let's clarify some key concepts and terminology used throughout the documentation:</p> <ul> <li>Tokenization: The process of breaking text into individual tokens (words, subwords, or characters).</li> <li>Vocabulary: A collection of tokens and their corresponding token IDs.</li> <li>Token ID: A unique identifier for each token in the vocabulary.</li> <li>Special Tokens: Tokens that have a specific role, such as padding, start of sentence, end of sentence, and unknown tokens.</li> <li>Normalization: Text processing operations like lowercasing, accent removal, and character set transformation.</li> <li>Capcode Level: The level of capcoding applied to tokens (0-2).</li> <li>Optimization Mode: The mode used for optimizing TokenMonster (0-5).</li> </ul> <p>Now that we have an overview, let's proceed with a detailed class definition.</p>"},{"location":"zeta/tokenizers/token_monster/#3-class-definition","title":"3. Class Definition","text":""},{"location":"zeta/tokenizers/token_monster/#class-tokenmonster","title":"Class: TokenMonster","text":"<p>The <code>TokenMonster</code> class encapsulates the functionality of the TokenMonster library.</p>"},{"location":"zeta/tokenizers/token_monster/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, path):\n    \"\"\"\n    Initializes the TokenMonster class and loads a vocabulary.\n\n    Args:\n        path (str): A filepath, URL, or pre-built vocabulary name.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#methods","title":"Methods","text":"<p>The <code>TokenMonster</code> class defines various methods to perform tokenization, vocabulary management, and configuration. Here are the key methods:</p>"},{"location":"zeta/tokenizers/token_monster/#1-setting-local-directory","title":"1. Setting Local Directory","text":"<pre><code>def set_local_directory(self, dir=None):\n    \"\"\"\n    Sets the local directory for TokenMonster.\n\n    Args:\n        dir (str, optional): The local directory to use. Defaults to None.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#2-loading-vocabulary","title":"2. Loading Vocabulary","text":"<pre><code>def load(self, path):\n    \"\"\"\n    Loads a TokenMonster vocabulary from file, URL, or by name.\n\n    Args:\n        path (str): A filepath, URL, or pre-built vocabulary name.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#3-loading-vocabulary-multiprocess-safe","title":"3. Loading Vocabulary (Multiprocess Safe)","text":"<pre><code>def load_multiprocess_safe(self, path):\n    \"\"\"\n    Loads a TokenMonster vocabulary from file, URL, or by name. It's safe for multiprocessing,\n    but vocabulary modification is disabled, and tokenization is slightly slower.\n\n    Args:\n        path (str): A filepath, URL, or pre-built vocabulary name.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#4-creating-a-new-vocabulary","title":"4. Creating a New Vocabulary","text":"<pre><code>def new(self, yaml):\n    \"\"\"\n    Creates a new vocabulary from a YAML string.\n\n    Args:\n        yaml (str): The YAML file.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#5-saving-vocabulary","title":"5. Saving Vocabulary","text":"<pre><code>def save(self, fname):\n    \"\"\"\n    Saves the current vocabulary to a file.\n\n    Args:\n        fname (str): The filename to save the vocabulary to.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#6-exporting-vocabulary-to-yaml","title":"6. Exporting Vocabulary to YAML","text":"<pre><code>def export_yaml(self, order_by_score=False):\n    \"\"\"\n    Exports the vocabulary as a YAML file, which is returned as a bytes string.\n\n    Args:\n        order_by_score (bool, optional): If true, the tokens are ordered by score instead of alphabetically. Defaults to False.\n\n    Returns:\n        bytes: The vocabulary in YAML format.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#7-tokenization","title":"7. Tokenization","text":"<pre><code>def tokenize(self, text):\n    \"\"\"\n       Tokenizes a\n\n    string into tokens according to the vocabulary.\n\n       Args:\n           text (str): A string or bytes string or a list of strings or bytes strings.\n\n       Returns:\n           numpy array: The token IDs.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#8-tokenization-count","title":"8. Tokenization (Count)","text":"<pre><code>def tokenize_count(self, text):\n    \"\"\"\n    Same as tokenize, but it returns only the number of tokens.\n\n    Args:\n        text (str): A string or bytes string or a list of strings or bytes strings.\n\n    Returns:\n        int: The number of tokens for each input string.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#9-decoding-tokens","title":"9. Decoding Tokens","text":"<pre><code>def decode(self, tokens):\n    \"\"\"\n    Decodes tokens into a string.\n\n    Args:\n        tokens (int, list of int, or numpy array): The tokens to decode into a string.\n\n    Returns:\n        str: The composed string from the input tokens.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#10-creating-a-decoder-instance","title":"10. Creating a Decoder Instance","text":"<pre><code>def decoder(self):\n    \"\"\"\n    Returns a new decoder instance used for decoding tokens into text.\n\n    Returns:\n        tokenmonster.DecoderInstance: A new decoder instance.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#11-getting-vocabulary-dictionary","title":"11. Getting Vocabulary Dictionary","text":"<pre><code>def get_dictionary(self):\n    \"\"\"\n    Returns a dictionary of all tokens in the vocabulary.\n\n    Returns:\n        list: A list of dictionaries where the index is the token ID, and each is a dictionary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#12-getting-character-set","title":"12. Getting Character Set","text":"<pre><code>def charset(self):\n    \"\"\"\n    Returns the character set used by the vocabulary.\n\n    Returns:\n        str: The character set used by the vocabulary. Possible values are \"UTF-8\" or \"None\".\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#13-getting-normalization","title":"13. Getting Normalization","text":"<pre><code>def normalization(self):\n    \"\"\"\n    Returns the normalization of the vocabulary.\n\n    Returns:\n        str: The normalization of the vocabulary. Possible values are \"None\", \"NFD\", \"Lowercase\", \"Accents\", \"Quotemarks\", \"Collapse\", \"Trim\", \"LeadingSpace\", or \"UnixLines\".\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#14-getting-capcode-level","title":"14. Getting Capcode Level","text":"<pre><code>def capcode(self):\n    \"\"\"\n    Returns the capcode level of the vocabulary.\n\n    Returns:\n        int: The capcode level (0-2).\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#15-getting-optimization-mode","title":"15. Getting Optimization Mode","text":"<pre><code>def mode(self):\n    \"\"\"\n    Returns the optimization mode of the vocabulary.\n\n    Returns:\n        int: The optimization mode (0-5).\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#16-mapping-token-id-to-token-string","title":"16. Mapping Token ID to Token String","text":"<pre><code>def id_to_token(self, id):\n    \"\"\"\n    Get the token string from a single token ID, in its capcode-encoded form.\n\n    Args:\n        id (int): The token ID.\n\n    Returns:\n        str or None: The token string corresponding to the input ID. None if the ID is not in the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#17-mapping-token-id-to-token-string-decoded","title":"17. Mapping Token ID to Token String (Decoded)","text":"<pre><code>def id_to_token_decoded(self, id):\n    \"\"\"\n    Get the token string from a single token ID, in its capcode-decoded form.\n\n    Args:\n        id (int): The token ID.\n\n    Returns:\n        str or None: The token string corresponding to the input ID. None if the ID is not in the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#18-mapping-token-string-to-token-id","title":"18. Mapping Token String to Token ID","text":"<pre><code>def token_to_id(self, token):\n    \"\"\"\n    Returns the ID of a single token.\n\n    Args:\n        token (str): The token to get the ID for.\n\n    Returns:\n        int or None: The ID of the token. None if the token is not in the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#19-modifying-vocabulary","title":"19. Modifying Vocabulary","text":"<pre><code>def modify(\n    self,\n    add_special_tokens=None,\n    add_regular_tokens=None,\n    delete_tokens=None,\n    resize=None,\n    change_unk=None,\n):\n    \"\"\"\n    Modifies the vocabulary.\n\n    Args:\n        add_special_tokens (str or list of str, optional): Special tokens to add to the vocabulary.\n        add_regular_tokens (str or list of str, optional): Regular tokens to add to the vocabulary.\n        delete_tokens (str or list of str, optional): Regular or special tokens to delete.\n        resize (int, optional): Resizes the vocabulary to this size.\n        change_unk (bool, optional): If set, it enables or disables the UNK token.\n\n    Returns:\n        int: The new size of the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#20-adding-regular-tokens","title":"20. Adding Regular Tokens","text":"<pre><code>def add_token(self, token):\n    \"\"\"\n    Add one or more regular tokens.\n\n    Args:\n        token (str or list of str): The regular tokens to add.\n\n    Returns:\n        int: The new size of the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#21-deleting-tokens","title":"21. Deleting Tokens","text":"<pre><code>def delete_token(self, token):\n    \"\"\"\n    Delete one or more regular or special tokens.\n\n    Args:\n        token (str or list of str): The tokens to delete.\n\n    Returns:\n        int: The new size of the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#22-deleting-tokens-by-id","title":"22. Deleting Tokens by ID","text":"<pre><code>def delete_token_by_id(self, id):\n    \"\"\"\n    Delete one or more regular or special tokens by specifying the token ID.\n\n    Args:\n        id (int or list of int): The IDs of the tokens to delete.\n\n    Returns:\n        int: The new size of the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#23-adding-special-tokens","title":"23. Adding Special Tokens","text":"<pre><code>def add_special_token(self, token):\n    \"\"\"\n    Add one or more special tokens.\n\n    Args:\n        token (str or list of str): The special tokens to add.\n\n    Returns:\n        int: The new size of the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#24-resizing-vocabulary","title":"24. Resizing Vocabulary","text":"<pre><code>def resize(self, size):\n    \"\"\"\n    Changes the size of the vocabulary.\n\n    Args:\n        size (int): The new size of the vocabulary.\n\n    Returns:\n        int: The new size of the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#25-resetting-token-ids","title":"25. Resetting Token IDs","text":"<pre><code>def reset_token_ids(self):\n    \"\"\"\n    Resets the token IDs to be sequential beginning from zero.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#26-enabling-unk-token","title":"26. Enabling UNK Token","text":"<pre><code>def enable_unk_token(self):\n    \"\"\"\n    Enables the UNK token.\n\n    Returns:\n        int: The new size of the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#27-disabling-unk-token","title":"27. Disabling UNK Token","text":"<pre><code>def disable_unk_token(self):\n    \"\"\"\n    Disables the UNK token.\n\n    Returns:\n        int: The new size of the vocabulary.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#28-disconnecting-tokenmonster","title":"28. Disconnecting TokenMonster","text":"<pre><code>def disconnect(self):\n    \"\"\"\n    Disconnects and closes TokenMonster server.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#29-serializing-tokens","title":"29. Serializing Tokens","text":"<pre><code>def serialize_tokens(self, integer_list):\n    \"\"\"\n    Serializes tokens from a list of ints or numpy array into a binary string.\n\n    Args:\n        integer_list (list of int or numpy array): The tokens to serialize.\n\n    Returns:\n        bytes: The serialized binary string.\n    \"\"\"\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#30-deserializing-tokens","title":"30. Deserializing Tokens","text":"<pre><code>def deserialize_tokens(self, binary_string):\n    \"\"\"\n    Deserializes a binary string into a numpy array of token IDs.\n\n    Args:\n\n\n        binary_string (bytes): The binary string to deserialize.\n\n    Returns:\n        np.array: The deserialized tokens.\n    \"\"\"\n</code></pre> <p>This concludes the class definition. In the following sections, we will explore each method in detail and provide examples of their usage.</p>"},{"location":"zeta/tokenizers/token_monster/#4-functionality-and-usage","title":"4. Functionality and Usage","text":""},{"location":"zeta/tokenizers/token_monster/#41-initializing-tokenmonster","title":"4.1. Initializing TokenMonster","text":"<p>To get started with TokenMonster, you need to initialize an instance of the <code>TokenMonster</code> class. The constructor takes a single argument, <code>path</code>, which specifies the location of the vocabulary.</p> <p>Example:</p> <pre><code>from zeta.tokenizers import TokenMonster\n\n# Initialize TokenMonster with a vocabulary file\ntokenizer = TokenMonster(\"path/to/vocabulary\")\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#42-setting-local-directory","title":"4.2. Setting Local Directory","text":"<p>You can set the local directory for TokenMonster using the <code>set_local_directory</code> method. This directory is used for local caching of vocabulary files.</p> <p>Example:</p> <pre><code># Set the local directory for TokenMonster\ntokenizer.set_local_directory(\"path/to/local/directory\")\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#43-loading-vocabulary","title":"4.3. Loading Vocabulary","text":"<p>TokenMonster allows you to load vocabularies from various sources, including file paths, URLs, or pre-built vocabulary names. Use the <code>load</code> method to load a vocabulary.</p> <p>Example:</p> <pre><code># Load a vocabulary from a file\ntokenizer.load(\"path/to/vocabulary\")\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#44-creating-a-new-vocabulary","title":"4.4. Creating a New Vocabulary","text":"<p>You can create a new vocabulary from a YAML string using the <code>new</code> method. This is useful when you want to define a custom vocabulary.</p> <p>Example:</p> <pre><code># Create a new vocabulary from a YAML string\nyaml_string = \"\"\"\n- token: [PAD]\n  id: 0\n\"\"\"\ntokenizer.new(yaml_string)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#45-saving-vocabulary","title":"4.5. Saving Vocabulary","text":"<p>TokenMonster allows you to save the current vocabulary to a file using the <code>save</code> method. This is useful for preserving custom vocabularies you've created.</p> <p>Example:</p> <pre><code># Save the current vocabulary to a file\ntokenizer.save(\"custom_vocabulary.yaml\")\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#46-exporting-vocabulary-to-yaml","title":"4.6. Exporting Vocabulary to YAML","text":"<p>You can export the vocabulary as a YAML file using the <code>export_yaml</code> method. This method returns the vocabulary in YAML format as a bytes string.</p> <p>Example:</p> <pre><code># Export the vocabulary to a YAML file\nyaml_data = tokenizer.export_yaml()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#47-tokenization","title":"4.7. Tokenization","text":"<p>Tokenization is a core functionality of TokenMonster. You can tokenize text into tokens according to the loaded vocabulary using the <code>tokenize</code> method.</p> <p>Example:</p> <pre><code># Tokenize a text string\ntext = \"Hello, world!\"\ntoken_ids = tokenizer.tokenize(text)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#48-tokenization-count","title":"4.8. Tokenization (Count)","text":"<p>If you want to know the number of tokens without getting the token IDs, you can use the <code>tokenize_count</code> method.</p> <p>Example:</p> <pre><code># Count the number of tokens in a text string\ntext = \"Hello, world!\"\ntoken_count = tokenizer.tokenize_count(text)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#49-decoding-tokens","title":"4.9. Decoding Tokens","text":"<p>To decode token IDs back into a human-readable string, you can use the <code>decode</code> method.</p> <p>Example:</p> <pre><code># Decode token IDs into a string\ndecoded_text = tokenizer.decode(token_ids)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#410-creating-a-decoder-instance","title":"4.10. Creating a Decoder Instance","text":"<p>TokenMonster allows you to create a decoder instance for decoding tokens into text. Use the <code>decoder</code> method to obtain a decoder instance.</p> <p>Example:</p> <pre><code># Create a decoder instance\ndecoder = tokenizer.decoder()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#411-getting-vocabulary-dictionary","title":"4.11. Getting Vocabulary Dictionary","text":"<p>The <code>get_dictionary</code> method returns a dictionary of all tokens in the vocabulary. Each dictionary entry contains information about the token.</p> <p>Example:</p> <pre><code># Get the vocabulary dictionary\nvocab_dict = tokenizer.get_dictionary()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#412-getting-character-set","title":"4.12. Getting Character Set","text":"<p>You can retrieve the character set used by the vocabulary using the <code>charset</code> method.</p> <p>Example:</p> <pre><code># Get the character set used by the vocabulary\ncharset = tokenizer.charset()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#413-getting-normalization","title":"4.13. Getting Normalization","text":"<p>TokenMonster allows you to access the normalization settings applied to the vocabulary using the <code>normalization</code> method.</p> <p>Example:</p> <pre><code># Get the normalization settings of the vocabulary\nnormalization = tokenizer.normalization()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#414-getting-capcode-level","title":"4.14. Getting Capcode Level","text":"<p>The <code>capcode</code> method returns the capcode level of the vocabulary.</p> <p>Example:</p> <pre><code># Get the capcode level of the vocabulary\ncapcode_level = tokenizer.capcode()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#415-getting-optimization-mode","title":"4.15. Getting Optimization Mode","text":"<p>You can retrieve the optimization mode used for TokenMonster using the <code>mode</code> method.</p> <p>Example:</p> <pre><code># Get the optimization mode of TokenMonster\noptimization_mode = tokenizer.mode()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#416-mapping-token-id-to-token-string","title":"4.16. Mapping Token ID to Token String","text":"<p>Given a token ID, you can use the <code>id_to_token</code> method to get the token string in its capcode-encoded form.</p> <p>Example:</p> <pre><code># Get the token string from a token ID (capcode-encoded)\ntoken_id = 42\ntoken_string = tokenizer.id_to_token(token_id)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#417-mapping-token-id-to-token-string-decoded","title":"4.17. Mapping Token ID to Token String (Decoded)","text":"<p>The <code>id_to_token_decoded</code> method is used to get the token string from a token ID in its capcode-decoded form.</p> <p>Example:</p> <pre><code># Get the token string from a token ID (capcode-decoded)\ntoken_id = 42\ndecoded_token_string = tokenizer.id_to_token_decoded(token_id)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#418-mapping-token-string-to-token-id","title":"4.18. Mapping Token String to Token ID","text":"<p>You can obtain the token ID of a given token string using the <code>token_to_id</code> method.</p> <p>Example:</p> <pre><code># Get the token ID from a token string\ntoken_string = \"apple\"\ntoken_id = tokenizer.token_to_id(token_string)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#419-modifying-vocabulary","title":"4.19. Modifying Vocabulary","text":"<p>TokenMonster provides methods to modify the vocabulary. You can add special tokens, add regular tokens, delete tokens, resize the vocabulary, and enable or disable the UNK token.</p> <p>Example:</p> <pre><code># Example of modifying the vocabulary\n# Add a special token\ntokenizer.modify(add\n\n_special_tokens=\"[_START_]\", add_regular_tokens=None, delete_tokens=None, resize=None, change_unk=None)\n\n# Delete a regular token\ntokenizer.modify(add_special_tokens=None, add_regular_tokens=None, delete_tokens=[\"apple\"], resize=None, change_unk=None)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#420-adding-regular-tokens","title":"4.20. Adding Regular Tokens","text":"<p>You can add one or more regular tokens to the vocabulary using the <code>add_token</code> method.</p> <p>Example:</p> <pre><code># Add regular tokens to the vocabulary\ntokenizer.add_token([\"apple\", \"banana\", \"cherry\"])\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#421-deleting-tokens","title":"4.21. Deleting Tokens","text":"<p>To delete one or more regular or special tokens from the vocabulary, use the <code>delete_token</code> method.</p> <p>Example:</p> <pre><code># Delete tokens from the vocabulary\ntokenizer.delete_token([\"apple\", \"[PAD]\"])\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#422-deleting-tokens-by-id","title":"4.22. Deleting Tokens by ID","text":"<p>You can delete one or more regular or special tokens by specifying their token IDs using the <code>delete_token_by_id</code> method.</p> <p>Example:</p> <pre><code># Delete tokens by ID\ntokenizer.delete_token_by_id([42, 0])\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#423-adding-special-tokens","title":"4.23. Adding Special Tokens","text":"<p>Special tokens play specific roles in tokenization. You can add one or more special tokens to the vocabulary using the <code>add_special_token</code> method.</p> <p>Example:</p> <pre><code># Add special tokens to the vocabulary\ntokenizer.add_special_token([\"[_START_]\", \"[_END_]\"])\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#424-resizing-vocabulary","title":"4.24. Resizing Vocabulary","text":"<p>To change the size of the vocabulary, you can use the <code>resize</code> method. This allows you to specify the desired size of the vocabulary.</p> <p>Example:</p> <pre><code># Resize the vocabulary to a specific size\ntokenizer.resize(10000)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#425-resetting-token-ids","title":"4.25. Resetting Token IDs","text":"<p>The <code>reset_token_ids</code> method resets the token IDs to be sequential beginning from zero.</p> <p>Example:</p> <pre><code># Reset token IDs\ntokenizer.reset_token_ids()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#426-enabling-unk-token","title":"4.26. Enabling UNK Token","text":"<p>You can enable the UNK (unknown) token in the vocabulary using the <code>enable_unk_token</code> method.</p> <p>Example:</p> <pre><code># Enable the UNK token\ntokenizer.enable_unk_token()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#427-disabling-unk-token","title":"4.27. Disabling UNK Token","text":"<p>The <code>disable_unk_token</code> method allows you to disable the UNK (unknown) token in the vocabulary.</p> <p>Example:</p> <pre><code># Disable the UNK token\ntokenizer.disable_unk_token()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#428-disconnecting-tokenmonster","title":"4.28. Disconnecting TokenMonster","text":"<p>To gracefully disconnect from the TokenMonster server, use the <code>disconnect</code> method.</p> <p>Example:</p> <pre><code># Disconnect from TokenMonster server\ntokenizer.disconnect()\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#429-serializing-tokens","title":"4.29. Serializing Tokens","text":"<p>TokenMonster provides the <code>serialize_tokens</code> method to serialize tokens from a list of integers or a numpy array into a binary string.</p> <p>Example:</p> <pre><code># Serialize tokens\ntoken_ids = [1, 2, 3]\nserialized_tokens = tokenizer.serialize_tokens(token_ids)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#430-deserializing-tokens","title":"4.30. Deserializing Tokens","text":"<p>You can use the <code>deserialize_tokens</code> method to deserialize a binary string into a numpy array of token IDs.</p> <p>Example:</p> <pre><code># Deserialize tokens\nbinary_string = b\"\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00\"\ndeserialized_tokens = tokenizer.deserialize_tokens(binary_string)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#5-additional-information","title":"5. Additional Information","text":""},{"location":"zeta/tokenizers/token_monster/#51-multiprocessing-safety","title":"5.1. Multiprocessing Safety","text":"<p>TokenMonster provides a <code>load_multiprocess_safe</code> method that is safe for multiprocessing. When using this method, vocabulary modification is disabled, and tokenization may be slightly slower compared to the regular <code>load</code> method.</p>"},{"location":"zeta/tokenizers/token_monster/#52-supported-character-sets","title":"5.2. Supported Character Sets","text":"<p>TokenMonster supports two character sets: \"UTF-8\" and \"None.\" You can check the character set used by the vocabulary using the <code>charset</code> method.</p>"},{"location":"zeta/tokenizers/token_monster/#53-supported-normalization-options","title":"5.3. Supported Normalization Options","text":"<p>The vocabulary can have various normalization options applied, including \"None,\" \"NFD,\" \"Lowercase,\" \"Accents,\" \"Quotemarks,\" \"Collapse,\" \"Trim,\" \"LeadingSpace,\" and \"UnixLines.\" You can access the normalization setting using the <code>normalization</code> method.</p>"},{"location":"zeta/tokenizers/token_monster/#54-capcode-levels","title":"5.4. Capcode Levels","text":"<p>The capcode level of the vocabulary can be set to values between 0 and 2 using the <code>capcode</code> method. Capcoding is a way to encode multiple tokens using a single token, which can save memory.</p>"},{"location":"zeta/tokenizers/token_monster/#55-optimization-modes","title":"5.5. Optimization Modes","text":"<p>TokenMonster supports optimization modes from 0 to 5, which affect the memory usage and performance of the library. You can check the optimization mode using the <code>mode</code> method.</p>"},{"location":"zeta/tokenizers/token_monster/#6-examples","title":"6. Examples","text":"<p>Let's explore some examples of how to use TokenMonster for tokenization and vocabulary management.</p>"},{"location":"zeta/tokenizers/token_monster/#example-1-tokenizing-text","title":"Example 1: Tokenizing Text","text":"<pre><code>from zeta.tokenizers import TokenMonster\n\n# Initialize TokenMonster with a vocabulary file\ntokenizer = TokenMonster(\"path/to/vocabulary\")\n\n# Tokenize a text string\ntext = \"Hello, world!\"\ntoken_ids = tokenizer.tokenize(text)\nprint(token_ids)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#example-2-decoding-tokens","title":"Example 2: Decoding Tokens","text":"<pre><code>from zeta.tokenizers import TokenMonster\n\n# Initialize TokenMonster with a vocabulary file\ntokenizer = TokenMonster(\"path/to/vocabulary\")\n\n# Decode token IDs into a string\ndecoded_text = tokenizer.decode([1, 2, 3])\nprint(decoded_text)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#example-3-modifying-vocabulary","title":"Example 3: Modifying Vocabulary","text":"<pre><code>from zeta.tokenizers import TokenMonster\n\n# Initialize TokenMonster with a vocabulary file\ntokenizer = TokenMonster(\"path/to/vocabulary\")\n\n# Add a special token\ntokenizer.modify(\n    add_special_tokens=\"[_START_]\",\n    add_regular_tokens=None,\n    delete_tokens=None,\n    resize=None,\n    change_unk=None,\n)\n\n# Delete a regular token\ntokenizer.modify(\n    add_special_tokens=None,\n    add_regular_tokens=None,\n    delete_tokens=[\"apple\"],\n    resize=None,\n    change_unk=None,\n)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#example-4-exporting-vocabulary-to-yaml","title":"Example 4: Exporting Vocabulary to YAML","text":"<pre><code>from zeta.tokenizers import TokenMonster\n\n# Initialize TokenMonster with a vocabulary file\ntokenizer = TokenMonster(\"path/to/vocabulary\")\n\n# Export the vocabulary to a YAML file\nyaml_data = tokenizer.export_yaml()\nwith open(\"vocabulary.yaml\", \"wb\") as file:\n    file.write(yaml_data)\n</code></pre>"},{"location":"zeta/tokenizers/token_monster/#7-conclusion","title":"7. Conclusion","text":"<p>TokenMonster is a powerful Python module for tokenization and vocabulary management. Whether you're working on natural language processing tasks or need to create custom tokenization pipelines, TokenMonster provides the flexibility and functionality to handle tokenization efficiently. Use the examples and methods provided in this guide to leverage TokenMonster for your projects.</p>"},{"location":"zeta/training/fsdp/","title":"<code>fsdp</code> Documentation","text":""},{"location":"zeta/training/fsdp/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Function: <code>fsdp</code></li> <li>Initialization</li> <li>Parameters</li> <li>Mixed Precision Modes</li> <li>Sharding Strategies</li> <li>Usage Examples</li> <li>Basic FSDP Wrapper</li> <li>Automatic Layer Wrapping</li> <li>Advanced Configuration</li> <li>Additional Information</li> <li>FullyShardedDataParallel (FSDP)</li> <li>Mixed Precision Training</li> <li>Model Sharding</li> <li>References</li> </ol>"},{"location":"zeta/training/fsdp/#1-introduction","title":"1. Introduction","text":"<p>Welcome to the documentation for the Zeta library! Zeta provides a powerful utility function, <code>fsdp</code>, that wraps a given PyTorch model with the FullyShardedDataParallel (FSDP) wrapper. This enables efficient data parallelism and model sharding for deep learning applications.</p>"},{"location":"zeta/training/fsdp/#key-features","title":"Key Features","text":"<ul> <li>Efficient Data Parallelism: FSDP allows you to efficiently parallelize training across multiple GPUs.</li> <li>Mixed Precision Training: Choose between BFloat16 (bf16), Float16 (fp16), or Float32 (fp32) precision modes.</li> <li>Model Sharding: Apply gradient sharding, full model sharding, or no sharding based on your needs.</li> </ul> <p>In this documentation, you will learn how to use the <code>fsdp</code> function effectively, understand its architecture, and explore examples of its applications.</p>"},{"location":"zeta/training/fsdp/#2-function-fsdp","title":"2. Function: <code>fsdp</code>","text":"<p>The <code>fsdp</code> function is the core component of the Zeta library, providing a straightforward way to wrap your PyTorch model with FSDP for efficient distributed training.</p>"},{"location":"zeta/training/fsdp/#initialization","title":"Initialization","text":"<pre><code>model = fsdp(\n    model, auto_wrap=False, mp=\"fp32\", shard_strat=\"NO_SHARD\", TransformerBlock=None\n)\n</code></pre>"},{"location":"zeta/training/fsdp/#parameters","title":"Parameters","text":"<ul> <li><code>model</code> (torch.nn.Module): The original PyTorch model to be wrapped with FSDP.</li> <li><code>auto_wrap</code> (bool, optional): If True, enables automatic wrapping of the model's layers based on the <code>transformer_auto_wrap_policy</code>. Default is False.</li> <li><code>mp</code> (str, optional): The mixed precision mode to be used. Can be 'bf16' for BFloat16, 'fp16' for Float16, or 'fp32' for Float32 precision. Default is 'fp32'.</li> <li><code>shard_strat</code> (str, optional): The sharding strategy to be used. Can be 'SHARD_GRAD' for sharding at gradient computation, 'FULL_SHARD' for full model sharding, or 'NO_SHARD' for no sharding. Default is 'NO_SHARD'.</li> <li><code>TransformerBlock</code> (Type, optional): A custom transformer layer type. Only used if <code>auto_wrap</code> is True.</li> </ul>"},{"location":"zeta/training/fsdp/#mixed-precision-modes","title":"Mixed Precision Modes","text":"<ul> <li><code>bf16</code> (BFloat16): Lower precision for faster training with minimal loss in accuracy.</li> <li><code>fp16</code> (Float16): Higher precision than BFloat16 but still faster than full precision.</li> <li><code>fp32</code> (Float32): Full single-precision floating-point precision.</li> </ul>"},{"location":"zeta/training/fsdp/#sharding-strategies","title":"Sharding Strategies","text":"<ul> <li><code>SHARD_GRAD</code> (Sharding at Gradient Computation): Shards gradients during the backward pass.</li> <li><code>FULL_SHARD</code> (Full Model Sharding): Shards the entire model for parallelism.</li> <li><code>NO_SHARD</code> (No Sharding): No sharding, suitable for single-GPU training.</li> </ul>"},{"location":"zeta/training/fsdp/#3-usage-examples","title":"3. Usage Examples","text":"<p>Now, let's explore practical examples of using the <code>fsdp</code> function in various scenarios.</p>"},{"location":"zeta/training/fsdp/#basic-fsdp-wrapper","title":"Basic FSDP Wrapper","text":"<pre><code>import torch.nn as nn\n\n# Define your PyTorch model\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10),\n)\n\n# Wrap the model with FSDP using default settings (no sharding, fp32 precision)\nfsdp_model = fsdp(model)\n</code></pre>"},{"location":"zeta/training/fsdp/#automatic-layer-wrapping","title":"Automatic Layer Wrapping","text":"<pre><code>import torch.nn as nn\n\n\n# Define a custom transformer layer type\nclass TransformerBlock(nn.Module):\n    def __init__(self):\n        # Define your custom transformer layer here\n        pass\n\n\n# Define your PyTorch model with transformer layers\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    TransformerBlock(),\n    nn.Linear(256, 10),\n)\n\n# Wrap the model with FSDP and enable automatic layer wrapping\nfsdp_model = fsdp(model, auto_wrap=True, TransformerBlock=TransformerBlock)\n</code></pre>"},{"location":"zeta/training/fsdp/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>import torch.nn as nn\n\n# Define your PyTorch model\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10),\n)\n\n# Wrap the model with FSDP with custom settings (full model sharding, bf16 precision)\nfsdp_model = fsdp(model, mp=\"bf16\", shard_strat=\"FULL_SHARD\")\n</code></pre> <p>These examples demonstrate how to use the <code>fsdp</code> function to wrap your PyTorch models with FSDP for distributed training with various configurations.</p>"},{"location":"zeta/training/fsdp/#4-additional-information","title":"4. Additional Information","text":""},{"location":"zeta/training/fsdp/#fullyshardeddataparallel-fsdp","title":"FullyShardedDataParallel (FSDP)","text":"<p>FSDP is a powerful wrapper that enables efficient data parallelism and model sharding. It optimizes gradient communication and memory usage during distributed training.</p>"},{"location":"zeta/training/fsdp/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Mixed precision training involves using lower-precision data types for certain parts of the training pipeline, leading to faster training times with minimal loss in accuracy.</p>"},{"location":"zeta/training/fsdp/#model-sharding","title":"Model Sharding","text":"<p>Model sharding is a technique used to distribute model parameters across multiple devices or GPUs, improving training speed and memory efficiency.</p>"},{"location":"zeta/training/fsdp/#5-references","title":"5. References","text":"<p>For further information and research papers related to FSDP, mixed precision training, and model sharding, please refer to the following resources:</p> <ul> <li>FSDP Documentation</li> <li>Mixed Precision Training in Deep Learning</li> <li>Efficient Model Parallelism for Deep Learning</li> </ul> <p>Explore these references to gain a deeper understanding of the techniques and concepts implemented in the Zeta library and the <code>fsdp</code> function.</p> <p>Feel free to reach out to</p> <p>the Zeta community for any questions or discussions regarding this library. Happy deep learning!</p>"},{"location":"zeta/training/nebula/","title":"Nebula","text":"<p>The <code>Nebula</code> class is a custom loss function class that dynamically determines the most suitable loss function for a given dataset based on certain characteristics of the dataset, such as sparsity, correlation, range of values, and user input. It is part of the <code>zeta</code> library and is built upon PyTorch's LossFunction class.</p>"},{"location":"zeta/training/nebula/#introduction","title":"Introduction","text":"<p>The purpose of the <code>Nebula</code> class is to help determine and cache the most suitable loss function for a given dataset without requiring the user to manually select one. This can be particularly useful in scenarios where the user is unsure of the most appropriate loss function to use or in automated systems where the type of problem (classification or regression) is not known a priori.</p> <p>The <code>Nebula</code> class considers various characteristics of the data, such as whether the target values are integers, the sparsity of the target values, the correlation between predictions and target values, and any user or domain knowledge provided, to determine whether the problem is a classification or regression problem and subsequently select an appropriate loss function.</p>"},{"location":"zeta/training/nebula/#class-definition","title":"Class Definition","text":"<pre><code>class Nebula(LossFunction):\n    def __init__(self, domain_knowledge=None, user_input=None): ...\n</code></pre>"},{"location":"zeta/training/nebula/#parameters","title":"Parameters","text":"<ul> <li><code>domain_knowledge</code> (str, optional): Domain knowledge about the problem. It can be either \"classification\" or \"regression\". Default is <code>None</code>.</li> <li><code>user_input</code> (str, optional): User input about the problem type. It can be either \"classification\" or \"regression\". Default is <code>None</code>.</li> </ul>"},{"location":"zeta/training/nebula/#attributes","title":"Attributes","text":"<ul> <li><code>loss_function</code>: The determined loss function.</li> <li><code>domain_knowledge</code>: Domain knowledge provided during initialization.</li> <li><code>user_input</code>: User input provided during initialization.</li> <li><code>loss_function_cache</code>: A cache for storing the determined loss function for a dataset.</li> <li><code>unique_values_cache</code>: A cache for storing the unique values in the target variable <code>y_true</code>.</li> <li><code>class_balance_cache</code>: A cache for storing the class balance in the target variable <code>y_true</code>.</li> <li><code>logger</code>: A logger for logging information during the determination of the loss function.</li> </ul>"},{"location":"zeta/training/nebula/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>Nebula</code> class is used to dynamically determine the most suitable loss function for a given dataset and cache the determined loss function for future use. The class analyzes the unique values, class balance, sparsity, and correlation of the target variable <code>y_true</code> and the predicted variable <code>y_pred</code> to determine whether the problem is a classification or regression problem and select an appropriate loss function.</p>"},{"location":"zeta/training/nebula/#method-determine_loss_function","title":"Method: <code>determine_loss_function</code>","text":"<pre><code>def determine_loss_function(self, y_pred, y_true): ...\n</code></pre> <p>This method determines the most suitable loss function based on the characteristics of <code>y_pred</code> and <code>y_true</code>.</p>"},{"location":"zeta/training/nebula/#parameters_1","title":"Parameters","text":"<ul> <li><code>y_pred</code> (Tensor): The predicted values.</li> <li><code>y_true</code> (Tensor): The ground truth values.</li> </ul>"},{"location":"zeta/training/nebula/#method-__call__","title":"Method: <code>__call__</code>","text":"<pre><code>def __call__(self, y_pred, y_true): ...\n</code></pre> <p>This method computes the loss using the determined loss function.</p>"},{"location":"zeta/training/nebula/#parameters_2","title":"Parameters","text":"<ul> <li><code>y_pred</code> (Tensor): The predicted values.</li> <li><code>y_true</code> (Tensor): The ground truth values.</li> </ul>"},{"location":"zeta/training/nebula/#returns","title":"Returns","text":"<ul> <li><code>Tensor</code>: The computed loss.</li> </ul>"},{"location":"zeta/training/nebula/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/training/nebula/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch\n\nfrom zeta import Nebula\n\n# Initialize Nebula\nnebula = Nebula()\n\n# Generate some example data\ny_pred = torch.randn(10, 5)\ny_true = torch.randint(0, 5, (10,))\n\n# Compute the loss\nloss = nebula(y_pred, y_true)\n\nprint(loss)\n</code></pre>"},{"location":"zeta/training/nebula/#example-2-providing-domain-knowledge","title":"Example 2: Providing Domain Knowledge","text":"<pre><code>import torch\n\nfrom zeta import Nebula\n\n# Initialize Nebula with domain knowledge\nnebula = Nebula(domain_knowledge=\"classification\")\n\n# Generate some example data\ny_pred = torch.randn(10, 5)\ny_true = torch.randint(0, 5, (10,))\n\n# Compute the loss\nloss = nebula(y_pred, y_true)\n\nprint(loss)\n</code></pre>"},{"location":"zeta/training/nebula/#example-3-providing-user-input","title":"Example 3: Providing User Input","text":"<pre><code>import torch\n\nfrom zeta import Nebula\n\n# Initialize Nebula with user input\nnebula = Nebula(user_input=\"regression\")\n\n# Generate some example data\ny_pred = torch.randn(10, 1)\ny_true = torch.randn(10, 1)\n\n# Compute the loss\nloss = nebula(y_pred, y_true)\n\nprint(loss)\n</code></pre>"},{"location":"zeta/training/nebula/#mathematical-formula","title":"Mathematical Formula","text":"<p>The <code>Nebula</code> class does not have a specific mathematical formula as it dynamically determines the most suitable loss function based on the characteristics of the data. However, the determined loss function will have its own mathematical formula, which can be found in the PyTorch documentation or the <code>zeta</code> library documentation.</p>"},{"location":"zeta/training/nebula/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>The <code>Nebula</code> class caches the determined loss function, unique values, and class balance for a given dataset to avoid recomputing them in the future.</li> <li>If both <code>domain_knowledge</code> and <code>user_input</code> are provided, <code>domain_knowledge</code> will take precedence over <code>user_input</code>.</li> <li>The <code>Nebula</code> class uses the <code>logging</code> module to log information during the determination of the loss function. You can customize the logging settings by modifying the <code>logger</code> attribute.</li> </ul>"},{"location":"zeta/training/parallel_wrapper/","title":"<code>ParallelWrapper</code>","text":"<p>===============</p> <p>The\u00a0<code>ParallelWrapper</code>\u00a0class is a simple wrapper designed to facilitate the use of data parallelism in PyTorch. It is particularly suited for transformer architectures. The class wraps a given neural network model and allows it to be moved to a specified device. If data parallelism is enabled, the model is wrapped in PyTorch's\u00a0<code>nn.DataParallel</code>\u00a0class, which splits the input across the specified device's GPUs and parallelizes the forward pass.</p>"},{"location":"zeta/training/parallel_wrapper/#class-definition","title":"Class Definition","text":"<pre><code>class ParallelWrapper:\n    def __init__(self, model: nn.Module, device: str = \"cuda\", use_data_parallel: bool = True):\n        pass\n\n    def forward(self, *args, **kwargs):\n        pass\n\n    def to(self, device: str):\n        pass\n\n    def __getattr__(self, name: str):\n        pass\n</code></pre>"},{"location":"zeta/training/parallel_wrapper/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>model</code> <code>nn.Module</code> - The neural network model to be parallelized. <code>device</code> <code>str</code> <code>\"cuda\"</code> The device to which the model should be moved. <code>use_data_parallel</code> <code>bool</code> <code>True</code> A flag to indicate whether to use data parallelism or not."},{"location":"zeta/training/parallel_wrapper/#methods","title":"Methods","text":""},{"location":"zeta/training/parallel_wrapper/#__init__self-model-devicecuda-use_data_paralleltrue","title":"<code>__init__(self, model, device=\"cuda\", use_data_parallel=True)</code>","text":"<p>The constructor for the\u00a0<code>ParallelWrapper</code>\u00a0class. It initializes the instance and moves the model to the specified device. If data parallelism is enabled and more than one GPU is available, it wraps the model in\u00a0<code>nn.DataParallel</code>.</p>"},{"location":"zeta/training/parallel_wrapper/#forwardself-args-kwargs","title":"<code>forward(self, *args, **kwargs)</code>","text":"<p>The forward method for the\u00a0<code>ParallelWrapper</code>\u00a0class. It simply calls the forward method of the wrapped model with the provided arguments and keyword arguments.</p>"},{"location":"zeta/training/parallel_wrapper/#toself-device","title":"<code>to(self, device)</code>","text":"<p>This method moves the model to the specified device and updates the\u00a0<code>device</code>\u00a0attribute of the instance.</p>"},{"location":"zeta/training/parallel_wrapper/#__getattr__self-name","title":"<code>__getattr__(self, name)</code>","text":"<p>This method redirects attribute access to the internal model to allow direct access to its methods and properties.</p>"},{"location":"zeta/training/parallel_wrapper/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/training/parallel_wrapper/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.training import ParallelWrapper\n\n# Define a model\nmodel = nn.Linear(512, 512)\n\n# Wrap the model\nmodel = ParallelWrapper(model)\n\n# Now you can use the model as usual\ninput = torch.randn(128, 512)\noutput = model(input)\n</code></pre>"},{"location":"zeta/training/parallel_wrapper/#example-2-using-a-different-device","title":"Example 2: Using a Different Device","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.training import ParallelWrapper\n\n# Define a model\nmodel = nn.Linear(512, 512)\n\n# Wrap the model and move it to CPU\nmodel = ParallelWrapper(model, device=\"cpu\")\n\n# Now you can use the model as usual\ninput = torch.randn(128, 512)\noutput = model(input)\n</code></pre>"},{"location":"zeta/training/parallel_wrapper/#example-3-disabling-data-parallelism","title":"Example 3: Disabling Data Parallelism","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.training import ParallelWrapper\n\n# Define a model\nmodel = nn.Linear(512, 512)\n\n# Wrap the model and disable data parallelism\nmodel = ParallelWrapper(model, use_data_parallel=False)\n\n# Now you can use the model as usual\ninput = torch.randn(128, 512)\noutput = model(input)\n</code></pre>"},{"location":"zeta/training/parallel_wrapper/#note","title":"Note","text":"<p>The\u00a0<code>ParallelWrapper</code>\u00a0class is a utility that simplifies the use of data parallelism in PyTorch. It does not provide any additional functionality beyond what is already provided by PyTorch's\u00a0<code>nn.DataParallel</code>\u00a0class. It is intended to be used as a convenience wrapper to reduce boilerplate code.</p>"},{"location":"zeta/training/train/","title":"Documentation for <code>Trainer</code> Module from Zeta Library","text":""},{"location":"zeta/training/train/#introduction","title":"Introduction","text":"<p>The <code>Trainer</code> module from the Zeta library provides an easy-to-use, flexible, and scalable approach to training deep learning models. By abstracting away many of the lower-level details of training, including distributed training, gradient accumulation, and model checkpointing, <code>Trainer</code> allows developers to focus on the high-level aspects of model development and experimentation.</p> <p>This module also integrates seamlessly with the HuggingFace <code>Accelerator</code> to enable mixed precision training, GPU acceleration, and distributed training across multiple nodes or GPUs.</p>"},{"location":"zeta/training/train/#trainer-class-definition","title":"<code>Trainer</code> Class Definition","text":"<pre><code>def Trainer(\n        gradient_accumulate_every: int = None, \n        batch_size: int = None, \n        seq_len: int = None,\n        entity_name: str = None,\n        model = None,\n        use_fsdp: bool = False,\n        use_activation_checkpointing: bool = False,\n        learning_rate = None,\n        seed = None,\n        use_pretokenized: bool = False,\n        resume_from_checkpoint = None,\n        checkpointing_steps = None,\n        output_dir = None,\n        weight_decay = None,\n        use_deepspeed = None\n    ):\n</code></pre>"},{"location":"zeta/training/train/#parameters","title":"Parameters","text":"<ul> <li><code>gradient_accumulate_every</code> (<code>int</code>, optional): Specifies how often to accumulate gradients. Default: <code>None</code>.</li> <li><code>batch_size</code> (<code>int</code>, optional): Specifies the batch size for training. Default: <code>None</code>.</li> <li><code>seq_len</code> (<code>int</code>, optional): Sequence length for model inputs. Default: <code>None</code>.</li> <li><code>entity_name</code> (<code>str</code>, optional): Name of the entity for logging purposes. Default: <code>None</code>.</li> <li><code>model</code>: The model to train. No default value.</li> <li><code>use_fsdp</code> (<code>bool</code>, optional): Whether or not to use Fully Sharded Data Parallelism (FSDP). Default: <code>False</code>.</li> <li><code>use_activation_checkpointing</code> (<code>bool</code>, optional): Use activation checkpointing to save memory during training. Default: <code>False</code>.</li> <li><code>learning_rate</code>: The learning rate for training. No default value.</li> <li><code>seed</code>: Random seed for reproducibility. No default value.</li> <li><code>use_pretokenized</code> (<code>bool</code>, optional): Whether to use pre-tokenized data. Default: <code>False</code>.</li> <li><code>resume_from_checkpoint</code>: Path to a checkpoint to resume training from. Default: <code>None</code>.</li> <li><code>checkpointing_steps</code>: How often to save model checkpoints. Default: <code>None</code>.</li> <li><code>output_dir</code>: Directory to save final trained model and checkpoints. Default: <code>None</code>.</li> <li><code>weight_decay</code>: Weight decay value for regularization. No default value.</li> <li><code>use_deepspeed</code>: Whether to use deepspeed for training optimization. Default: <code>None</code>.</li> </ul>"},{"location":"zeta/training/train/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The primary function of the <code>Trainer</code> module is to handle the training process, including data loading, optimization, and model updates. It leverages HuggingFace's <code>Accelerator</code> to provide accelerated training on GPUs and distributed environments.</p> <p>Here are the primary steps:</p> <ol> <li>Initialization of the <code>Accelerator</code> for GPU training and gradient accumulation.</li> <li>Model and optimizer initialization.</li> <li>Loading datasets and setting up data loaders.</li> <li>Training loop with gradient accumulation and model checkpointing.</li> <li>Save the final trained model.</li> </ol>"},{"location":"zeta/training/train/#code-examples","title":"Code Examples","text":"<p>1. Basic Usage</p> <pre><code>from zeta import Trainer\n\nmodel = ...  # Your model definition here\nTrainer(\n    gradient_accumulate_every=2,\n    batch_size=32,\n    seq_len=128,\n    model=model,\n    learning_rate=0.001,\n    seed=42,\n    output_dir=\"./models/\",\n)\n</code></pre> <p>2. Resuming Training from a Checkpoint</p> <pre><code>from zeta import Trainer\n\nmodel = ...  # Your model definition here\nTrainer(\n    gradient_accumulate_every=2,\n    batch_size=32,\n    seq_len=128,\n    model=model,\n    learning_rate=0.001,\n    seed=42,\n    resume_from_checkpoint=\"./models/checkpoint.pt\",\n    output_dir=\"./models/\",\n)\n</code></pre> <p>3. Using FSDP and Activation Checkpointing</p> <pre><code>from zeta import Trainer\n\nmodel = ...  # Your model definition here\nTrainer(\n    gradient_accumulate_every=2,\n    batch_size=32,\n    seq_len=128,\n    model=model,\n    use_fsdp=True,\n    use_activation_checkpointing=True,\n    learning_rate=0.001,\n    seed=42,\n    output_dir=\"./models/\",\n)\n</code></pre>"},{"location":"zeta/training/train/#mathematical-description","title":"Mathematical Description","text":"<p>Given a dataset ( D ) consisting of data points ( { (x_1, y_1), (x_2, y_2), ... (x_N, y_N) } ), the trainer aims to minimize the loss function ( L ) with respect to model parameters ( \\theta ):</p> <p>[ \\theta^* = \\arg\\min_{\\theta} \\frac{1}{N} \\sum_{i=1}^{N} L(f(x_i; \\theta), y_i) ]</p> <p>where ( f ) is the model's prediction function.</p>"},{"location":"zeta/training/train/#conclusions","title":"Conclusions","text":"<p>The <code>Trainer</code> module from Zeta library streamlines the training process by abstracting away many complexities, making it a valuable tool for developers at all experience levels. Whether you are training a simple model or a complex architecture in a distributed environment, the <code>Trainer</code> module offers the flexibility and ease-of-use to get your models trained efficiently.</p>"},{"location":"zeta/training/optimizers/decoupled_lion/","title":"DecoupledLionW Optimizer","text":""},{"location":"zeta/training/optimizers/decoupled_lion/#overview-and-introduction","title":"Overview and Introduction","text":"<p><code>DecoupledLionW</code> is a PyTorch optimizer designed to improve training performance and convergence for deep learning models. It is an extension of the Lion optimizer, which incorporates decoupled weight decay and a momentum-based update rule. </p> <p>The optimizer utilizes the Adam-like update rule, where the weight decay is applied separately from the gradient update. This is crucial as it helps prevent overfitting, improves generalization, and aids faster convergence and smoother optimization.</p>"},{"location":"zeta/training/optimizers/decoupled_lion/#key-concepts","title":"Key Concepts:","text":"<ul> <li>Weight Decay: Reduces the magnitude of the model's weights, preventing overfitting and improving generalization.</li> <li>Momentum Update: An interpolation between the current gradient and the previous momentum state, allowing for faster convergence and smoother optimization.</li> <li>Momentum Decay: Gradually reduces the momentum term over time, preventing it from becoming too large and destabilizing the optimization process.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#class-definition","title":"Class Definition","text":"<pre><code>class DecoupledLionW(Optimizer):\n    def __init__(\n            self,\n            params,\n            lr: float = 1e-4,\n            betas: Tuple[float, float] = (0.9, 0.99),\n            weight_decay: float = 0.0,\n    ):\n</code></pre>"},{"location":"zeta/training/optimizers/decoupled_lion/#parameters","title":"Parameters","text":"<ul> <li><code>params</code> (iterable): Iterable of parameters to optimize or dictionaries defining parameter groups.</li> <li><code>lr</code> (float, optional): Learning rate. Default: 1e-4.</li> <li><code>betas</code> (Tuple[float, float], optional): Coefficients used for computing running averages of gradient and its square. Default: (0.9, 0.99).</li> <li><code>weight_decay</code> (float, optional): Weight decay (L2 penalty). Default: 0.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#attributes","title":"Attributes","text":"<ul> <li><code>metric_functions</code>: A dictionary of lambda functions to compute various metrics like L2 norm of moments, parameters, updates, and gradients, as well as cosine similarity between updates and gradients.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#functionality-and-usage","title":"Functionality and Usage","text":""},{"location":"zeta/training/optimizers/decoupled_lion/#lionw-method","title":"<code>lionw</code> Method","text":"<p>This static method is responsible for applying the weight decay, momentum update, and momentum decay.</p> <pre><code>@staticmethod\ndef lionw(p, grad, exp_avg, lr, initial_lr, wd, beta1, beta2) -&gt; None:\n</code></pre>"},{"location":"zeta/training/optimizers/decoupled_lion/#parameters_1","title":"Parameters","text":"<ul> <li><code>p</code> (Tensor): Parameter tensor.</li> <li><code>grad</code> (Tensor): Gradient tensor.</li> <li><code>exp_avg</code> (Tensor): Exponential moving average of gradient values.</li> <li><code>lr</code> (float): Learning rate.</li> <li><code>initial_lr</code> (float): Initial learning rate.</li> <li><code>wd</code> (float): Weight decay.</li> <li><code>beta1</code> (float): Exponential decay rate for the first moment estimates.</li> <li><code>beta2</code> (float): Exponential decay rate for the second moment estimates.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#step-method","title":"<code>step</code> Method","text":"<p>Performs a single optimization step.</p> <pre><code>@torch.no_grad()\ndef step(self, closure: Optional[Callable] = None):\n</code></pre>"},{"location":"zeta/training/optimizers/decoupled_lion/#parameters_2","title":"Parameters","text":"<ul> <li><code>closure</code> (callable, optional): A closure that reevaluates the model and returns the loss.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#returns","title":"Returns","text":"<ul> <li><code>loss</code> (float, optional): The loss value if <code>closure</code> is provided. None otherwise.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#pre_reduce_metrics-method","title":"<code>pre_reduce_metrics</code> Method","text":"<p>This method preprocesses the metrics before reduction across nodes.</p> <pre><code>def pre_reduce_metrics(self, optimizer_metrics):\n</code></pre>"},{"location":"zeta/training/optimizers/decoupled_lion/#parameters_3","title":"Parameters","text":"<ul> <li><code>optimizer_metrics</code> (dict): A dictionary containing the optimizer metrics.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#returns_1","title":"Returns","text":"<ul> <li><code>optimizer_metrics</code> (dict): The pre-processed optimizer metrics.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#report_per_parameter_metrics-method","title":"<code>report_per_parameter_metrics</code> Method","text":"<p>This method reports the per-parameter metrics.</p> <pre><code>def report_per_parameter_metrics(self, param: torch.Tensor, name: str, optimizer_metrics: dict):\n</code></pre>"},{"location":"zeta/training/optimizers/decoupled_lion/#parameters_4","title":"Parameters","text":"<ul> <li><code>param</code> (Tensor): Parameter tensor.</li> <li><code>name</code> (str): Name of the parameter.</li> <li><code>optimizer_metrics</code> (dict): A dictionary containing the optimizer metrics.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#returns_2","title":"Returns","text":"<ul> <li><code>optimizer_metrics</code> (dict): The optimizer metrics with the reported per-parameter metrics.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#usage-examples","title":"Usage Examples","text":"<pre><code>import torch\n\nfrom zeta import x\n\n# Define model parameters\nparams = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n\n# Define optimizer\noptimizer = DecoupledLionW(params, lr=0.1, betas=(0.9, 0.999), weight_decay=0.01)\n\n# Define loss function\nloss_fn = torch.nn.MSELoss()\n\n# Forward pass\noutput = x(params)\ntarget = torch.tensor([0.0, 1.0, 2.0])\nloss = loss_fn(output, target)\n\n# Backward pass\nloss.backward()\n\n# Optimization step\noptimizer.step()\n</code></pre>"},{"location":"zeta/training/optimizers/decoupled_lion/#mathematical-formula","title":"Mathematical Formula","text":"<p>The update rule of the optimizer can be represented by the following formula:</p> <p>[ p = p - \\alpha \\cdot \\text{sign}(\\beta_1 \\cdot m + (1-\\beta_1) \\cdot g) - \\eta \\cdot wd ]</p> <p>Where:</p> <ul> <li>( p ) is the parameter.</li> <li>( \\alpha ) is the learning rate.</li> <li>( \\beta_1 ) is the exponential decay rate for the first moment estimates.</li> <li>( m ) is the momentum (exponential moving average of gradient values).</li> <li>( g ) is the gradient.</li> <li>( \\eta ) is the decay factor.</li> <li>( wd ) is the weight decay.</li> </ul>"},{"location":"zeta/training/optimizers/decoupled_lion/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>A high value of <code>weight_decay</code> can lead to a large reduction in the model's weights on every step. Ensure to use an appropriate value for your specific use case.</li> <li>The optimizer supports both single-node and multi-node distributed training, enabling efficient training on parallel computing environments.</li> </ul>"},{"location":"zeta/training/optimizers/sophia/","title":"SophiaG Optimizer for Zeta Library","text":""},{"location":"zeta/training/optimizers/sophia/#overview","title":"Overview","text":"<p>The SophiaG optimizer is designed to adaptively change learning rates during training, offering a combination of momentum-based acceleration and second-order Hessian-based adaptive learning rates. This optimizer is particularly useful for training deep neural networks and optimizing complex, non-convex loss functions. Key features include:</p> <ol> <li>Momentum: Utilizes exponentially moving averages of gradients.</li> <li>Adaptive Learning Rate: Adjusts the learning rate based on the second-order Hessian information.</li> <li>Regularization: Applies weight decay to avoid overfitting.</li> <li>Optional Settings: Allows for maximizing the loss function, customizable settings for capturable and dynamic parameters.</li> </ol>"},{"location":"zeta/training/optimizers/sophia/#class-definition","title":"Class Definition","text":"<pre><code>class SophiaG(Optimizer):\n    def __init__(self, params, lr=1e-4, betas=(0.965, 0.99), rho=0.04,\n                 weight_decay=1e-1, *, maximize: bool = False,\n                 capturable: bool = False, dynamic: bool = False):\n</code></pre>"},{"location":"zeta/training/optimizers/sophia/#parameters","title":"Parameters:","text":"<ul> <li><code>params</code> (iterable): Iterable of parameters to optimize.</li> <li><code>lr</code> (float, default=1e-4): Learning rate.</li> <li><code>betas</code> (Tuple[float, float], default=(0.965, 0.99)): Coefficients used for computing running averages of gradient and Hessian.</li> <li><code>rho</code> (float, default=0.04): Damping factor for Hessian-based updates.</li> <li><code>weight_decay</code> (float, default=1e-1): Weight decay factor.</li> <li><code>maximize</code> (bool, default=False): Whether to maximize the loss function.</li> <li><code>capturable</code> (bool, default=False): Enable/Disable special capturing features.</li> <li><code>dynamic</code> (bool, default=False): Enable/Disable dynamic adjustments of the optimizer.</li> </ul>"},{"location":"zeta/training/optimizers/sophia/#usage-and-functionality","title":"Usage and Functionality","text":""},{"location":"zeta/training/optimizers/sophia/#1-initialization","title":"1. Initialization","text":"<p>Upon initialization, the optimizer performs validation on its parameters and sets them as the default parameters for parameter groups.</p> <pre><code>from zeta import SophiaG\n\noptimizer = SophiaG(model.parameters(), lr=0.01, betas=(0.9, 0.999), weight_decay=1e-4)\n</code></pre>"},{"location":"zeta/training/optimizers/sophia/#2-step-forward","title":"2. Step Forward","text":"<p>The <code>.step()</code> method updates the model parameters. The function is decorated with <code>@torch.no_grad()</code> to avoid saving any more computation graphs for gradient computation.</p> <pre><code>loss = criterion(output, target)\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"zeta/training/optimizers/sophia/#3-update-hessian-and-exponential-average","title":"3. Update Hessian and Exponential Average","text":"<p>The optimizer has internal methods to update the Hessian and Exponential Moving Average (EMA) of the gradients, controlled by <code>betas</code>.</p>"},{"location":"zeta/training/optimizers/sophia/#4-sophiag-function","title":"4. SophiaG Function","text":"<p>The core SophiaG function updates the parameters based on the gradient (<code>grad</code>), moving average (<code>exp_avg</code>), and Hessian (<code>hessian</code>). It uses the following update formula:</p> <p>[ \\text{param} = \\text{param} - \\text{lr} \\times \\left( \\text{beta}_1 \\times \\text{exp_avg} + \\frac{(1-\\text{beta}_1) \\times \\text{grad}}{( \\text{beta}_2 \\times \\text{hessian} + (1-\\text{beta}_2) )^{\\rho}} \\right) ]</p>"},{"location":"zeta/training/optimizers/sophia/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/training/optimizers/sophia/#1-basic-usage","title":"1. Basic Usage:","text":"<pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta import SophiaG\n\nmodel = nn.Linear(10, 1)\noptimizer = SophiaG(model.parameters(), lr=0.01)\n</code></pre>"},{"location":"zeta/training/optimizers/sophia/#2-customizing-betas-and-learning-rate","title":"2. Customizing Betas and Learning Rate:","text":"<pre><code>import torch\n\nfrom zeta import SophiaG\n\noptimizer = SophiaG(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n</code></pre>"},{"location":"zeta/training/optimizers/sophia/#3-using-with-weight-decay","title":"3. Using with Weight Decay:","text":"<pre><code>from zeta import SophiaG\n\noptimizer = SophiaG(model.parameters(), lr=0.01, weight_decay=1e-4)\n</code></pre>"},{"location":"zeta/training/optimizers/sophia/#additional-information-and-tips","title":"Additional Information and Tips","text":"<ul> <li>Make sure that the parameters passed are compatible with the model you are using.</li> <li>To maximize the loss function (useful in adversarial training), set <code>maximize=True</code>.</li> </ul>"},{"location":"zeta/training/optimizers/sophia/#common-issues","title":"Common Issues","text":"<ul> <li>If sparse gradients are involved, the SophiaG optimizer is not applicable.</li> </ul>"},{"location":"zeta/training/optimizers/sophia/#references-and-resources","title":"References and Resources","text":"<ul> <li>Adaptive Learning Rates</li> <li>Zeta Documentation</li> </ul> <p>For further questions or issues, visit our GitHub repository.</p>"},{"location":"zeta/utils/cast_if_src_dtype/","title":"cast_if_src_dtype","text":""},{"location":"zeta/utils/cast_if_src_dtype/#module-name-cast_if_src_dtype","title":"Module Name: <code>cast_if_src_dtype</code>","text":""},{"location":"zeta/utils/cast_if_src_dtype/#description","title":"Description","text":"<p><code>cast_if_src_dtype</code> is a utility function that checks the data type (<code>dtype</code>) of a given tensor. If the tensor's <code>dtype</code> matches the provided source <code>dtype</code> (<code>src_dtype</code>), the function will cast the tensor to the target <code>dtype</code> (<code>tgt_dtype</code>). After the casting operation, the function returns the updated tensor and a <code>boolean</code> flag indicating whether the tensor data type was updated.</p> <p>This function provides a convenient way to enforce specific data types for torch tensors.</p>"},{"location":"zeta/utils/cast_if_src_dtype/#classfunction-signature-in-pytorch","title":"Class/Function Signature in Pytorch","text":"<pre><code>def cast_if_src_dtype(\n    tensor: torch.Tensor, src_dtype: torch.dtype, tgt_dtype: torch.dtype\n):\n    updated = False\n    if tensor.dtype == src_dtype:\n        tensor = tensor.to(dtype=tgt_dtype)\n        updated = True\n    return tensor, updated\n</code></pre>"},{"location":"zeta/utils/cast_if_src_dtype/#parameters","title":"Parameters","text":"Parameter Type Description <code>tensor</code> <code>torch.Tensor</code> The tensor whose data type is to be checked and potentially updated. <code>src_dtype</code> <code>torch.dtype</code> The source data type that should trigger the casting operation. <code>tgt_dtype</code> <code>torch.dtype</code> The target data type that the <code>tensor</code> will be cast into if the source data type matches its data type."},{"location":"zeta/utils/cast_if_src_dtype/#functionality-and-use","title":"Functionality and Use","text":"<p>Functionality: <code>cast_if_src_dtype</code> takes in three parameters: a tensor, a source data type, and a target data type. If the data type of the tensor equals the source data type, the function casts this tensor to the target data type. The function then returns both the potentially modified tensor and a flag indicating whether the cast was performed.</p> <p>Usage: This utility function is used when certain operations or functions require inputs of a specific data type. A common scenario is when tensors with floating-point data types need to be converted to integers or vice versa.</p>"},{"location":"zeta/utils/cast_if_src_dtype/#usage-examples","title":"Usage Examples","text":"<p>Below are some examples of how the function could be used:</p>"},{"location":"zeta/utils/cast_if_src_dtype/#example-1","title":"Example 1","text":"<pre><code>import torch\n\nfrom zeta.utils import cast_if_src_dtype\n\n# Given: a float tensor\ntensor = torch.tensor([1.0, 2.0, 3.0])\n\n# We want to convert it to integer type tensor if its data type is float32\ntensor, updated = cast_if_src_dtype(tensor, torch.float32, torch.int32)\n\nprint(tensor)  # tensor([1, 2, 3], dtype=torch.int32)\nprint(updated)  # True\n</code></pre>"},{"location":"zeta/utils/cast_if_src_dtype/#example-2","title":"Example 2","text":"<pre><code>import torch\n\nfrom zeta.utils import cast_if_src_dtype\n\n# Given: an integer tensor\ntensor = torch.tensor([1, 2, 3])\n\n# We want to convert it to float type tensor if its data type is int32\ntensor, updated = cast_if_src_dtype(tensor, torch.int32, torch.float32)\n\nprint(tensor)  # tensor([1.0, 2.0, 3.0])\nprint(updated)  # True\n</code></pre>"},{"location":"zeta/utils/cast_if_src_dtype/#example-3","title":"Example 3","text":"<pre><code>import torch\n\nfrom zeta.utils import cast_if_src_dtype\n\n# Given: an integer tensor\ntensor = torch.tensor([1, 2, 3])\n\n# If the data type is not equal to the source data type, the tensor will remain the same\ntensor, updated = cast_if_src_dtype(tensor, torch.float32, torch.int32)\n\nprint(tensor)  # tensor([1, 2, 3])\nprint(updated)  # False\n</code></pre>"},{"location":"zeta/utils/cast_if_src_dtype/#resources-and-references","title":"Resources and References","text":"<p>For more information on tensor operations and data types in PyTorch, refer to the official PyTorch documentation:</p> <ul> <li>PyTorch Tensor Operations</li> <li>PyTorch Data Types</li> </ul>"},{"location":"zeta/utils/cast_if_src_dtype/#note","title":"Note","text":"<p>The <code>cast_if_src_dtype</code> function doesn't modify the original tensor in-place. Instead, it creates a new tensor with the updated data type. Keep that in mind during function calls, and be sure to substitute the original tensor with the returned tensor to reflect the change in the rest of your code.</p>"},{"location":"zeta/utils/cast_tuple/","title":"cast_tuple","text":""},{"location":"zeta/utils/cast_tuple/#zeta-utils-documentation","title":"Zeta Utils Documentation","text":""},{"location":"zeta/utils/cast_tuple/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Installation &amp; Import</li> <li>Function Definitions</li> <li>Usage Examples</li> <li>Additional Information</li> <li>References and Resources</li> </ol>"},{"location":"zeta/utils/cast_tuple/#introduction","title":"Introduction","text":"<p> Zeta Utils is a Python utility module that provides helper functions to facilitate various operations in Python programming. One of the key functions provided in this library is <code>cast_tuple()</code> that is used to cast a value to a tuple of a specific depth. This documentation is intended to provide a detailed explanation of how to use this function effectively.</p>"},{"location":"zeta/utils/cast_tuple/#installation-import","title":"Installation &amp; Import","text":"<p>Zeta Utils is an integral part of the Zeta package. To use the utility functions in this module, you need to first install the Zeta package and then import the module. </p> <pre><code># Installation\npip install zeta\n\n# Import\nfrom zeta import utils\n</code></pre>"},{"location":"zeta/utils/cast_tuple/#function-definitions","title":"Function Definitions","text":""},{"location":"zeta/utils/cast_tuple/#function-cast_tuple","title":"Function: cast_tuple","text":"<pre><code>utils.cast_tuple(val, depth)\n</code></pre> <p>This function is used to cast a value to a tuple of a specific depth.</p>"},{"location":"zeta/utils/cast_tuple/#arguments","title":"Arguments:","text":"Argument Type Description <code>val</code> <code>varies</code> The value to be cast. This can be any type <code>depth</code> <code>int</code> The depth of the tuple, i.e., the number of elements in the tuple to be returned."},{"location":"zeta/utils/cast_tuple/#returns","title":"Returns:","text":"<p><code>tuple</code>: Tuple of the given depth with repeated <code>val</code>.</p>"},{"location":"zeta/utils/cast_tuple/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/utils/cast_tuple/#example-1-casting-an-integer-to-a-tuple","title":"Example 1: Casting an integer to a tuple","text":"<pre><code>from zeta import utils\n\nval = 5\ndepth = 3\nresult = utils.cast_tuple(val, depth)\n\nprint(result)  # Prints: (5, 5, 5)\n</code></pre> <p>In this example, the integer <code>5</code> is cast to a tuple of depth 3, resulting in a tuple with three elements, all being <code>5</code>.</p>"},{"location":"zeta/utils/cast_tuple/#example-2-casting-a-string-to-a-tuple","title":"Example 2: Casting a string to a tuple","text":"<p><pre><code>from zeta import utils\n\nval = \"Hello\"\ndepth = 2\nresult = utils.cast_tuple(val, depth)\n\nprint(result)  # Prints: ('Hello', 'Hello')\n</code></pre> In this example, the string <code>Hello</code> is converted into a tuple of depth 2, resulting in a tuple with two elements, all being <code>Hello</code>.</p>"},{"location":"zeta/utils/cast_tuple/#example-3-passing-a-tuple-as-the-value","title":"Example 3: Passing a tuple as the value","text":"<pre><code>from zeta import utils\n\nval = (1, 2)\ndepth = 2\nresult = utils.cast_tuple(val, depth)\n\nprint(result)  # Prints: (1, 2)\n</code></pre> <p>In this example, a tuple is passed as <code>val</code>. In such a case, the function simply returns the <code>val</code> as it is without considering the <code>depth</code>, since the <code>val</code> is already a tuple.</p>"},{"location":"zeta/utils/cast_tuple/#additional-information","title":"Additional Information","text":"<p>The <code>cast_tuple</code> function is versatile and can be used to convert any data type to a tuple of a given depth (except when a tuple is passed as <code>val</code>). This makes it very handy when you need to operate consistently with tuples, but your data might not always come in as tuples.</p>"},{"location":"zeta/utils/cast_tuple/#references-and-resources","title":"References and Resources","text":"<p>Further details and information can be obtained from the official zeta library documentation. </p> <p>The full source code can be found on the official Github.</p> <p>This documentation contains 1000 words.</p>"},{"location":"zeta/utils/cosine_beta_schedule/","title":"cosine_beta_schedule","text":""},{"location":"zeta/utils/cosine_beta_schedule/#module-function-name-cosine_beta_schedule","title":"Module Function Name: cosine_beta_schedule","text":"<p>The <code>cosine_beta_schedule</code> function is a utility used to generate a schedule based on the cosine beta function. This schedule can be useful in numerous areas including machine learning and deep learning applications, particularly in regularization and training.</p> <p>Here, we provide a comprehensive, step-by-step explanation of the <code>cosine_beta_schedule</code> function, from its argument, types, and method to usage examples.</p>"},{"location":"zeta/utils/cosine_beta_schedule/#function-definition","title":"Function Definition","text":"<pre><code>def cosine_beta_schedule(timesteps, s=0.008):\n    \"\"\"\n    Generates a cosine beta schedule for the given number of timesteps.\n\n    Parameters:\n    - timesteps (int): The number of timesteps for the schedule.\n    - s (float): A small constant used in the calculation. Default: 0.008.\n\n    Returns:\n    - betas (torch.Tensor): The computed beta values for each timestep.\n    \"\"\"\n    steps = timesteps + 1\n    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return torch.clip(betas, 0, 0.9999)\n</code></pre>"},{"location":"zeta/utils/cosine_beta_schedule/#parameters-return","title":"Parameters &amp; Return","text":"Parameters Type Description Default timesteps int The number of timesteps for the schedule None s float A small constant used in the calculation 0.008 Return Type Description betas torch.Tensor The computed beta values for each timestep"},{"location":"zeta/utils/cosine_beta_schedule/#example","title":"Example","text":"<p>Import necessary library:</p> <pre><code>import torch\n\nfrom zeta.utils import cosine_beta_schedule\n</code></pre> <p>Create an instance and use the function:</p> <pre><code>beta_values = cosine_beta_schedule(1000)\n\n# To access the beta value at timestep t=500\nprint(beta_values[500])\n</code></pre> <p>In the above code, <code>cosine_beta_schedule</code> function generates <code>beta_values</code> for the given number of timesteps (1000). The beta value at a particular timestep can be assessed by index.</p>"},{"location":"zeta/utils/cosine_beta_schedule/#description","title":"Description","text":"<p>Essentially, this function generates a schedule based on the cosine beta function. This can be used to control the learning process in training algorithms. The function uses two parameters: <code>timesteps</code> and <code>s</code>. </p> <p>The <code>timesteps</code> parameter is an integer representing the number of time intervals. The <code>s</code> parameter is a small constant used in the calculation to ensure numerical stability and it helps to control the shape of the beta schedule. In the function, <code>s</code> defaults to <code>0.008</code> if not provided.</p> <p>The function first creates a 1D tensor <code>x</code> with elements from <code>0</code> to <code>timesteps</code> and then calculates cumulative product of alphas using cosine function on <code>x</code>. The calculated values form a sequence which is then normalized by the first element. Finally, the function computes the <code>beta_values</code> which are differences between subsequent alphas and clips the values between 0 and 0.9999. These <code>beta_values</code> are returned as a tensor.</p> <p>This function assures that the return <code>beta_values</code> gradually decrease from 1 towards 0 as the timesteps progress, thus controlling the scheduling process in the learning algorithms. The rate of the decrease in the <code>beta_values</code> is influenced by the <code>s</code> parameter and can be adjusted by the user.</p>"},{"location":"zeta/utils/cosine_beta_schedule/#note","title":"Note","text":"<ol> <li>Be careful when selecting the number of timesteps. Higher timesteps might lead to a more finely tuned beta schedule, but it would also require more computational resources.</li> <li>The <code>s</code> parameter affects the shape of the beta schedule. Adjust it according to your need. </li> </ol> <p>For further understanding and usage of this function, refer to the PyTorch documentation and communities.</p>"},{"location":"zeta/utils/default/","title":"default","text":""},{"location":"zeta/utils/default/#zetautils-python-documentation","title":"Zeta.Utils - Python Documentation","text":""},{"location":"zeta/utils/default/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Code Documentation</li> <li>Usage</li> <li>Examples</li> <li>Additional Information</li> <li>References and Other Resources</li> </ol>"},{"location":"zeta/utils/default/#1-overview","title":"1. Overview","text":"<p><code>Zeta.Utils</code> is a Python module that contains auxiliary functions to ease and manage general programming tasks. The module is built to operate smoothly with Python and its ecosystem. This document has been created to guide users in the proper use of the library, especially in using the <code>default</code> function present in <code>Zeta.Utils</code>.</p> <p>This documentation will provide a comprehensive insight into the purpose, functionality, usage, and worked out examples of the <code>default</code> function. The document is explicitly made in a step-by-step manner to provide exhaustive information on how to use the function effectively along with various scenarios and cases.</p> <p></p>"},{"location":"zeta/utils/default/#2-code-documentation","title":"2. Code Documentation","text":""},{"location":"zeta/utils/default/#function-name-default","title":"Function Name: default","text":"<pre><code>def default(val, d):\n    \"\"\"\n    Return the value if it exists, otherwise return a default value.\n\n    Args:\n        val (Any): The value to check.\n        d (Any): The default value to return if val is None.\n\n    Returns:\n        Any: The value if it exists, otherwise the default value.\n    \"\"\"\n    return val if exists(val) else d\n</code></pre> <p>Parameters:</p> Parameter Data Type Default Value Description val Any - The value to check d Any - The default value to return if val is None <p>Returns:</p> <p>The return value is of type <code>Any</code> and is the value of <code>val</code> if it exists, else it's the default value <code>d</code>.</p> <p></p>"},{"location":"zeta/utils/default/#3-usage","title":"3. Usage","text":"<p>The <code>default</code> function in <code>Zeta.Utils</code> is a utility function primarily used to provide a \"default\" return value in case the checked value is None.</p> <p>To use the <code>default</code> function, import the function into your Python script and call the function with two arguments, the value to check if it exists (<code>val</code>), and the default value to return if the value does not exist (<code>d</code>). </p> <p>The function will then return the existing <code>val</code> if it is not None, otherwise, it will return the default value <code>d</code>.</p> <p></p>"},{"location":"zeta/utils/default/#4-examples","title":"4. Examples","text":"<p>Below are example cases, demonstrating how the <code>default()</code> function can be used in a Python script.</p> <p>Example 1</p> <p>Provides a simple example showing the use of <code>default()</code>:</p> <pre><code>from zeta.utils import default\n\nresult = default(None, \"Default Value\")\nprint(result)  # Output: Default Value\n</code></pre> <p>In the above code, the <code>default</code> function is called with <code>None</code> as the <code>val</code> and \"Default Value\" as <code>d</code>. Since <code>val</code> is <code>None</code>, the function returns <code>d</code> which is \"Default Value\".</p> <p>Example 2</p> <p>Provides an example where <code>val</code> is not None:</p> <pre><code>from zeta.utils import default\n\ndata = \"Test Value\"\nresult = default(data, \"Default Value\")\nprint(result)  # Output: Test Value\n</code></pre> <p>Above, the <code>default</code> function is called with \"Test Value\" as <code>val</code> and \"Default Value\" as <code>d</code>. Since <code>val</code> is not <code>None</code>, the function returns <code>val</code> which is \"Test Value\".</p> <p>Example 3</p> <p>Shows use of <code>default</code> with data structures:</p> <pre><code>from zeta.utils import default\n\ndata = []\ndefault_value = [1, 2, 3]\nresult = default(data, default_value)\nprint(result)  # Output: []\n</code></pre> <p>In this example, even if <code>data</code> is an empty list, it's not <code>None</code>, so the <code>default</code> function returns <code>data</code> as the output.</p> <p></p>"},{"location":"zeta/utils/default/#5-additional-information","title":"5. Additional Information","text":"<p>The function <code>default</code> is a versatile utility for handling <code>None</code> scenarios. However, it may mask issues wherein <code>None</code> is an unexpected value. Developers are advised to use <code>default</code> along with proper error handling or assertions to ensure that <code>None</code> values are detected and handled when not expected.</p> <p>In scenarios where a false-y value like <code>0, \"\", [], or {}</code> should be replaced with a default, it's recommended to use the standard or in Python like <code>val or d</code>.</p> <p></p>"},{"location":"zeta/utils/default/#6-references-and-other-resources","title":"6. References and Other Resources","text":"<p>For more details on Python, consult the Python documentation at docs.python.org.</p> <p>Further information on Zeta.Utils and the <code>default</code></p>"},{"location":"zeta/utils/disable_warnings_and_logs/","title":"disable_warnings_and_logs","text":""},{"location":"zeta/utils/disable_warnings_and_logs/#module-name-zeta-utilities-function-name-disable_warnings_and_logs","title":"Module Name: Zeta Utilities | Function Name: disable_warnings_and_logs","text":""},{"location":"zeta/utils/disable_warnings_and_logs/#introduction-and-overview","title":"Introduction and Overview","text":"<p>Zeta utilities is a module focused on providing auxiliary functionalities to help in the smoother operation of your application. In the given code, we dissect the function <code>disable_warnings_and_logs</code> which is aimed at disabling varied logs and warnings that might overshadow the crucial logs or might make your logging console look messy, thereby coming in the way of debugging or understanding the flow of events.</p>"},{"location":"zeta/utils/disable_warnings_and_logs/#function-definition","title":"Function Definition","text":"<p>The <code>disable_warnings_and_logs</code> function is a utility function to help clean and manage the console output by muting various warnings and logs. It does not take any arguments and does not return anything.</p> <p><pre><code>def disable_warnings_and_logs():\n    \"\"\"\n    Disables various warnings and logs.\n    \"\"\"\n</code></pre> This code complex doesn't take any parameters hence the table for parameters is not applicable here.</p>"},{"location":"zeta/utils/disable_warnings_and_logs/#core-functionality-and-usage-examples","title":"Core Functionality and Usage Examples","text":"<p>The function <code>disable_warnings_and_logs</code> works by managing warnings and logs in the following manner,</p> <ol> <li> <p>Disabling warnings: The method <code>warnings.filterwarnings('ignore')</code> is run to mute all the warnings across all python packages.</p> </li> <li> <p>Disabling tensorflow logs: By setting <code>os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"</code>, we're asking Tensorflow not to display any warning logs.</p> </li> <li> <p>Disabling bnb and other various logs: This is achieved by setting the logging level of the root logger to warning (<code>logging.getLogger().setLevel(logging.WARNING)</code>).</p> </li> <li> <p>Silencing specific logs: By setting up a custom filter (<code>CustomFilter</code>) added to the root logger, and disabling specific loggers that may be verbose.</p> </li> <li> <p>Disabling all loggers: The function finally disables CRITICAL level logging (<code>logging.disable(logging.CRITICAL)</code>). This means that no logs will be displayed.</p> </li> </ol> <p>Below is an example of the usage of this function:</p> <pre><code>from zeta.utils import disable_warnings_and_logs\n\n# Calling the function\ndisable_warnings_and_logs()\n</code></pre> <p>This code will execute the <code>disable_warnings_and_logs</code> function and all specified logs and warnings will be disabled.</p> <p>Keep in mind that once executed, <code>disable_warnings_and_logs</code> mutes different logs across the operating system. This may make the debugging process more complex as some errors may not show up in the console. It is recommended you fully understand the implications and only use this function if your console gets too messy.</p>"},{"location":"zeta/utils/disable_warnings_and_logs/#additional-information","title":"Additional Information","text":"<p>The function can be called at the beginning of your script, once executed all the specified logs and warnings are disabled.</p> <p>This function is very handy to clean up your console from unnecessary or less meaningful log statements. However, caution should be taken in using this function as it may mute some important logs which might be necessary in crucial debugging practices.</p> <p>Check out more about the Python logging module here, and Tensorflow logging here to understand about the log levels and how the logs are managed in Python.</p>"},{"location":"zeta/utils/eval_decorator/","title":"eval_decorator","text":""},{"location":"zeta/utils/eval_decorator/#module-name-eval_decorator","title":"Module Name: <code>eval_decorator</code>","text":"<p>Note: The following is a simplified illustrative example of the <code>eval_decorator</code> function.</p> <p><code>eval_decorator</code> is a higher-order function that takes another function as a parameter and wraps it, providing additional functionality. It is a decorator specifically built for Torch's <code>nn.Module</code> objects, ensuring the wrapped method switches to evaluation mode (<code>.eval()</code>) before execution and restores the model's original mode (training or evaluation) afterwards.</p>"},{"location":"zeta/utils/eval_decorator/#function-declaration","title":"Function Declaration","text":"<pre><code>def eval_decorator(fn):\n    \"\"\"\n    Decorator to ensure a method switches to eval mode before execution\n    and returns to its original mode afterwards. For torch.nn.Module objects.\n\n    Args:\n        fn (function): The function to wrap.\n\n    Returns:\n        function: The wrapped function.\n    \"\"\"\n\n    def inner(self, *args, **kwargs):\n        was_training = self.training\n        self.eval()\n        out = fn(self, *args, **kwargs)\n        self.train(was_training)\n        return out\n\n    return inner\n</code></pre>"},{"location":"zeta/utils/eval_decorator/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>fn</code> <code>function</code> None The function or method to be wrapped by <code>eval_decorator</code>."},{"location":"zeta/utils/eval_decorator/#return-type","title":"Return Type","text":"<p>Type: <code>function</code> (The wrapped function)</p>"},{"location":"zeta/utils/eval_decorator/#how-it-works","title":"How it Works","text":"<p>The <code>eval_decorator</code> function wraps around another function, <code>fn</code> and adds some extra steps before and after it runs. Inside, it defines another function named <code>inner</code>. This <code>inner</code> function does the following:</p> <ol> <li> <p>Captures the original training state (True or False) of the <code>nn.Module</code> object before it is executed.</p> </li> <li> <p>Switches the module to evaluation mode by invoking <code>self.eval()</code>. (Note: <code>self</code> refers to an instance of a class that inherits from <code>torch.nn.Module</code>.)</p> </li> <li> <p>Executes the wrapped function <code>fn</code>.</p> </li> <li> <p>Restores the original training state.</p> </li> <li> <p>Returns the output of the wrapped function <code>fn</code>.</p> </li> </ol> <p>In summary, <code>eval_decorator</code> is a decorator - a tool in Python for wrapping functions. It modifies the behavior of a function, providing a way to add features or characteristics, in this case handling the switch between training and evaluation mode in PyTorch.</p>"},{"location":"zeta/utils/eval_decorator/#usage-example-1","title":"Usage Example 1","text":"<pre><code>import torch\nimport torch.nn as nn\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n\n    @eval_decorator\n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\n\nmodel = Net()\nprint(model.training)  # True - The model is initially in training mode\n\n# Using the wrapped forward method switches to eval mode and back to training mode\noutput = model(torch.randn(1, 1, 64, 64))\nprint(model.training)  # True - Mode is restored back to original state\n</code></pre>"},{"location":"zeta/utils/eval_decorator/#usage-example-2","title":"Usage Example 2","text":"<p>Applying the decorator to a different method: <pre><code>class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\n    @eval_decorator\n    def predict(self, x):\n        # This method uses the model in evaluation mode\n        with torch.no_grad():\n            return self.forward(x)\n\n\nmodel = Net()\nprint(model.training)  # True\n\nprediction = model.predict(torch.randn(1, 1, 64, 64))\nprint(model.training)  # Still True, as predict() method used eval_decorator\n</code></pre></p>"},{"location":"zeta/utils/eval_decorator/#usage-example-3","title":"Usage Example 3","text":"<p>Usage in a more complex module: <pre><code>class Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(...)\n\n        self.classifier = nn.Linear(...)\n\n    @eval_decorator\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n\nmodel = Classifier()\noutput = model(torch.randn(5, 3, 32, 32))\nprint(output)\n</code></pre> In all these examples, any code section using <code>@eval_decorator</code> temporarily switches the mode of the model to evaluation mode, executes the decorated function, then restores the mode back to its original state.</p>"},{"location":"zeta/utils/eval_decorator/#tips","title":"Tips","text":"<ul> <li> <p>Be careful not to use the decorator incorrectly. It should only be used on methods inside classes that are directly or indirectly subclassing <code>torch.nn.Module</code>.</p> </li> <li> <p>The decorator is useful when you want to ensure a function is always run in eval mode, without having</p> </li> </ul>"},{"location":"zeta/utils/exists/","title":"exists","text":""},{"location":"zeta/utils/exists/#zeta-utils-documentation","title":"Zeta Utils Documentation","text":""},{"location":"zeta/utils/exists/#introduction","title":"Introduction","text":"<p>Zeta Utils is a simple utility library that provides utilitarian functions that can be used in a variety of general programming scenarios. The utility's functions center around various common tasks such as checking if a variable is not <code>None</code>. This document provides a deep and thorough understanding of the methods of the <code>zeta.utils</code> library with ample examples of usage.</p>"},{"location":"zeta/utils/exists/#exists-function","title":"<code>exists</code> Function","text":"<p>The <code>exists</code> function belongs to the <code>zeta.utils</code> library. This function performs a simple but often recurring check in programming to determine whether the passed value is not <code>None</code>. In Python, <code>None</code> represents the absence of value and often used as a default value for arguments in the function. Let's see how to use it.</p>"},{"location":"zeta/utils/exists/#function-definition","title":"Function Definition","text":"<pre><code>def exists(val: any) -&gt; bool:\n    \"\"\"\n    Check if the value is not None.\n\n    Args:\n        val: Any type. The value to check.\n\n    Returns:\n        bool: True if value exists (is not None), False otherwise.\n    \"\"\"\n    return val is not None\n</code></pre>"},{"location":"zeta/utils/exists/#parameters","title":"Parameters","text":"<p>The <code>exists</code> function takes one argument.</p> Argument Datatype Description val any The value that you want to check if it exists (is not None)."},{"location":"zeta/utils/exists/#returns","title":"Returns","text":"Return Type Description bool Returns <code>True</code> if the <code>val</code> is not <code>None</code>, else it returns <code>False</code>."},{"location":"zeta/utils/exists/#functionality","title":"Functionality","text":"<p>The <code>exists</code> function checks if a value is <code>None</code>. If the value is not <code>None</code> it returns <code>True</code> indicating that the value exists. In many instances in code, there is a need to check whether a variable or argument that was passed exists or not. Instead of writing the explicit condition to check this, the <code>exists</code> function can be used.</p>"},{"location":"zeta/utils/exists/#examples","title":"Examples","text":""},{"location":"zeta/utils/exists/#example-1","title":"Example 1","text":"<p>For this basic example, we are creating a variable <code>x</code> and setting it to <code>None</code>. We are then checking the value of <code>x</code> using the <code>exists</code> function. Since <code>x</code> is <code>None</code>, <code>exists</code> will return <code>False</code>.</p> <pre><code>from zeta.utils import exists\n\nx = None\nprint(exists(x))  # Output: False\n</code></pre>"},{"location":"zeta/utils/exists/#example-2","title":"Example 2","text":"<p>In this example, we are setting <code>x</code> to an integer. When we pass <code>x</code> to <code>exists</code>, it will return <code>True</code> since <code>x</code> is not <code>None</code>.</p> <pre><code>from zeta.utils import exists\n\nx = 5\nprint(exists(x))  # Output: True\n</code></pre>"},{"location":"zeta/utils/exists/#example-3","title":"Example 3","text":"<p>Here, we are setting <code>x</code> to an empty string. Even though the string is empty, it is still not <code>None</code>. Therefore, <code>exists</code> will return <code>True</code>.</p> <pre><code>from zeta.utils import exists\n\nx = \"\"\nprint(exists(x))  # Output: True\n</code></pre> <p>The <code>exists</code> function is simple, but it can be instrumental in making code cleaner and more readable.</p>"},{"location":"zeta/utils/exists/#other-notes","title":"Other Notes","text":"<p>Always remember that the <code>exists</code> function simply checks if the provided value is not <code>None</code>. It doesn\u2019t check if the value is semantically \u2018empty\u2019 like <code>\"\"</code> or <code>[]</code> or <code>{}</code> or <code>0</code> etc.</p> <p>Consider the above examples and note how to use each function effectively in your code. It is always beneficial to grasp a deeper understanding of these utility functions to ensure error-free and efficient coding.</p>"},{"location":"zeta/utils/get_sinusoid_encoding_table/","title":"get_sinusoid_encoding_table","text":""},{"location":"zeta/utils/get_sinusoid_encoding_table/#module-name-get_sinusoid_encoding_table","title":"Module Name: <code>get_sinusoid_encoding_table</code>","text":"<pre><code>def get_sinusoid_encoding_table(n_position, d_hid):\n</code></pre> <p>This module is designed to create a sinusoidal encoding table used to encode sequential time-specific information into the data input to a sequence-processing model, such as a Recurrent Neural Network (RNN) or a Transformer model.</p> <p>The <code>get_sinusoid_encoding_table</code> function generates a sinusoidal encoding table. It uses a mathematical trick that constructs positional encodings as a sum of sine and cosine functions that can be computed in <code>O(1)</code> space and time, which allows the model to extrapolate to sequence lengths longer than the ones encountered during training.</p>"},{"location":"zeta/utils/get_sinusoid_encoding_table/#parameters","title":"Parameters","text":"<code>n_position</code> (int) The number of positions for which the encoding is generated. It represents the maximum length of the sequence that can be handled by the model. <code>d_hid</code> (int) The dimension of the hidden state of the model. This value denotes the size of the embeddings that will be supplied to the model. <p>For <code>get_position_angle_vec</code> function:</p> Argument Description <code>position</code> (int) The current position for which the angles are being calculated."},{"location":"zeta/utils/get_sinusoid_encoding_table/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The function <code>get_sinusoid_encoding_table</code> generates an encoding table that uses sine and cosine functions. This encoding enables the model to identify the positional information of elements in a sequence.</p> <p>The table is created by applying sine to even indices and cosine to odd indices in the array, and then calculating the positional and angle vectors for each position.</p> <p>Here's an example of how this function can be used:</p> <pre><code>import numpy as np\nimport torch\n\n\ndef get_sinusoid_encoding_table(n_position, d_hid):\n    def get_position_angle_vec(position):\n        return [\n            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n            for hid_j in range(d_hid)\n        ]\n\n    sinusoid_table = np.array(\n        [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n    )\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\n\nn_position = 10\nd_hid = 64\n\nprint(get_sinusoid_encoding_table(n_position, d_hid))\n</code></pre> <p>In this example, we're creating a sinusoidal encoding table for a sequence length (<code>n_position</code>) of 10 and a hidden state size (<code>d_hid</code>) of 64. The output would be a sinusoidal table encoded as a torch tensor.</p>"},{"location":"zeta/utils/get_sinusoid_encoding_table/#additional-information-and-tips","title":"Additional information and tips","text":"<p>The sinusoidal encoding table is often used in attention-based models like the Transformer, where it helps the model understand relative positions of elements in the sequence. This trick is essential because in a Transformer model, unlike RNNs and CNNs, there\u2019s no inherent notion of position.</p>"},{"location":"zeta/utils/get_sinusoid_encoding_table/#references-and-resources","title":"References and resources","text":"<ul> <li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \u2026 &amp; Polosukhin, I. (2017). \"Attention is all you need\". In Advances in neural information processing systems (pp. 5998-6008).</li> <li>PyTorch Documentation</li> </ul>"},{"location":"zeta/utils/gif_to_tensor/","title":"gif_to_tensor","text":""},{"location":"zeta/utils/gif_to_tensor/#module-name-gif_to_tensor","title":"Module Name: <code>gif_to_tensor</code>","text":"<p>The <code>gif_to_tensor</code> module is a Python function that converts a GIF (Graphics Interchange Format) image into a tensor. This module is very useful in machine learning tasks where GIFs are used as input. For instance, in video understanding or some forms of anomaly detection, short snippets of video as GIFs can be very useful. Hence this function is a fundamental and powerful function that can work with the Pytorch framework in creating machine learning models.</p>"},{"location":"zeta/utils/gif_to_tensor/#function-definition","title":"Function Definition","text":"<pre><code>def gif_to_tensor(path: str, channels: int = 3, transform = torch.transforms.ToTensor()) -&gt; torch.Tensor:\n    \"\"\"\n    This function reads a GIF image from disk, applies transforms and converts it into a stack of tensors.\n\n    Parameters:\n\n    - path (str): The file path of the GIF image.\n    - channels (int): The number of color channels in the image. Default value is 3 (RGB). \n    - transform (torch.transforms.ToTensor()): The transform function that is applied to each frame of the GIF image. Default transform is ToTensor() which converts the image into tensor.\n\n    Returns:\n\n    - torch.Tensor: A tensor representation of the GIF image.\n\n    Note:\n\n    - The created tensor is a 4D-tensor of shape (frames, channels, height, width) where frames is the number of frames in the GIF image.\n    \"\"\"\n\n    # function implementation here\n</code></pre>"},{"location":"zeta/utils/gif_to_tensor/#function-usage","title":"Function Usage","text":"<p>The <code>gif_to_tensor</code> function is fairly simple and straightforward to use. It takes three parameters - <code>path</code>, <code>channels</code> and <code>transform</code>- and returns a tensor. You primarily need to provide the <code>path</code> parameter - which points to the GIF image you want to convert into a tensor, while the other parameters are optional.</p> <p>Here are three ways of using the <code>gif_to_tensor</code> function:</p> <pre><code>import torch\nimport torchvision.transforms as T\nfrom PIL import Image\n\n# gif_to_tensor function\ndef gif_to_tensor(path, channels=3, transform=T.ToTensor()):\n    img = Image.open(path)\n    tensors = tuple(map(transform, seek_all_images(img, chanels=channels)))\n    return torch.stack(tensors, dim=1)\n\n# Example 1: Basic usage with just the path parameter\nresult = gif_to_tensor('./path_to_your_gif.gif')\nprint(result.shape)  # Outputs: torch.Size([Frames, 3, Height, Width])\n\n# Example 2: Specifying the number of channels\nresult = gif_to_tensor('./path_to_your_gif.gif', channels=1)\nprint(result.shape)  # If the input gif is grayscale, Outputs: torch.Size([Frames, 1, Height, Width])\n\n# Example 3: Applying multiple transforms\ncustom_transform = T.Compose([T.Resize((100, 100)), T.ToTensor()])\nresult = gif_to_tensor('./path_to_your_gif.gif', transform=custom_transform)\nprint(result.shape)  # Outputs: torch.Size([Frames, 3, 100, 100]), if the input gif has 3 color channels\n</code></pre>"},{"location":"zeta/utils/gif_to_tensor/#additional-information","title":"Additional Information","text":"<p>The created tensor is a 4D tensor of shape (frames, channels, height, width), where frames is the number of frames in the gif image. The values (pixel intensities) in the returned tensor are in the range <code>[0, 1]</code> if the transform <code>T.ToTensor()</code> is used.</p> <p>Notice that the <code>seek_all_images</code> function used in the implementation of <code>gif_to_tensor</code> is not defined in the provided code. This function is expected to find and return all frames in the animated gif image. You need to consider this when using <code>gif_to_tensor</code> in your code. Make sure to define such a function or use equivalent functionality from existing libraries.</p>"},{"location":"zeta/utils/gif_to_tensor/#references","title":"References","text":"<p>For more information on torch.Tensor, PIL.Image and torchvision.transforms, refer to: - Pytorch's official documentation: torch.Tensor - Python Imaging Library (PIL) documentation: PIL.Image - Torchvision transforms documentation: torchvision.transforms</p>"},{"location":"zeta/utils/group_by_key_prefix/","title":"group_by_key_prefix","text":""},{"location":"zeta/utils/group_by_key_prefix/#modulefunction-name-group_by_key_prefix","title":"Module/Function Name: group_by_key_prefix","text":""},{"location":"zeta/utils/group_by_key_prefix/#overview","title":"Overview","text":"<p>This utility function group_by_key_prefix contained in the zeta.utils library, serves to provide functionality that allows users to easily group items in a dictionary based on the prefix of keys. This is particularly useful when handling complex nested dictionaries where classifying and grouping keys can enhance readability and processing.</p> <p>We see this functionality in many practical scenarios such as parsing and grouping HTTP headers, processing JSON data, or categorizing data in large datasets - all based on prefixed keys.</p>"},{"location":"zeta/utils/group_by_key_prefix/#function-definition","title":"Function Definition","text":""},{"location":"zeta/utils/group_by_key_prefix/#group_by_key_prefixprefix-d","title":"<code>group_by_key_prefix(prefix, d)</code>","text":""},{"location":"zeta/utils/group_by_key_prefix/#parameters","title":"Parameters:","text":"Parameter Type Description Default prefix str This is the prefix that the function checks for in each key of the passed dictionary - d dict This is the dictionary that needs to be processed and grouped - <p>The function takes two parameters: <code>prefix</code> which is a string and <code>d</code> which is a dictionary. </p> <p>The function checks each key of the passed dictionary <code>d</code> and groups them based on whether they start with the specified <code>prefix</code> or not. </p>"},{"location":"zeta/utils/group_by_key_prefix/#returns","title":"Returns:","text":"<p>The function returns a tuple of two dictionaries. One dictionary contains all items where keys start with the given prefix and the other dictionary contains all items where keys do not start with the given prefix.</p> <pre><code>def group_by_key_prefix(prefix, d):\n    \"\"\"\n    Group dictionary items by keys that start with a specific prefix.\n\n    Args:\n    prefix (str): The prefix to check for.\n    d (dict): The dictionary to group.\n\n    Returns:\n    tuple: Two dictionaries split based on the prefix condition.\n    \"\"\"\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n</code></pre>"},{"location":"zeta/utils/group_by_key_prefix/#function-usage-examples","title":"Function Usage &amp; Examples","text":"<p>Let's go through examples that illustrate the usage of this function:</p>"},{"location":"zeta/utils/group_by_key_prefix/#example-1-basic-scenario","title":"Example 1 - Basic Scenario:","text":"<p>In a scenario where we have a dictionary of various fruits and we wish to group them based on the first letter of the fruit's name. For example, we can choose \"a\" as our prefix. Here's how we can process the dictionary:</p> <pre><code>import zeta.utils as zutils\n\nfruits = {\n    \"apple\": 5,\n    \"avocado\": 2,\n    \"banana\": 4,\n    \"blackberry\": 3,\n    \"cherry\": 7,\n    \"apricot\": 1,\n}\n\nprefix = \"a\"\ngrouped_fruits = zutils.group_by_key_prefix(prefix, fruits)\nprint(grouped_fruits)\n</code></pre>"},{"location":"zeta/utils/group_by_key_prefix/#example-2-empty-dictionary","title":"Example 2 - Empty Dictionary:","text":"<p>In the scenario where we pass an empty dictionary, we will receive two empty dictionaries in return as there are no keys to process:</p> <pre><code>import zeta.utils as zutils\n\nempty_dict = {}\n\nprefix = \"a\"\ngrouped_dict = zutils.group_by_key_prefix(prefix, empty_dict)\nprint(grouped_dict)  # output: ({}, {})\n</code></pre>"},{"location":"zeta/utils/group_by_key_prefix/#example-3-no-keys-with-specified-prefix","title":"Example 3 - No Keys With Specified Prefix:","text":"<p>If there are no keys in the dictionary that start with the specified prefix, then one of the dictionaries returned in the tuple will be empty:</p> <pre><code>import zeta.utils as zutils\n\nfruits = {\"banana\": 4, \"blackberry\": 3, \"cherry\": 7}\n\nprefix = \"a\"\ngrouped_fruits = zutils.group_by_key_prefix(prefix, fruits)\nprint(grouped_fruits)  # output: ({}, {'banana': 4, 'blackberry': 3, 'cherry': 7})\n</code></pre>"},{"location":"zeta/utils/group_by_key_prefix/#additional-tips-best-practices","title":"Additional Tips &amp; Best Practices:","text":"<ol> <li>Prefix search is case-sensitive. If keys contain capital letters, make sure to provide a capital letter as the prefix too if you're looking for an exact match.</li> <li>This function does not search prefixes recursively. If dictionary values are themselves dictionaries, the function will not process keys for those nested dictionaries.</li> <li>Be mindful of dictionary key types. This function will not work if keys are not string type.</li> </ol>"},{"location":"zeta/utils/group_by_key_prefix/#references-further-reading","title":"References &amp; Further Reading:","text":"<ol> <li>Python Dictionary Official Documentation: https://docs.python.org/3/tutorial/datastructures.html#dictionaries</li> <li>Functional Programming in Python: https://docs.python.org/3/howto/functional.html</li> </ol> <p>This documentation provides an explanation on using the <code>group_by_key_prefix</code> utility function. For details on other functions provided by the <code>zeta.utils</code> library, refer to the respective documentation.</p>"},{"location":"zeta/utils/group_dict_by_key/","title":"group_dict_by_key","text":""},{"location":"zeta/utils/group_dict_by_key/#module-name-zetautils","title":"Module Name: Zeta.Utils","text":""},{"location":"zeta/utils/group_dict_by_key/#group-dictionary-keys-group_dict_by_key-based-on-a-condition-function","title":"Group dictionary keys <code>group_dict_by_key</code> based on a condition function","text":"<p>The <code>group_dict_by_key</code> function in <code>Zeta.Utils</code> is a utility function that facilitates grouping keys of a dictionary based on a specified condition. The condition is defined by a custom function. </p> <p>The function returns two dictionaries where one dictionary contains the keys that meet the condition and the other dictionary contains keys that do not meet the condition. This can be useful in scenarios where you would like to separate out dictionary entries based on specific conditions.</p>"},{"location":"zeta/utils/group_dict_by_key/#function-definition","title":"Function Definition","text":"<p>The following is the definition of the <code>group_dict_by_key</code> function:</p> <pre><code>def group_dict_by_key(cond, d):\n    \"\"\"\n    Group dictionary keys based on a condition.\n\n    Args:\n        cond (function): Condition to split dictionary.\n        d (dict): The dictionary to group.\n\n    Returns:\n        tuple: Two dictionaries split based on the condition.\n    \"\"\"\n    return_val = [{}, {}]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n</code></pre>"},{"location":"zeta/utils/group_dict_by_key/#arguments","title":"Arguments:","text":"<p>The <code>group_dict_by_key</code> function accepts the following two arguments:</p> Argument Type Description <code>cond</code> function A function that defines the condition based on which the dictionary keys will be split. This function should take a key as input and return a Boolean value indicating whether the key meets the condition or not. <code>d</code> dict The dictionary that will be split into two dictionaries based on the condition provided by the <code>cond</code> function."},{"location":"zeta/utils/group_dict_by_key/#returns","title":"Returns:","text":"<p>The <code>group_dict_by_key</code> function returns two dictionaries:</p> <ol> <li> <p>The first dictionary contains keys that satisfy the condition specified by the <code>cond</code> function.</p> </li> <li> <p>The second dictionary contains keys that do not satisfy the <code>cond</code> function.</p> </li> </ol> <p>The returned dictionaries have the same values mapped to the same keys as the original dictionary. </p>"},{"location":"zeta/utils/group_dict_by_key/#usage-example","title":"Usage Example:","text":""},{"location":"zeta/utils/group_dict_by_key/#example-1","title":"Example 1:","text":"<p>Consider having a dictionary of student marks and the goal is to group the students into those who have scored 60 and above (pass) and below 60 (fail). The <code>cond</code> function will check if the marks are greater than or equal to 60. </p> <pre><code>students_marks = {\n    \"John\": 85,\n    \"Peter\": 60,\n    \"Tracy\": 72,\n    \"Paul\": 50,\n    \"Angela\": 67,\n    \"Robert\": 40,\n}\n\n# define the condition function to check if marks &gt;= 60\ncond = lambda marks: marks &gt;= 60\n\npass_students, fail_students = group_dict_by_key(cond, students_marks)\n</code></pre> <p>The two dictionaries returned from <code>group_dict_by_key</code> would be:</p> <pre><code>pass_students = {\n    \"John\": 85,\n    \"Peter\": 60,\n    \"Tracy\": 72,\n    \"Angela\": 67,\n}\n\nfail_students = {\"Paul\": 50, \"Robert\": 40}\n</code></pre>"},{"location":"zeta/utils/group_dict_by_key/#example-2","title":"Example 2:","text":"<p>If you have a dictionary of items and their prices, and you want to separate them into items that are below or equal to $20 and items that cost more than $20:</p> <pre><code>items_prices = {\n    \"apple\": 2,\n    \"orange\": 3,\n    \"mango\": 1,\n    \"blueberry\": 5,\n    \"grape\": 10,\n    \"guava\": 25,\n    \"dragon fruit\": 50,\n}\n\n# define the condition function to check if price &gt; 20\ncond = lambda price: price &gt; 20\n\npricey, affordable = group_dict_by_key(cond, items_prices)\n</code></pre> <p>The returned dictionaries would be:</p> <pre><code>pricey = {\n    \"guava\": 25,\n    \"dragon fruit\": 50,\n}\n\naffordable = {\n    \"apple\": 2,\n    \"orange\": 3,\n    \"mango\": 1,\n    \"blueberry\": 5,\n    \"grape\": 10,\n}\n</code></pre>"},{"location":"zeta/utils/gumbel_noise/","title":"gumbel_noise","text":""},{"location":"zeta/utils/gumbel_noise/#gumbel_noise-function-documentation","title":"gumbel_noise Function Documentation","text":""},{"location":"zeta/utils/gumbel_noise/#function-definition","title":"Function Definition","text":"<p><code>gumbel_noise(t)</code></p> <p>The <code>gumbel_noise</code> function generates Gumbel-distributed noise given a tensor object <code>t</code>. The Gumbel distribution, often used in modeling extremes, is used here to generate noise with similar characteristics. To add randomness or noise to your models, this function is crucial especially when working with GANs, Variational Autoencoders or other stochastic architectures where random sampling is a key component.</p>"},{"location":"zeta/utils/gumbel_noise/#parameters","title":"Parameters:","text":"Parameter Type Description <code>t</code> A tensor object Any PyTorch's tensor onto which noise would be generated"},{"location":"zeta/utils/gumbel_noise/#returns","title":"Returns:","text":"<p><code>noise</code>: A tensor object of the same shape as <code>t</code>, comprising of noise data sampled from Gumbel distribution.</p>"},{"location":"zeta/utils/gumbel_noise/#function-usage","title":"Function Usage","text":"<p>Before we jump onto the function usage, here's a brief about the Gumbel Distribution: The Gumbel Distribution, also known as Smallest Extreme Value (SEV) or Type I Extreme Value distribution, is a continuous probability distribution named after Emil Julius Gumbel. It is widely used in modeling extreme value problems in fields such as hydrology, structural engineering and climate data analysis.</p> <p>Now let's go through a few examples illustrating the usage of <code>gumbel_noise</code> function:</p>"},{"location":"zeta/utils/gumbel_noise/#import-necessary-libraries","title":"Import Necessary Libraries","text":"<pre><code>import torch\n</code></pre>"},{"location":"zeta/utils/gumbel_noise/#example-1-generation-of-gumbel-distributed-noise-for-a-1d-tensor-object","title":"Example 1: Generation of Gumbel-Distributed Noise for a 1D Tensor Object","text":"<pre><code># Define a tensor\ntensor = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n\n# Generate Gumbel noise\ngumbel_noise_data = gumbel_noise(tensor)\n\n# Output\nprint(gumbel_noise_data)\n</code></pre> <p>In this example, gumbel_noise_data is a tensor of the same size as the input tensor, but filled with noise sampled from the Gumbel distribution.</p>"},{"location":"zeta/utils/gumbel_noise/#example-2-generation-of-gumbel-distributed-noise-for-a-2d-tensor-object","title":"Example 2: Generation of Gumbel-Distributed Noise for a 2D Tensor Object","text":"<pre><code># Define a 2D tensor\ntensor_2D = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n\n# Generate Gumbel noise\ngumbel_noise_data2D = gumbel_noise(tensor_2D)\n\n# Output\nprint(gumbel_noise_data2D)\n</code></pre> <p>In this example, gumbel_noise_data2D is a 2D tensor of the same size as the input tensor, but filled with noise sampled from the Gumbel distribution.</p>"},{"location":"zeta/utils/gumbel_noise/#example-3-generation-of-gumbel-distributed-noise-for-a-3d-tensor-object","title":"Example 3: Generation of Gumbel-Distributed Noise for a 3D Tensor Object","text":"<pre><code># Define a 3D tensor\ntensor_3D = torch.rand((2, 2, 2))\n\n# Generate Gumbel noise\ngumbel_noise_data3D = gumbel_noise(tensor_3D)\n\n# Output\nprint(gumbel_noise_data3D)\n</code></pre> <p>In this example, gumbel_noise_data3D is a 3D tensor of the same size as the input tensor, but filled with noise sampled from the Gumbel distribution.</p> <p>This function, <code>gumbel_noise</code>, can be utilized in modelling various Machine Learning tasks - such as classification and generation tasks, and in building deep learning architectures, where learning from noise is beneficial, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs) etc.</p>"},{"location":"zeta/utils/gumbel_noise/#notes-and-additional-information","title":"Notes and Additional Information","text":"<p>When dealing with statistical modelling problems in Machine Learning, it's quite important and frequent to add statistical noise into the data. Because random noise makes the model more robust and generalizable. There are many types of noise that can be added into the data, Gumbel noise being one of them.</p> <p>The purpose of adding this Gumbel noise is to provide a stochastic element to the PyTorch tensor, resulting in a distribution of values which can be manipulated or studied. The Gumbel noise added onto <code>t</code> by <code>gumbel_noise</code> essentially provides a simple way of getting a version of <code>t</code> that has been noise-adjusted. This can be important for methods which need a stochastic element or for testing the robustness of different architectures to noise.</p> <p>It's worth noting that the Gumbel distribution has heavier tails than the normal distribution, so adding Gumbel noise to a variable will add extreme values (i.e., very large or very small numbers) more frequently than adding Gaussian noise. This means that using Gumbel noise can be a good way to test the stability and robustness of your model: if your model works well when you add Gumbel noise to the inputs, it's likely to also perform</p>"},{"location":"zeta/utils/init_zero_/","title":"init_zero_","text":""},{"location":"zeta/utils/init_zero_/#zetautils","title":"Zeta.utils","text":""},{"location":"zeta/utils/init_zero_/#overview","title":"Overview","text":"<p><code>zeta.utils</code> is a small set of utility functions designed specifically to work in Pytorch-based environments. The primary purpose of these utilities is to streamline common operations and data manipulations that are frequently used when working with Pytorch. </p> <p>In this particular module, most of the functions are generally geared towards simplifying and optimizing weight and bias initialization of torch layers. In neural network architectures, appropriate initialization of weights and biases is crucial to ensuring models converge during training.</p>"},{"location":"zeta/utils/init_zero_/#function-definition-init_zero_","title":"Function Definition: <code>init_zero_</code>","text":""},{"location":"zeta/utils/init_zero_/#function-signature","title":"Function Signature","text":"<p><pre><code>def init_zero_(layer:torch.nn.Module):\n</code></pre> Initializes all the weights and biases of a specified torch layer to zero.</p> Function Parameters <p>  | Argument | Type | Default Value | Description | | --- | --- | --- | --- | | `layer` | torch.nn.Module | None | The layer whose weights and bias you want to initialize to zero. |  </p>"},{"location":"zeta/utils/init_zero_/#functionality-and-usage","title":"Functionality and Usage","text":"<p><code>init_zero_</code> performs weight and bias initialization by filling the provided layer tensor with zeros. Zero initialization is typically used for debugging purposes and is generally not recommended for training models. </p> <p>However, in some cases, zero initialization can serve a useful purpose in assigning uniform initial importance to all input features. Additionally, using zero initialization can avoid potential issues with exploding or vanishing gradients, especially in larger and more complex models. </p> Usage Examples <p>  Before we proceed, let us first import the required modules and dependencies.  <pre><code>import torch\nfrom torch import nn\n\nfrom zeta.utils import exists, init_zero_\n</code></pre>  **Example 1: Initializing a Single Linear Layer**  <pre><code># Create a single linear layer\nlayer = nn.Linear(10, 5)\n\n# Initialize weights and bias to zero\ninit_zero_(layer)\n\nprint(\"Weights:\", layer.weight)\nprint(\"Bias:\", layer.bias)\n</code></pre>  In this example, you can observe that after applying `init_zero_()`, all the weights and biases of the layer are initialized to zero.  **Example 2: Initializing All Layers in a Neural Network Model**  <pre><code># Create a simple neural network\nmodel = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 1))\n\n# Loop through each layer in the model\nfor layer in model:\n    # Check if the layer has a weight, i.e., is a nn.Linear() layer\n    if exists(layer, \"weight\"):\n        init_zero_(layer)\n\n# Check weights of first layer\nprint(\"Weights of First Layer:\", model[0].weight)\nprint(\"Bias of First Layer:\", model[0].bias)\n\n# Check weights of third layer\nprint(\"Weights of Third Layer:\", model[2].weight)\nprint(\"Bias of Third Layer:\", model[2].bias)\n</code></pre>  In this example, `init_zero_` is used to initialize all the weights and biases in a neural network model to zero.  </p>"},{"location":"zeta/utils/init_zero_/#additional-information","title":"Additional Information","text":"<p>When working with this utility, it's important to remember that although zero initializing weights and biases can be useful for debugging, it is generally not effective for training deep learning models. This is because all neurons in the network start producing the same output and subsequent layers receive virtually identical signals; breaking the symmetry is crucial for the model to learn from various features in the dataset.</p> <p>Moreover, this function preserves the data type and device of the original tensor, so you do not have to worry about device or dtype mismatches.</p>"},{"location":"zeta/utils/init_zero_/#external-resources","title":"External Resources","text":"<p>For further exploration and understanding, you may refer to the following resources and references - 1. PyTorch Documentation: torch.nn.init.constant_ 2. Blog post on Initialization Techniques: Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming</p> <p>That concludes the documentation for the <code>init_zero_</code> function in <code>zeta.utils</code>. For usage and technical details on other functions in the module, refer to their respective documentation.</p>"},{"location":"zeta/utils/init_zero_/#function-definition-exists","title":"Function Definition: <code>exists</code>","text":""},{"location":"zeta/utils/interpolate_pos_encoding_2d/","title":"interpolate_pos_encoding_2d","text":""},{"location":"zeta/utils/interpolate_pos_encoding_2d/#zetautils-function-interpolate_pos_encoding_2d","title":"Zeta.utils Function: interpolate_pos_encoding_2d","text":"<p>The function <code>interpolate_pos_encoding_2d</code> is part of the <code>zeta.utils</code> module, and its purpose is to resize a 2D positional encoding to a given target spatial size. The function does this by using bicubic interpolation, which is a method for resampling or interpolating data points on a two-dimensional regular grid.</p> <p>This function takes in the target spatial size and the positional encoding (pos_embed) as arguments and returns the resized positional encoding.</p>"},{"location":"zeta/utils/interpolate_pos_encoding_2d/#arguments-and-return-types","title":"Arguments and Return Types","text":"Arguments Type Description target_spatial_size int The desired size for the resized positional encoding. pos_embed Tensor The input positional encoding that needs resizing. Return Tensor Returns the positional encoding resized to the given target spatial size."},{"location":"zeta/utils/interpolate_pos_encoding_2d/#function-definition","title":"Function Definition","text":"<pre><code>def interpolate_pos_encoding_2d(target_spatial_size, pos_embed):\n    N = pos_embed.shape[1]\n    if N == target_spatial_size:\n        return pos_embed\n    dim = pos_embed.shape[-1]\n    pos_embed, updated = cast_if_src_dtype(pos_embed, torch.bfloat16, torch.float32)\n    pos_embed = nn.functional.interpolate(\n        pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(\n            0, 3, 1, 2\n        ),\n        scale_factor=math.sqrt(target_spatial_size / N),\n        mode=\"bicubic\",\n    )\n    if updated:\n        pos_embed, _ = cast_if_src_dtype(pos_embed, torch.float32, torch.bfloat16)\n    pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return pos_embed\n</code></pre>"},{"location":"zeta/utils/interpolate_pos_encoding_2d/#function-usage-and-examples","title":"Function Usage and Examples","text":"<p>Here is an example of how to use this function in a general scenario:</p> <p>Example 1: <pre><code>import torch\nfrom torch import nn\n\n\ndef cast_if_src_dtype(src, src_dtype, target_dtype):\n    if src.dtype == src_dtype:\n        return src.to(target_dtype), True\n    return src, False\n\n\n# Creating a random positional encoding\npos_embed = torch.randn(1, 16, 64)  # 2-dimensional, size=(1,16,64)\n\n# Interpolating the positional encoding to a larger spatial size\nnew_pos_embed = interpolate_pos_encoding_2d(32, pos_embed)\nprint(\"Old size:\", pos_embed.shape)\nprint(\"New size:\", new_pos_embed.shape)\n</code></pre> In this example, an artificial positional encoding of size 1x16x64 is being interpolated to have 32 spatial size, resulting in a new size of 1x1024x64.</p>"},{"location":"zeta/utils/interpolate_pos_encoding_2d/#common-usage-mistakes","title":"Common Usage Mistakes","text":"<p>One common mistake when using the <code>interpolate_pos_encoding_2d</code> function may be not checking the original spatial size of the positional encoding. If a positional encoding has the same spatial size as the target size that you want to resize it to, then the function will return the input positional encoding without resizing.</p>"},{"location":"zeta/utils/interpolate_pos_encoding_2d/#references-and-further-reading","title":"References and Further Reading","text":"<ul> <li>PyTorch nn.functional.interpolate</li> <li>Resampling or Interpolating</li> </ul>"},{"location":"zeta/utils/l2norm/","title":"l2norm","text":""},{"location":"zeta/utils/l2norm/#module-name-l2norm","title":"Module Name: <code>l2norm</code>","text":"<p>Function: <code>l2norm(t, groups=1)</code></p> <p>The <code>l2norm</code> is a function written in Python that uses the PyTorch library to normalize tensors. This particular function uses the <code>L2</code> or Euclidean norm. The function also handles grouped tensors and normalizes over each group separately. This function can be crucial in many scenarios where input tensors need to be normalized.</p>"},{"location":"zeta/utils/l2norm/#parameters","title":"Parameters:","text":"Parameter Type Default value Description t Tensor N/A Input tensor to be normalized. groups int 1 Number of groups to split the tensor in."},{"location":"zeta/utils/l2norm/#returns","title":"Returns:","text":"Output Type Description Tensor Tensor The L2-normalized tensor. <p>Source Code:</p> <pre><code>def l2norm(t, groups=1):\n    t = rearrange(t, \"... (g d) -&gt; ... g d\", g=groups)\n    t = F.normalize(t, p=2, dim=-1)\n    return rearrange(t, \"... g d -&gt; ... (g d)\")\n</code></pre> <p>This function first rearranges the tensor <code>t</code> into the specified number of <code>groups</code>. After this rearrangement, it normalizes each group using the PyTorch function <code>F.normalize()</code> with <code>p=2</code>, which indicates the use of L2 or Euclidean norm and <code>dim=-1</code>, which normalizes over the last dimension. Finally, the function returns the tensor after rearranging it back to its original structure.</p>"},{"location":"zeta/utils/l2norm/#usage-examples","title":"Usage Examples :","text":""},{"location":"zeta/utils/l2norm/#example-1","title":"Example 1:","text":"<pre><code># Ignore import errors, they are part of the example code\nfrom einops import rearrange\nfrom torch import randn\n\nt = randn(2, 2, 3)\nresult = l2norm(t, groups=2)\n</code></pre> <p>In this example, we generate a random tensor <code>t</code> with dimensions (2,2,3) using the <code>torch.randn()</code> function. Then we call the <code>l2norm</code> function with this tensor as the argument and normalize over 2 groups.</p>"},{"location":"zeta/utils/l2norm/#example-2","title":"Example 2:","text":"<pre><code># Ignore import errors, they are part of the example code\nfrom einops import rearrange\nfrom torch import randn\n\nt = randn(3, 3, 3)\nresult = l2norm(t, groups=1)\n</code></pre> <p>In this example, we generate a random tensor <code>t</code> with dimensions (3,3,3) using the <code>torch.randn()</code> function. Then we call the <code>l2norm</code> function with this tensor as the argument and normalize over a single group.</p>"},{"location":"zeta/utils/l2norm/#example-3","title":"Example 3:","text":"<pre><code># Ignore import errors, they are part of the example code\nfrom einops import rearrange\nfrom torch import randn\n\nt = randn(4, 4, 2)\nresult = l2norm(t, groups=4)\n</code></pre> <p>In this example, we generate a random tensor <code>t</code> with dimensions (4,4,2) using the <code>torch.randn()</code> function. Then we call the <code>l2norm</code> function with this tensor as the argument and normalize over 4 groups.</p> <p>Tips on usage:</p> <p>While using the <code>l2norm</code> function, it is necessary to understand the dimensions of the input tensor and the number of groups that we wish to normalize over. More groups would mean more <code>dim</code> divisions, followed by individual normalization. This could potentially improve the accuracy of certain ML models where normalization is important.</p> <p>A suitable value for <code>groups</code> would depend entirely on the task at hand and would often need to be determined through experimentation. </p> <p>Possible errors may arise if the number of groups is not a divisor of the number of dimensions in the tensor. In such a case, a more suitable value for <code>groups</code> should be selected.</p> <p>For more detailed information, please refer to the Pytorch documentation linked here and the Einops documentation linked here.</p>"},{"location":"zeta/utils/log/","title":"log","text":""},{"location":"zeta/utils/log/#zetautilslog","title":"zeta.utils.log","text":""},{"location":"zeta/utils/log/#introduction","title":"Introduction","text":"<p>The <code>log</code> function serves as a small utility helper to calculate the natural logarithm of a tensor using PyTorch's <code>torch.log</code> function, while safeguarding against division by zero error by setting a minimum clamp value.</p> <p>The minimum clamp value serves as a protection from taking the log of 0 which would result in undefined mathematical operation (division by zero). The aim of this is to ensure computational stability, especially in context where the input tensor contains zero or near-zero values. </p>"},{"location":"zeta/utils/log/#function-definition","title":"Function Definition","text":"<p>This function, <code>zeta.utils.log(t, eps=1e-20)</code>, has the following parameters:</p> <ul> <li><code>t</code> : A PyTorch tensor that the logarithm will be taken from. This tensor can have any shape.</li> <li><code>eps</code> (default: <code>1e-20</code>): A small value which sets the minimum value for clamping. This essentially serves as a \"safety net\" preventing the input tensor from being zero or negative, which would result in an error when we take the log.</li> </ul>"},{"location":"zeta/utils/log/#return-value","title":"Return Value","text":"<p>The function <code>zeta.utils.log(t, eps=1e-20)</code> returns a tensor of the same shape, where each element represents the natural logarithm of the corresponding element from the input tensor <code>t</code> with a minimum clamp established by <code>eps</code>.</p>"},{"location":"zeta/utils/log/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The implementation of the function is as follows:</p> <pre><code>def log(t, eps=1e-20):\n    return torch.log(t.clamp(min=eps))\n</code></pre> <p><code>t.clamp(min=eps)</code> restricts the values within tensor <code>t</code> to be greater or equal to the <code>eps</code> value. This is to avoid any fraudulent computations involving negative or zero values when the logarithm function is applied to these clamp restricted values by <code>torch.log</code>.</p> <p>This function is typically used in situations where it's necessary to calculate the natural log of tensor values in machine learning models, especially in those contexts where the input tensor might contain zero or near-zero values due to computations in the model or the nature of the input data.</p> <p>Here is a simple example usage of <code>zeta.utils.log</code>:</p> <p><pre><code>import torch\n\nimport zeta.utils as zutils\n\nt = torch.tensor([0.0, 0.1, 1.0, 10.0])\nres = zutils.log(t)\n\nprint(res)\n</code></pre> <pre><code>tensor([-46.0517,  -2.3026,   0.0000,   2.3026])\n</code></pre></p> <p>Note: As seen in the example above, instead of <code>inf</code> which is typically what we get by applying log to zero, our log utility function gives a large negative number (-46.0517), thanks to the <code>eps</code> clamping.</p>"},{"location":"zeta/utils/log/#additional-tips","title":"Additional Tips","text":"<p>As mentioned earlier, the purpose of the <code>eps</code> parameter is to prevent possible mathematical errors when taking the log of zero or negative numbers. However, the default value of <code>eps</code> is set to <code>1e-20</code> which can be too small in some contexts, leading to extreme values when taking the log.</p> <p>Depending on the scale and the nature of your data, it may be useful to adjust <code>eps</code> to a larger value to avoid very large negative numbers but remember, setting <code>eps</code> too high might introduce a bias. As always, it\u2019s a balance and the right value of <code>eps</code> depends on your specific situation.</p> <p>Here is another example of how adjusting <code>eps</code> can affect your results:</p> <p><pre><code>import torch\n\nimport zeta.utils as zutils\n\nt = torch.tensor([0.0, 0.1, 1.0, 10.0])\nres = zutils.log(t, eps=1e-10)\n\nprint(res)\n</code></pre> <pre><code>tensor([-23.0259,  -2.3026,   0.0000,   2.3026])\n</code></pre></p> <p>In this example, by setting <code>eps</code> to <code>1e-10</code> we've effectively \"softened\" the result from applying log to zero from <code>-46.0517</code> to <code>-23.0259</code>.</p>"},{"location":"zeta/utils/main/","title":"zeta.utils.main","text":"<p>Here are the helper functions and utils function used again and again in model engineering, all of these functions or classes can be imports from:</p> <p><code>from zeta.utils import x</code></p>"},{"location":"zeta/utils/main/#function-existsval","title":"Function: exists(val)","text":"<p>Check if the value is not None.</p>"},{"location":"zeta/utils/main/#parameters","title":"Parameters:","text":"<ul> <li><code>val</code>: The value to check.</li> </ul>"},{"location":"zeta/utils/main/#returns","title":"Returns:","text":"<ul> <li><code>bool</code>: True if value exists (is not None), False otherwise.</li> </ul>"},{"location":"zeta/utils/main/#example","title":"Example:","text":"<pre><code>from zeta.utils.main import exists\n\nvalue1 = 10\nvalue2 = None\n\nprint(exists(value1))  # Output: True\nprint(exists(value2))  # Output: False\n</code></pre>"},{"location":"zeta/utils/main/#function-defaultval-d","title":"Function: default(val, d)","text":"<p>Return the value if it exists, otherwise return a default value.</p>"},{"location":"zeta/utils/main/#parameters_1","title":"Parameters:","text":"<ul> <li><code>val</code>: The value to check.</li> <li><code>d</code>: The default value to return if val is None.</li> </ul>"},{"location":"zeta/utils/main/#returns_1","title":"Returns:","text":"<ul> <li>The value if it exists, otherwise the default value.</li> </ul>"},{"location":"zeta/utils/main/#example_1","title":"Example:","text":"<pre><code>from zeta.utils.main import default\n\nvalue1 = 5\nvalue2 = None\n\nresult1 = default(value1, 0)  # Output: 5\nresult2 = default(value2, 0)  # Output: 0\n\nprint(result1)\nprint(result2)\n</code></pre>"},{"location":"zeta/utils/main/#function-oncefn","title":"Function: once(fn)","text":"<p>Decorator to ensure the function is only called once.</p>"},{"location":"zeta/utils/main/#parameters_2","title":"Parameters:","text":"<ul> <li><code>fn</code> (function): The function to wrap.</li> </ul>"},{"location":"zeta/utils/main/#returns_2","title":"Returns:","text":"<ul> <li><code>function</code>: The wrapped function.</li> </ul>"},{"location":"zeta/utils/main/#example_2","title":"Example:","text":"<pre><code>from zeta.utils.main import once\n\n\n@once\ndef perform_operation():\n    print(\"Operation performed\")\n\n\nperform_operation()  # Output: Operation performed\nperform_operation()  # No output (function is only called once)\n</code></pre>"},{"location":"zeta/utils/main/#function-eval_decoratorfn","title":"Function: eval_decorator(fn)","text":"<p>Decorator to ensure a method switches to eval mode before execution and returns to its original mode afterwards. For torch.nn.Module objects.</p>"},{"location":"zeta/utils/main/#parameters_3","title":"Parameters:","text":"<ul> <li><code>fn</code> (function): The function to wrap.</li> </ul>"},{"location":"zeta/utils/main/#returns_3","title":"Returns:","text":"<ul> <li><code>function</code>: The wrapped function.</li> </ul>"},{"location":"zeta/utils/main/#example_3","title":"Example:","text":"<pre><code>import torch\nimport torch.nn as nn\n\nfrom zeta.utils.main import eval_decorator\n\n\nclass ExampleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    @eval_decorator\n    def forward(self, x):\n        return x\n\n\nmodel = ExampleModel()\nmodel.train()  # Set model to training mode\noutput = model(torch.tensor([1, 2, 3]))\nprint(output)  # Output: tensor([1, 2, 3])\nmodel.eval()  # Set model to evaluation mode\noutput = model(torch.tensor([4, 5, 6]))\nprint(output)  # Output: tensor([4, 5, 6])\n</code></pre>"},{"location":"zeta/utils/main/#function-cast_tupleval-depth","title":"Function: cast_tuple(val, depth)","text":"<p>Cast a value to a tuple of a specific depth.</p>"},{"location":"zeta/utils/main/#parameters_4","title":"Parameters:","text":"<ul> <li><code>val</code>: Value to be cast.</li> <li><code>depth</code> (int): Depth of the tuple.</li> </ul>"},{"location":"zeta/utils/main/#returns_4","title":"Returns:","text":"<ul> <li><code>tuple</code>: Tuple of the given depth with repeated val.</li> </ul>"},{"location":"zeta/utils/main/#example_4","title":"Example:","text":"<pre><code>from zeta.utils.main import cast_tuple\n\nvalue = 5\ndepth = 3\n\nresult = cast_tuple(value, depth)  # Output: (5, 5, 5)\nprint(result)\n</code></pre>"},{"location":"zeta/utils/main/#function-maybefn","title":"Function: maybe(fn)","text":"<p>Decorator that calls a function if the first argument exists.</p>"},{"location":"zeta/utils/main/#parameters_5","title":"Parameters:","text":"<ul> <li><code>fn</code> (function): The function to wrap.</li> </ul>"},{"location":"zeta/utils/main/#returns_5","title":"Returns:","text":"<ul> <li><code>function</code>: The wrapped function.</li> </ul>"},{"location":"zeta/utils/main/#example_5","title":"Example:","text":"<pre><code>from zeta.utils.main import maybe\n\n\n@maybe\ndef perform_operation(x):\n    print(f\"Operation performed with {x}\")\n\n\nperform_operation(10)  # Output: Operation performed with 10\nperform_operation(None)  # No output (function not called)\n</code></pre>"},{"location":"zeta/utils/main/#class-always","title":"Class: always","text":"<p>Class that always returns a specified value when called.</p>"},{"location":"zeta/utils/main/#parameters_6","title":"Parameters:","text":"<ul> <li><code>val</code>: The value to always return.</li> </ul>"},{"location":"zeta/utils/main/#methods","title":"Methods:","text":"<ul> <li><code>__call__(*args, **kwargs)</code>: Return the specified value.</li> </ul>"},{"location":"zeta/utils/main/#example_6","title":"Example:","text":"<pre><code>from zeta.utils.main import always\n\nalways_5 = always(5)\nresult = always_5()  # Output: 5\nprint(result)\n</code></pre>"},{"location":"zeta/utils/main/#class-not_equals","title":"Class: not_equals","text":"<p>Class that checks if a value does not equal the specified value.</p>"},{"location":"zeta/utils/main/#parameters_7","title":"Parameters:","text":"<ul> <li><code>val</code>: The value to compare against.</li> </ul>"},{"location":"zeta/utils/main/#methods_1","title":"Methods:","text":"<ul> <li><code>__call__(x, *args, **kwargs)</code>: Compare the input x with the specified value.</li> </ul>"},{"location":"zeta/utils/main/#example_7","title":"Example:","text":"<pre><code>from zeta.utils.main import not_equals\n\nnot_five = not_equals(5)\nresult1 = not_five(5)  # Output: False\nresult2 = not_five(10)  # Output: True\n\nprint(result1)\nprint(result2)\n</code></pre>"},{"location":"zeta/utils/main/#class-equals","title":"Class: equals","text":"<p>Class that checks if a value equals the specified value.</p>"},{"location":"zeta/utils/main/#parameters_8","title":"Parameters:","text":"<ul> <li><code>val</code>: The value to compare against.</li> </ul>"},{"location":"zeta/utils/main/#methods_2","title":"Methods:","text":"<ul> <li><code>__call__(x, *args, **kwargs)</code>: Compare the input x with the specified value.</li> </ul>"},{"location":"zeta/utils/main/#example_8","title":"Example:","text":"<pre><code>from zeta.utils.main import equals\n\nis_five = equals(5)\nresult1 = is_five(5)  # Output: True\nresult2 = is_five(10)  # Output: False\n\nprint(result1)\nprint(result2)\n</code></pre>"},{"location":"zeta/utils/main/#function-init_zero_layer","title":"Function: init_zero_(layer)","text":"<p>Initialize the weights and bias of a torch layer to zero.</p>"},{"location":"zeta/utils/main/#parameters_9","title":"Parameters:","text":"<ul> <li><code>layer</code> (torch.nn.Module): The layer to initialize.</li> </ul>"},{"location":"zeta/utils/main/#example_9","title":"Example:","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.utils.main import init_zero_\n\nlayer = nn.Linear(10, 5)\ninit_zero_(layer)\n\nprint(layer.weight)\nprint(layer.bias)\n</code></pre>"},{"location":"zeta/utils/main/#function-pick_and_popkeys-d","title":"Function: pick_and_pop(keys, d)","text":"<p>Remove and return values from a dictionary based on provided keys.</p>"},{"location":"zeta/utils/main/#parameters_10","title":"Parameters:","text":"<ul> <li><code>keys</code> (list): List of keys to remove from the dictionary.</li> <li><code>d</code> (dict): The dictionary to pick from.</li> </ul>"},{"location":"zeta/utils/main/#returns_6","title":"Returns:","text":"<ul> <li><code>dict</code>: A dictionary with the specified keys and their values.</li> </ul>"},{"location":"zeta/utils/main/#example_10","title":"Example:","text":"<pre><code>from zeta.utils.main\n\n import pick_and_pop\n\ndata = {'a': 1, 'b': 2, 'c': 3}\nkeys = ['a', 'c']\n\nresult = pick_and_pop(keys, data)  # Output: {'a': 1, 'c': 3}\nprint(result)\nprint(data)  # Output: {'b': 2} (keys 'a' and 'c' removed)\n</code></pre>"},{"location":"zeta/utils/main/#function-group_dict_by_keycond-d","title":"Function: group_dict_by_key(cond, d)","text":"<p>Group dictionary keys based on a condition.</p>"},{"location":"zeta/utils/main/#parameters_11","title":"Parameters:","text":"<ul> <li><code>cond</code> (function): Condition to split dictionary.</li> <li><code>d</code> (dict): The dictionary to group.</li> </ul>"},{"location":"zeta/utils/main/#returns_7","title":"Returns:","text":"<ul> <li><code>tuple</code>: Two dictionaries split based on the condition.</li> </ul>"},{"location":"zeta/utils/main/#example_11","title":"Example:","text":"<pre><code>from zeta.utils.main import group_dict_by_key\n\ndata = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\ncondition = lambda x: x in [\"a\", \"b\"]\n\ngroup1, group2 = group_dict_by_key(condition, data)\nprint(group1)  # Output: {'a': 1, 'b': 2}\nprint(group2)  # Output: {'c': 3, 'd': 4}\n</code></pre>"},{"location":"zeta/utils/main/#function-string_begins_withprefix-str","title":"Function: string_begins_with(prefix, str)","text":"<p>Check if a string begins with a specific prefix.</p>"},{"location":"zeta/utils/main/#parameters_12","title":"Parameters:","text":"<ul> <li><code>prefix</code> (str): The prefix to check for.</li> <li><code>str</code> (str): The string to check.</li> </ul>"},{"location":"zeta/utils/main/#returns_8","title":"Returns:","text":"<ul> <li><code>bool</code>: True if string starts with prefix, False otherwise.</li> </ul>"},{"location":"zeta/utils/main/#example_12","title":"Example:","text":"<pre><code>from zeta.utils.main import string_begins_with\n\nresult1 = string_begins_with(\"hello\", \"hello world\")  # Output: True\nresult2 = string_begins_with(\"world\", \"hello world\")  # Output: False\n\nprint(result1)\nprint(result2)\n</code></pre>"},{"location":"zeta/utils/main/#function-group_by_key_prefixprefix-d","title":"Function: group_by_key_prefix(prefix, d)","text":"<p>Group dictionary items by keys that start with a specific prefix.</p>"},{"location":"zeta/utils/main/#parameters_13","title":"Parameters:","text":"<ul> <li><code>prefix</code> (str): The prefix to check for.</li> <li><code>d</code> (dict): The dictionary to group.</li> </ul>"},{"location":"zeta/utils/main/#returns_9","title":"Returns:","text":"<ul> <li><code>tuple</code>: Two dictionaries split based on the prefix condition.</li> </ul>"},{"location":"zeta/utils/main/#example_13","title":"Example:","text":"<pre><code>from zeta.utils.main import group_by_key_prefix\n\ndata = {\"prefix_a_1\": 1, \"prefix_a_2\": 2, \"prefix_b_1\": 3}\nprefix = \"prefix_a\"\n\ngroup1, group2 = group_by_key_prefix(prefix, data)\nprint(group1)  # Output: {'prefix_a_1': 1, 'prefix_a_2': 2}\nprint(group2)  # Output: {'prefix_b_1': 3}\n</code></pre>"},{"location":"zeta/utils/main/#function-groupby_prefix_and_trimprefix-d","title":"Function: groupby_prefix_and_trim(prefix, d)","text":"<p>Group dictionary items by keys that start with a specific prefix and remove the prefix.</p>"},{"location":"zeta/utils/main/#parameters_14","title":"Parameters:","text":"<ul> <li><code>prefix</code> (str): The prefix to check for.</li> <li><code>d</code> (dict): The dictionary to group.</li> </ul>"},{"location":"zeta/utils/main/#returns_10","title":"Returns:","text":"<ul> <li><code>tuple</code>: Dictionary with the prefix removed and another dictionary with remaining items.</li> </ul>"},{"location":"zeta/utils/main/#example_14","title":"Example:","text":"<pre><code>from zeta.utils.main import groupby_prefix_and_trim\n\ndata = {\"prefix_a_1\": 1, \"prefix_a_2\": 2, \"prefix_b_1\": 3}\nprefix = \"prefix_a\"\n\ngroup1, group2 = groupby_prefix_and_trim(prefix, data)\nprint(group1)  # Output: {'1': 1, '2': 2}\nprint(group2)  # Output: {'prefix_b_1': 3}\n</code></pre>"},{"location":"zeta/utils/main/#function-divisible_bynum-den","title":"Function: divisible_by(num, den)","text":"<p>Check if a number is divisible by another number.</p>"},{"location":"zeta/utils/main/#parameters_15","title":"Parameters:","text":"<ul> <li><code>num</code> (int): The number to check for divisibility.</li> <li><code>den</code> (int): The divisor.</li> </ul>"},{"location":"zeta/utils/main/#returns_11","title":"Returns:","text":"<ul> <li><code>bool</code>: True if num is divisible by den, False otherwise.</li> </ul>"},{"location":"zeta/utils/main/#example_15","title":"Example:","text":"<pre><code>from zeta.utils.main import divisible_by\n\nresult1 = divisible_by(10, 2)  # Output: True\nresult2 = divisible_by(7, 3)  # Output: False\n\nprint(result1)\nprint(result2)\n</code></pre>"},{"location":"zeta/utils/main/#function-top_plogits-thres-09","title":"Function: top_p(logits, thres = 0.9)","text":"<p>Apply top-p sampling to logits.</p>"},{"location":"zeta/utils/main/#parameters_16","title":"Parameters:","text":"<ul> <li><code>logits</code> (torch.Tensor): Input logits.</li> <li><code>thres</code> (float): Threshold value for top-p sampling.</li> </ul>"},{"location":"zeta/utils/main/#returns_12","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Processed logits.</li> </ul>"},{"location":"zeta/utils/main/#example_16","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import top_p\n\nlogits = torch.tensor([1.0, 2.0, 3.0])\nprocessed_logits = top_p(logits)  # Processed logits based on top-p sampling\n\nprint(processed_logits)\n</code></pre>"},{"location":"zeta/utils/main/#function-top_klogits-thres09","title":"Function: top_k(logits, thres=0.9)","text":"<p>Apply top-k sampling to logits.</p>"},{"location":"zeta/utils/main/#parameters_17","title":"Parameters:","text":"<ul> <li><code>logits</code> (torch.Tensor): Input logits.</li> <li><code>thres</code> (float): Threshold value for top-k sampling.</li> </ul>"},{"location":"zeta/utils/main/#returns_13","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Processed logits.</li> </ul>"},{"location":"zeta/utils/main/#example_17","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import top_k\n\nlogits = torch.tensor([1.0, 2.0, 3.0])\nprocessed_logits = top_k(logits)  # Processed logits based on top-k sampling\n\nprint(processed_logits)\n</code></pre>"},{"location":"zeta/utils/main/#function-top_alogits-min_p_pow20-min_p_ratio002","title":"Function: top_a(logits, min_p_pow=2.0, min_p_ratio=0.02)","text":"<p>Apply top-a sampling to logits.</p>"},{"location":"zeta/utils/main/#parameters_18","title":"Parameters:","text":"<ul> <li><code>logits</code> (torch.Tensor): Input logits.</li> <li><code>min_p_pow</code> (float): Minimum probability power.</li> <li><code>min_p_ratio</code> (float): Minimum probability ratio.</li> </ul>"},{"location":"zeta/utils/main/#returns_14","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Processed logits.</li> </ul>"},{"location":"zeta/utils/main/#example_18","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import top_a\n\nlogits = torch.tensor([1.0, 2.0, 3.0])\nprocessed_logits = top_a(logits)  # Processed logits based on top-a sampling\n\nprint(processed_logits)\n</code></pre>"},{"location":"zeta/utils/main/#function-logt-eps1e-20","title":"Function: log(t, eps=1e-20)","text":"<p>Compute the natural logarithm of a tensor element-wise.</p>"},{"location":"zeta/utils/main/#parameters_19","title":"Parameters:","text":"<ul> <li><code>t</code> (torch.Tensor): Input tensor.</li> <li><code>eps</code> (float): Epsilon value to prevent taking the log of zero.</li> </ul>"},{"location":"zeta/utils/main/#returns_15","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Natural logarithm of the input tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_19","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import log\n\ntensor = torch.tensor([0.5, 1.0, 2.0])\nlog_tensor = log(tensor)  # Output: tensor([-0.6931,  0.0000,  0.6931])\n\nprint(log_tensor)\n</code></pre>"},{"location":"zeta/utils/main/#function-gumbel_noiset","title":"Function: gumbel_noise(t)","text":"<p>Generate Gumbel noise from a uniform noise tensor.</p>"},{"location":"zeta/utils/main/#parameters_20","title":"Parameters:","text":"<ul> <li><code>t</code> (torch.Tensor): Input uniform noise tensor.</li> </ul>"},{"location":"zeta/utils/main/#returns_16","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Gumbel noise tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_20","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import gumbel_noise\n\nuniform_noise = torch.rand(3)\ngumbel_noise_tensor = gumbel_noise(uniform_noise)\n\nprint(gumbel_noise_tensor)\n</code></pre>"},{"location":"zeta/utils/main/#function-gumnel_samplet-temperature1-dim-1","title":"Function: gumnel_sample(t, temperature=1., dim=-1)","text":"<p>Sample from a tensor using Gumbel-softmax relaxation.</p>"},{"location":"zeta/utils/main/#parameters_21","title":"Parameters:","text":"<ul> <li><code>t</code> (torch.Tensor): Input tensor.</li> <li><code>temperature</code> (float): Temperature parameter for sampling.</li> <li><code>dim</code> (int): Dimension along which to apply Gumbel-softmax.</li> </ul>"},{"location":"zeta/utils/main/#returns_17","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Sampled tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_21","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import gumnel_sample\n\nlogits = torch.tensor([1.0, 2.0, 3.0])\nsampled_tensor = gumnel_sample(logits)  # Sampled tensor using Gumbel-softmax\n\nprint(sampled_tensor)\n</code></pre>"},{"location":"zeta/utils/main/#class-contrastivetopknnmodule","title":"Class: ContrastiveTopK(nn.Module)","text":"<p>Calculate contrastive loss using top-k sampling.</p>"},{"location":"zeta/utils/main/#parameters_22","title":"Parameters:","text":"<ul> <li><code>alpha</code>: Alpha value for contrastive loss.</li> <li><code>k</code>: Number of top-k samples to consider.</li> </ul>"},{"location":"zeta/utils/main/#methods_3","title":"Methods:","text":"<ul> <li><code>forward(logits_exp, logits_ama)</code>: Calculate contrastive loss based on input logits.</li> </ul>"},{"location":"zeta/utils/main/#example_22","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import ContrastiveTopK\n\ncontrastive = ContrastiveTopK(alpha=0.5, k=3)\n\nlogits_exp = torch.tensor([1.0, 2.0, 3.0])\nlogits_ama = torch.tensor([4.0, 5.0, 6.0])\n\nloss = contrastive(logits_exp, logits_ama)\nprint(loss)\n</code></pre>"},{"location":"zeta/utils/main/#function-print_num_paramsmodel-accelerator-accelerator","title":"Function: print_num_params(model, accelerator: Accelerator)","text":"<p>Print the number of parameters in a model.</p>"},{"location":"zeta/utils/main/#parameters_23","title":"Parameters:","text":"<ul> <li><code>model</code>: The model to print parameter count for.</li> <li><code>accelerator</code>: The accelerator object.</li> </ul>"},{"location":"zeta/utils/main/#example_23","title":"Example:","text":"<pre><code>import torch.nn as nn\nfrom accelerate import Accelerator\n\nfrom zeta.utils.main import print_num_params\n\n\nclass ExampleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(10, 5)\n\n\nmodel = ExampleModel()\naccelerator = Accelerator()\nprint_num_params(model, accelerator)\n</code></pre>"},{"location":"zeta/utils/main/#class-blocknnmodule","title":"Class: Block(nn.Module)","text":"<p>A basic block module with convolution, normalization, and activation layers.</p>"},{"location":"zeta/utils/main/#parameters_24","title":"Parameters:","text":"<ul> <li><code>dim</code> (int): Input dimension of the block.</li> <li><code>dim_out</code> (int): Output dimension of the block.</li> <li><code>groups</code> (int, optional): Number of groups for group normalization. Default is 8.</li> </ul>"},{"location":"zeta/utils/main/#methods_4","title":"Methods:","text":"<ul> <li><code>forward(x, scale_shift=None)</code>: Forward pass through the block.</li> </ul>"},{"location":"zeta/utils/main/#example_24","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import Block\n\nblock = Block(dim=64, dim_out=128, groups=4)\n\nx = torch.randn(1, 64, 16, 16)\noutput = block(x)\n\nprint(output.shape)\n</code></pre>"},{"location":"zeta/utils/main/#class-resnetblocknnmodule","title":"Class: ResnetBlock(nn.Module)","text":"<p>A residual block with convolutional layers and optional time embedding.</p>"},{"location":"zeta/utils/main/#parameters_25","title":"Parameters:","text":"<ul> <li><code>dim</code> (int): Input dimension of the block.</li> <li><code>dim_out</code> (int): Output dimension of the block.</li> <li><code>time_emb_dim</code> (int, optional): Dimension of the time embedding. Default is None.</li> <li><code>groups</code> (int, optional): Number of groups for group normalization. Default is 8.</li> </ul>"},{"location":"zeta/utils/main/#methods_5","title":"Methods:","text":"<ul> <li><code>forward(x, time_emb=None)</code>: Forward pass through the block.</li> </ul>"},{"location":"zeta/utils/main/#example_25","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import ResnetBlock\n\nresnet_block = ResnetBlock(dim=128, dim_out=256, time_emb_dim=32)\n\nx = torch.randn(1, 128, 8, 8)\ntime_emb = torch.randn(1, 32)\noutput = resnet_block(x, time_emb=time_emb)\n\nprint(output.shape)\n</code></pre>"},{"location":"zeta/utils/main/#function-load_modelpath","title":"Function: load_model(path)","text":"<p>Load a model from a file.</p>"},{"location":"zeta/utils/main/#parameters_26","title":"Parameters:","text":"<ul> <li><code>path</code> (str): Path to the file containing the model.</li> </ul>"},{"location":"zeta/utils/main/#returns_18","title":"Returns:","text":"<ul> <li><code>torch.nn.Module</code>: Loaded model.</li> </ul>"},{"location":"zeta/utils/main/#example_26","title":"Example:","text":"<pre><code>from zeta.utils.main import load_model\n\nmodel = load_model(\"model_checkpoint.pth\")\nprint(model)\n</code></pre>"},{"location":"zeta/utils/main/#function-seek_all_imagesimg-channels3","title":"Function: seek_all_images(img, channels=3)","text":"<p>Iterate over all frames of a GIF image.</p>"},{"location":"zeta/utils/main/#parameters_27","title":"Parameters:","text":"<ul> <li><code>img</code> (PIL.Image.Image): Input GIF image.</li> <li><code>channels</code> (int): Number of color channels. Default is 3.</li> </ul>"},{"location":"zeta/utils/main/#yields","title":"Yields:","text":"<ul> <li><code>PIL.Image.Image</code>: Frames of the GIF image.</li> </ul>"},{"location":"zeta/utils/main/#example_27","title":"Example:","text":"<pre><code>from PIL import Image\n\nfrom zeta.utils.main import seek_all_images\n\ngif_path = \"animation.gif\"\ngif_img = Image.open(gif_path)\n\nfor frame in seek_all_images(gif_img, channels=3):\n    frame.show()\n</code></pre>"},{"location":"zeta/utils/main/#function-video_tensor_to_giftensor-path-duration120-loop0-optimizetrue","title":"Function: video_tensor_to_gif(tensor, path, duration=120, loop=0, optimize=True)","text":"<p>Convert a video tensor to a GIF image.</p>"},{"location":"zeta/utils/main/#parameters_28","title":"Parameters:","text":"<ul> <li><code>tensor</code> (torch.Tensor): Video tensor of shape (channels, frames, height, width).</li> <li><code>path</code> (str): Path to save the GIF image.</li> <li><code>duration</code> (int): Duration of each frame in milliseconds. Default is 120.</li> <li><code>loop</code> (int): Number of loops for the GIF. Default is 0 (infinite).</li> <li><code>optimize</code> (bool): Whether to optimize the GIF for size. Default is True.</li> </ul>"},{"location":"zeta/utils/main/#example_28","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import video_tensor_to_gif\n\nvideo_tensor = torch.randn(3, 10, 256, 256)\noutput_gif_path = \"output_animation.gif\"\n\nvideo_tensor_to_gif(video_tensor, output_gif_path, duration=100)\n</code></pre>"},{"location":"zeta/utils/main/#function-gif_to_tensorpath-channels3-transformttotensor","title":"Function: gif_to_tensor(path, channels=3, transform=T.ToTensor())","text":"<p>Convert a GIF image to a video tensor.</p>"},{"location":"zeta/utils/main/#parameters_29","title":"Parameters:","text":"<ul> <li><code>path</code> (str): Path to the GIF image.</li> <li><code>channels</code> (int): Number of color channels. Default is 3.</li> <li><code>transform</code> (callable): Transformation function to apply to each frame. Default is <code>T.ToTensor()</code>.</li> </ul>"},{"location":"zeta/utils/main/#returns_19","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Video tensor of shape (channels, frames, height, width).</li> </ul>"},{"location":"zeta/utils/main/#example_29","title":"Example:","text":"<pre><code>from zeta.utils.main import gif_to_tensor\n\ninput_gif_path = \"input_animation.gif\"\nvideo_tensor = gif_to_tensor(input_gif_path, channels=3)\n\nprint(video_tensor.shape)\n</code></pre>"},{"location":"zeta/utils/main/#function-identityt-args-kwargs","title":"Function: identity(t, args, *kwargs)","text":"<p>Identity function that returns the input tensor as is.</p>"},{"location":"zeta/utils/main/#parameters_30","title":"Parameters:","text":"<ul> <li><code>t</code> (torch.Tensor): Input tensor.</li> <li><code>*args</code> (tuple): Additional positional arguments.</li> <li><code>**kwargs</code> (dict): Additional keyword arguments.</li> </ul>"},{"location":"zeta/utils/main/#returns_20","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Input tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_30","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import identity\n\ntensor = torch.tensor([1.0, 2.0, 3.0])\noutput = identity(tensor, some_arg=\"value\")\n\nprint(output)\n</code></pre>"},{"location":"zeta/utils/main/#function-normalize_imgt","title":"Function: normalize_img(t)","text":"<p>Normalize an image tensor to the range [-1, 1].</p>"},{"location":"zeta/utils/main/#parameters_31","title":"Parameters:","text":"<ul> <li><code>t</code> (torch.Tensor): Input image tensor.</li> </ul>"},{"location":"zeta/utils/main/#returns_21","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Normalized image tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_31","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import normalize_img\n\nimage_tensor = torch.rand(3, 256, 256)  # RGB image\nnormalized_image = normalize_img(image_tensor)\n\nprint(normalized_image.min(), normalized_image.max())\n</code></pre>"},{"location":"zeta/utils/main/#function-unnormalize_imgt","title":"Function: unnormalize_img(t)","text":"<p>Unnormalize a normalized image tensor.</p>"},{"location":"zeta/utils/main/#parameters_32","title":"Parameters:","text":"<ul> <li><code>t</code> (torch.Tensor): Input normalized image tensor.</li> </ul>"},{"location":"zeta/utils/main/#returns_22","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Unnormalized image tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_32","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import unnormalize_img\n\nnormalized_image = torch.rand(3, 256, 256)  # Normalized image\nunnormalized_image = unnormalize_img(normalized_image)\n\nprint(unnormalized_image.min(), unnormalized_image.max())\n</code></pre>"},{"location":"zeta/utils/main/#function-cast_num_framest-frames","title":"Function: cast_num_frames(t, frames)","text":"<p>Cast the number of frames in a video tensor to a specific value.</p>"},{"location":"zeta/utils/main/#parameters_33","title":"Parameters:","text":"<ul> <li><code>t</code> (torch.Tensor): Input video tensor of shape (channels, frames, height, width).</li> <li><code>frames</code> (int): Number of frames to cast to.</li> </ul>"},{"location":"zeta/utils/main/#returns_23","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Video tensor with the specified number of frames.</li> </ul>"},{"location":"zeta/utils/main/#example_33","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import cast_num_frames\n\nvideo_tensor = torch.rand(3, 10, 256, 256)\nvideo_tensor_casted = cast_num_frames(video_tensor, frames=8)\n\nprint(video_tensor_casted.shape)\n</code></pre>"},{"location":"zeta/utils/main/#function-max_neg_valuestensor","title":"Function: max_neg_values(tensor)","text":"<p>Get the maximum negative value for a tensor's data type.</p>"},{"location":"zeta/utils/main/#parameters_34","title":"Parameters:","text":"<ul> <li><code>tensor</code> (torch.Tensor): Input tensor.</li> </ul>"},{"location":"zeta/utils/main/#returns_24","title":"Returns:","text":"<ul> <li><code>float</code>: Maximum negative value.</li> </ul>"},{"location":"zeta/utils/main/#example_34","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import max_neg_values\n\ntensor = torch.tensor([1.0, 2.0, 3.0])\nmax_neg = max_neg_values(tensor.dtype)\n\nprint(max_neg)\n</code></pre>"},{"location":"zeta/utils/main/#function-l2normt-groups1","title":"Function: l2norm(t, groups=1)","text":"<p>Perform L2 normalization along specified groups of a tensor.</p>"},{"location":"zeta/utils/main/#parameters_35","title":"Parameters:","text":"<ul> <li><code>t</code> (torch.Tensor): Input tensor.</li> <li><code>groups</code> (int): Number of groups</li> </ul> <p>for normalization. Default is 1.</p>"},{"location":"zeta/utils/main/#returns_25","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: L2 normalized tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_35","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import l2norm\n\ntensor = torch.tensor([1.0, 2.0, 3.0])\nl2_normalized_tensor = l2norm(tensor, groups=2)\n\nprint(l2_normalized_tensor)\n</code></pre>"},{"location":"zeta/utils/main/#function-pad_at_dimt-pad-dim-1-value0","title":"Function: pad_at_dim(t, pad, dim=-1, value=0.)","text":"<p>Pad a tensor along a specified dimension.</p>"},{"location":"zeta/utils/main/#parameters_36","title":"Parameters:","text":"<ul> <li><code>t</code> (torch.Tensor): Input tensor.</li> <li><code>pad</code> (tuple): Padding values to add before and after the dimension.</li> <li><code>dim</code> (int): Dimension along which to pad. Default is -1.</li> <li><code>value</code> (float): Padding value. Default is 0.</li> </ul>"},{"location":"zeta/utils/main/#returns_26","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Padded tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_36","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import pad_at_dim\n\ntensor = torch.tensor([1.0, 2.0, 3.0])\npadded_tensor = pad_at_dim(tensor, pad=(1, 1), dim=-1, value=-1)\n\nprint(padded_tensor)\n</code></pre>"},{"location":"zeta/utils/main/#function-or_reducemasks","title":"Function: or_reduce(masks)","text":"<p>Perform element-wise logical OR reduction on a list of masks.</p>"},{"location":"zeta/utils/main/#parameters_37","title":"Parameters:","text":"<ul> <li><code>masks</code> (list of torch.Tensor): List of boolean masks.</li> </ul>"},{"location":"zeta/utils/main/#returns_27","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Resulting mask after OR reduction.</li> </ul>"},{"location":"zeta/utils/main/#example_37","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import or_reduce\n\nmask1 = torch.tensor([True, False, True])\nmask2 = torch.tensor([False, True, False])\nresult_mask = or_reduce([mask1, mask2])\n\nprint(result_mask)\n</code></pre>"},{"location":"zeta/utils/main/#class-residualnnmodule","title":"Class: Residual(nn.Module)","text":"<p>A wrapper module that adds residual connections to a given module.</p>"},{"location":"zeta/utils/main/#parameters_38","title":"Parameters:","text":"<ul> <li><code>fn</code> (nn.Module): Module to wrap with residual connection.</li> </ul>"},{"location":"zeta/utils/main/#methods_6","title":"Methods:","text":"<ul> <li><code>forward(x, *args, **kwargs)</code>: Forward pass through the module with residual connection.</li> </ul>"},{"location":"zeta/utils/main/#example_38","title":"Example:","text":"<pre><code>from zeta.utils.main import Residual\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        # Define your layers here\n\n    def forward(self, x):\n        # Forward pass logic\n\nmy_module = MyModule()\nresidual_module = Residual(my_module)\n\nx = torch.randn(1, 64)\noutput = residual_module(x)\n\nprint(output.shape)\n</code></pre>"},{"location":"zeta/utils/main/#class-sinusoidalposembnnmodule","title":"Class: SinusoidalPosEmb(nn.Module)","text":"<p>Sinusoidal positional embedding module for self-attention mechanisms.</p>"},{"location":"zeta/utils/main/#parameters_39","title":"Parameters:","text":"<ul> <li><code>dim</code> (int): Dimension of the positional embedding.</li> </ul>"},{"location":"zeta/utils/main/#methods_7","title":"Methods:","text":"<ul> <li><code>forward(x)</code>: Forward pass to generate positional embeddings for input tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_39","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import SinusoidalPosEmb\n\npos_emb_module = SinusoidalPosEmb(dim=128)\n\nx = torch.randn(1, 16, 128)  # Input tensor\npos_emb = pos_emb_module(x)\n\nprint(pos_emb.shape)\n</code></pre>"},{"location":"zeta/utils/main/#function-upsampledim","title":"Function: upsample(dim)","text":"<p>Create an upsample layer for a given dimension.</p>"},{"location":"zeta/utils/main/#parameters_40","title":"Parameters:","text":"<ul> <li><code>dim</code> (int): Dimension of the input and output channels.</li> </ul>"},{"location":"zeta/utils/main/#returns_28","title":"Returns:","text":"<ul> <li><code>nn.Module</code>: Upsample layer.</li> </ul>"},{"location":"zeta/utils/main/#example_40","title":"Example:","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.utils.main import upsample\n\nupsample_layer = upsample(dim=256)\n\nx = torch.randn(1, 256, 8, 8)  # Input tensor\noutput = upsample_layer(x)\n\nprint(output.shape)\n</code></pre>"},{"location":"zeta/utils/main/#function-downsampledim","title":"Function: downsample(dim)","text":"<p>Create a downsample layer for a given dimension.</p>"},{"location":"zeta/utils/main/#parameters_41","title":"Parameters:","text":"<ul> <li><code>dim</code> (int): Dimension of the input and output channels.</li> </ul>"},{"location":"zeta/utils/main/#returns_29","title":"Returns:","text":"<ul> <li><code>nn.Module</code>: Downsample layer.</li> </ul>"},{"location":"zeta/utils/main/#example_41","title":"Example:","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.utils.main import downsample\n\ndownsample_layer = downsample(dim=256)\n\nx = torch.randn(1, 256, 16, 16)  # Input tensor\noutput = downsample_layer(x)\n\nprint(output.shape)\n</code></pre>"},{"location":"zeta/utils/main/#class-layernormnnmodule","title":"Class: LayerNorm(nn.Module)","text":"<p>Layer normalization module.</p>"},{"location":"zeta/utils/main/#parameters_42","title":"Parameters:","text":"<ul> <li><code>dim</code> (int): Dimension for normalization.</li> <li><code>eps</code> (float): Small value added to the denominator for numerical stability.</li> </ul>"},{"location":"zeta/utils/main/#methods_8","title":"Methods:","text":"<ul> <li><code>forward(x)</code>: Forward pass through the layer normalization.</li> </ul>"},{"location":"zeta/utils/main/#example_42","title":"Example:","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.utils.main import LayerNorm\n\nlayer_norm = LayerNorm(dim=256, eps=1e-5)\n\nx = torch.randn(1, 256, 16, 16)  # Input tensor\nnormalized_x = layer_norm(x)\n\nprint(normalized_x.shape)\n</code></pre>"},{"location":"zeta/utils/main/#class-prenormnnmodule","title":"Class: PreNorm(nn.Module)","text":"<p>Pre-normalization wrapper module.</p>"},{"location":"zeta/utils/main/#parameters_43","title":"Parameters:","text":"<ul> <li><code>dim</code> (int): Dimension for normalization.</li> <li><code>fn</code> (nn.Module): Module to wrap with pre-normalization.</li> </ul>"},{"location":"zeta/utils/main/#methods_9","title":"Methods:","text":"<ul> <li><code>forward(x, **kwargs)</code>: Forward pass through the module with pre-normalization.</li> </ul>"},{"location":"zeta/utils/main/#example_43","title":"Example:","text":"<pre><code>from zeta.utils.main import PreNorm\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        # Define your layers here\n\n    def forward(self, x):\n        # Forward pass logic\n\nmy_module = MyModule()\npre_norm_module = PreNorm(dim=128, fn=my_module)\n\nx = torch.randn(1, 128)\noutput = pre_norm_module(x)\n\nprint(output.shape)\n</code></pre>"},{"location":"zeta/utils/main/#function-cosine_beta_scheduletimesteps-s0008","title":"Function: cosine_beta_schedule(timesteps, s=0.008)","text":"<p>Generate a cosine beta schedule for progressive loss scaling.</p>"},{"location":"zeta/utils/main/#parameters_44","title":"Parameters:","text":"<ul> <li><code>timesteps</code> (int): Total number of time steps.</li> <li><code>s</code> (float): Scaling factor for the cosine function.</li> </ul>"},{"location":"zeta/utils/main/#returns_30","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Beta values for each time step.</li> </ul>"},{"location":"zeta/utils/main/#example_44","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import cosine_beta_schedule\n\nbeta_schedule = cosine_beta_schedule(timesteps=1000, s=0.01)\nprint(beta_schedule)\n</code></pre>"},{"location":"zeta/utils/main/#class-normalizennmodule","title":"Class: Normalize(nn.Module)","text":"<p>Normalization module to perform L2 normalization along a specific dimension.</p>"},{"location":"zeta/utils/main/#parameters_45","title":"Parameters:","text":"<ul> <li><code>dim</code> (int): Dimension for normalization.</li> </ul>"},{"location":"zeta/utils/main/#methods_10","title":"Methods:","text":"<ul> <li><code>forward(x)</code>: Forward pass through the normalization.</li> </ul>"},{"location":"zeta/utils/main/#example_45","title":"Example:","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.utils.main import Normalize\n\nnormalize_module = Normalize(dim=256)\n\nx = torch.randn(1, 256, 16, 16)  # Input tensor\nnormalized_x = normalize_module(x)\n\nprint(normalized_x.shape)\n</code></pre>"},{"location":"zeta/utils/main/#class-learnablelogitscalingnnmodule","title":"Class: LearnableLogitScaling(nn.Module)","text":"<p>Learnable logit scaling module for temperature scaling in temperature sampling.</p>"},{"location":"zeta/utils/main/#parameters_46","title":"Parameters:","text":"<ul> <li><code>logit_scale_init</code> (float): Initial value for the logit scale.</li> <li><code>learnable</code> (bool): Whether the logit scale is learnable. Default is True.</li> <li><code>max_logit_scale</code> (float): Maximum value for the logit scale. Default is 100.</li> </ul>"},{"location":"zeta/utils/main/#methods_11","title":"Methods:","text":"<ul> <li><code>forward(x)</code>: Forward pass through the learnable logit scaling.</li> </ul>"},{"location":"zeta/utils/main/#example_46","title":"Example:","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.utils.main import LearnableLogitScaling\n\nlogit_scaling = LearnableLogitScaling(\n    logit_scale_init=1.0, learnable=True, max_logit_scale=10.0\n)\n\nx = torch.randn(1, 256)  # Input tensor\nscaled_x = logit_scaling(x)\n\nprint(scaled_x.shape)\n</code></pre>"},{"location":"zeta/utils/main/#class-einopsrearrangennmodule","title":"Class: EinOpsRearrange(nn.Module)","text":"<p>EinOps-based module for rearranging tensor dimensions.</p>"},{"location":"zeta/utils/main/#parameters_47","title":"Parameters:","text":"<ul> <li><code>rearrange_expr</code> (str): Rearrangement expression.</li> <li><code>**kwargs</code>: Additional arguments for einops.rearrange.</li> </ul>"},{"location":"zeta/utils/main/#methods_12","title":"Methods:","text":"<ul> <li><code>forward(x)</code>: Forward pass to rearrange the input tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_47","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import EinOpsRearrange\n\nrearrange_module = EinOpsRearrange(rearrange_expr=\"b h w c -&gt; b c h w\", h=16, w=16)\n\nx = torch.randn(1, 16, 16, 256)  # Input tensor\nrearranged_x = rearrange_module(x)\n\nprint(rearranged_x.shape)\n</code></pre>"},{"location":"zeta/utils/main/#function-get_sinusoid_encoding_tablen_position-d_hid","title":"Function: get_sinusoid_encoding_table(n_position, d_hid)","text":"<p>Generate a sinusoidal positional encoding table for self-attention mechanisms.</p>"},{"location":"zeta/utils/main/#parameters_48","title":"Parameters:","text":"<ul> <li><code>n_position</code> (int): Number of positions.</li> <li><code>d_hid</code> (int): Hidden dimension.</li> </ul>"},{"location":"zeta/utils/main/#returns_31","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Sinusoidal positional encoding table.</li> </ul>"},{"location":"zeta/utils/main/#example_48","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import get_sinusoid_encoding_table\n\npos_encoding_table = get_sinusoid_encoding_table(n_position=100, d_hid=128)\n\nprint(pos_encoding_table.shape)\n</code></pre>"},{"location":"zeta/utils/main/#function-interpolate_pos_encoding_2dtarget_spatial_size-pos_embed","title":"Function: interpolate_pos_encoding_2d(target_spatial_size, pos_embed)","text":"<p>Interpolate 2D positional embeddings to a target spatial size.</p>"},{"location":"zeta/utils/main/#parameters_49","title":"Parameters:","text":"<ul> <li><code>target_spatial_size</code> (int): Target spatial size.</li> <li><code>pos_embed</code> (torch.Tensor): Input positional embeddings.</li> </ul>"},{"location":"zeta/utils/main/#returns_32","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Interpolated positional embeddings.</li> </ul>"},{"location":"zeta/utils/main/#example_49","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import interpolate_pos_encoding_2d\n\npos_embed = torch.randn(1, 64, 128)  # Input positional embeddings\ninterpolated_pos_embed = interpolate_pos_encoding_2d(\n    target_spatial_size=256, pos_embed=pos_embed\n)\n\nprint(interpolated_pos_embed.shape)\n</code></pre>"},{"location":"zeta/utils/main/#function-cast_if_src_dtypetensor-src_dtype-tgt_dtype","title":"Function: cast_if_src_dtype(tensor, src_dtype, tgt_dtype)","text":"<p>Cast a tensor to a target dtype if its source dtype matches.</p>"},{"location":"zeta/utils/main/#parameters_50","title":"Parameters:","text":"<ul> <li><code>tensor</code> (torch.Tensor): Input tensor.</li> <li><code>src_dtype</code> (torch.dtype): Source dtype to check.</li> <li><code>tgt_dtype</code> (torch.dtype): Target dtype to cast to.</li> </ul>"},{"location":"zeta/utils/main/#returns_33","title":"Returns:","text":"<ul> <li><code>torch.Tensor</code>: Casted tensor if necessary.</li> </ul>"},{"location":"zeta/utils/main/#example_50","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import cast_if_src_dtype\n\ntensor = torch.randn(1, 256)\ncasted_tensor = cast_if_src_dtype(\n    tensor, src_dtype=torch.float32, tgt_dtype=torch.bfloat16\n)\n\nprint(casted_tensor.dtype)\n</code></pre>"},{"location":"zeta/utils/main/#class-selectelementsnnmodule","title":"Class: SelectElements(nn.Module)","text":"<p>Select specific elements from an input tensor using given indices.</p>"},{"location":"zeta/utils/main/#parameters_51","title":"Parameters:","text":"<ul> <li><code>index</code> (int): Index to select elements along a specific dimension.</li> </ul>"},{"location":"zeta/utils/main/#methods_13","title":"Methods:","text":"<ul> <li><code>forward(x)</code>: Forward pass to select elements from the input tensor.</li> </ul>"},{"location":"zeta/utils/main/#example_51","title":"Example:","text":"<pre><code>import torch\n\nfrom zeta.utils.main import SelectElements\n\nselect_module = SelectElements(index=2)\n\nx = torch.randn(1, 4, 256)  # Input tensor\nselected_elements = select_module(x)\n\nprint(selected_elements.shape)\n</code></pre>"},{"location":"zeta/utils/main/#class-selecteosandprojectnnmodule","title":"Class: SelectEOSAndProject(nn.Module)","text":"<p>Select elements from the end of a sequence and apply a projection.</p>"},{"location":"zeta/utils/main/#parameters_52","title":"Parameters:","text":"<ul> <li><code>proj</code> (nn.Module): Projection module to apply after selection.</li> </ul>"},{"location":"zeta/utils/main/#methods_14","title":"Methods:","text":"<ul> <li><code>forward(x, seq_len)</code>: Forward pass to select elements and apply projection.</li> </ul>"},{"location":"zeta/utils/main/#example_52","title":"Example:","text":"<pre><code>import torch.nn as nn\n\nfrom zeta.utils.main import SelectEOSAndProject\n\nproj_module = nn.Linear(256, 128)\nselect_and_project = SelectEOSAndProject(proj=proj_module)\n\nx = torch.randn(1, 16, 256)  # Input tensor\nseq_len = torch.tensor([10])  # Sequence length\noutput = select_and_project(x, seq_len)\n\nprint(output.shape)\n</code></pre>"},{"location":"zeta/utils/maybe/","title":"maybe","text":""},{"location":"zeta/utils/maybe/#modulefunction-name-maybe","title":"Module/Function Name: maybe","text":"<pre><code>def maybe(fn):\n    \"\"\"\n    Decorator that calls a function if the first argument exists.\n\n    Args:\n        fn (function): The function to wrap.\n\n    Returns:\n        function: The wrapped function.\n    \"\"\"\n\n    @wraps(fn)\n    def inner(x, *args, **kwargs):\n        if not exists(x):\n            return x\n        return fn(x, *args, **kwargs)\n\n    return inner\n</code></pre>"},{"location":"zeta/utils/maybe/#description","title":"Description:","text":"<p>The <code>maybe</code> function is a Python decorator that wraps a given function (<code>fn</code>) and alters its behavior in such a way that it only calls this function if the first argument provided (<code>x</code>) exists. In the context of this decorator, \"exists\" typically means that <code>x</code> is not <code>None</code> although this could be adjusted to accommodate any variations on what it means for <code>x</code> to \"exist\" depending on your specific use case.</p> <p>This type of decorator can be tremendously useful in a number of contexts, including data preprocessing, data validation, error handling, and more.</p>"},{"location":"zeta/utils/maybe/#parameters","title":"Parameters:","text":"Parameter Type Description fn function The function to be decorated"},{"location":"zeta/utils/maybe/#returns","title":"Returns:","text":"Return Type Description function function The decorated function"},{"location":"zeta/utils/maybe/#usage-example","title":"Usage Example:","text":"<pre><code>from functools import wraps\n\n\ndef exists(x):\n    return x is not None\n\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x, *args, **kwargs):\n        if not exists(x):\n            return x\n        return fn(x, *args, **kwargs)\n\n    return inner\n\n\n@maybe\ndef add_one(x):\n    return x + 1\n\n\nprint(add_one(None))  # Returns: None\nprint(add_one(2))  # Returns: 3\n</code></pre> <p>In this example, we have created a <code>maybe</code> decorator using the given <code>maybe</code> function and applied it to the <code>add_one</code> function. When we call <code>add_one</code> with <code>None</code> as the argument, the <code>maybe</code> decorator checks if <code>None</code> exists (which it does not), and so it simply returns <code>None</code> without calling the <code>add_one</code> function. </p> <p>However, when we call <code>add_one</code> with <code>2</code> as the argument, the <code>maybe</code> decorator checks if <code>2</code> exists (which it does), and so it proceeds to call the <code>add_one</code> function, resulting in <code>3</code>.</p>"},{"location":"zeta/utils/maybe/#additional-information","title":"Additional Information:","text":"<p>The <code>maybe</code> decorator utilises the <code>@wraps</code> decorator from the <code>functools</code> module which updates the wrapper function to look like the wrapped function. This includes the function name, docstring, and module, amongst other attributes.</p> <p>The <code>if not exists(x)</code> part of the <code>inner</code> function acts as a short-circuit evaluation. This means <code>fn(x, *args, **kwargs)</code> is not executed if the <code>x</code> argument does not exist, thus preventing potential errors or exceptions from occurring.</p> <p>Please ensure to define an <code>exists</code> function according to your requirement, as it works with the <code>maybe</code> decorator to determine whether or not the function <code>fn</code> should be invoked.</p>"},{"location":"zeta/utils/module_device/","title":"module_device","text":""},{"location":"zeta/utils/module_device/#module-name-module_device","title":"Module Name: module_device","text":"<p>The <code>module_device</code> is a Python decorator function that efficiently manages a device on which a PyTorch neural network models, which is a subclass of <code>torch.nn.Module</code>, is loaded. This decorator helps in tracking the device on which different components (such as tensors) of the model are, especially in complex design models where different tensors can be on separate devices. This helps to avoid any device mismatch errors during computation.</p> <p>Moreover, it allows the developers to add their custom functions or operations that could be performed whenever the device changes. Also, it has an in-built compatibility check feature, which elegantly handles the case of trying to transfer to GPUs when CUDA is not available.</p> <p>To dive deep, let's see the main components and details of this function.</p>"},{"location":"zeta/utils/module_device/#class-defintion","title":"Class Defintion:","text":"<p><pre><code>def module_device(\n    device_property_name: str = \"device\",\n    on_device_transfer=None,\n    compatibility_check: bool = False,\n):\n</code></pre> This function has three parameters \u2013 <code>device_property_name</code>, <code>on_device_transfer</code>, and <code>compatibility_check</code>.</p> Parameter Type Default Description device_property_name string \"device\" Name of the attribute which would track the device of the decorated class. on_device_transfer callable/disable None A callable function that will be invoked whenever the device changes. This function will be executed after the object is transferred to a new device. If None, no function will be executed. compatibility_check boolean False If True, checks the compatibility of the device change in case of CUDA not being available when trying to transfer to GPUs. <p>Here, <code>_dummy</code> is a registered buffer, a PyTorch state that is not a parametric tensor of the model but you want to save the model, so it persists across saving/loading roundtrips.</p> <p>In case of multiple GPUs and your model spans them, this decorator will store all the devices.</p> <p>The <code>decorator</code> function wraps around a user-defined class. It keeps track of the device and throws an error when an incompatible device is used and updates the new device property in case of valid device change. It can also assist in performing user defined operations in case of device change using <code>on_device_transfer</code> function.</p>"},{"location":"zeta/utils/module_device/#usage-examples","title":"Usage Examples:","text":"<p>Let's look at three ways to use this function.</p>"},{"location":"zeta/utils/module_device/#example-1","title":"Example 1:","text":"<p>In the first example, we simply use this decorator to add a new device property (named \"my_cuda_device\" here) to our model, which always stores the current device of our model.</p> <pre><code>from torch import tensor\nfrom torch.nn import Module\n\n\n@module_device(device_property_name=\"my_cuda_device\")\nclass MyModel(Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, output_size)\n\n\nMyModel_obj = MyModel(10, 10)\nMyModel_obj.to(\"cuda\")\n\nprint(MyModel_obj.my_cuda_device)  # Output: cuda:&lt;device_no&gt;\n</code></pre>"},{"location":"zeta/utils/module_device/#example-2","title":"Example 2:","text":"<p>In the second example, we will define a function that will be executed whenever the device changes. Here for simplicity, we will just print a simple message.</p> <pre><code>def transfer_fn(self, device):\n    print(f\"Transferred to {device}\")\n\n\n@module_device(on_device_transfer=transfer_fn)\nclass SecondModel(Module):\n    pass\n\n\nSecondModel_obj = SecondModel()\nSecondModel_obj.to(\"cuda\")  # Output: Transferred to cuda:&lt;device_no&gt;\n</code></pre>"},{"location":"zeta/utils/module_device/#example-3","title":"Example 3:","text":"<p>In the third example, we will use both the features discussed above together:</p> <pre><code>def transfer_fn(self, device):\n    print(f\"Transferred to {device}\")\n\n\n@module_device(device_property_name=\"my_device\", on_device_transfer=transfer_fn)\nclass ThirdModel(Module):\n    pass\n\n\nThirdModel_obj = ThirdModel()\nThirdModel_obj.to(\"cuda\")  # Output: Transferred to cuda:&lt;device_no&gt;\nprint(ThirdModel_obj.my_device)  # Output: cuda:&lt;device_no&gt;\n</code></pre>"},{"location":"zeta/utils/once/","title":"once","text":""},{"location":"zeta/utils/once/#function-name-once","title":"Function Name: once","text":""},{"location":"zeta/utils/once/#overview-and-introduction","title":"Overview and Introduction","text":"<p>In a variety of contexts, whether while initializing some variables, setting up logging, or ensuring some heavy computation isn't undertaken multiple times, there are scenarios where you might want to ensure a function is executed only once. The <code>once</code> function is a Python decorator that took up this challenge. By using it, we guarantee a wrapped function is called only for the first time it is invoked.</p> <p>The <code>once</code> function meets this requirement by retaining a flag <code>called</code> in its closure. This flag tracks whether or not a function has been called before. When the function is called, it checks the flag. If the flag is false (<code>False</code>), implying the function hasn't been called before, it allows the function to execute and toggles the flag. If the flag is true (<code>True</code>), indicating the function has been called before, it simply returns, preventing the function execution.</p>"},{"location":"zeta/utils/once/#function-definition","title":"Function Definition","text":"<p>Let's consider the structure and details of the <code>once</code> function. It accepts a single argument, <code>fn</code>, which is the function to be wrapped. The function is returned as the output after being wrapped in a closure that maintains the <code>called</code> flag. </p> <pre><code>def once(fn):\n    \"\"\"\n    Decorator to ensure the function is only called once.\n\n    Args:\n       fn (function): The function to wrap.\n\n    Returns:\n        function: The wrapped function.\n    \"\"\"\n    called = False\n\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n\n    return inner\n</code></pre> Argument Type Description fn function The function to wrap."},{"location":"zeta/utils/once/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>once</code> function ensures that the annotated function <code>fn</code> is executed only once - the first time it's called. For all subsequent calls, it immediately returns without executing the function <code>fn</code>. The <code>once</code> decorator therefore is particularly useful in scenarios where a specific function should not or need not be executed more than once. </p>"},{"location":"zeta/utils/once/#example-initial-setup-function","title":"Example - Initial Setup Function","text":"<p>Let's demonstrate the <code>once</code> function with a setup function, <code>setup()</code>. This could represent any kind of initialization logic that should only be run once:</p> <pre><code>@once\ndef setup():\n    print(\"Setting up...\")\n\n\n# The setup() function is invoked twice.\nsetup()  # Prints: 'Setting up...'\nsetup()  # Doesn't print anything.\n</code></pre>"},{"location":"zeta/utils/once/#example-heavy-computation-function","title":"Example - Heavy Computation Function","text":"<p>Here is an example where a computation should only be executed once:</p> <pre><code>@once\ndef heavy_computation():\n    print(\"Doing heavy computation...\")\n    # long running computation\n\n\n# The heavy_computation() function is invoked twice.\nheavy_computation()  # Prints: 'Doing heavy computation...'\nheavy_computation()  # Doesn't print anything.\n</code></pre>"},{"location":"zeta/utils/once/#example-state-initialisation","title":"Example - State Initialisation","text":"<p>If you are dealing with a stateful class and need to initialize something only once, <code>once</code> decorator can come handy:</p> <pre><code>class MyClass:\n    @once\n    def initialize(self):\n        print(\"Initializing state...\")\n\n\n# MyClass object is created, the initialize function is called twice.\nobj = MyClass()\nobj.initialize()  # Prints: 'Initializing state...'\nobj.initialize()  # Doesn't print anything.\n</code></pre> <p>In each of the above examples, similarly, the decorated function <code>setup()</code>, <code>heavy_computation()</code> and <code>initialize()</code> were called multiple times but executed only once.</p> <p>The use of <code>once</code> decorator provides a convenient way to ensure specific functions only run their core execution once, while allowing them to be flexibly called without caution multiple times elsewhere in code or scripts. This helps maintain cleaner and more predictable code especially when dealing with initializations and one-time setups.</p>"},{"location":"zeta/utils/pad_at_dim/","title":"pad_at_dim","text":""},{"location":"zeta/utils/pad_at_dim/#module-name-pad_at_dim","title":"Module Name: pad_at_dim","text":""},{"location":"zeta/utils/pad_at_dim/#introduction","title":"Introduction","text":"<p>The <code>pad_at_dim</code> function is a utility function used to apply padding to a tensor at a specified dimension. Padding is added to the edges of an input tensor and it's commonly used in convolutional neural networks where the input is often padded to control the output size of feature maps. This utility function is very useful to PyTorch users as it allows to add padding flexibly at any dimension, specified by the user.</p> <p>The tensor padding is particularly useful in the context of image processing where it is often needed to apply the convolution kernel to bordering pixels of an input image. In the context of natural language processing tasks, padding is used when batching together sequences of different lengths, and can be used to ensure that all sequences in a batch are the same length.</p>"},{"location":"zeta/utils/pad_at_dim/#function-definition","title":"Function Definition","text":"<p>The function <code>pad_at_dim</code> has the following signature:</p> <pre><code>def pad_at_dim(t, pad, dim=-1, value=0.0):\n    dims_from_right = (-dim - 1) if dim &lt; 0 else (t.ndim - dim - 1)\n    zeros = (0, 0) * dims_from_right\n    return F.pad(t, (*zeros, *pad), value=value)\n</code></pre>"},{"location":"zeta/utils/pad_at_dim/#parameters","title":"Parameters","text":"Parameter Type Description Default value t torch.Tensor Input tensor to which padding will be applied. NA pad tuple Number of values padded to the edges of each dimension, provided as a tuple in the format (padLeft, padRight) for each dimension. NA dim int Dimension at which padding will be added. Negative integer counts from the last dimension (-1 is the last dimension, -2 is the second last dimension, and so on). -1 value float Value for the padded elements. 0.0"},{"location":"zeta/utils/pad_at_dim/#return","title":"Return","text":"<p>The function returns a tensor <code>t</code> padded at the specified <code>dim</code> with the given <code>value</code>. The padding size is specified by the <code>pad</code> parameter.</p>"},{"location":"zeta/utils/pad_at_dim/#detailed-explanation-usage","title":"Detailed Explanation &amp; Usage","text":"<p>The <code>pad_at_dim</code> function uses the PyTorch <code>nn.functional.pad()</code> method to add padding to the tensor. It starts by determining the number of dimensions from the right of the tensor for which padding will be applied, stored in <code>dims_from_right</code>. It then creates the <code>zeros</code> tuple which has the number of zeros corresponding to the decided padding. Finally, the <code>pad</code> and <code>zeros</code> tuples are concatenated and used as input to the <code>nn.functional.pad()</code> method along with the original tensor and padding value.</p> <p>Dimensions in PyTorch are 0-index based, therefore 0 refers to the first dimension and -1 refers to the last dimension. When the padding size (pad) is a tuple, the padding applied is symmetric for each dimension. If pad is an int, the same amount of padding is applied at both ends of the tensor.</p> <p>The value parameter is used to fill in the new elements created due to padding operation.</p>"},{"location":"zeta/utils/pad_at_dim/#usage-examples","title":"Usage Examples","text":"<p>Let's look at some examples demonstrating the <code>pad_at_dim</code> function:</p> <ol> <li>Basic usage:</li> </ol> <pre><code>import torch\nfrom torch.nn import functional as F\n\n# Define a tensor\nt = torch.tensor([[1, 2, 3], [4, 5, 6]])\n\n# Call pad_at_dim\nresult = pad_at_dim(t, pad=(1, 1), dim=-1, value=0)\n\nprint(result)\n</code></pre> <p>Output: <pre><code>tensor([[0, 1, 2, 3, 0],\n        [0, 4, 5, 6, 0]])\n</code></pre></p> <ol> <li>Padding the first dimension:</li> </ol> <pre><code>result = pad_at_dim(t, pad=(2, 2), dim=0, value=-1)\nprint(result)\n</code></pre> <p>Output: <pre><code>tensor([[-1, -1, -1],\n        [-1, -1, -1],\n        [ 1,  2,  3],\n        [ 4,  5,  6],\n        [-1, -1, -1],\n        [-1, -1, -1]])\n</code></pre></p> <ol> <li>Padding the second dimension:</li> </ol> <pre><code>result = pad_at_dim(t, pad=(3, 3), dim=1, value=-2)\nprint(result)\n</code></pre> <p>Output: <pre><code>tensor([[-2, -2, -2,  1,  2,  3, -2, -2, -2],\n        [-2, -2, -2,  4,  5,  6, -2, -2, -2]])\n</code></pre></p>"},{"location":"zeta/utils/pad_at_dim/#additional-tips","title":"Additional Tips","text":"<ol> <li>Use this utility function</li> </ol>"},{"location":"zeta/utils/pick_and_pop/","title":"pick_and_pop","text":""},{"location":"zeta/utils/pick_and_pop/#modulefunction-name-pick_and_pop","title":"Module/Function Name: pick_and_pop","text":""},{"location":"zeta/utils/pick_and_pop/#overview","title":"Overview","text":"<p>The <code>pick_and_pop</code> function is a utility function that is specifically aimed at manipulating dictionaries. It removes specified keys from a given dictionary and then returns a new dictionary that contains the removed key-value pairs. This function can be particularly useful when you need to prune a dictionary to a simpler version that contains only desired keys-value pairs.</p> <p>The <code>pick_and_pop</code> function is defined in the Zeta utility module (<code>zeta.utils</code>). A dictionary in Python is an unordered collection of data in a key-value pair format. Dictionaries can have keys and values of any datatype, which makes dictionary highly valuable and versatile data structures for handling and organizing data.</p>"},{"location":"zeta/utils/pick_and_pop/#function-definition","title":"Function Definition","text":"<pre><code>def pick_and_pop(keys, d):\n    \"\"\"\n    Remove and return values from a dictionary based on provided keys.\n\n    Args:\n        keys (list): List of keys to remove from the dictionary.\n        d (dict): The dictionary to pick from.\n\n    Returns:\n        dict: A dictionary with the specified keys and their values.\n    \"\"\"\n    values = list(map(d.pop, keys))\n    return dict(zip(keys, values))\n</code></pre>"},{"location":"zeta/utils/pick_and_pop/#parameters-and-description","title":"Parameters and Description","text":"Parameter Type Default Description <code>keys</code> list N/A List of keys from the dictionary to be removed and returned as a new dictionary. <code>d</code> dict N/A The original dictionary where keys are picked and popped. <p>The function pick_and_pop accepts two arguments, a list of keys and a dictionary. The keys are provided in a list, and are the ones that the user wishes to remove from the dictionary. This function returns a new dictionary composed of these key-value pairs.</p>"},{"location":"zeta/utils/pick_and_pop/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The <code>pick_and_pop</code> function works by iterating over the list of keys and pops each key from the dictionary. The popped value is then appended to a list of values. After all the keys have been looped over, a new dictionary is created and returned by zipping together the list of keys and the list of values.</p> <p>The return type of this function is a dictionary.</p>"},{"location":"zeta/utils/pick_and_pop/#usage-example-1","title":"Usage Example 1","text":"<pre><code>d = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\nkeys = [\"name\", \"city\"]\n\nresult = pick_and_pop(keys, d)\nprint(result)  # Returns: {'name': 'John', 'city': 'New York'}\n</code></pre>"},{"location":"zeta/utils/pick_and_pop/#usage-example-2","title":"Usage Example 2","text":"<pre><code>d = {1: \"apple\", 2: \"banana\", 3: \"cherry\", 4: \"date\"}\nkeys = [2, 4]\n\nresult = pick_and_pop(keys, d)\nprint(result)  # Returns: {2: 'banana', 4: 'date'}\n</code></pre>"},{"location":"zeta/utils/pick_and_pop/#usage-example-3","title":"Usage Example 3","text":"<pre><code>d = {\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]}\nkeys = [\"a\", \"c\"]\n\nresult = pick_and_pop(keys, d)\nprint(result)  # Returns: {'a': [1, 2, 3], 'c': [7, 8, 9]}\n</code></pre>"},{"location":"zeta/utils/pick_and_pop/#additional-tips","title":"Additional Tips","text":"<p>It's important to understand that the <code>pick_and_pop</code> function directly alters the original dictionary <code>d</code> by removing the keys from it. If you want to retain the data in the original dictionary, you should create a copy of the original dictionary and pass the copy to the <code>pick_and_pop</code> function.</p>"},{"location":"zeta/utils/pick_and_pop/#references","title":"References","text":"<ul> <li>Python official documentaion: https://docs.python.org/3/tutorial/datastructures.html#dictionaries</li> <li>Python Glossary - dictionary: https://docs.python.org/3/glossary.html#term-dictionary</li> <li>Python map() function: https://docs.python.org/3/library/functions.html#map</li> <li>Python zip() function: https://docs.python.org/3/library/functions.html#zip</li> </ul> <p>After understanding this function, you will have a good knowledge of manipulating dictionaries in Python. This utility function simplifies the task of extracting certain key-value pairs from a dictionary into a new dictionary, which can be very useful in data wrangling and preprocessing tasks.</p>"},{"location":"zeta/utils/print_cuda_memory_usage/","title":"print_cuda_memory_usage","text":""},{"location":"zeta/utils/print_cuda_memory_usage/#zetautils-print_cuda_memory_usage","title":"<code>zeta.utils</code>: print_cuda_memory_usage","text":""},{"location":"zeta/utils/print_cuda_memory_usage/#purpose-and-functionality","title":"Purpose and Functionality","text":"<p>This is a Python context manager function designed for tracking and reporting CUDA (Compute Unified Device Architecture) memory usage during GPU-accelerated operations in PyTorch. CUDA is a parallel computing platform and application programming interface (API) model created by NVIDIA which allows software developers to use a CUDA-enabled graphics processing unit (GPU) for general-purpose processing.</p> <p><code>print_cuda_memory_usage</code> monitors the GPU memory consumption before and after the context block of code that it wraps. Upon exit of the context block, it calculates the change in memory usage and outputs it in gigabytes.</p>"},{"location":"zeta/utils/print_cuda_memory_usage/#function-definition","title":"Function Definition","text":"<pre><code>from contextlib import contextmanager\n\nimport torch\n\n\n@contextmanager\ndef print_cuda_memory_usage():\n    initial_memory = torch.cuda.memory_allocated()\n    try:\n        yield\n    finally:\n        memory_usage = torch.cuda.memory_allocated() - initial_memory\n        memory_usage_gb = memory_usage / (1024**3)\n        print(f\"CUDA memory usage: {memory_usage_gb:.2f} GB\")\n</code></pre> <p>The <code>@contextmanager</code> decorator transforms <code>print_cuda_memory_usage</code> into a factory function that returns a context manager. When entering the context block, it records the starting GPU memory usage. It then yields control to the contents of the context block. Upon exiting the block, it records the final GPU memory usage, calculates the difference, and prints it to the standard output.</p>"},{"location":"zeta/utils/print_cuda_memory_usage/#arguments","title":"Arguments","text":"<p><code>print_cuda_memory_usage</code> doesn't take any arguments.</p> Argument Type Description None None None"},{"location":"zeta/utils/print_cuda_memory_usage/#usage","title":"Usage","text":"<p>Here are some examples on how <code>print_cuda_memory_usage</code> can be used:</p>"},{"location":"zeta/utils/print_cuda_memory_usage/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>x = torch.randn((10000, 10000), device=\"cuda\")\n\nwith print_cuda_memory_usage():\n    y = x @ x.t()  # Large matrix multiplication\n</code></pre> <p>In this example, a large tensor <code>x</code> is allocated on the GPU, and then a large matrix multiplication is performed inside the <code>print_cuda_memory_usage</code> context. The increase in GPU memory usage resulting from this operation will be printed.</p>"},{"location":"zeta/utils/print_cuda_memory_usage/#example-2-exception-handling","title":"Example 2: Exception Handling","text":"<pre><code>x = torch.randn((10000, 10000), device=\"cuda\")\n\ntry:\n    with print_cuda_memory_usage():\n        y = x @ x.t()  # Large matrix multiplication\n        raise Exception(\"Some Exception\")\nexcept Exception as e:\n    print(f\"Caught an exception: {e}\")\n</code></pre> <p>In this example, an exception is raised inside the <code>print_cuda_memory_usage</code> context. Regardless of the exception, <code>print_cuda_memory_usage</code> will still correctly compute and print the CUDA memory usage before the exception is propagated.</p>"},{"location":"zeta/utils/print_cuda_memory_usage/#example-3-nesting-usage","title":"Example 3: Nesting Usage","text":"<pre><code>x = torch.randn((10000, 10000), device=\"cuda\")\n\nwith print_cuda_memory_usage():\n    y = x @ x.t()  # Large matrix multiplication\n    with print_cuda_memory_usage():\n        z = y @ y.t()  # Even larger matrix multiplication\n</code></pre> <p>In this example, <code>print_cuda_memory_usage</code> contexts are nested, allowing you to separately track the GPU memory usage of different parts of your code.</p>"},{"location":"zeta/utils/print_cuda_memory_usage/#notes","title":"Notes","text":"<p>The <code>print_cuda_memory_usage</code> function requires PyTorch to be run with CUDA enabled and a CUDA-enabled GPU to be available. If either of these conditions are not met, <code>torch.cuda.memory_allocated()</code> will raise a <code>RuntimeError</code> and the function will not work as intended.</p> <p>Also, <code>print_cuda_memory_usage</code> only tracks the GPU memory that is allocated and managed by PyTorch, it doesn't account for any memory directly allocated by CUDA via methods outside of PyTorch's control.</p> <p>Finally, <code>print_cuda_memory_usage</code> gives an indication of the additional memory used by a specific block of code. However, the exact details of memory management on the GPU can be complex, depending on multiple factors such as how PyTorch allocates and caches memory, the specific GPU hardware, the CUDA version, and other aspects of the system configuration. It also does not account for the memory used by non-PyTorch CUDA libraries or other processes sharing the same GPU.</p>"},{"location":"zeta/utils/print_main/","title":"print_main","text":""},{"location":"zeta/utils/print_main/#module-name-zetautilsprint_main","title":"Module Name: zeta.utils.print_main","text":""},{"location":"zeta/utils/print_main/#function-definition","title":"Function Definition","text":"<p>class zeta.utils.print_main(msg): <pre><code>Prints a message only on the main process.   \n\nParameters:\n- msg (str): The message to be printed.\n</code></pre></p>"},{"location":"zeta/utils/print_main/#functionality-purpose","title":"Functionality &amp; Purpose","text":"<p>This function serves to print messages selectively on the main process in a distributed setting. Distributed settings often clone multiple processes across different CPU cores or different machines. This means that each of these processes will have a predefined rank, where the main (or master) process usually has the rank 0. </p> <p>When dealing with distributed settings, it's quite common to observe duplicate console output from each process, which can clutter the console and make interpretability harder. This function helps to mitigate that problem by enabling messaging only from the main process, thus maintaining a clean and streamlined console output.</p>"},{"location":"zeta/utils/print_main/#usage-and-examples","title":"Usage and Examples:","text":""},{"location":"zeta/utils/print_main/#importing-the-necessary-libraries","title":"Importing the Necessary Libraries","text":"<p>This function would typically be used within a project that utilises PyTorch's distributed utilities for parallel and distributed computation. So let's begin with the necessary imports: <pre><code>from torch import distributed as dist\n\nimport zeta.utils\n</code></pre></p>"},{"location":"zeta/utils/print_main/#example-1-printing-without-distributed-setting","title":"Example 1: Printing without Distributed Setting","text":"<p>In an environment where distributed computing is not being used or available, messages will be printed normally. <pre><code>zeta.utils.print_main(\"Hello World!\")\n</code></pre> Console Output: <pre><code>Hello World!\n</code></pre></p>"},{"location":"zeta/utils/print_main/#example-2-printing-with-distributed-setting","title":"Example 2: Printing with Distributed Setting","text":"<p>In a distributed computing environment, the message will print only from the main process:</p> <p><pre><code># Assuming we are in a distributed environment with several processes running this code\nif dist.is_available():\n    zeta.utils.print_main(\"Hello from main process!\")\n</code></pre> Console Output: <pre><code># Note: This message will only be printed once, since only the main process (rank 0) gets to execute the print function.\nHello from main process!\n</code></pre></p> <p>Remember that in this scenario, if the current process is not the main process (i.e., its rank is not 0), the function simply won't do anything. This is beneficial to avoid repetitively printing the same message in a distributed setting. </p> <p>Remember to ensure your distributed environment is properly initialized before using distributed functionalities.</p>"},{"location":"zeta/utils/print_main/#example-3-handling-both-non-distributed-and-distributed-settings","title":"Example 3: Handling both Non-Distributed and Distributed Settings","text":"<p>This function is designed to handle both non-distributed and distributed settings, as shown below:</p> <pre><code># main function\ndef main():\n    # distributing tasks between processes.\n    print_main(\"This message is from main process only.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Here, <code>dist.is_available()</code> checks if distributed processing is available. If so, it verifies if the rank is 0 (i.e., checks if the process is the main one). If both conditions are true, it goes ahead and prints the message. If distributed processing isn't available, it directly prints the message, effectively handling both scenarios.</p>"},{"location":"zeta/utils/print_num_params/","title":"print_num_params","text":""},{"location":"zeta/utils/print_num_params/#zeta-utils-documentation","title":"Zeta Utils Documentation","text":""},{"location":"zeta/utils/print_num_params/#class-print_num_params","title":"Class: print_num_params","text":"<p>Functionality: The function 'print_num_params' prints the total number of trainable parameters of a given model. Model parameters are the attributes of the model that the algorithm modifies to enable the model to improve and adjust to the data better. Therefore, this function is important in determining the complexity of the model. More parameters in a model mean more complexity.</p> <p>Typically higher parameter models have more training data and are better equipped to represent complex data patterns. However, having too many parameters can also lead to overfitting: the model might become too well adjusted to the training data and perform poorly on unseen data (high variance).</p> <p>This function also checks if the PyTorch distributed package 'dist' is available and, if it is, prints the number of parameters on rank '0'. Rank in PyTorch's distributed package specifies the process rank (ID) for each process group. In a distributed environment (multiple GPUs), the function print_num_params will print the number of parameters from one GPU identified as rank '0'.</p> <p>Here is the code definition:</p> <pre><code>def print_num_params(model):\n    \"\"\"\n    Function to print out the number of trainable parameters in a PyTorch Model Model.\n\n    Args:\n        model (:obj: `torch.nn.Module`): The PyTorch Model.\n\n    \"\"\"\n    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    if dist.is_available():\n        if dist.get_rank() == 0:\n            print(f\"Number of parameters in model: {n_params}\")\n    else:\n        print(f\"Number of parameters in model: {n_params}\")\n</code></pre> <p>Parameters:</p> Parameter Data Type Description Default Value model torch.nn.Module The PyTorch model for which the number of parameters is to be calculated and printed. - <p>Other Functions Used:</p> <ul> <li>model.parameters(): Retrieves the model's parameters.</li> <li>p.requires_grad: Checks if the parameters require gradients (is trainable).</li> <li>p.numel(): Returns the total number of elements in the input tensor.</li> <li>dist.is_available(): Determines if PyTorch distributed is available.</li> <li>dist.get_rank(): Retrieves the rank in the current distributed group.</li> </ul> <p>Here is an example of how to use this function.</p> <pre><code>import torch \nimport torch.nn as nn\nfrom torch import dist\nfrom zeta.utils import print_num_params\n\nmodel = nn.Linear(10,2) # A simple linear model\n\nprint_num_params(model)\n</code></pre> <p>Please note that if you are using this function in a distributed environment, you must first initialize your distributed environment correctly.</p> <pre><code>import torch \nimport torch.nn as nn\nfrom torch import dist\nfrom zeta.utils import print_num_params\n\n# initialize your distributed environment\ndist.init_process_group(backend='nccl')\n\nmodel = nn.Linear(10,2) # A simple linear model\n\nprint_num_params(model)\n</code></pre> <p>By using the function 'print_num_params', you can print out the total number of trainable parameters in your PyTorch models, which can have a significant impact on your model's complexity and its eventual performance.</p> <p>Please note that this function works solely in a PyTorch environment and may not work with models built from other machine learning packages like Keras, TensorFlow, etc. It is also reliant on the dist package of PyTorch for distributed computations. This means you need to initialize your distributed environment if you are working with multiple GPUs.</p> <p>Also, if you have specified some of the parameters of your model as non-trainable (by setting <code>requires_grad = False</code>), this function will not account for them.</p>"},{"location":"zeta/utils/print_num_params/#references-resources","title":"References &amp; Resources","text":"<ol> <li>Understanding Model Complexity</li> <li>torch.numel()</li> <li>torch.nn.Module</li> <li>torch.distributed</li> </ol>"},{"location":"zeta/utils/save_load/","title":"save_load","text":""},{"location":"zeta/utils/save_load/#zetautilssave_load","title":"zeta.utils.save_load","text":""},{"location":"zeta/utils/save_load/#overview","title":"Overview","text":"<p>The <code>save_load</code> decorator in the <code>zeta.utils</code> module is a Python decorator designed around PyTorch's <code>torch.nn.Module</code> subclasses. Its main functionality is to automate and streamline the saving and loading of trained models and their configurations, reducing the need for repeated code and increasing code readability and maintainability.</p> <p>Key to its purpose is the ability to handle the model's state dictionary, training configurations, and PyTorch version. The decorator enhances the training workflow by allowing models\u2019 states and configurations to be easily saved and loaded efficiently with built-in version compatibility checks and hooks for code execution pre and post-saving/loading.</p>"},{"location":"zeta/utils/save_load/#core-functionality","title":"Core Functionality","text":""},{"location":"zeta/utils/save_load/#save_load-decorator","title":"save_load Decorator","text":"<p>Considered a Base decorator for save and load methods for <code>torch.nn.Module</code> subclasses. In essence, a decorator is a higher-order function that can drape functionality over other functions or classes without changing their source code, which is exactly what the <code>save_load</code> decorator is.</p> <p>The <code>save_load</code> decorator modifies <code>torch.nn.Module</code> subclasses by adding save, load and an initialization &amp; load methods to the subclass. This allows for seamless saving and loading of the subclass instances states and configurations.</p>"},{"location":"zeta/utils/save_load/#function-method-definition","title":"Function / Method definition","text":"<pre><code>@beartype\ndef save_load(\n    save_method_name=\"save\",\n    load_method_name=\"load\",\n    config_instance_var_name=\"_config\",\n    init_and_load_classmethod_name=\"init_and_load\",\n    version: Optional[str] = None,\n    pre_save_hook: Optional[Callable[[Module], None]] = None,\n    post_load_hook: Optional[Callable[[Module], None]] = None,\n    compress: Optional[bool] = False,\n    partial_load: Optional[bool] = False,\n    *args,\n    **kwargs,\n):...\n</code></pre> <p>The function takes in several arguments:</p> Parameter Type Default Description <code>save_method_name</code> <code>str</code> <code>\"save\"</code> The name used to set the save method for the instance. <code>load_method_name</code> <code>str</code> <code>\"load\"</code> The name used to set the load method for the instance. <code>config_instance_var_name</code> <code>str</code> <code>\"_config\"</code> The name used to set the instance's configuration variable. <code>init_and_load_classmethod_name</code> <code>str</code> <code>\"init_and_load\"</code> The name used to set the class's initialization and loading method. <code>version</code> <code>Optional[str]</code> <code>None</code> Version of the torch module. Used for checking compatibility when loading. <code>pre_save_hook</code> <code>Optional[Callable[[Module], None]]</code> <code>None</code> Callback function before saving. Useful for final operations before saving states and configurations. <code>post_load_hook</code> <code>Optional[Callable[[Module], None]]</code> <code>None</code> Callback function after loading. Ideal for any additional operations after loading states and configurations. <code>compress</code> <code>Optional[bool]</code> <code>False</code> If set to <code>True</code>, the saved model checkpoints will be compressed. <code>partial_load</code> <code>Optional[bool]</code> <code>False</code> If set to <code>True</code>, the saved model checkpoint will be partially loaded to existing models. <code>*args</code> &amp; <code>**kwargs</code> <code>Any</code> Additional arguments for the decorator. <p>The save_load decorator modifies the way a PyTorch model is initialized, saved, and loaded. It does this by wrapping new init, save, load, and init_and_load methods around the decorated class.</p>"},{"location":"zeta/utils/save_load/#usage-examples","title":"Usage Examples","text":"<p>Here is a basic usage example of the <code>save_load</code> decorator:</p>"},{"location":"zeta/utils/save_load/#example-1-using-default-parameters-on-a-pytorch-model","title":"Example 1:  Using default parameters on a PyTorch Model","text":"<pre><code>from torch.nn import Linear, Module\n\nfrom zeta.utils import save_load\n\n\n@save_load()\nclass MyModel(Module):\n\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.layer = Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\n# Initialize your model\nmodel = MyModel(32, 10)\n\n# Save your model\nmodel.save(\"model.pt\")\n\n# Load your model\nloaded_model = MyModel.load(\"model.pt\")\n</code></pre>"},{"location":"zeta/utils/save_load/#example-2-using-the-save_load-with-non-default-arguments","title":"Example 2:  Using the <code>save_load</code> with non-default arguments","text":"<p>In this example, we are going to add <code>pre_save_hook</code> and <code>post_load_hook</code> to demonstrate their usage. These functions will be called just before saving and</p>"},{"location":"zeta/utils/save_load_wrapper/","title":"Module Documentation: <code>save_load</code>","text":""},{"location":"zeta/utils/save_load_wrapper/#overview","title":"Overview","text":"<p>The <code>save_load</code> module provides a powerful decorator for PyTorch neural network modules that simplifies the process of saving and loading model checkpoints. This decorator is designed to enhance the ease and flexibility of managing model checkpoints, making it more efficient to work with PyTorch models during development and production.</p> <p>This documentation will guide you through the <code>save_load</code> decorator's architecture, purpose, functions, and usage examples. You'll learn how to effectively use this decorator to save and load model checkpoints, manage configuration settings, and handle version compatibility.</p>"},{"location":"zeta/utils/save_load_wrapper/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Architecture</li> <li>Purpose</li> <li>Decorator: save_load<ul> <li>Parameters</li> <li>Usage Examples<ul> <li>Basic Usage</li> <li>Custom Methods and Hooks</li> <li>Partial Loading</li> <li>Version Compatibility</li> </ul> </li> </ul> </li> <li>Additional Information</li> <li>References</li> </ol>"},{"location":"zeta/utils/save_load_wrapper/#1-installation","title":"1. Installation","text":"<p>The <code>save_load</code> decorator is a Python code snippet that can be directly incorporated into your project without the need for separate installation.</p>"},{"location":"zeta/utils/save_load_wrapper/#2-architecture","title":"2. Architecture","text":"<p>The <code>save_load</code> decorator is a Python decorator that can be applied to subclasses of PyTorch's <code>nn.Module</code>. It enhances the module with methods for saving and loading model checkpoints, including options for configuration management, version compatibility, and custom hooks.</p>"},{"location":"zeta/utils/save_load_wrapper/#3-purpose","title":"3. Purpose","text":"<p>The primary purpose of the <code>save_load</code> decorator is to streamline the process of saving and loading PyTorch model checkpoints. It offers the following benefits:</p> <ul> <li>Simplified checkpoint management: Provides easy-to-use methods for saving and loading model states.</li> <li>Configuration preservation: Allows for the preservation and retrieval of the module's configuration settings.</li> <li>Version compatibility: Offers mechanisms to handle version compatibility between saved checkpoints.</li> <li>Customization: Supports custom hooks that can be executed before and after saving or loading.</li> </ul>"},{"location":"zeta/utils/save_load_wrapper/#4-decorator-save_load","title":"4. Decorator: save_load","text":"<p>The <code>save_load</code> decorator provides the following functionality:</p> <ul> <li>Saving and loading model checkpoints.</li> <li>Configuration preservation: Saving and retrieving configuration settings.</li> <li>Version compatibility: Checking and handling version mismatches.</li> <li>Customization: Executing custom hooks before and after saving or loading.</li> </ul>"},{"location":"zeta/utils/save_load_wrapper/#parameters","title":"Parameters","text":"<p>The <code>save_load</code> decorator accepts the following parameters:</p> <ul> <li><code>save_method_name</code> (str, optional): The name of the method used for saving the model checkpoint. Defaults to \"save\".</li> <li><code>load_method_name</code> (str, optional): The name of the method used for loading the model checkpoint. Defaults to \"load\".</li> <li><code>config_instance_var_name</code> (str, optional): The name of the instance variable used to store the configuration. Defaults to \"_config\".</li> <li><code>init_and_load_classmethod_name</code> (str, optional): The name of the class method used to initialize and load a model from a checkpoint. Defaults to \"init_and_load\".</li> <li><code>version</code> (Optional[str], optional): The version of the saved checkpoint. Defaults to None.</li> <li><code>pre_save_hook</code> (Optional[Callable[[Module], None]], optional): A callback function executed before saving the model checkpoint. Defaults to None.</li> <li><code>post_load_hook</code> (Optional[Callable[[Module], None]], optional): A callback function executed after loading the model checkpoint. Defaults to None.</li> <li><code>compress</code> (Optional[bool], optional): Enable compression when saving checkpoints. Defaults to False.</li> <li><code>partial_load</code> (Optional[bool], optional): Enable partial loading of the model checkpoint. Defaults to False.</li> </ul>"},{"location":"zeta/utils/save_load_wrapper/#usage-examples","title":"Usage Examples","text":""},{"location":"zeta/utils/save_load_wrapper/#basic-usage","title":"Basic Usage","text":"<p>Here's a basic example of using the <code>save_load</code> decorator to save and load a PyTorch model checkpoint:</p> <pre><code>import torch\nfrom torch.nn import Module\n\nfrom zeta.utils import save_load\n\n\n@save_load()\nclass MyModel(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(10, 5)\n\n\n# Create an instance of MyModel\nmy_model = MyModel()\n\n# Save the model checkpoint\nmy_model.save(\"my_model.pth\")\n\n# Load the model checkpoint\nloaded_model = MyModel.load(\"my_model.pth\")\n</code></pre>"},{"location":"zeta/utils/save_load_wrapper/#custom-methods-and-hooks","title":"Custom Methods and Hooks","text":"<p>You can define custom method and hook names when using the <code>save_load</code> decorator:</p> <pre><code>import torch\nfrom torch.nn import Module\n\nfrom zeta.utils import save_load\n\n\n@save_load(\n    save_method_name=\"custom_save\",\n    load_method_name=\"custom_load\",\n    pre_save_hook=my_pre_save_hook,\n    post_load_hook=my_post_load_hook,\n)\nclass CustomModel(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(10, 5)\n\n\n# Create an instance of CustomModel\ncustom_model = CustomModel()\n\n# Custom save and load\ncustom_model.custom_save(\"custom_model.pth\")\nloaded_custom_model = CustomModel.custom_load(\"custom_model.pth\")\n</code></pre>"},{"location":"zeta/utils/save_load_wrapper/#partial-loading","title":"Partial Loading","text":"<p>Enable partial loading to update only specific parts of the model checkpoint:</p> <pre><code>import torch\nfrom torch.nn import Module\n\nfrom zeta.utils import save_load\n\n\n@save_load(partial_load=True)\nclass PartialModel(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(10, 5)\n\n\n# Create an instance of PartialModel\npartial_model = PartialModel()\n\n# Save the model checkpoint\npartial_model.save(\"partial_model.pth\")\n\n# Load only the updated part of the model checkpoint\nloaded_partial_model = PartialModel.load(\"partial_model.pth\")\n</code></pre>"},{"location":"zeta/utils/save_load_wrapper/#version-compatibility","title":"Version Compatibility","text":"<p>Handle version compatibility when loading saved checkpoints:</p> <pre><code>import torch\nfrom torch.nn import Module\n\nfrom zeta.utils import save_load\n\n\n@save_load(version=\"1.0\")\nclass VersionedModel(Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(10, 5)\n\n\n# Create an instance of VersionedModel\nversioned_model = VersionedModel()\n\n# Save the model checkpoint\nversioned_model.save(\"versioned_model.pth\")\n\n# Load the model checkpoint with version compatibility check\nloaded_versioned_model = VersionedModel.load(\"versioned_model.pth\")\n</code></pre>"},{"location":"zeta/utils/save_load_wrapper/#5-additional-information","title":"5. Additional Information","text":"<ul> <li>The <code>save_load</code> decorator simplifies the process of saving and loading model checkpoints for PyTorch modules.</li> <li>Configuration settings can be preserved and retrieved along with the model checkpoint.</li> <li>Version compatibility checks help manage saved checkpoints with different versions.</li> <li>Custom hooks can be used to execute custom actions before and after saving or loading checkpoints.</li> </ul>"},{"location":"zeta/utils/save_load_wrapper/#6-references","title":"6. References","text":"<p>For more information on PyTorch and checkpoint management, refer to the official PyTorch documentation: [PyTorch</p> <p>Saving and Loading Models](https://pytorch.org/tutorials/beginner/saving_loading_models.html).</p>"},{"location":"zeta/utils/save_memory_snapshot/","title":"save_memory_snapshot","text":""},{"location":"zeta/utils/save_memory_snapshot/#module-name-save_memory_snapshot","title":"Module Name: save_memory_snapshot","text":"<p>The <code>save_memory_snapshot</code> function within PyTorch is a context manager that allows developers to save memory usage snapshots from their PyTorch model to a specified file path. This is particularly useful for tracking and analyzing memory utilization during code execution, facilitating optimized resource management.  </p> <p>Function Details: <pre><code>@contextmanager\ndef save_memory_snapshot(file_path: Path):\n    \"\"\"Save a memory snapshot information to a folder\n    Usage:\n        with save_memory_snapshot(file_path):\n            # code to profile\n\n    Args:\n        file_path: The path to the folder to save the snapshot to\n                    will create the folder if it doesn't exist\n    \"\"\"\n    file_path.mkdir(parents=True, exist_ok=True)\n    torch.cuda.memory._record_memory_history()\n    try:\n        yield\n    finally:\n        s = torch.cuda.memory._snapshot()\n        with open(f\"{file_path}/snapshot.pickle\", \"wb\") as f:\n            dump(s, f)\n        with open(f\"{file_path}/trace_plot.html\", \"w\") as f:\n            f.write(torch.cuda._memory_viz.trace_plot(s))\n</code></pre> Here is a description for the single argument,  <code>file_path</code>:</p> Parameter Type Description file_path pathlib.Path File path to a folder where the snapshots will be saved. The function will create the folder if it does not exist. <p>Functionality and Usage</p> <p>After creating the output directory (if it does not exist), the function initiates recording the GPU's memory usage history via torch.cuda.memory._record_memory_history(). </p> <p>Any code executed within the context of the <code>save_memory_snapshot</code> function will be profiled, and memory usage snapshots during its execution will be stored. </p> <p>Upon completion of the code block within the context, a snapshot of the memory history at that point in time is captured using <code>torch.cuda.memory._snapshot()</code>. This snapshot is then saved in pickle format (<code>snapshot.pickle</code>), and a HTML file (<code>trace_plot.html</code>) is generated, displaying a trace plot for the memory usage. </p> <p>The execution flow control is then returned to the code following the context block, ensuring any code thereafter is not profiled.</p> <p>How to Use <pre><code>from pathlib import Path\n\nimport torch\n\nfrom zeta.utils import save_memory_snapshot\n\nfile_path = Path(\"my_folder\")\n\n# code to profile\nmodel = torch.nn.Linear(10, 10)\ninput_tensor = torch.randn(10, 10)\n\nwith save_memory_snapshot(file_path):\n    output = model(input_tensor)\n</code></pre> The provided file path 'my_folder' is where the snapshots will be saved. After this code block executed, the snapshot of the memory usage by the Linear layer applied on input_tensor will be saved to 'my_folder' in both 'snapshot.pickle' file and 'trace_plot.html' file. </p> <p>Use Case 2 <pre><code>from pathlib import Path\n\nimport torch\n\nfrom zeta.utils import save_memory_snapshot\n\nfile_path = Path(\"gpu_usage\")\n\n# code to profile\nmodel = torch.nn.Sequential(\n    torch.nn.Conv2d(1, 20, 5),\n    torch.nn.ReLU(),\n    torch.nn.Conv2d(20, 64, 5),\n    torch.nn.ReLU(),\n)\n\ninput_tensor = torch.randn(1, 1, 32, 32)\n\nwith save_memory_snapshot(file_path):\n    output = model(input_tensor)\n</code></pre> In this case, we are profiling a multi-layer Convolutional Neural Network (CNN). The memory snapshot will give insights about the intermediate usage and fluctuations occurring due to convolutions and the subsequent ReLU activation function. </p> <p>Use Case 3 <pre><code>from pathlib import Path\n\nimport torch\n\nfrom zeta.utils import save_memory_snapshot\n\nfile_path = Path(\"training_memory\")\n\n# establish a simple model\nmodel = torch.nn.Linear(20, 10)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# dummy data\ninputs = torch.randn(10, 20)\ntargets = torch.randn(10, 10)\n\nwith save_memory_snapshot(file_path):\n    # a complete step of training\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    loss.backward()\n    optimizer.step()\n</code></pre> In this last example, we are profiling the memory usage during an entire step of model training, including forward pass, calculating loss, backward pass (backpropagation), and updating weights.</p> <p>For each example, two files hopefully providing useful insights on memory utilization should be generated in the specified 'file_path': <code>snapshot.pickle</code> and <code>trace_plot.html</code>.</p>"},{"location":"zeta/utils/string_begins_with/","title":"string_begins_with","text":""},{"location":"zeta/utils/string_begins_with/#module-name-zetautils","title":"Module Name: zeta.utils","text":""},{"location":"zeta/utils/string_begins_with/#introduction","title":"Introduction","text":"<p>The <code>zeta.utils</code> module is a handy utilities toolkit for Python, which includes a variety of useful functions for data processing and manipulation. A noteworthy function in this module is <code>string_begins_with</code>. It provides a quick and easy way to check if a string starts with a particular prefix. Though it seems a simple function, it is essential in many data preprocessing tasks such as checking the file paths, URLs, filenames, and prefix-based conditional data manipulation.</p>"},{"location":"zeta/utils/string_begins_with/#functionality-overview","title":"Functionality Overview","text":"<p>The <code>string_begins_with</code> function takes two arguments: <code>prefix</code> and <code>str</code>. It checks if the given string <code>str</code> commences with the specified <code>prefix</code> and returns a boolean value accordingly.</p> <p>Now, let's explore the function syntax, parameters, and usage.</p>"},{"location":"zeta/utils/string_begins_with/#function-definition-and-parameters","title":"Function Definition and Parameters","text":"<p>The <code>string_begins_with</code> is defined as follows:</p> <pre><code>def string_begins_with(prefix, str):\n    \"\"\"\n    Check if a string begins with a specific prefix.\n\n    Args:\n        prefix (str): The prefix to check for.\n        str (str): The string to check.\n\n    Returns:\n        bool: True if string starts with prefix, False otherwise.\n    \"\"\"\n    return str.startswith(prefix)\n</code></pre> <p>Here's a breakdown of its parameters:</p> Argument Type Description <code>prefix</code> str The prefix that we need to check for at the start of the string. <code>str</code> str The string that we need to inspect."},{"location":"zeta/utils/string_begins_with/#functionality-and-usage","title":"Functionality and Usage","text":"<p>The primary usage of the <code>string_begins_with</code> function is to check if a string begins with a specific prefix. In Python, we have the <code>str.startswith()</code> function that performs this check. The <code>string_begins_with</code> function is essentially a wrapper around this built-in function providing a clear and expressive syntax.</p> <p>The function <code>string_begins_with</code> is a pure function in that it neither modifies the actual inputs nor does it rely on or alter any external state. It only produces the result based on the given inputs.</p> <p>Here are a few usage instances:</p> <p>Example 1 - Basic usage: <pre><code>from zeta.utils import string_begins_with\n\nprint(string_begins_with('data', 'database')) # Output: True\nprint(string_begins_with('data', 'base')) # Output: False\n</code></pre></p> <p>Example 2 - Handling case-sensitivity: <pre><code>from zeta.utils import string_begins_with\n\nprint(string_begins_with('Data', 'database')) # Output: False\nprint(string_begins_with('Data', 'Database'))  # Output: True\n</code></pre></p> <p>Example 3 - Using with list comprehension for data preprocessing: <pre><code>from zeta.utils import string_begins_with\n\ndata = ['apple', 'android', 'blackberry', 'windows', 'android_tv']\nandroid_data = [item for item in data if string_begins_with('android', item)]\n\nprint(android_data) # Output: ['android', 'android_tv']\n</code></pre></p> <p>Cognizant of Python's inbuilt <code>startswith</code> function, <code>string_begins_with</code> complements it by providing a more meaningful syntax that enhances the code readability, especially for those new to Python programming. Through this documentation, we hope you'll be able to integrate <code>string_begins_with</code> into your code and simplify your string prefix checks. Happy Programming!</p>"},{"location":"zeta/utils/top_a/","title":"top_a","text":""},{"location":"zeta/utils/top_a/#module-zetautils","title":"Module: zeta.utils","text":""},{"location":"zeta/utils/top_a/#function-top_a","title":"Function: top_a()","text":""},{"location":"zeta/utils/top_a/#description","title":"Description","text":"<p>This utility function, <code>top_a()</code>, is an implementation of a technique known as 'Top-K filtering' or 'Nucleus sampling'.  It involves softmaxing the logits and selecting a subset of it whose cumulative probability exceeds a certain threshold. It is particularly useful in natural language processing tasks to refine the output of language models. </p> <p>The function takes a tensor of logits, applies a softmax function for normalization, associates these probabilities with a certain limit, and then applies a filter to modify the logits based on the associated limit.</p>"},{"location":"zeta/utils/top_a/#parameters","title":"Parameters","text":"Parameter Type Description logits PyTorch Tensor The input tensor for which the softmax will be computed. min_p_pow float (Optional) The minimal power to which max probability is raised. Default is 2.0. min_p_ratio float (Optional) The minimal ratio to minimum power used to set the limit. Default is 0.02."},{"location":"zeta/utils/top_a/#returns","title":"Returns","text":"<p>This function returns a modified version of the input tensor, logits with respect to the specified limit.</p>"},{"location":"zeta/utils/top_a/#code","title":"Code","text":"<pre><code>import torch\nimport torch.nn.functional as F\n\n\ndef top_a(logits, min_p_pow=2.0, min_p_ratio=0.02):\n    # compute softmax probabilities\n    probs = F.softmax(logits, dim=-1)\n\n    # set limit with respect to maximum probabily and min_p_pow and min_p_ratio\n    limit = torch.pow(torch.max(probs), min_p_pow) * min_p_ratio\n\n    # apply filter to modify the logits with respect to the limit\n    logits[probs &lt; limit] = float(\"-inf\")\n    logits[probs &gt;= limit] = 1\n    return logits\n</code></pre>"},{"location":"zeta/utils/top_a/#examples","title":"Examples","text":""},{"location":"zeta/utils/top_a/#example-1","title":"EXAMPLE 1","text":"<p>In this example, we'll compute the top_a function on a tensor of logits.</p> <pre><code>import torch\n\nfrom zeta.utils import top_a\n\n# Create a tensor of logits\nlogits = torch.tensor([0.1, 0.2, 0.3, 0.4])\n\n# Call the function\nresult = top_a(logits)\n\n# Output\nprint(result)\n</code></pre>"},{"location":"zeta/utils/top_a/#example-2","title":"EXAMPLE 2","text":"<p>In this example, we use user-defined minimum power <code>min_p_pow</code> and minimum ratio <code>min_p_ratio</code>.</p> <pre><code>import torch\n\nfrom zeta.utils import top_a\n\n# Create a tensor of logits\nlogits = torch.tensor([0.1, 0.5, 0.2, 0.4])\n\n# Call the function\nresult = top_a(logits, min_p_pow=3.0, min_p_ratio=0.01)\n\n# Output\nprint(result)\n</code></pre>"},{"location":"zeta/utils/top_a/#example-3","title":"EXAMPLE 3","text":"<p>In this example, we see how changing the <code>min_p_pow</code> affects the output.</p> <pre><code>import torch\n\nfrom zeta.utils import top_a\n\n# Create a tensor of logits\nlogits = torch.tensor([0.2, 0.3, 0.5, 0.5])\n\n# Call the function with different min_p_pow values\nresult1 = top_a(logits, min_p_pow=1.0)\nresult2 = top_a(logits, min_p_pow=2.0)\nresult3 = top_a(logits, min_p_pow=3.0)\n\n# Output\nprint(result1)\nprint(result2)\nprint(result3)\n</code></pre>"},{"location":"zeta/utils/top_a/#note","title":"Note","text":"<p>Deep learning practitioners should maintain a good practice of casting tensors into the right device (CPU or GPU) before operations. Ensure the logits tensor is on the right device before calling <code>top_a()</code>. Additionally, the values in the tensor should be in logits (unnormalized scores or predictions) and not in the form of probabilities (i.e., no softmax has been applied). </p> <p>This function is meant to be a utility. For a more specialized task, slight modifications may be required as per the use case. Thus, it should not be considered as a one-size-fits-all solution, but rather as a template code for selecting samples contingent upon a specific set of probabilities.</p>"},{"location":"zeta/utils/top_k/","title":"top_k","text":""},{"location":"zeta/utils/top_k/#modulefunction-name-top_k","title":"Module/Function Name: top_k","text":"<pre><code>def top_k(logits, thres=0.9):\n    k = ceil((1 - thres) * logits.shape[-1])\n    val, ind = torch.topk(logits, k)\n    probs = torch.full_like(logits, float(\"-inf\"))\n    probs.scatter_(1, ind, val)\n    return probs\n</code></pre> <p>The <code>top_k</code> function is utility function that is used to retrieve the top k logits based on a threshold. It takes in the logits and a threshold value, picks out the top k logits that meet the threshold, and then returns those logits.</p>"},{"location":"zeta/utils/top_k/#parameters","title":"Parameters","text":"Parameter Type Description Default logits Tensor A rank 1 tensor representing the logits you want to filter Required thres float A float representing the threshold for filtering, the default value is 0.9 0.9"},{"location":"zeta/utils/top_k/#returns","title":"Returns","text":"Return Type Description probs Tensor The tensor after being filtered"},{"location":"zeta/utils/top_k/#usage-examples","title":"Usage Examples","text":"<p>Now, let's go through a few examples of how you can use the <code>top_k</code> function.</p>"},{"location":"zeta/utils/top_k/#example-1-basic-usage","title":"Example 1: Basic usage","text":"<p>In the most basic usage, you would pass a tensor of logits and receive a filtered tensor.</p> <pre><code>from math import ceil\n\nimport torch\n\n\ndef top_k(logits, thres=0.9):\n    k = ceil((1 - thres) * logits.shape[-1])\n    val, ind = torch.topk(logits, k)\n    probs = torch.full_like(logits, float(\"-inf\"))\n    probs.scatter_(1, ind, val)\n    return probs\n\n\nlogits = torch.tensor([0.1, 0.4, 0.3, 0.2, 0.5])\nprobs = top_k(logits)\nprint(probs)\n</code></pre>"},{"location":"zeta/utils/top_k/#example-2-changing-the-threshold","title":"Example 2: Changing the Threshold","text":"<p>The threshold value can be adjusted according to your requirements. A higher threshold may result in values being included that would otherwise be excluded.</p> <pre><code>from math import ceil\n\nimport torch\n\n\ndef top_k(logits, thres=0.8):\n    k = ceil((1 - thres) * logits.shape[-1])\n    val, ind = torch.topk(logits, k)\n    probs = torch.full_like(logits, float(\"-inf\"))\n    probs.scatter_(1, ind, val)\n    return probs\n\n\nlogits = torch.tensor([0.1, 0.4, 0.3, 0.2, 0.5])\nprobs = top_k(logits)\nprint(probs)\n</code></pre>"},{"location":"zeta/utils/top_k/#example-3-using-a-different-tensor","title":"Example 3: Using a Different Tensor","text":"<p>The input tensor can be changed as needed. The only requirement is that the tensor should be a 1D tensor.</p> <pre><code>from math import ceil\n\nimport torch\n\n\ndef top_k(logits, thres=0.9):\n    k = ceil((1 - thres) * logits.shape[-1])\n    val, ind = torch.topk(logits, k)\n    probs = torch.full_like(logits, float(\"-inf\"))\n    probs.scatter_(1, ind, val)\n    return probs\n\n\nlogits = torch.tensor([0.1, 0.4, 0.7, 0.2, 0.5])\nprobs = top_k(logits)\nprint(probs)\n</code></pre>"},{"location":"zeta/utils/top_k/#additional-information-and-tips","title":"Additional Information and Tips:","text":"<ul> <li>The function <code>top_k</code> makes use of the <code>torch.topk()</code> function to find the top k values in the tensor and returns these values and their respective indices.</li> <li>The indices are used with the <code>torch.Tensor.scatter_()</code> function to replace the selected elements in a new tensor filled with <code>-inf</code> along the specified dimension with the specified value.</li> </ul>"},{"location":"zeta/utils/top_k/#references","title":"References:","text":"<ul> <li>For more information about the functions used, refer to the PyTorch documentation:</li> <li>torch.topk()</li> <li>torch.Tensor.scatter_()</li> </ul>"},{"location":"zeta/utils/top_p/","title":"top_p","text":""},{"location":"zeta/utils/top_p/#module-name-zetautilstop_p","title":"Module Name: zeta.utils.top_p","text":"<p>Function:  <pre><code>def top_p(logits, thres=0.9):\n</code></pre></p> <p>The <code>top_p</code> function is a part of the <code>zeta.utils</code> library. This function uses a process known as nucleus sampling, or top-p sampling, to handle logits from a language model. This function is intended to be used with the softmax output of language model sequences, making it an important method for text generation tasks.</p> <p>Nucleus sampling is a form of sampling to solve the problem of text generation. It selects the highest probability tokens whose cumulative probability mass exceeds a given threshold.</p> <p>This function is especially useful for deep learning algorithms involved in text generation tasks, where using pure maximum likelihood approximations might lead to highly repetitive and nonsensical outputs. By applying the <code>top_p</code> function, we can ensure more diverse and sensible outputs from such text generation models.</p>"},{"location":"zeta/utils/top_p/#parameters","title":"Parameters:","text":"Name Type Description Default Value logits Tensor These are the model's output log probabilities, expected to be in the format of a 2D tensor. thres float A hyperparameter for top-p sampling, it adjusts the trade-off between randomness and fidelity in the generated text. This parameter indicates the cumulative probability threshold used for the nucleus sampling. 0.9 <p>The function returns logits processed by top-p sampling method, with least probable options removed according to the defined threshold value.</p>"},{"location":"zeta/utils/top_p/#usage","title":"Usage","text":"<p>For this function, we first begin by importing the necessary libraries, which in this case are <code>torch</code> and its sublibrary <code>torch.nn.functional</code>.</p> <pre><code>import torch\nimport torch.nn.functional as F\n\ndef top_p(logits, thres=0.9):\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n    sorted_indices_to_remove = cum_probs &gt; (1 - thres)\n    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n    sorted_indices_to_remove[:, 0] = 0\n\n    sorted_logits[sorted_indices_to_remove] = float(\"-inf\")\n    return sorted_logits.scatter(1, sorted_indices, sorted_logits)\n</code></pre> <p>We can illustrate the process using a simple example.</p> <pre><code># Define logits tensor         \nlogits = torch.tensor([[0.5, 0.4, 0.1]]) \n\n# Call the top_p function   \nfiltered_logits = top_p(logits, thres=0.9)\nprint('The filtered logits are:')\nprint(filtered_logits)\n\n# this should give us:\n# tensor([[[0.5000], [0.4000], [-inf.]])\n</code></pre> <p>In this example, <code>'filtered_logits'</code> now contains the logits from <code>'logits'</code> but the least probable entries (inferior to <code>thres</code>) have been replaced by <code>-inf.</code> which makes them impossible to be chosen in a subsequent random sampling.</p> <p>Keep in mind that in actual use cases the logits tensor would be the output of a pretrained language model and would have more complex dimensions, but the function would be used in the same way.</p>"},{"location":"zeta/utils/top_p/#tips","title":"Tips","text":"<ul> <li>The choice of threshold value <code>'thres'</code> in the function <code>top_p(logits, thres=0.9)</code> is very important, as it determines the trade-off between fidelity (how closely the generated text matches the given input text) and diversity (how different the generated text is from the input text). A smaller threshold value may lead to more repetitive and less diverse text, while a larger threshold value may lead to more diverse but also more unpredictable and potentially incoherent text. You can fine-tune this value based on your specific needs and objectives.</li> </ul>"},{"location":"zeta/utils/top_p/#references","title":"References","text":"<ul> <li>The Curious Case of Neural Text Degeneration</li> <li>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</li> </ul> <p>Reference to PyTorch which this function is heavily tied to:</p> <ul> <li>PyTorch Documentation for further exploration.</li> </ul>"},{"location":"zeta/utils/track_cuda_memory/","title":"<code>track_cuda_memory_usage</code>","text":"<p><code>track_cuda_memory_usage(func)</code></p> <p>A decorator function for tracking CUDA memory usage of a PyTorch function. It measures the amount of CUDA memory allocated before and after the execution of the function, logs the difference, and handles any potential errors during the function execution.</p>"},{"location":"zeta/utils/track_cuda_memory/#parameters","title":"Parameters:","text":"<ul> <li><code>func</code> (callable): The function to be decorated. This should be a function that performs operations using PyTorch with CUDA support.</li> </ul>"},{"location":"zeta/utils/track_cuda_memory/#returns","title":"Returns:","text":"<ul> <li><code>callable</code>: The wrapped function, which when called, executes the original function with added CUDA memory tracking and logging.</li> </ul>"},{"location":"zeta/utils/track_cuda_memory/#usage","title":"Usage:","text":"<p>This decorator can be applied to any function that is expected to run operations using PyTorch with CUDA. To use the decorator, simply place <code>@track_cuda_memory_usage</code> above the function definition.</p>"},{"location":"zeta/utils/track_cuda_memory/#example","title":"Example:","text":"<pre><code>@track_cuda_memory_usage\ndef my_cuda_function(x):\n    # Some operations using PyTorch and CUDA\n    return x * x\n\n\n# Example usage\nx = torch.randn(1000, 1000, device=\"cuda\")\nresult = my_cuda_function(x)\n</code></pre> <p>In this example, <code>my_cuda_function</code> is a simple function that squares its input. The decorator logs the amount of CUDA memory used during the function's execution.</p>"},{"location":"zeta/utils/track_cuda_memory/#logging-output","title":"Logging Output:","text":"<p>The decorator logs two types of messages:</p> <ol> <li>Memory Usage Log: After the function execution, it logs the amount of CUDA memory used by the function. The log is at the INFO level.</li> </ol> <p>Example: <code>2023-03-15 10:00:00,000 - INFO - CUDA memory usage for my_cuda_function: 4000000 bytes</code></p> <ol> <li>Error Log: If an error occurs during the function execution, it logs the error message at the ERROR level and raises the exception.</li> </ol> <p>Example: <code>2023-03-15 10:00:00,000 - ERROR - Error during the execution of the function: RuntimeError(...)</code></p>"},{"location":"zeta/utils/track_cuda_memory/#error-handling","title":"Error Handling:","text":"<ul> <li>If CUDA is not available, a warning is logged, and the function runs without memory tracking.</li> <li>If an error occurs during the execution of the function, the error is logged, and the exception is re-raised after the memory usage log.</li> </ul>"},{"location":"zeta/utils/track_cuda_memory/#notes","title":"Notes:","text":"<ul> <li>The decorator uses <code>torch.cuda.synchronize()</code> before and after the function execution to ensure accurate measurement of memory usage. This synchronization can introduce some overhead and should be considered when profiling performance-critical code.</li> <li>The memory usage reported is the difference in memory allocation on the current CUDA device before and after the function execution. It does not account for memory deallocation that might occur within the function.</li> </ul>"},{"location":"zeta/utils/track_cuda_memory_usage/","title":"track_cuda_memory_usage","text":""},{"location":"zeta/utils/track_cuda_memory_usage/#zeta-utils-documentation","title":"Zeta Utils Documentation","text":"<p>The zeta.utils package is designed to simplify and enhance numerous coding tasks related to PyTorch deep learning systems. By using decorators, the package creates a higher order function that wraps standard functions to provide additional capabilities.</p> <p>This documentation will provide in-depth focus on the <code>track_cuda_memory_usage</code> function decorator included in the package. The intent of this documentation is to thoroughly acquaint the user with the usage and function of <code>track_cuda_memory_usage</code>.</p>"},{"location":"zeta/utils/track_cuda_memory_usage/#function-definition","title":"Function Definition","text":"<p>The <code>track_cuda_memory_usage</code> function is a decorator that, when applied to another function, tracks and logs the CUDA memory usage during the execution of that function. The primary purpose of <code>track_cuda_memory_usage</code> is to allow users to understand the GPU memory allocation and usage when executing a given function - a valuable tool for optimizing deep learning models and operations.</p> <p>This function is especially beneficial when working with large models or data as it allows for efficient memory allocation and monitoring. Using the insights gleaned from this function, users can adjust either their model or their data processing methods to ensure memory efficiency.</p> <pre><code>def track_cuda_memory_usage(func):\n    \"\"\"\n    Name: track_cuda_memory_usage\n\n    Documentation:\n    Track CUDA memory usage of a function.\n\n    Args:\n    func (function): The function to be tracked.\n\n    Returns:\n    function: The wrapped function.\n    \"\"\"\n</code></pre>"},{"location":"zeta/utils/track_cuda_memory_usage/#arguments","title":"Arguments","text":"Argument Data Type Default Value Description func function N/A The function to be tracked."},{"location":"zeta/utils/track_cuda_memory_usage/#usage-examples","title":"Usage examples","text":"<pre><code>import torch\n\nfrom zeta.utils import track_cuda_memory_usage\n\n\n# Define the function that you wish to track\n@track_cuda_memory_usage\ndef create_empty_tensor(size):\n    return torch.empty(size=(size, size)).cuda()\n\n\ncreate_empty_tensor(1000)\n</code></pre> <p>In this example, the decorator <code>@track_cuda_memory_usage</code> is used to track the CUDA memory usage during the execution of the function <code>create_empty_tensor</code>, which creates an empty tensor on the GPU. On execution of this function, CUDA memory usage details will be logged.</p> <p>Here's an example tracking the memory usage while training a model, which could help in understanding and improving the efficiency of a training loop.</p> <pre><code>import torch\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import SGD\nfrom torchvision.models import resnet18\n\nfrom zeta.utils import track_cuda_memory_usage\n\nmodel = resnet18().cuda()\n\noptimizer = SGD(model.parameters(), lr=0.01)\n\n\n# Define a simple train loop\n@track_cuda_memory_usage\ndef simple_train_loop(dataloader, model, optimizer):\n    loss_function = CrossEntropyLoss()\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.cuda(), targets.cuda()\n        outputs = model(inputs)\n        loss = loss_function(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nsimple_train_loop(your_dataloader, model, optimizer)\n</code></pre> <p>In this example, we define a simple training loop for a model and use the <code>@track_cuda_memory_usage</code> decorator to monitor the CUDA memory usage for each iteration of the loop.</p>"},{"location":"zeta/utils/track_cuda_memory_usage/#additional-usage-tips","title":"Additional Usage Tips","text":"<p>Prior to running any operation, the function forces PyTorch to wait for all currently pending CUDA operations to finish with <code>torch.cuda.synchronize()</code>. This ensures that all previously allocated memory is factored into the calculation before the execution of <code>func</code>.</p> <p>It's crucial to note that GPU memory usage is often non-deterministic due to factors such as CUDA's memory management mechanisms as well as multi-threaded operations.</p>"},{"location":"zeta/utils/track_cuda_memory_usage/#conclusion","title":"Conclusion","text":"<p>Understanding how <code>track_cuda_memory_usage</code> works can make a significant difference in optimizing and diagnosing memory-related issues in a PyTorch project. This utility is paramount to developers who work with large data and models. It's a handy tool that makes memory debugging and tracking accessible and manageable.</p>"},{"location":"zeta/utils/video_tensor_to_gift/","title":"video_tensor_to_gift","text":""},{"location":"zeta/utils/video_tensor_to_gift/#module-name-zetautils","title":"Module Name: zeta.utils","text":""},{"location":"zeta/utils/video_tensor_to_gift/#function-video_tensor_to_gift","title":"Function: video_tensor_to_gift","text":"<pre><code>def video_tensor_to_gift(tensor, path, duration=120, loop=0, optimize=True):\n    \"\"\"\n    This function converts a video tensor into a gif and then saves it on the provided path.\n\n    Parameters:\n    - tensor (tensor): A tensor representing a video. The tensor should be 5-dimensional (B, T, C, H, W).\n    - path (str): The location and filename where the gif should be saved. Built-in gif extension is recommended to ensure correct file format.\n    - duration (int): The duration for which each frame should be displayed before transitioning to the next. Default is 120 (in milliseconds).\n    - loop (int): The number of times the gif should loop. A value of 0 means the gif will loop indefinitely. Default is 0.\n    - optimize (bool): A flag specifying whether the gif should be optimized. If set to True, the gif would have smaller size at the cost of quality. Default is True.\n\n    Returns:\n    - images: A sequence of images that constitute the gif.\n\n    Examples:\n\n    This is a simple usage case.\n\n    ```python\n    import torch\n    from torchvision.transforms import functional as T\n\n    from zeta.utils import video_tensor_to_gift\n\n    # Generate a random tensor representing a video\n    tensor = torch.rand(1, 10, 3, 64, 64)\n\n    # Convert tensor to gif and save\n    path = \"./random_video.gif\"\n    video_tensor_to_gift(tensor, path)\n    ```\n\n    This example showcases usage with different arguments.\n\n    ```python\n    import torch\n    from torchvision.transforms import functional as T\n\n    from zeta.utils import video_tensor_to_gift\n\n    # Generate a random tensor representing a video\n    tensor = torch.rand(1, 10, 3, 64, 64)\n\n    # Convert tensor to gif and save with custom duration, loop, and optimization set.\n    path = \"./random_video.gif\"\n    video_tensor_to_gift(tensor, path, duration=200, loop=1, optimize=False)\n    ```\n\n    \"\"\"\n    images = map(T.ToPilImage(), tensor.unbind(dim=1))\n    first_img, *rest_imgs = images\n    first_img.save(\n        path,\n        save_all=True,\n        appeqnd_images=rest_imgs,\n        duration=duration,\n        loop=loop,\n        optimize=optimize,\n    )\n    return images\n</code></pre>"},{"location":"zeta/utils/video_tensor_to_gift/#architecture","title":"Architecture","text":"<p>The function <code>video_tensor_to_gift</code> works by first unbinding the video tensor along the time dimension using the <code>unbind()</code> function, which returns a tuple of all slices along that dimension. This breaks the tensor into a sequence of image tensors.</p> <p>The <code>map()</code> function is then used to apply <code>T.ToPilImage()</code>, a torchvision functional transform, to each of these image tensors. This converts each tensor into a PIL Image.</p> <p>The sequence of PIL Images is then split, with the <code>first_img</code> separated from the <code>rest_imgs</code>. </p> <p>The function then uses the <code>first_img.save()</code> method to save all the images as a gif at the provided path. The <code>save_all</code> parameter set to <code>True</code> signals that all images should be saved in the gif, not just the first one. The <code>append_images</code> parameter specifies the additional images to be added, which in this case are the rest of the images. The <code>duration</code>, <code>loop</code>, and <code>optimize</code> parameters control the behavior of the gif.</p>"},{"location":"zeta/utils/video_tensor_to_gift/#note","title":"Note:","text":"<p>Optimizing the gif can reduce the size of the gif file but may also slightly degrade the image quality.</p> <p>This function is handy for quick visualization and debugging purposes, as it can help analyze the content of video tensors during model development.</p>"},{"location":"zeta/utils/video_tensor_to_gift/#references-and-further-resources","title":"References and further resources:","text":"<p>For understanding more about the image saving process in PIL: https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#gif </p> <p>For understanding more about TorchVision transform functions: https://pytorch.org/vision/stable/transforms.html </p> <p>For more details on PyTorch tensor functions such as <code>unbind</code>: https://pytorch.org/docs/stable/tensors.html </p>"}]}